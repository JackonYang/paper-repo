{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144961113"
                        ],
                        "name": "I. Elfadel",
                        "slug": "I.-Elfadel",
                        "structuredName": {
                            "firstName": "Ibrahim",
                            "lastName": "Elfadel",
                            "middleNames": [
                                "Abe",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Elfadel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "These have been used for content addressable memories (Waugh &\nWestervelt, 1993; Elfadel, 1995) and have been applied to clustering for unsupervised texture segmentation (Hofmann & Buhmann, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49743260,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cbc6853ac9b52712d048ea1275b0c22033f156af",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the problem of mapping hybrid (i.e., both discrete and continuous) constrained optimization problems onto analog networks. The saddle-point paradigm of mean-field methods in statistical physics provides a systematic procedure for finding such a mapping via the notion of effective energy. Specifically, it is shown that within this paradigm, to each closed bounded constraint set is associated a smooth convex potential function. Using the conjugate (or the Legendre-Fenchel transform) of the convex potential, the effective energy can be transformed to yield a cost function that is a natural generalization of the analog Hopfield energy. Descent dynamics and deterministic annealing can then be used to find the global minimum of the original minimization problem. When the conjugate is hard to compute explicitly, it is shown that a minimax dynamics, similar to that of Arrow and Hurwicz in Lagrangian optimization, can be used to find the saddle points of the effective energy. As an illustration of its wide applicability, the effective energy framework is used to derive Hopfield-like energy functions and descent dynamics for two classes of networks previously considered in the literature, winner-take-all networks and rotor networks, even when the cost function of the original optimization problem is not quadratic."
            },
            "slug": "Convex-Potentials-and-their-Conjugates-in-Analog-Elfadel",
            "title": {
                "fragments": [],
                "text": "Convex Potentials and their Conjugates in Analog Mean-Field Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The saddle-point paradigm of mean-field methods in statistical physics provides a systematic procedure for finding a mapping of constrained optimization problems onto analog networks via the notion of effective energy, and it is shown that within this paradigm, to each closed bounded constraint set is associated a smooth convex potential function."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705072"
                        ],
                        "name": "E. Mjolsness",
                        "slug": "E.-Mjolsness",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mjolsness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mjolsness"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "We will concentrate on Legendre minimization (Rangarajan et al., 1996, 1999) instead of Legendre min-max emphasized in Mjolsness and Garrett (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 169
                            }
                        ],
                        "text": "The next example concerns mean field methods to model combinatorial optimization problems such as the quadratic assignment problem (Rangarajan, Gold, & Mjolsness, 1996; Rangarajan et al., 1999) or the traveling salesman problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "(By { \u2202F \u2217 \u2202 y }\u22121( x) we mean the value y such that \u2202F \u2217 \u2202 y ( y) = x.)\nThe Legendre minimization algorithms (Rangarajan et al., 1996, 1999) exploit Legendre transforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 112
                            }
                        ],
                        "text": "The CCCP viewpoint emphasizes the geometry of the approach and complements the algebraic manipulations given in Rangarajan et al. (1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10519742,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6dab94c9938d29b24732b250fd22fe5ce0a04a5b",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The softassign quadratic assignment algorithm is a discrete-time, continuous-state, synchronous updating optimizing neural network. While its effectiveness has been shown in the traveling salesman problem, graph matching, and graph partitioning in thousands of simulations, its convergence properties have not been studied. Here, we construct discrete-time Lyapunov functions for the cases of exact and approximate doubly stochastic constraint satisfaction, which show convergence to a fixed point. The combination of good convergence properties and experimental success makes the softassign algorithm an excellent choice for neural quadratic assignment optimization."
            },
            "slug": "Convergence-Properties-of-the-Softassign-Quadratic-Rangarajan-Yuille",
            "title": {
                "fragments": [],
                "text": "Convergence Properties of the Softassign Quadratic Assignment Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "D discrete-time Lyapunov functions are constructed for the cases of exact and approximate doubly stochastic constraint satisfaction, which show convergence to a fixed point."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 31
                            }
                        ],
                        "text": "See, for example, recent work (Yuille, 2002) where CCCP was used to construct a double loop algorithm to minimize the Bethe and Kikuchi free energies (Yedidia, Freeman, & Weiss, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 253
                            }
                        ],
                        "text": "As we will show, the entropy terms arising in mean field algorithms make it particularly easy to apply CCCP. CCCP has also been applied to develop an algorithm that minimizes the Bethe and Kikuchi free energies and whose empirical convergence is rapid (Yuille, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 143
                            }
                        ],
                        "text": "CCCP has also been applied to develop an algorithm that minimizes the Bethe and Kikuchi free energies and whose empirical convergence is rapid (Yuille, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 782136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "974fee9c3836d91aee9492c07379898ccc2c3a85",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This article introduces a class of discrete iterative algorithms that are provably convergent alternatives to belief propagation (BP) and generalized belief propagation (GBP). Our work builds on recent results by Yedidia, Freeman, and Weiss (2000), who showed that the fixed points of BP and GBP algorithms correspond to extrema of the Bethe and Kikuchi free energies, respectively. We obtain two algorithms by applying CCCP to the Bethe and Kikuchi free energies, respectively (CCCP is a procedure, introduced here, for obtaining discrete iterative algorithms by decomposing a cost function into a concave and a convex part). We implement our CCCP algorithms on two- and three-dimensional spin glasses and compare their results to BP and GBP. Our simulations show that the CCCP algorithms are stable and converge very quickly (the speed of CCCP is similar to that of BP and GBP). Unlike CCCP, BP will often not converge for these problems (GBP usually, but not always, converges). The results found by CCCP applied to the Bethe or Kikuchi free energies are equivalent, or slightly better than, those found by BP or GBP, respectively (when BP and GBP converge). Note that for these, and other problems, BP and GBP give very accurate results (see Yedidia et al., 2000), and failure to converge is their major error mode. Finally, we point out that our algorithms have a large range of inference and learning applications."
            },
            "slug": "CCCP-Algorithms-to-Minimize-the-Bethe-and-Kikuchi-Yuille",
            "title": {
                "fragments": [],
                "text": "CCCP Algorithms to Minimize the Bethe and Kikuchi Free Energies: Convergent Alternatives to Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A class of discrete iterative algorithms that are provably convergent alternatives to believe propagation (BP) and generalized belief propagation (GBP) and are pointed out that have a large range of inference and learning applications."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705072"
                        ],
                        "name": "E. Mjolsness",
                        "slug": "E.-Mjolsness",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mjolsness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mjolsness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37123238"
                        ],
                        "name": "Charles Garrett",
                        "slug": "Charles-Garrett",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Garrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Garrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "In the latter (Mjolsness & Garrett, 1990), the introduction of auxiliary variables converts the problem to a min-max problem where the goal is to find a saddle point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 119
                            }
                        ],
                        "text": "We will concentrate on Legendre minimization (Rangarajan et al., 1996, 1999) instead of Legendre min-max emphasized in Mjolsness and Garrett (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 108
                            }
                        ],
                        "text": "The Legendre transform can be used to reformulate optimization problems by introducing auxiliary variables (Mjolsness & Garrett, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43144545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9b251a729004e468680c55c99435dd8dd15b78",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algebraic-transformations-of-objective-functions-Mjolsness-Garrett",
            "title": {
                "fragments": [],
                "text": "Algebraic transformations of objective functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678170728"
                        ],
                        "name": "J. Kosowsky",
                        "slug": "J.-Kosowsky",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kosowsky",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosowsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "The linear assignment problem can also be solved using these methods (Kosowsky & Yuille, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "Sinkhorn\u2019s algorithm can be used to solve the linear assignment problem (Kosowsky & Yuille, 1994), and CCCP variants of Sinkhorn can be used to solve additional constraint problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "It is straightforward to verify that Sinkhorn\u2019s algorithm is equivalent (Kosowsky & Yuille, 1994) to minimizing an energy function E\u0302[r, s] = \u2212\u2211a log ra \u2212 \u2211i log si + \u2211ia Miarasi with respect to r and s alternatively, where {ra} and {si} are the diagonal elements of E and D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 71
                            }
                        ],
                        "text": "These algorithms are derived using similar techniques to those used by Kosowsky and Yuille (1994) to rederive Sinkhorn\u2019s algorithm; hence, they can be considered generalizations of Sinkhorn."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 3
                            }
                        ],
                        "text": "As Kosowsky and Yuille (1994) showed, the minima of Eef f [S; p, q] at sufficiently large \u03b2 correspond to the solutions of the linear assignment problem whose goal is to select the permutation matrix {\u220fia} that minimizes the energy E[ \u220f ] = \u2211ia\u220fia Aia, where {Aia} is a set of assignment values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 83
                            }
                        ],
                        "text": "Therefore, Sinkhorn\u2019s algorithm can be used to solve the linear assignment problem (Kosowsky & Yuille, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6967348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "429b1cf154d3d5e24d98b6d83f875cd91aab72fd",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-invisible-hand-algorithm:-Solving-the-problem-Kosowsky-Yuille",
            "title": {
                "fragments": [],
                "text": "The invisible hand algorithm: Solving the assignment problem with statistical physics"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678170728"
                        ],
                        "name": "J. Kosowsky",
                        "slug": "J.-Kosowsky",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kosowsky",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosowsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 16
                            }
                        ],
                        "text": "As described in Yuille and Kosowsky (1994), to ensure that the minima of E[V] and Eef f [S; T] all coincide (as T \u2192 0), it is sufficient that Cijab be negative definite."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5200734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed223a6a3f853a6720406d6c22680e02e3953b9c",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been significant interest in adapting techniques from statistical physics, in particular mean field theory, to provide deterministic heuristic algorithms for obtaining approximate solutions to optimization problems. Although these algorithms have been shown experimentally to be successful there has been little theoretical analysis of them. In this paper we demonstrate connections between mean field theory methods and other approaches, in particular, barrier function and interior point methods. As an explicit example, we summarize our work on the linear assignment problem. In this previous work we defined a number of algorithms, including deterministic annealing, for solving the assignment problem. We proved convergence, gave bounds on the convergence times, and showed relations to other optimization algorithms."
            },
            "slug": "Statistical-Physics-Algorithms-That-Converge-Yuille-Kosowsky",
            "title": {
                "fragments": [],
                "text": "Statistical Physics Algorithms That Converge"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Close connections are demonstrated between mean field theory methods and other approaches, in particular, barrier function and interior point methods, for obtaining approximate solutions to optimization problems."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953625"
                        ],
                        "name": "K. Lange",
                        "slug": "K.-Lange",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lange",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49406642"
                        ],
                        "name": "D. Hunter",
                        "slug": "D.-Hunter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hunter",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102934819"
                        ],
                        "name": "I. Yang",
                        "slug": "I.-Yang",
                        "structuredName": {
                            "firstName": "Ilsoon",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "Other equivalent techniques are known as surrogate functions and majorization (Lange et al., 2000) or as lower-bound maximization (Luttrell, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "These techniques are more general than CCCP, and it has been shown that algorithms like EM can be derived from them (Minka, 1998; Lange et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122337001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9e0444b694a804a9088a622a6123e10a04430ae",
            "isKey": false,
            "numCitedBy": 775,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The well-known EM algorithm is an optimization transfer algorithm that depends on the notion of incomplete or missing data. By invoking convexity arguments, one can construct a variety of other optimization transfer algorithms that do not involve missing data. These algorithms all rely on a majorizing or minorizing function that serves as a surrogate for the objective function. Optimizing the surrogate function drives the objective function in the correct direction. This article illustrates this general principle by a number of specific examples drawn from the statistical literature. Because optimization transfer algorithms often exhibit the slow convergence of EM algorithms, two methods of accelerating optimization transfer are discussed and evaluated in the context of specific problems."
            },
            "slug": "Optimization-Transfer-Using-Surrogate-Objective-Lange-Hunter",
            "title": {
                "fragments": [],
                "text": "Optimization Transfer Using Surrogate Objective Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Because optimization transfer algorithms often exhibit the slow convergence of EM algorithms, two methods of accelerating optimization transfer are discussed and evaluated in the context of specific problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34890235"
                        ],
                        "name": "S. Ikeda",
                        "slug": "S.-Ikeda",
                        "structuredName": {
                            "firstName": "Shiro",
                            "lastName": "Ikeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ikeda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145876882"
                        ],
                        "name": "Toshiyuki TANAKA",
                        "slug": "Toshiyuki-TANAKA",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "TANAKA",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki TANAKA"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6181671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c39ae3e4c93a93f3bd882103c04ec9181e98447",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) is a universal method of stochastic reasoning. It gives exact inference for stochastic models with tree interactions and works surprisingly well even if the models have loopy interactions. Its performance has been analyzed separately in many fields, such as AI, statistical physics, information theory, and information geometry. This article gives a unified framework for understanding BP and related methods and summarizes the results obtained in many fields. In particular, BP and its variants, including tree reparameterization and concave-convex procedure, are reformulated with information-geometrical terms, and their relations to the free energy function are elucidated from an information-geometrical viewpoint. We then propose a family of new algorithms. The stabilities of the algorithms are analyzed, and methods to accelerate them are investigated."
            },
            "slug": "Stochastic-Reasoning,-Free-Energy,-and-Information-Ikeda-TANAKA",
            "title": {
                "fragments": [],
                "text": "Stochastic Reasoning, Free Energy, and Information Geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for understanding BP and related methods is given and a family of new algorithms, including tree reparameterization and concave-convex procedure, are proposed from an information-geometrical viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "Hathaway (1986) and Neal and Hinton (1998) showed that EM is equivalent to minimizing the following effective energy with respect to the variables y and P\u0302(V),\nEem[ y, P\u0302] = \u2212 \u2211\nV\nP\u0302(V) log P( y, V) + \u2211\nV\nP\u0302(V) log P\u0302(V)\n+ \u03bb {\u2211 P\u0302(V) \u2212 1 } , (4.1)\nwhere \u03bb is a Lagrange multiplier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 158
                            }
                        ],
                        "text": "(4.2)\nThese update rules are guaranteed to lower Eem[ y, P\u0302] and give convergence to a saddle point or a local minimum (Dempster et al., 1977; Hathaway, 1986; Neal & Hinton, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": true,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35180576"
                        ],
                        "name": "L. Bregman",
                        "slug": "L.-Bregman",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Bregman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bregman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 34
                            }
                        ],
                        "text": "This relates to Bregman distances (Bregman, 1967), and, indeed, the original derivation of self-annealing involved adding a Bregman distance to the en-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 120
                            }
                        ],
                        "text": "3 Examples of CCCP This section illustrates CCCP by examples from neural networks, meanfield algorithms, self-annealing (which relate to Bregman distances; Bregman, 1967), EM, and mixture models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 35
                            }
                        ],
                        "text": "This relates to Bregman distances (Bregman, 1967), and, indeed, the original derivation of self-annealing involved adding a Bregman distance to the en-\nergy function, followed by taking Legendre transforms (see section 4.2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "We first illustrate CCCP by giving examples of neural network, mean field, and self-annealing (which relate to Bregman distances; Bregman, 1967) algorithms, which can be reexpressed in this form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 137
                            }
                        ],
                        "text": "This section illustrates CCCP by examples from neural networks, meanfield algorithms, self-annealing (which relate to Bregman distances; Bregman, 1967), EM, and mixture models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121309410,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "44a6b76e5cbc61330663d0a9f393caf91a3a1be8",
            "isKey": true,
            "numCitedBy": 2442,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-relaxation-method-of-finding-the-common-point-Bregman",
            "title": {
                "fragments": [],
                "text": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "It also relates to linear prediction (Kivinen & Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2400759"
                        ],
                        "name": "C. Byrne",
                        "slug": "C.-Byrne",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Byrne",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Byrne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119387499,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "892ea09990d74615de1fd2c195f569f2010b2765",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The deblurring problem is that of recovering the function from (noisy) values . The discrete finite version of the problem is to solve the system of linear equations Hc=d for c, where H is a matrix and d and c are vectors. When the kernel is a function of the difference , the deblurring problem becomes a deconvolution problem. The use of iterative algorithms to effect deblurring subject to non-negativity constraints on c has been presented by Snyder et al for the case of non-negative kernel function h. In this paper we extend these algorithms to include upper and lower bounds on the entries of the desired solution. We show that any linear deblurring problem involving a real kernel h can be transformed into a linear deblurring problem involving a non-negative kernel. Therefore our algorithms apply to general deblurring and deconvolution problems. These algorithms converge to a solution of the system of equations y = Px, with , for , , satisfying the vector inequalities , whenever such a solution exists. When there is no solution satisfying the constraints the simultaneous versions converge to an approximate solution that minimizes a cost function related to the Kullback-Leibler cross-entropy and the Fermi-Dirac generalized entropy."
            },
            "slug": "Iterative-algorithms-for-deblurring-and-with-Byrne",
            "title": {
                "fragments": [],
                "text": "Iterative algorithms for deblurring and deconvolution with constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that any linear deblurring problem involving a real kernel h can be transformed into a lineardeblurring problems involving a non-negative kernel, and these algorithms apply to general deblurred and deconvolution problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40227361"
                        ],
                        "name": "D. Willshaw",
                        "slug": "D.-Willshaw",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Willshaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Willshaw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 17
                            }
                        ],
                        "text": "The elastic net (Durbin & Willshaw, 1987) attempts to solve the traveling salesman problem (TSP) by finding the shortest tour through a set of cities at positions { xi}."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "Our final example is the elastic net (Durbin & Willshaw, 1987; Durbin, Szeliski, & Yuille, 1989) in the formulation presented in Yuille (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 169
                            }
                        ],
                        "text": "By setting E[ y] = Eef f [S\u2217( y), y] where S\u2217( y) = arg minS Eef f [S, y], we obtain the original elastic net cost function E[ y] = \u2212T\u2211i log\u2211a e\u2212| xi\u2212 ya|2/T +\u03b3 \u2211a,b yTa Aab yb (Durbin & Willshaw, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4321691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a94be030ccd68f3a5a3bf9245137fe114c549819",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The travelling salesman problem1 is a classical problem in the field of combinatorial optimization, concerned with efficient methods for maximizing or minimizing a function of many independent variables. Given the positions of N cities, which in the simplest case lie in the plane, what is the shortest closed tour in which each city can be visited once? We describe how a parallel analogue algorithm, derived from a formal model2\u20133 for the establishment of topographically ordered projections in the brain4\u201310, can be applied to the travelling salesman problem1,11,12. Using an iterative procedure, a circular closed path is gradually elongated non-uniformly until it eventually passes sufficiently near to all the cities to define a tour. This produces shorter tour lengths than another recent parallel analogue algorithm13, scales well with the size of the problem, and is naturally extendable to a large class of optimization problems involving topographic mappings between geometrical structures14."
            },
            "slug": "An-analogue-approach-to-the-travelling-salesman-an-Durbin-Willshaw",
            "title": {
                "fragments": [],
                "text": "An analogue approach to the travelling salesman problem using an elastic net method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes how a parallel analogue algorithm, derived from a formal model for the establishment of topographically ordered projections in the brain, can be applied to the travelling salesman problem, and produces shorter tour lengths than another recent parallel analogue algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2400759"
                        ],
                        "name": "C. Byrne",
                        "slug": "C.-Byrne",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Byrne",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Byrne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Byrne (2000) has also developed an interior point algorithm for minimizing convex cost functions that is equivalent to CCCP and has been applied to image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59942575,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d66dfb9b0bd2f68906c429f123f987869981de5a",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative algorithms for image reconstruction often involve minimizing some cost function h(x) that measures the degree of agreement between the measured data and a theoretical parametrized model. In addition, one may wish to have x satisfy certain constraints. It is usually the case that the cost function is the sum of simpler functions: h(x) = \u2211i = 1Ihi(x). Partitioning the set {i = 1,...,I} as the union of the disjoint sets Bn,n = 1,...,N, we let hn(x) = \u2211iBnhi(x). The method presented here is block iterative, in the sense that at each step only the gradient of a single hn(x) is employed. Convergence can be significantly accelerated, compared to that of the single-block (N = 1) method, through the use of appropriately chosen scaling factors. The algorithm is an interior point method, in the sense that the images xk + 1 obtained at each step of the iteration satisfy the desired constraints. Here the constraints are imposed by having the next iterate xk + 1 satisfy the gradient equation \u2207F(xk + 1) = \u2207F(xk)-tn\u2207hn(xk), for appropriate scalars tn, where the convex function F is defined and differentiable only on vectors satisfying the constraints. Special cases of the algorithm that apply to tomographic image reconstruction, and permit inclusion of upper and lower bounds on individual pixels, are presented. The focus here is on the development of the underlying convergence theory of the algorithm. Behaviour of special cases has been considered elsewhere."
            },
            "slug": "Block-iterative-interior-point-optimization-methods-Byrne",
            "title": {
                "fragments": [],
                "text": "Block-iterative interior point optimization methods for image reconstruction from limited data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675444"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "This algorithm in theorem 8 has similar form to an algorithm proposed by Saul and Lee (2002) which generalizes GIS (Darroch & Ratcliff, 1972) to mixture models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7993922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4d15d8d3063ce4abb29a79c6f6b73ad53f40e5",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a learning algorithm for the classification of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classifiers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm\u2014its guarantee of monotonic improvement, and its absence of tuning parameters\u2014with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classification than comparably sized models trained by EM."
            },
            "slug": "Multiplicative-Updates-for-Classification-by-Models-Saul-Lee",
            "title": {
                "fragments": [],
                "text": "Multiplicative Updates for Classification by Mixture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A learning algorithm that retains the main virtues of the Expectation-Maximization algorithm\u2014its guarantee of monotonic improvement, and its absence of tuning parameters\u2014with the added advantage of optimizing a discriminative objective function for mixture models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 69
                            }
                        ],
                        "text": "The elastic net can be formulated as minimizing an effective energy (Yuille, 1990):\nEef f [S, y] = \u2211\nia Sia( xi \u2212 ya)2 + \u03b3 \u2211 a,b yTa Aab yb + T \u2211 i,a Sia log Sia\n+ \u2211\ni \u03bbi (\u2211 a Sia \u2212 1 ) , (3.7)\nwhere the {Aab} are components of a positive definite matrix representing a spring energy and {\u03bba} are\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 37
                            }
                        ],
                        "text": "The elastic net can be reformulated (Yuille, 1990) as minimizing an effective energy Eef f [S, y] where the variables {Sia} determine soft correspondence between the cities and the nodes of the net."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "The elastic net can be formulated as minimizing an effective energy (Yuille, 1990): Eef f [S, y] = \u2211"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 129
                            }
                        ],
                        "text": "Our final example is the elastic net (Durbin & Willshaw, 1987; Durbin, Szeliski, & Yuille, 1989) in the formulation presented in Yuille (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45670932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fbb0b127de1ea9be27d5f8c1e30d0cf394a503",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how to formulate matching and combinatorial problems of vision and neural network theory by generalizing elastic and deformable templates models to include binary matching elements. Techniques from statistical physics, which can be interpreted as computing marginal probability distributions, are then used to analyze these models and are shown to (1) relate them to existing theories and (2) give insight into the relations between, and relative effectivenesses of, existing theories. In particular we exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements. The binary elements can then be removed analytically before minimization. This is demonstrated to be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions. We give applications to winner-take-all networks, correspondence for stereo and long-range motion, the traveling salesman problem, deformable template matching, learning, content addressable memories, and models of brain development. The biological plausibility of these networks is briefly discussed."
            },
            "slug": "Generalized-Deformable-Models,-Statistical-Physics,-Yuille",
            "title": {
                "fragments": [],
                "text": "Generalized Deformable Models, Statistical Physics, and Matching Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Techniques from statistical physics are used to exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements and be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "It also relates to linear prediction (Kivinen & Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15718458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG *. They both maintain a weight vector using simple updates. For the GD algorithm, the weight vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied by a parameter called the learning rate. The EG* uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line loss bounds for EG* and compare them to previously known bounds for the GD algorithm. The bounds suggest that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller loss if only few of the input variables are relevant for the predictions. Experiments show that the worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm."
            },
            "slug": "Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient updates for linear prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35380459"
                        ],
                        "name": "S. Gold",
                        "slug": "S.-Gold",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705072"
                        ],
                        "name": "E. Mjolsness",
                        "slug": "E.-Mjolsness",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mjolsness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mjolsness"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "We will concentrate on Legendre minimization (Rangarajan et al., 1996, 1999) instead of Legendre min-max emphasized in Mjolsness and Garrett (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "(By { \u2202F \u2217 \u2202 y }\u22121( x) we mean the value y such that \u2202F \u2217 \u2202 y ( y) = x.)\nThe Legendre minimization algorithms (Rangarajan et al., 1996, 1999) exploit Legendre transforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 39
                            }
                        ],
                        "text": "By contrast, in Legendre minimization (Rangarajan et al., 1996), the problem remains a minimization one (and so it becomes easier to analyze convergence)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 160519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b782c8204b8b8c86a088a443a948221a239ec26",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel optimizing network architecture with applications in vision, learning, pattern recognition, and combinatorial optimization. This architecture is constructed by combining the following techniques: (1) deterministic annealing, (2) self-amplification, (3) algebraic transformations, (4) clocked objectives, and (5) softassign. Deterministic annealing in conjunction with self-amplification avoids poor local minima and ensures that a vertex of the hypercube is reached. Algebraic transformations and clocked objectives help partition the relaxation into distinct phases. The problems considered have doubly stochastic matrix constraints or minor variations thereof. We introduce a new technique, softassign, which is used to satisfy this constraint. Experimental results on different problems are presented and discussed."
            },
            "slug": "A-Novel-Optimizing-Network-Architecture-with-Rangarajan-Gold",
            "title": {
                "fragments": [],
                "text": "A Novel Optimizing Network Architecture with Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A novel optimizing network architecture with applications in vision, learning, pattern recognition, and combinatorial optimization is presented and a new technique, softassign, is introduced, which is used to satisfy this constraint."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "P[ y] = e\u2212E[ y]/Z can be interpreted (Durbin et al., 1989) as a constrained mixture model (Jordan & Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "This is an example of constrained mixture models (Jordan & Jacobs, 1994) and uses an EM algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 166
                            }
                        ],
                        "text": "This corresponds to a probability distribution P( y) = e\u2212E[ y]/Z on the node positions, which can be interpreted (Durbin et al., 1989) as a constrained mixture model (Jordan & Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 39
                            }
                        ],
                        "text": ", 2000) or as lower-bound maximization (Luttrell, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 131
                            }
                        ],
                        "text": "Other equivalent techniques are known as surrogate functions and majorization (Lange et al., 2000) or as lower-bound maximization (Luttrell, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": "CCCP can be viewed as a special case of variational bounding (Rustagi, 1976; Jordan, Ghahramani, Jaakkola, & Saul, 1999) and related techniques including lower-bound and upper-bound minimization (Luttrell, 1994), surrogate functions, and majorization (Lange, Hunter, & Yang, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13628599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bd17b8e29e91ff57c608a2c5254f6787d13d269",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability calculus, based on the axioms of inference, is the only consistent scheme for performing inference; this is also known as Bayesian inference. The objects which this approach manipulates, namely probability density functions (PDFs), may be created in a variety of ways, but the focus of this paper is on the use of adaptive PDF networks. Adaptive mixture distribution (MD) networks are already widely used. In this paper, an extension of the standard MD approach is presented, it is called a partitioned mixture distribution (PMD). PMD networks are designed specifically to scale sensibly to high-dimensional problems, such as image processing. Several numerical simulations are performed, which demonstrate that the emergent properties of PMD networks are similar to those of biological low-level vision processing systems. The use of PDFs as a vehicle for solving inference problems is discussed, and the standard theory of MDs is summarised. The new theory of PMDs is then presented."
            },
            "slug": "An-adaptive-Bayesian-network-for-low-level-image-Luttrell",
            "title": {
                "fragments": [],
                "text": "An adaptive Bayesian network for low-level image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents a partitioned mixture distribution (PMD) approach, designed specifically to scale sensibly to high-dimensional problems, such as image processing, and demonstrates that the emergent properties of PMD networks are similar to those of biological low-level vision processing systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 117
                            }
                        ],
                        "text": "These techniques are more general than CCCP, and it has been shown that algorithms like EM can be derived from them (Minka, 1998; Lange et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7463275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16c55848a4835f6cd00e092bd0983de7951aa949",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The Expectation Maximization algorithm given by Dempster et al has enjoyed considerable popularity for solving MAP estimation problems This note derives EM from the lower bounding viewpoint Luttrell which better illustrates the convergence properties of the algorithm and its variants The algorithm is illustrated with two examples pooling data from multiple noisy sources and tting a mixture density"
            },
            "slug": "Expectation-Maximization-as-lower-bound-Minka",
            "title": {
                "fragments": [],
                "text": "Expectation-Maximization as lower bound maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This note derives EM from the lower bounding viewpoint Luttrell which better illustrates the convergence properties of the Expectation Maximization algorithm and its variants."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 253
                            }
                        ],
                        "text": "\u2026Sia \u22121) gives us an update rule (compare example 1),\nSt+1ia = Stiae (\u22121/\u03b3 ){\u2211jbCijabStjb+\u03b8ia}\u2211 c Stice (\u22121/\u03b3 ){\u2211jbCijbcStbj+\u03b8ic} , (3.6)\nwhich, by expanding the exponential by a Taylor series, gives the equations for relaxation labeling (Rosenfeld et al., 1976; see Rangarajan, 2000, for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 205
                            }
                        ],
                        "text": "Self-annealing acts as if it has a temperature parameter that it continuously decreases or, equivalently, as if it has a barrier function whose strength is reduced automatically as the algorithm proceeds (Rangarajan, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "Our next example is self-annealing (Rangarajan, 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 265
                            }
                        ],
                        "text": "It can also be applied to the relaxation labeling problems studied in computer vision, and indeed the classic relaxation algorithm (Rosenfeld, Hummel, & Zucker, 1976) can be obtained as an approximation to selfannealing by performing a Taylor series approximation (Rangarajan, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15973890,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d39f230f4e58fd362058d81e25cf1ae690645b37",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-annealing-and-self-annihilation:-unifying-and-Rangarajan",
            "title": {
                "fragments": [],
                "text": "Self-annealing and self-annihilation: unifying deterministic annealing and relaxation labeling"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 37
                            }
                        ],
                        "text": "P[ y] = e\u2212E[ y]/Z can be interpreted (Durbin et al., 1989) as a constrained mixture model (Jordan & Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 113
                            }
                        ],
                        "text": "This corresponds to a probability distribution P( y) = e\u2212E[ y]/Z on the node positions, which can be interpreted (Durbin et al., 1989) as a constrained mixture model (Jordan & Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9262827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477db9d84d8875da57664d39af140d884d858222",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the elastic net approach (Durbin and Willshaw 1987) to the traveling salesman problem of finding the shortest path through a set of cities. The elastic net approach jointly minimizes the length of an arbitrary path in the plane and the distance between the path points and the cities. The tradeoff between these two requirements is controlled by a scale parameter K. A global minimum is found for large K, and is then tracked to a small value. In this paper, we show that (1) in the small K limit the elastic path passes arbitrarily close to all the cities, but that only one path point is attracted to each city, (2) in the large K limit the net lies at the center of the set of cities, and (3) at a critical value of K the energy function bifurcates. We also show that this method can be interpreted in terms of extremizing a probability distribution controlled by K. The minimum at a given K corresponds to the maximum a posteriori (MAP) Bayesian estimate of the tour under a natural statistical interpretation. The analysis presented in this paper gives us a better understanding of the behavior of the elastic net, allows us to better choose the parameters for the optimization, and suggests how to extend the underlying ideas to other domains."
            },
            "slug": "An-Analysis-of-the-Elastic-Net-Approach-to-the-Durbin-Szeliski",
            "title": {
                "fragments": [],
                "text": "An Analysis of the Elastic Net Approach to the Traveling Salesman Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The analysis presented in this paper gives a better understanding of the behavior of the elastic net, allows us to better choose the parameters for the optimization, and suggests how to extend the underlying ideas to other domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60578841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c9529259e180dea589447d9b7414a998286e1c2",
            "isKey": false,
            "numCitedBy": 1466,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams."
            },
            "slug": "Learning-in-Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents an introduction to inference for Bayesian networks and a view of the EM algorithm that justifies incremental, sparse and other variants, as well as an information-theoretic analysis of hard and soft assignment methods for clustering."
            },
            "venue": {
                "fragments": [],
                "text": "NATO ASI Series"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 171
                            }
                        ],
                        "text": "These have been used for content addressable memories (Waugh &\nWestervelt, 1993; Elfadel, 1995) and have been applied to clustering for unsupervised texture segmentation (Hofmann & Buhmann, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15479156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a58c3eafcc642ffa2e571e069e53f20bb1d1150",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean-field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images."
            },
            "slug": "Pairwise-Data-Clustering-by-Deterministic-Annealing-Hofmann-Buhmann",
            "title": {
                "fragments": [],
                "text": "Pairwise Data Clustering by Deterministic Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A deterministic annealing approach to pairwise clustering is described which shares the robustness properties of maximum entropy inference and the resulting Gibbs probability distributions are estimated by mean-field approximation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30489560"
                        ],
                        "name": "R. Hummel",
                        "slug": "R.-Hummel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hummel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hummel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 225
                            }
                        ],
                        "text": "\u2026Sia \u22121) gives us an update rule (compare example 1),\nSt+1ia = Stiae (\u22121/\u03b3 ){\u2211jbCijabStjb+\u03b8ia}\u2211 c Stice (\u22121/\u03b3 ){\u2211jbCijbcStbj+\u03b8ic} , (3.6)\nwhich, by expanding the exponential by a Taylor series, gives the equations for relaxation labeling (Rosenfeld et al., 1976; see Rangarajan, 2000, for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18603445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5df9cdc54c8085781c4199a95576a14cb7198b9d",
            "isKey": true,
            "numCitedBy": 1518,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a set of objects in a scene whose identifications are ambiguous, it is often possible to use relationships among the objects to reduce or eliminate the ambiguity. A striking example of this approach was given by Waltz [13]. This paper formulates the ambiguity-reduction process in terms of iterated parallel operations (i.e., relaxation operations) performed on an array of (object, identification) data. Several different models of the process are developed, convergence properties of these models are established, and simple examples are given."
            },
            "slug": "Scene-Labeling-by-Relaxation-Operations-Rosenfeld-Hummel",
            "title": {
                "fragments": [],
                "text": "Scene Labeling by Relaxation Operations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper formulates the ambiguity-reduction process in terms of iterated parallel operations (i.e., relaxation operations) performed on an array of object, identification data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643177"
                        ],
                        "name": "S. Barnett",
                        "slug": "S.-Barnett",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Barnett",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Barnett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7278262"
                        ],
                        "name": "D. Pegg",
                        "slug": "D.-Pegg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pegg",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pegg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9321869"
                        ],
                        "name": "J. Noh",
                        "slug": "J.-Noh",
                        "structuredName": {
                            "firstName": "Ji-whan",
                            "lastName": "Noh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Noh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49812593"
                        ],
                        "name": "A. Foug\u00e8res",
                        "slug": "A.-Foug\u00e8res",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Foug\u00e8res",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Foug\u00e8res"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69890621"
                        ],
                        "name": "L. Mandel",
                        "slug": "L.-Mandel",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Mandel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mandel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "These have been used for content addressable memories (Waugh &\nWestervelt, 1993; Elfadel, 1995) and have been applied to clustering for unsupervised texture segmentation (Hofmann & Buhmann, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125625318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b2517dcab431d19e3a0f172f0eddbda70f5cbd9",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the attractors of associative-memory neural networks in which analog neurons compete locally. These networks are well suited for a variety of feature-extraction, pattern-classification, and data-compression tasks. For networks storing a finite number of patterns, we present bifurcation diagrams for the pattern overlaps. For networks storing an extensive number of patterns, we present phase diagrams showing attractor types as a function of pattern-storage fraction and neuron-transfer-function steepness. We also report results for the storage capacity of k-winner associative memories in the limit of infinite neuron gain. Numerical investigations of computer-generated networks confirm the phase diagrams"
            },
            "slug": "Analog-neural-networks-with-local-competition.-II:-Barnett-Pegg",
            "title": {
                "fragments": [],
                "text": "Analog neural networks with local competition. II: Application to associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The attractors of associative-memory neural networks in which analog neurons compete locally are analyzed, showing attractor types as a function of pattern-storage fraction and neuron-transfer-function steepness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695292"
                        ],
                        "name": "R. Hathaway",
                        "slug": "R.-Hathaway",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hathaway",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hathaway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Hathaway (1986) and Neal and Hinton (1998) showed that EM is equivalent to minimizing the following effective energy with respect to the variables y and P\u0302(V),\nEem[ y, P\u0302] = \u2212 \u2211\nV\nP\u0302(V) log P( y, V) + \u2211\nV\nP\u0302(V) log P\u0302(V)\n+ \u03bb {\u2211 P\u0302(V) \u2212 1 } , (4.1)\nwhere \u03bb is a Lagrange multiplier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "(4.2)\nThese update rules are guaranteed to lower Eem[ y, P\u0302] and give convergence to a saddle point or a local minimum (Dempster et al., 1977; Hathaway, 1986; Neal & Hinton, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 113
                            }
                        ],
                        "text": "These update rules are guaranteed to lower Eem[ y, P\u0302] and give convergence to a saddle point or a local minimum (Dempster et al., 1977; Hathaway, 1986; Neal & Hinton, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119523289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f866fa9a6c13d72a87b8ff2da7c582e987db53a6",
            "isKey": true,
            "numCitedBy": 265,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Another-interpretation-of-the-EM-algorithm-for-Hathaway",
            "title": {
                "fragments": [],
                "text": "Another interpretation of the EM algorithm for mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 18
                            }
                        ],
                        "text": "The EM algorithm (Dempster et al., 1977) seeks to estimate a variable y\u2217 = arg max y log \u2211 {V} P( y, V), where { y}, {V} are variables that depend on the specific problem formulation (we will soon illustrate them for the elastic net)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 99
                            }
                        ],
                        "text": "This is an example of constrained mixture models (Jordan & Jacobs, 1994) and uses an EM algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 119
                            }
                        ],
                        "text": "(4.2)\nThese update rules are guaranteed to lower Eem[ y, P\u0302] and give convergence to a saddle point or a local minimum (Dempster et al., 1977; Hathaway, 1986; Neal & Hinton, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 113
                            }
                        ],
                        "text": "These update rules are guaranteed to lower Eem[ y, P\u0302] and give convergence to a saddle point or a local minimum (Dempster et al., 1977; Hathaway, 1986; Neal & Hinton, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": true,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986184"
                        ],
                        "name": "G. Strang",
                        "slug": "G.-Strang",
                        "structuredName": {
                            "firstName": "Gilbert",
                            "lastName": "Strang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Strang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11609445"
                        ],
                        "name": "L. Freund",
                        "slug": "L.-Freund",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "Two properites can be derived from this definition (Strang, 1986): Property 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 52
                            }
                        ],
                        "text": "Two properites can be derived from this definition (Strang, 1986):\nProperty 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62132950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac84c7e453c6848f5b873492e56f74d11bde71aa",
            "isKey": false,
            "numCitedBy": 1733,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to applied mathematics , Introduction to applied mathematics , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Introduction-to-applied-mathematics-Strang-Freund",
            "title": {
                "fragments": [],
                "text": "Introduction to applied mathematics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "We can express GIS as a CCCP algorithm in the variables {r\u00b5 = e\u03bb\u00b5} by decomposing the cost function E[ r] into a convex term \u2212\u2211\u00b5 h\u00b5 log r\u00b5 and a concave term log Z[{log r\u00b5}]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": "These include the generalized iterative scaling (GIS) algorithm (Darroch & Ratcliff, 1972) and Sinkhorn\u2019s algorithm for obtaining doubly stochastic matrices (Sinkhorn, 1964)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Section 5 shows that other algorithms such as GIS and Sinkhorn can be expressed in CCCP by a change of variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The GIS algorithm is given by\n\u03bbt+1\u00b5 = \u03bbt\u00b5 \u2212 log ht\u00b5 + log h\u00b5, \u2200\u00b5, (5.1)\nwhere ht\u00b5 = \u2211\nx P( x; \u03bbt)\u03c6\u00b5( x)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "We now show that GIS can be reformulated as CCCP, which gives a\nsimple convergence proof of the algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 77
                            }
                        ],
                        "text": "In this section, we first show that both generalized iterative scaling (GIS; Darroch & Ratcliff, 1972) and Sinkhorn\u2019s algorithm (Sinkhorn, 1964) can be formulated as CCCP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "This gives\n1 rt+1\u00b5 = 1 rt\u00b5 1 h\u00b5 \u2211 x P( x : rt)\u03c6\u03bd( x), (5.4)\nwhich is the GIS algorithm after setting r\u00b5 = e\u03bb\u00b5, \u2200\u00b5.\n5.2 Sinkhorn\u2019s Algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "This section shows that the GIS algorithm (Darroch & Ratcliff, 1972) for estimating parameters of probability distributions can also be expressed as CCCP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "All problems of this type can be converted to a standard form where \u03c6\u00b5( x) \u2265 0, \u2200 \u00b5, x, h\u00b5 \u2265 0, \u2200 \u00b5, and\u2211\n\u00b5 \u03c6\u00b5( x) = 1, \u2200 x and \u2211\n\u00b5 h\u00b5 = 1 (Darroch & Ratcliff, 1972)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 116
                            }
                        ],
                        "text": "This algorithm in theorem 8 has similar form to an algorithm proposed by Saul and Lee (2002) which generalizes GIS (Darroch & Ratcliff, 1972) to mixture models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 137
                            }
                        ],
                        "text": "It is guaranteed to converge to the (unique) minimum of the energy function log Z[\u03bb] \u2212 h \u00b7 \u03bb and hence gives a solution to \u2211\nx P( x; \u03bb) \u03c6( x) = h, (Darroch & Ratcliff, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": true,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102566654"
                        ],
                        "name": "Richard Sinkhorn",
                        "slug": "Richard-Sinkhorn",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sinkhorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Sinkhorn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 40
                            }
                        ],
                        "text": "Then Sinkhorn\u2019s theorem states: Theorem (Sinkhorn, 1964)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "These include the generalized iterative scaling (GIS) algorithm (Darroch & Ratcliff, 1972) and Sinkhorn\u2019s algorithm for obtaining doubly stochastic matrices (Sinkhorn, 1964)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "Sinkhorn\u2019s algorithm was designed to make matrices doubly stochastic (Sinkhorn, 1964)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 40
                            }
                        ],
                        "text": "Then Sinkhorn\u2019s theorem states:\nTheorem (Sinkhorn, 1964)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "In this section, we first show that both generalized iterative scaling (GIS; Darroch & Ratcliff, 1972) and Sinkhorn\u2019s algorithm (Sinkhorn, 1964) can be formulated as CCCP."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120846714,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "441f33fab0614fa0696be54a046cbc692b7e70a2",
            "isKey": true,
            "numCitedBy": 830,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Relationship-Between-Arbitrary-Positive-Matrices-Sinkhorn",
            "title": {
                "fragments": [],
                "text": "A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87075478"
                        ],
                        "name": "B. Beavis",
                        "slug": "B.-Beavis",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Beavis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Beavis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102424944"
                        ],
                        "name": "I. Dobbs",
                        "slug": "I.-Dobbs",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Dobbs",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dobbs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125692686,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "7fe26ea9b2d662cebb559261f53df5e94f9f307a",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-and-Stability-Theory-for-Economic-AND-Beavis-Dobbs",
            "title": {
                "fragments": [],
                "text": "Optimization and Stability Theory for Economic Analysis: DYNAMICS AND STABILITY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124525506,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "493cd5e15d6b8726ce27ed0595f1993c27525670",
            "isKey": false,
            "numCitedBy": 17356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-recipes-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091885188"
                        ],
                        "name": "Marcus",
                        "slug": "Marcus",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Marcus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100767079"
                        ],
                        "name": "Westervelt",
                        "slug": "Westervelt",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Westervelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Westervelt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 68
                            }
                        ],
                        "text": "An original motivation for them was based on a convexity principle (Marcus & Westervelt, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39969227,
            "fieldsOfStudy": [
                "Physics",
                "Medicine"
            ],
            "id": "10b142806ef289156a06478cfa9bfc8ca43255ba",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-of-iterated-map-neural-networks.-Marcus-Westervelt",
            "title": {
                "fragments": [],
                "text": "Dynamics of iterated-map neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, General physics"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95716490"
                        ],
                        "name": "Waugh",
                        "slug": "Waugh",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Waugh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Waugh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100767079"
                        ],
                        "name": "Westervelt",
                        "slug": "Westervelt",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Westervelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Westervelt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "These have been used for content addressable memories (Waugh &\nWestervelt, 1993; Elfadel, 1995) and have been applied to clustering for unsupervised texture segmentation (Hofmann & Buhmann, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13001108,
            "fieldsOfStudy": [
                "Mathematics",
                "Medicine"
            ],
            "id": "e92ecd9b2b11717181e1df98a9c73bc93a5bf135",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analog-neural-networks-with-local-competition.-I.-Waugh-Westervelt",
            "title": {
                "fragments": [],
                "text": "Analog neural networks with local competition. I. Dynamics and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 81
                            }
                        ],
                        "text": "After much of this work was done, we obtained an unpublished technical report by Geman (1984) that states theorem 2 and has results for a subclass of EM algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 110
                            }
                        ],
                        "text": "(After completing this work we found that a version of theorem 2 appeared in an unpublished technical report: Geman, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parameter estimation for Markov random fields with hidden variables and experiments with the EM algorithm (Working paper no. 21)"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Mathematics and Statistics,"
            },
            "year": 1984
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 20
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Concave-Convex-Procedure-Yuille-Rangarajan/ffb607e61e10a3bb54463b334aaf5ea9c7c04be6?sort=total-citations"
}