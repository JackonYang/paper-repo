{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3006928"
                        ],
                        "name": "N. Krishnamoorthy",
                        "slug": "N.-Krishnamoorthy",
                        "structuredName": {
                            "firstName": "Niveda",
                            "lastName": "Krishnamoorthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Krishnamoorthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3163967"
                        ],
                        "name": "Girish Malkarnenkar",
                        "slug": "Girish-Malkarnenkar",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Malkarnenkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Malkarnenkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Subsets of this data were previously used by [20] and [14], however, contrary to these works, we use all the videos, and not only the 20 objects included in the PASCAL dataset [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026object descriptors are based on the well-known Deformable Parts Model (DPM) [11] as it achieves state-of-the-art performance on the PASCAL (VOC) Challenge, and also forms the substrate for the ObjectBank representation [18], which has itself demonstrated strong performance as a concept\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8740928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b9f8101c61b415f946625b69f69fc9e3d0d6fc4",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-of-the-art object and activity detectors with \"real-world' knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time.\n \n"
            },
            "slug": "Generating-Natural-Language-Video-Descriptions-Krishnamoorthy-Malkarnenkar",
            "title": {
                "fragments": [],
                "text": "Generating Natural-Language Video Descriptions Using Text-Mined Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work combines the output of state-of-the-art object and activity detectors with \"real-world' knowledge to select the most probable subject-verb-object triplet for describing a video, and shows that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363529"
                        ],
                        "name": "P. Das",
                        "slug": "P.-Das",
                        "structuredName": {
                            "firstName": "Pradipto",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026123"
                        ],
                        "name": "Chenliang Xu",
                        "slug": "Chenliang-Xu",
                        "structuredName": {
                            "firstName": "Chenliang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenliang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38972663"
                        ],
                        "name": "Richard F. Doell",
                        "slug": "Richard-F.-Doell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Doell",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard F. Doell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Introduction\nDespite recent advances in activity recognition [27, 24, 2, 5] automatic understanding and description of activities \u201cin-the-wild\u201d remains challenging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12284555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level."
            },
            "slug": "A-Thousand-Frames-in-Just-a-Few-Words:-Lingual-of-Das-Xu",
            "title": {
                "fragments": [],
                "text": "A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions that captures the most relevant contents of a video in a natural language description."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805076"
                        ],
                        "name": "G. Toderici",
                        "slug": "G.-Toderici",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Toderici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Toderici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1842163"
                        ],
                        "name": "J. Yagnik",
                        "slug": "J.-Yagnik",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Yagnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yagnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "A matching function \u03bcLt : V \u2192 [0, 1] is then defined over a hierarchy H with respect to a ground truth set leaf nodes Lt \u2282 L (L is the set of all leafs in the tree) as:\n(1)\u03bcLt(v) = max l\u2208Lt\n{st(v, l)}\nwhere st is the similarity between any two nodes in the hierarchy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There has been existing work on \u201cin-the-wild\u201d video tagging on YouTube (e.g., [1]) but their focus is on mining text tags, and not on describing actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14967958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cc75d0939454a42f81a0ed692bf9fd1cef11f8f",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a new method for automatic discovery and organization of descriptive concepts (labels) within large real-world corpora of user-uploaded multimedia, such as YouTube. com. Conversely, it also provides validation of existing labels, if any. While training, our method does not assume any explicit manual annotation other than the weak labels already available in the form of video title, description, and tags. Prior work related to such auto-annotation assumed that a vocabulary of labels of interest (e. g., indoor, outdoor, city, landscape) is specified a priori. In contrast, the proposed method begins with an empty vocabulary. It analyzes audiovisual features of 25 million YouTube. com videos -- nearly 150 years of video data -- effectively searching for consistent correlation between these features and text metadata. It autonomously extends the label vocabulary as and when it discovers concepts it can reliably identify, eventually leading to a vocabulary with thousands of labels and growing. We believe that this work significantly extends the state of the art in multimedia data mining, discovery, and organization based on the technical merit of the proposed ideas as well as the enormous scale of the mining exercise in a very challenging, unconstrained, noisy domain."
            },
            "slug": "Video2Text:-Learning-to-Annotate-Video-Content-Aradhye-Toderici",
            "title": {
                "fragments": [],
                "text": "Video2Text: Learning to Annotate Video Content"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper discusses a new method for automatic discovery and organization of descriptive concepts (labels) within large real-world corpora of user-uploaded multimedia, such as YouTube."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Data Mining Workshops"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145096334"
                        ],
                        "name": "K. Reddy",
                        "slug": "K.-Reddy",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Reddy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Introduction\nDespite recent advances in activity recognition [27, 24, 2, 5] automatic understanding and description of activities \u201cin-the-wild\u201d remains challenging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Previously published activity recognition methods that work on datasets such as KTH [26], Drinking and Smoking [16] and UCF50 [24] have a very limited recognition vocabulary of activity classes, ranging from 6 to 12 action classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10875786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7eee57d8a8c057ab1f599105d16d0e8489a61e0",
            "isKey": false,
            "numCitedBy": 547,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Action recognition on large categories of unconstrained videos taken from the web is a very challenging problem compared to datasets like KTH (6 actions), IXMAS (13 actions), and Weizmann (10 actions). Challenges like camera motion, different viewpoints, large interclass variations, cluttered background, occlusions, bad illumination conditions, and poor quality of web videos cause the majority of the state-of-the-art action recognition approaches to fail. Also, an increased number of categories and the inclusion of actions with high confusion add to the challenges. In this paper, we propose using the scene context information obtained from moving and stationary pixels in the key frames, in conjunction with motion features, to solve the action recognition problem on a large (50 actions) dataset with videos from the web. We perform a combination of early and late fusion on multiple features to handle the very large number of categories. We demonstrate that scene context is a very important feature to perform action recognition on very large datasets. The proposed method does not require any kind of video stabilization, person detection, or tracking and pruning of features. Our approach gives good performance on a large number of action categories; it has been tested on the UCF50 dataset with 50 action categories, which is an extension of the UCF YouTube Action (UCF11) dataset containing 11 action categories. We also tested our approach on the KTH and HMDB51 datasets for comparison."
            },
            "slug": "Recognizing-50-human-action-categories-of-web-Reddy-Shah",
            "title": {
                "fragments": [],
                "text": "Recognizing 50 human action categories of web videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes using the scene context information obtained from moving and stationary pixels in the key frames, in conjunction with motion features, to solve the action recognition problem on a large (50 actions) dataset with videos from the web."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056669450"
                        ],
                        "name": "Duo Ding",
                        "slug": "Duo-Ding",
                        "structuredName": {
                            "firstName": "Duo",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duo Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740721"
                        ],
                        "name": "Florian Metze",
                        "slug": "Florian-Metze",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Metze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Metze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37715825"
                        ],
                        "name": "Shourabh Rawat",
                        "slug": "Shourabh-Rawat",
                        "structuredName": {
                            "firstName": "Shourabh",
                            "lastName": "Rawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shourabh Rawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145610328"
                        ],
                        "name": "Peter F. Schulam",
                        "slug": "Peter-F.-Schulam",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Schulam",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter F. Schulam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144804455"
                        ],
                        "name": "S. Burger",
                        "slug": "S.-Burger",
                        "structuredName": {
                            "firstName": "Susanne",
                            "lastName": "Burger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Burger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128651"
                        ],
                        "name": "Ehsan Younessian",
                        "slug": "Ehsan-Younessian",
                        "structuredName": {
                            "firstName": "Ehsan",
                            "lastName": "Younessian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehsan Younessian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056689403"
                        ],
                        "name": "Lei Bao",
                        "slug": "Lei-Bao",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Bao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Bao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772930"
                        ],
                        "name": "Michael G. Christel",
                        "slug": "Michael-G.-Christel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Christel",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael G. Christel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8028285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df74e5c1078a800647630ea7277ad844b00096f",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Given the deluge of multimedia content that is becoming available over the Internet, it is increasingly important to be able to effectively examine and organize these large stores of information in ways that go beyond browsing or collaborative filtering. In this paper we review previous work on audio and video processing, and define the task of Topic-Oriented Multimedia Summarization (TOMS) using natural language generation: given a set of automatically extracted features from a video (such as visual concepts and ASR transcripts) a TOMS system will automatically generate a paragraph of natural language (\"a recounting\"), which summarizes the important information in a video belonging to a certain topic area, and provides explanations for why a video was matched and retrieved. We see this as a first step towards systems that will be able to discriminate visually similar, but semantically different videos, compare two videos and provide textual output or summarize a large number of videos at once. In this paper, we introduce our approach of solving the TOMS problem. We extract visual concept features and ASR transcription features from a given video, and develop a template-based natural language generation system to produce a textual recounting based on the extracted features. We also propose possible experimental designs for continuously evaluating and improving TOMS systems, and present results of a pilot evaluation of our initial system."
            },
            "slug": "Beyond-audio-and-video-retrieval:-towards-Ding-Metze",
            "title": {
                "fragments": [],
                "text": "Beyond audio and video retrieval: towards multimedia summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper extracts visual concept features and ASR transcription features from a given video, and develops a template-based natural language generation system to produce a textual recounting based on the extracted features."
            },
            "venue": {
                "fragments": [],
                "text": "ICMR '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120684987"
                        ],
                        "name": "Muhammad Usman Ghani Khan",
                        "slug": "Muhammad-Usman-Ghani-Khan",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Khan",
                            "middleNames": [
                                "Usman",
                                "Ghani"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Usman Ghani Khan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703592"
                        ],
                        "name": "Y. Gotoh",
                        "slug": "Y.-Gotoh",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gotoh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "In this paper, we take steps towards scalable \u201cin-thewild\u201d description of short videos by making two observations: 1) that there are several valid ways to describe the same activity, and 2) the description does not need to be\n1550-5499/13 $31.00 \u00a9 2013 IEEE DOI 10.1109/ICCV.2013.337\n2712\nvery\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15575166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9076183dcdb28ebb3e547f7ffd3f9d5d0faec531",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution addresses generation of natural language descriptions for human actions, behaviour and their relations with other objects observed in video streams. The work starts with implementation of conventional image processing techniques to extract high level features from video. These features are converted into natural language descriptions using context free grammar. Although feature extraction processes are erroneous at various levels, we explore approaches to putting them together to produce a coherent description. Evaluation is made by calculating ROUGE scores between human annotated and machine generated descriptions. Further we introduce a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions."
            },
            "slug": "Describing-Video-Contents-in-Natural-Language-Khan-Gotoh",
            "title": {
                "fragments": [],
                "text": "Describing Video Contents in Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This contribution addresses generation of natural language descriptions for human actions, behaviour and their relations with other objects observed in video streams using context free grammar and introduces a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Previously published activity recognition methods that work on datasets such as KTH [26], Drinking and Smoking [16] and UCF50 [24] have a very limited recognition vocabulary of activity classes, ranging from 6 to 12 action classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9689275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6cd1a1c1e6bde8e06120c431d4e816e4d67f249",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We address recognition and localization of human actions in realistic scenarios. In contrast to the previous work studying human actions in controlled settings, here we train and test algorithms on real movies with substantial variation of actions in terms of subject appearance, motion, surrounding scenes, viewing angles and spatio-temporal extents. We introduce a new annotated human action dataset and use it to evaluate several existing methods. We in particular focus on boosted space-time window classifiers and introduce \"keyframe priming\" that combines discriminative models of human motion and shape within an action. Keyframe priming is shown to significantly improve the performance of action detection. We present detection results for the action class \"drinking\" evaluated on two episodes of the movie \"Coffee and Cigarettes\"."
            },
            "slug": "Retrieving-actions-in-movies-Laptev-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "Retrieving actions in movies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new annotated human action dataset is introduced and a new \"keyframe priming\" that combines discriminative models of human motion and shape within an action is shown to significantly improve the performance of action detection."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49693392"
                        ],
                        "name": "A. Kojima",
                        "slug": "A.-Kojima",
                        "structuredName": {
                            "firstName": "Atsuhiro",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kojima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114145871"
                        ],
                        "name": "Takeshi Tamura",
                        "slug": "Takeshi-Tamura",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950023"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16139212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d53a97a3dd7760b193c0d9a5293b60feff239059",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for describing human activities from video images based on concept hierarchies of actions. Major difficulty in transforming video images into textual descriptions is how to bridge a semantic gap between them, which is also known as inverse Hollywood problem. In general, the concepts of events or actions of human can be classified by semantic primitives. By associating these concepts with the semantic features extracted from video images, appropriate syntactic components such as verbs, objects, etc. are determined and then translated into natural language sentences. We also demonstrate the performance of the proposed method by several experiments."
            },
            "slug": "Natural-Language-Description-of-Human-Activities-on-Kojima-Tamura",
            "title": {
                "fragments": [],
                "text": "Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method for describing human activities from video images based on concept hierarchies of actions based on semantic primitives, which demonstrates the performance of the proposed method by several experiments."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "In this paper, we take steps towards scalable \u201cin-thewild\u201d description of short videos by making two observations: 1) that there are several valid ways to describe the same activity, and 2) the description does not need to be\n1550-5499/13 $31.00 \u00a9 2013 IEEE DOI 10.1109/ICCV.2013.337\n2712\nvery\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Most prior work on natural-language description of visual data has focused on static images [30, 15, 19, 10, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10702193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbdbe747c6aa8b35b981d21e475ff1506a1bae66",
            "isKey": true,
            "numCitedBy": 319,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches."
            },
            "slug": "Composing-Simple-Image-Descriptions-using-Web-scale-Li-Kulkarni",
            "title": {
                "fragments": [],
                "text": "Composing Simple Image Descriptions using Web-scale N-grams"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams, which indicates that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1631318420"
                        ],
                        "name": "Tanvi S. Motwani",
                        "slug": "Tanvi-S.-Motwani",
                        "structuredName": {
                            "firstName": "Tanvi",
                            "lastName": "Motwani",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanvi S. Motwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "We followed Motwani and Mooney\u2019s [20] approach to automatically extract semantic SVO triplets from the human generated sentences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Subsets of this data were previously used by [20] and [14], however, contrary to these works, we use all the videos, and not only the 20 objects included in the PASCAL dataset [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Ours is also the first approach to evaluate on generic, large and diverse set of challenging YouTube videos that cover an unprecedented range of activities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215542366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47af965b9567bb0389cea161c9bd43a23a2aedd6",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing activities in real-world videos is a challenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling training videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correlations between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity recognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach."
            },
            "slug": "Improving-Video-Activity-Recognition-using-Object-Motwani-Mooney",
            "title": {
                "fragments": [],
                "text": "Improving Video Activity Recognition using Object Recognition and Text Mining"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling training videos."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "While object recognition methods have gone large-scale, with datasets such as ImageNet[7] and LabelMe [25], activity datasets have lagged behind."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1888731"
                        ],
                        "name": "Mohsen Hejrati",
                        "slug": "Mohsen-Hejrati",
                        "structuredName": {
                            "firstName": "Mohsen",
                            "lastName": "Hejrati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohsen Hejrati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21160985"
                        ],
                        "name": "M. Sadeghi",
                        "slug": "M.-Sadeghi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Sadeghi",
                            "middleNames": [
                                "Amin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadeghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052690705"
                        ],
                        "name": "Peter Young",
                        "slug": "Peter-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125805"
                        ],
                        "name": "Cyrus Rashtchian",
                        "slug": "Cyrus-Rashtchian",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Rashtchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyrus Rashtchian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Most prior work on natural-language description of visual data has focused on static images [30, 15, 19, 10, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13272863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us-ingdata. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche."
            },
            "slug": "Every-Picture-Tells-a-Story:-Generating-Sentences-Farhadi-Hejrati",
            "title": {
                "fragments": [],
                "text": "Every Picture Tells a Story: Generating Sentences from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A system that can compute a score linking an image to a sentence, which can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005568"
                        ],
                        "name": "Asaad Hakeem",
                        "slug": "Asaad-Hakeem",
                        "structuredName": {
                            "firstName": "Asaad",
                            "lastName": "Hakeem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asaad Hakeem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085175"
                        ],
                        "name": "N. Haering",
                        "slug": "N.-Haering",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Haering",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Haering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "The existing work on describing videos with sentences [12, 17, 13, 8, 14, 5] deals with constrained domains with a limited set of actions or objects, and does not exploit text\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1717158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8136e4ed207be52d840edb78bfbfabeafc32ca28",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a framework that performs automatic semantic annotation of visual events (SAVE). This is an enabling technology for content-based video annotation, query and retrieval with applications in Internet video search and video data mining. The method involves identifying objects in the scene, describing their inter-relations, detecting events of interest, and representing them semantically in a human readable and query-able format. The SAVE framework is composed of three main components. The first component is an image parsing engine that performs scene content extraction using bottom-up image analysis and a stochastic attribute image grammar, where we define a visual vocabulary from pixels, primitives, parts, objects and scenes, and specify their spatio-temporal or compositional relations; and a bottom-up top-down strategy is used for inference. The second component is an event inference engine, where the video event markup language (VEML) is adopted for semantic representation, and a grammar-based approach is used for event analysis and detection. The third component is the text generation engine that generates text report using head-driven phrase structure grammar (HPSG). The main contribution of this paper is a framework for an end-to-end system that infers visual events and annotates a large collection of videos. Experiments with maritime and urban scenes indicate the feasibility of the proposed approach."
            },
            "slug": "SAVE:-A-framework-for-semantic-annotation-of-visual-Lee-Hakeem",
            "title": {
                "fragments": [],
                "text": "SAVE: A framework for semantic annotation of visual events"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A framework for an end-to-end system that infers visual events and annotates a large collection of videos and experiments with maritime and urban scenes indicate the feasibility of the proposed approach."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712738"
                        ],
                        "name": "C. Sch\u00fcldt",
                        "slug": "C.-Sch\u00fcldt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Sch\u00fcldt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sch\u00fcldt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Previously published activity recognition methods that work on datasets such as KTH [26], Drinking and Smoking [16] and UCF50 [24] have a very limited recognition vocabulary of activity classes, ranging from 6 to 12 action classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8777811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b480f6a3750b4cebaf1db205692c8321d45926a2",
            "isKey": false,
            "numCitedBy": 3080,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition."
            },
            "slug": "Recognizing-human-actions:-a-local-SVM-approach-Sch\u00fcldt-Laptev",
            "title": {
                "fragments": [],
                "text": "Recognizing human actions: a local SVM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition and presents the presented results of action recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Introduction\nDespite recent advances in activity recognition [27, 24, 2, 5] automatic understanding and description of activities \u201cin-the-wild\u201d remains challenging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Since the words at higherlevel nodes of our trees do not tend to appear in the human descriptions, here we use the first level of our hierarchies, which group the flat raw labels (see Section 4)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13537104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3afbb0e64fcb70496b44b30b76fac9456cc51e34",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports."
            },
            "slug": "Action-recognition-by-dense-trajectories-Wang-Kl\u00e4ser",
            "title": {
                "fragments": [],
                "text": "Action recognition by dense trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a novel descriptor based on motion boundary histograms, which is robust to camera motion and consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7607499"
                        ],
                        "name": "Yezhou Yang",
                        "slug": "Yezhou-Yang",
                        "structuredName": {
                            "firstName": "Yezhou",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yezhou Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756655"
                        ],
                        "name": "C. L. Teo",
                        "slug": "C.-L.-Teo",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Teo",
                            "middleNames": [
                                "Lik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Teo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697493"
                        ],
                        "name": "Y. Aloimonos",
                        "slug": "Y.-Aloimonos",
                        "structuredName": {
                            "firstName": "Yiannis",
                            "lastName": "Aloimonos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aloimonos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 112
                            }
                        ],
                        "text": "Most prior work on natural-language description of visual data has focused on static images [30, 15, 19, 10, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1539668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone."
            },
            "slug": "Corpus-Guided-Sentence-Generation-of-Natural-Images-Yang-Teo",
            "title": {
                "fragments": [],
                "text": "Corpus-Guided Sentence Generation of Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results show that the strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110901865"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Most prior work on natural-language description of visual data has focused on static images [30, 15, 19, 10, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6023198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05e074abddd3fe987b9bebd46f6cf4bf8465c37e",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding. The proposed I2T framework follows three steps: 1) input images (or video frames) are decomposed into their constituent visual patterns by an image parsing engine, in a spirit similar to parsing sentences in natural language; 2) the image parsing results are converted into semantic representation in the form of Web ontology language (OWL), which enables seamless integration with general knowledge bases; and 3) a text generation engine converts the results from previous steps into semantically meaningful, human readable, and query-able text reports. The centerpiece of the I2T framework is an and-or graph (AoG) visual knowledge representation, which provides a graphical representation serving as prior knowledge for representing diverse visual patterns and provides top-down hypotheses during the image parsing. The AoG embodies vocabularies of visual elements including primitives, parts, objects, scenes as well as a stochastic image grammar that specifies syntactic relations (i.e., compositional) and semantic relations (e.g., categorical, spatial, temporal, and functional) between these visual elements. Therefore, the AoG is a unified model of both categorical and symbolic representations of visual knowledge. The proposed I2T framework has two objectives. First, we use semiautomatic method to parse images from the Internet in order to build an AoG for visual knowledge representation. Our goal is to make the parsing process more and more automatic using the learned AoG model. Second, we use automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications. In the case studies at the end of this paper, we demonstrate two automatic I2T systems: a maritime and urban scene video surveillance system and a real-time automatic driving scene understanding system."
            },
            "slug": "I2T:-Image-Parsing-to-Text-Description-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "I2T: Image Parsing to Text Description"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding and uses automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21570451"
                        ],
                        "name": "Andrei Barbu",
                        "slug": "Andrei-Barbu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Barbu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Barbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48540451"
                        ],
                        "name": "Alexander Bridge",
                        "slug": "Alexander-Bridge",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Bridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Bridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3190146"
                        ],
                        "name": "Zachary Burchill",
                        "slug": "Zachary-Burchill",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Burchill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Burchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2179730"
                        ],
                        "name": "D. Coroian",
                        "slug": "D.-Coroian",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Coroian",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Coroian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38414598"
                        ],
                        "name": "Aaron Michaux",
                        "slug": "Aaron-Michaux",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Michaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Michaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587937"
                        ],
                        "name": "Sam Mussman",
                        "slug": "Sam-Mussman",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Mussman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam Mussman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38052303"
                        ],
                        "name": "S. Narayanaswamy",
                        "slug": "S.-Narayanaswamy",
                        "structuredName": {
                            "firstName": "Siddharth",
                            "lastName": "Narayanaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Narayanaswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2968009"
                        ],
                        "name": "D. Salvi",
                        "slug": "D.-Salvi",
                        "structuredName": {
                            "firstName": "Dhaval",
                            "lastName": "Salvi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Salvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073229458"
                        ],
                        "name": "Lara Schmidt",
                        "slug": "Lara-Schmidt",
                        "structuredName": {
                            "firstName": "Lara",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lara Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060623"
                        ],
                        "name": "Jiangnan Shangguan",
                        "slug": "Jiangnan-Shangguan",
                        "structuredName": {
                            "firstName": "Jiangnan",
                            "lastName": "Shangguan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangnan Shangguan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32655613"
                        ],
                        "name": "Jarrell W. Waggoner",
                        "slug": "Jarrell-W.-Waggoner",
                        "structuredName": {
                            "firstName": "Jarrell",
                            "lastName": "Waggoner",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jarrell W. Waggoner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117074540"
                        ],
                        "name": "Song Wang",
                        "slug": "Song-Wang",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111524777"
                        ],
                        "name": "Jinlian Wei",
                        "slug": "Jinlian-Wei",
                        "structuredName": {
                            "firstName": "Jinlian",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinlian Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1813304"
                        ],
                        "name": "Yifan Yin",
                        "slug": "Yifan-Yin",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48806246"
                        ],
                        "name": "Zhiqi Zhang",
                        "slug": "Zhiqi-Zhang",
                        "structuredName": {
                            "firstName": "Zhiqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiqi Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Introduction\nDespite recent advances in activity recognition [27, 24, 2, 5] automatic understanding and description of activities \u201cin-the-wild\u201d remains challenging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "In this paper, we take steps towards scalable \u201cin-thewild\u201d description of short videos by making two observations: 1) that there are several valid ways to describe the same activity, and 2) the description does not need to be\n1550-5499/13 $31.00 \u00a9 2013 IEEE DOI 10.1109/ICCV.2013.337\n2712\nvery\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3841790,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "793c1c908672ea71aef9e1b41a46272aa27598f7",
            "isKey": true,
            "numCitedBy": 125,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases,spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture."
            },
            "slug": "Video-In-Sentences-Out-Barbu-Bridge",
            "title": {
                "fragments": [],
                "text": "Video In Sentences Out"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A system that produces sentential descriptions of video: who did what to whom, and where and how they did it, with an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Second, for each video we extract 2 frames per second, and for each frame, apply the object detectors proposed in [11] and [18], and select the maximum score assigned to each object in any frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The flat (FL) baseline predicts the most confident output for each SVM trained over the whole set of labels without any hierarchy or grouping."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The hierarchical (HE) baseline is an implementation of the \u201cHedging Your Bets\u201d method [6],\nMethod 0-1 LossS% V% O% Prior 78.36 13.43 6.12\nFL 78.51 22.09 12.84 OU 80.90 29.10 17.01\nTable 1: Binary 0-1 accuracy of predicting subject, verb and object labels with Prior:most frequent triplet, FL:flat\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The hierarchical (HE) baseline is an implementation of the \u201cHedging Your Bets\u201d method [6],\nMethod 0-1 LossS% V% O% Prior 78.36 13.43 6.12\nFL 78.51 22.09 12.84 OU 80.90 29.10 17.01\nTable 1: Binary 0-1 accuracy of predicting subject, verb and object labels with Prior:most frequent triplet, FL:flat visual classifiers, OU:first level of our semantic hierarchies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "The av-\nRetrieval Method FL HE OU Ground Truth Average Rating 1.81 1.54 1.99 3.90\nerage ratings of the videos retrieved by each method is presented in Table 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Second, for each video we extract 2 frames per second, and for each frame, apply the object detectors proposed in [11] and [18], and select the maximum score assigned to each object in any frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "There are many objects for which we lack a pre-trained model, and we have models for several objects that don\u2019t appear in the dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 591187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6286a82f72f632672c1890f3dd6bbb15b8e5168b",
            "isKey": true,
            "numCitedBy": 996,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns."
            },
            "slug": "Object-Bank:-A-High-Level-Image-Representation-for-Li-Su",
            "title": {
                "fragments": [],
                "text": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A high-level image representation, called the Object Bank, is proposed, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "We use two measures of accuracy, the first is a 0-1 loss defined over the leaf or internal nodes (similar to the accuracy defined in [6]) and the second is based on a similarity function between pairs of nodes in the hierarchy as described below."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "For example, if \u201cperson\u201d and \u201ccar\u201d are the top subject and object detections and \u201cmove\u201d is the top verb detection, we can expand \u201cmove\u201d with similar verbs like \u201cride\u201d and \u201cdrive\u201d to describe the video as \u201cA person is driving a car\u201d without needing any training videos for \u201cride\u201d or \u201cdrive\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This also indicates that WUP is a good intrinsic measure of evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There has been existing work on \u201cin-the-wild\u201d video tagging on YouTube (e.g., [1]) but their focus is on mining text tags, and not on describing actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "Figure 2 illustrates the advantage of our approach: while the conventional method attempts (unsuccessfully) to predict the most accurate basic category classifiers (leaf nodes), our model \u201cbacks off\u201d to the more general phrase that is more accurate visually and with respect to the human annotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14503974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb4f27d02b997025f9163b62c04ae7ef61802e3a",
            "isKey": true,
            "numCitedBy": 185,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "As visual recognition scales up to ever larger numbers of categories, maintaining high accuracy is increasingly difficult. In this work, we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition, motivated by the observation that object categories form a semantic hierarchy consisting of many levels of abstraction. A classifier can select the appropriate level, trading off specificity for accuracy in case of uncertainty. By optimizing this trade-off, we obtain classifiers that try to be as specific as possible while guaranteeing an arbitrarily high accuracy. We formulate the problem as maximizing information gain while ensuring a fixed, arbitrarily small error rate with a semantic hierarchy. We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that, under practical conditions, it converges to an optimal solution. Experiments demonstrate the effectiveness of our algorithm on datasets ranging from 65 to over 10,000 categories."
            },
            "slug": "Hedging-your-bets:-Optimizing-accuracy-specificity-Deng-Krause",
            "title": {
                "fragments": [],
                "text": "Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and proves that, under practical conditions, it converges to an optimal solution."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153642390"
                        ],
                        "name": "David L. Chen",
                        "slug": "David-L.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David L. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "We use the English portion of the YouTube data collected by Chen et al. [4], consisting of 1,970 short video clips, each with an average of 16 natural-language descriptions provided by Amazon Mechanical Turk workers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215717103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "554a31ce91189cf6022ac677413ef2f8b9b40ca7",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments."
            },
            "slug": "Collecting-Highly-Parallel-Data-for-Paraphrase-Chen-Dolan",
            "title": {
                "fragments": [],
                "text": "Collecting Highly Parallel Data for Paraphrase Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel data collection framework is presented that produces highly parallel text data relatively inexpensively and on a large scale that allows for simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128210"
                        ],
                        "name": "Visruth Premraj",
                        "slug": "Visruth-Premraj",
                        "structuredName": {
                            "firstName": "Visruth",
                            "lastName": "Premraj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visruth Premraj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985883"
                        ],
                        "name": "S. Dhar",
                        "slug": "S.-Dhar",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Dhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Most prior work on natural-language description of visual data has focused on static images [30, 15, 19, 10, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10116609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169b847e69c35cfd475eb4dcc561a24de11762ca",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work."
            },
            "slug": "Baby-talk:-Understanding-and-generating-simple-Kulkarni-Premraj",
            "title": {
                "fragments": [],
                "text": "Baby talk: Understanding and generating simple image descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision that is very effective at producing relevant sentences for images."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Subsets of this data were previously used by [20] and [14], however, contrary to these works, we use all the videos, and not only the 20 objects included in the PASCAL dataset [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11692,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875887"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Then we combine activity and object descriptors using a multi-channel approach [31] and pass it to a non-linear SVM [3] (see Section 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "The table measures the similarity to both the \u2019Most Common\u2019 gold-standard answer (single triplet for each video), and the \u2019Valid Answer\u2019 (any of the Subjects, Verbs and Objects mentioned by humans)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1486613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d",
            "isKey": false,
            "numCitedBy": 2175,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover\u2019s Distance and the \u03c72 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter."
            },
            "slug": "Local-Features-and-Kernels-for-Classification-of-A-Zhang-Marszalek",
            "title": {
                "fragments": [],
                "text": "Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A large-scale evaluation of an approach that represents images as distributions of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover\u2019s Distance and the \u03c72 distance."
            },
            "venue": {
                "fragments": [],
                "text": "2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146254443"
                        ],
                        "name": "Zhibiao Wu",
                        "slug": "Zhibiao-Wu",
                        "structuredName": {
                            "firstName": "Zhibiao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibiao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In order to account for such similarities, we use WUP similarity [28] between the predicted and correct items (other similarity functions defined over semantic hierarchies could also be used):\n(3)sWUP(v, l) = 2 \u00b7 depth(lcs)\ndepth(v) + depth(l)\nwhere lcs is the least common ancestor of v and l, and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12009057,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb",
            "isKey": false,
            "numCitedBy": 3703,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentences as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection."
            },
            "slug": "Verb-Semantics-and-Lexical-Selection-Wu-Palmer",
            "title": {
                "fragments": [],
                "text": "Verb Semantics and Lexical Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT), and sees the approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984521"
                        ],
                        "name": "Siddharth Patwardhan",
                        "slug": "Siddharth-Patwardhan",
                        "structuredName": {
                            "firstName": "Siddharth",
                            "lastName": "Patwardhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siddharth Patwardhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3075310"
                        ],
                        "name": "Jason Michelizzi",
                        "slug": "Jason-Michelizzi",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Michelizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Michelizzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "During this filtering, we allow synonyms of nouns by including all words with a Lesk similarity (as implemented in [21]) of at least 0.5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1499545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495f3405da229b903797472c64d09d83659fdb34",
            "isKey": true,
            "numCitedBy": 1783,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related."
            },
            "slug": "WordNet::Similarity-Measuring-the-Relatedness-of-Pedersen-Patwardhan",
            "title": {
                "fragments": [],
                "text": "WordNet: : Similarity - Measuring the Relatedness of Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets)."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "\u202681.03 45.71 28.45 OU 92.57 46.83 46.66 93.72 61.19 58.41\nTable 2: Comparison of WUP Similarity\nwhich combines the outputs of the SVMs according to the WordNet hierarchy and chooses the appropriate level of generalization by setting the accuracy to some prespecified value (0.9 in our experiments)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Then we combine activity and object descriptors using a multi-channel approach [31] and pass it to a non-linear SVM [3] (see Section 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 961425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "isKey": true,
            "numCitedBy": 40077,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "slug": "LIBSVM:-A-library-for-support-vector-machines-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "LIBSVM: A library for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061462269"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "This idea can be used to expand \u201ccoarse\u201d activity detections, obtained by training classifiers on available (possibly limited) activity training data, with \u201cfiner\u201d activities unseen at training time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56563878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7",
            "isKey": false,
            "numCitedBy": 5101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Outputs-for-Support-vector-Machines-Platt",
            "title": {
                "fragments": [],
                "text": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "While object recognition methods have gone large-scale, with datasets such as ImageNet[7] and LabelMe [25], activity datasets have lagged behind."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Construction and Analysis of a Large Scale Image Ontology"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Sciences Society"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "We tested different ways to build the hierarchies for subjects, verbs and objects using the idea of distributional clustering [22] and co-occurrence of the labels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183\u2013190"
            },
            "venue": {
                "fragments": [],
                "text": "Association for Computational Linguistics,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Video 2 text : Learning to annotate video content . In Data Mining Workshops , 2009 . ICDMW \u2019 09"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Retrieving actions in movies . In International Conference on Computer Vision ,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 22
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/YouTube2Text:-Recognizing-and-Describing-Arbitrary-Guadarrama-Krishnamoorthy/d6a7a563640bf53953c4fda0997e4db176488510?sort=total-citations"
}