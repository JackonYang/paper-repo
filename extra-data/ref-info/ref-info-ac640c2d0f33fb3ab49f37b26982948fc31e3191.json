{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956994"
                        ],
                        "name": "Andrew Owens",
                        "slug": "Andrew-Owens",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Owens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045089"
                        ],
                        "name": "Jiajun Wu",
                        "slug": "Jiajun-Wu",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 117
                            }
                        ],
                        "text": "Recently there has been quite a bit of interest in the computer vision community on several versions of this problem [1, 2, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11614363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93a87dfa72f22fba14ef243a62c7d0a6906dfed7",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The sound of crashing waves, the roar of fast-moving cars \u2013 sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds."
            },
            "slug": "Ambient-Sound-Provides-Supervision-for-Visual-Owens-Wu",
            "title": {
                "fragments": [],
                "text": "Ambient Sound Provides Supervision for Visual Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work trains a convolutional neural network to predict a statistical summary of the sound associated with a video frame, and shows that this representation is comparable to that of other state-of-the-art unsupervised learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780072"
                        ],
                        "name": "Terri L. Bonebright",
                        "slug": "Terri-L.-Bonebright",
                        "structuredName": {
                            "firstName": "Terri",
                            "lastName": "Bonebright",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terri L. Bonebright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "1To our delight, Foley artists really do knock two coconuts together to fake the sound of horses galloping [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64445931,
            "fieldsOfStudy": [
                "Psychology",
                "Physics"
            ],
            "id": "66c7585be7ff16589203517f96c45db7218b6f7a",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This study examined whether visual context has an effect on the identification of everyday sounds. Scenes portraying actions that lead to everyday sounds were paired with the actual sounds, acoustically similar sounds and acoustically contrasting sounds Participants identified sounds, rated their confidence on the identifications, the veracity of the sounds and their familiarity with the sounds. Results showed that participants identified the actual and contrasting sounds correctly more often than the similar sounds, which were frequently incorrectly identified as the sound that occurred from the action in the visual scene. However, the confidence ratings for the identifications were lower for the similar sounds, and they rated them as less realistic than the actual sounds. Thus, even though similar sounds were frequently misidentified as the actual sound taking place in the scene, participants did recognize that such sounds were not quite correct for the visual action being portrayed."
            },
            "slug": "COCONUTS-OR-HORSE-HOOFS-VISUAL-CONTEXT-EFFECTS-ON-Bonebright",
            "title": {
                "fragments": [],
                "text": "COCONUTS OR HORSE HOOFS ? VISUAL CONTEXT EFFECTS ON IDENTIFICATION AND PERCEIVED VERACITY OF EVERYDAY SOUNDS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780072"
                        ],
                        "name": "Terri L. Bonebright",
                        "slug": "Terri-L.-Bonebright",
                        "structuredName": {
                            "firstName": "Terri",
                            "lastName": "Bonebright",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terri L. Bonebright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1To our delight, Foley artists really do knock two coconuts together to fake the sound of horses galloping [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145807443,
            "fieldsOfStudy": [
                "Psychology",
                "Physics"
            ],
            "id": "f75b3e88be498ce45856d474f9ed907ff9ffab21",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This study examined whether visual context has an effect on the identification of everyday sounds. Scenes portraying actions that lead to everyday sounds were paired with the actual sounds, acoustically similar sounds and acoustically contrasting sounds Participants identified sounds, rated their confidence on the identifications, the veracity of the sounds and their familiarity with the sounds. Results showed that participants identified the actual and contrasting sounds correctly more often than the similar sounds, which were frequently incorrectly identified as the sound that occurred from the action in the visual scene. However, the confidence ratings for the identifications were lower for the similar sounds, and they rated them as less realistic than the actual sounds. Thus, even though similar sounds were frequently misidentified as the actual sound taking place in the scene, participants did recognize that such sounds were not quite correct for the visual action being portrayed."
            },
            "slug": "Were-those-coconuts-or-horse-hoofs-Visual-context-Bonebright",
            "title": {
                "fragments": [],
                "text": "Were those coconuts or horse hoofs? Visual context effects on identification and veracity of everyday sounds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "The first is the simple parametric synthesis approach of [42, 32], which iteratively imposes the subband envelopes on a sample of white noise (we used just one iteration)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "Following work in sound synthesis [42, 32], we compute our sound features by decomposing the waveform into subband envelopes \u2013 a simple representation obtained by filtering the waveform and applying a nonlinearity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8520803,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9921e7412a49ced5cdede8797e32476c737d4ec0",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sound-Texture-Perception-via-Statistics-of-the-from-McDermott-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487205"
                        ],
                        "name": "William W. Gaver",
                        "slug": "William-W.-Gaver",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gaver",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Gaver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "In our case, we predict a signal \u2013 sound \u2013 known to be a useful representation for many tasks [14, 37], and we show that the output (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Material properties, such as stiffness and density [37, 31, 14], can likewise be determined from impact sounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16586995,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0a36667de28f912ba999fe92ae718284012a67be",
            "isKey": false,
            "numCitedBy": 918,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Everyday listening is the experience of hearing events in the world rather than sounds per se. In this article, I take an ecological approach to everyday listening to overcome constraints on its study implied by more traditional approaches. In particular, I am concerned with developing a new framework for describing sound in terms of audible source attributes. An examination of the continuum of structured energy from event to audition suggests that sound conveys information about events at locations in an environment. Qualitative descriptions of the physics of sound~producing events, complemented by protocol studies, suggest a tripartite division of sound-producing events into those involving vibrating solids, gasses, or liquids. Within each of these categories, basic-level events are defined by the simple interactions that can cause these materials to sound, whereas more complex events can be described in terms of temporal patterning, compound, or hybrid sources. The results of these investigations are u..."
            },
            "slug": "What-in-the-World-Do-We-Hear-An-Ecological-Approach-Gaver",
            "title": {
                "fragments": [],
                "text": "What in the World Do We Hear? An Ecological Approach to Auditory Event Perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31638576"
                        ],
                        "name": "Anurag Arnab",
                        "slug": "Anurag-Arnab",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Arnab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Arnab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145466505"
                        ],
                        "name": "Michael Sapienza",
                        "slug": "Michael-Sapienza",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sapienza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Sapienza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143777501"
                        ],
                        "name": "S. Golodetz",
                        "slug": "S.-Golodetz",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Golodetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golodetz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39596866"
                        ],
                        "name": "Julien P. C. Valentin",
                        "slug": "Julien-P.-C.-Valentin",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Valentin",
                            "middleNames": [
                                "P.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien P. C. Valentin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3336488"
                        ],
                        "name": "O. Miksik",
                        "slug": "O.-Miksik",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Miksik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Miksik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79406746"
                        ],
                        "name": "S. Izadi",
                        "slug": "S.-Izadi",
                        "structuredName": {
                            "firstName": "Shahram",
                            "lastName": "Izadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Izadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "We also tried concatenating vision and sound features (similar to [2]), finding that this significantly improved the accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] recently presented a semantic segmentation model that incorporates audio from impact sounds, and showed that audio information could"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10439697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e3e6c68a2106129c66a05522b58af3d0c2600f",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "It is not always possible to recognise objects and infer material properties for a scene from visual cues alone, since objects can look visually similar whilst being made of very different materials. In this paper, we therefore present an approach that augments the available dense visual cues with sparse auditory cues in order to estimate dense object and material labels. Since estimates of object class and material properties are mutually informative, we optimise our multi-output labelling jointly using a random-field framework. We evaluate our system on a new dataset with paired visual and auditory data that we make publicly available. We demonstrate that this joint estimation of object and material labels significantly outperforms the estimation of either category in isolation."
            },
            "slug": "Joint-Object-Material-Category-Segmentation-from-Arnab-Sapienza",
            "title": {
                "fragments": [],
                "text": "Joint Object-Material Category Segmentation from Audio-Visual Cues"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents an approach that augments the available dense visual cues with sparse auditory cues in order to estimate dense object and material labels and demonstrates that this joint estimation of object andmaterial labels significantly outperforms the estimation of either category in isolation."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47940821"
                        ],
                        "name": "Hang Zhao",
                        "slug": "Hang-Zhao",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158271"
                        ],
                        "name": "Chuang Gan",
                        "slug": "Chuang-Gan",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41020711"
                        ],
                        "name": "Andrew Rouditchenko",
                        "slug": "Andrew-Rouditchenko",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rouditchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rouditchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 117
                            }
                        ],
                        "text": "Recently there has been quite a bit of interest in the computer vision community on several versions of this problem [1, 2, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4748509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe018f22600d07cbd0452a070e03708886470015",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources."
            },
            "slug": "The-Sound-of-Pixels-Zhao-Gan",
            "title": {
                "fragments": [],
                "text": "The Sound of Pixels"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Qualitative results suggest the PixelPlayer model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources, and experimental results show that the proposed Mix-and-Separate framework outperforms several baselines on source separation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715858"
                        ],
                        "name": "J. Sinapov",
                        "slug": "J.-Sinapov",
                        "structuredName": {
                            "firstName": "Jivko",
                            "lastName": "Sinapov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sinapov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32011288"
                        ],
                        "name": "Mark Wiemer",
                        "slug": "Mark-Wiemer",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Wiemer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Wiemer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3245286"
                        ],
                        "name": "A. Stoytchev",
                        "slug": "A.-Stoytchev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Stoytchev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stoytchev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 74
                            }
                        ],
                        "text": "Other work recognizes objects using audio produced by robotic interaction [41, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Our dataset is also related to robotic manipulation datasets [41, 35, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8239485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29361be99bcb16256b510d512d62aff60246a1b3",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Human beings can perceive object properties such as size, weight, and material type based solely on the sounds that the objects make when an action is performed on them. In order to be successful, the household robots of the near future must also be capable of learning and reasoning about the acoustic properties of everyday objects. Such an ability would allow a robot to detect and classify various interactions with objects that occur outside of the robot's field of view. This paper presents a framework that allows a robot to infer the object and the type of behavioral interaction performed with it from the sounds generated by the object during the interaction. The framework is evaluated on a 7-d.o.f. Barrett WAM robot which performs grasping, shaking, dropping, pushing and tapping behaviors on 36 different household objects. The results show that the robot can learn models that can be used to recognize objects (and behaviors performed on objects) from the sounds generated during the interaction. In addition, the robot can use the learned models to estimate the similarity between two objects in terms of their acoustic properties."
            },
            "slug": "Interactive-learning-of-the-acoustic-properties-of-Sinapov-Wiemer",
            "title": {
                "fragments": [],
                "text": "Interactive learning of the acoustic properties of household objects"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A framework that allows a robot to infer the object and the type of behavioral interaction performed with it from the sounds generated by the object during the interaction is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Robotics and Automation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2963422"
                        ],
                        "name": "S. Cavaco",
                        "slug": "S.-Cavaco",
                        "structuredName": {
                            "firstName": "Sofia",
                            "lastName": "Cavaco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cavaco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Work in psychology has studied low-dimensional representations for impact sounds [7], and recent work in neuroimaging has shown that silent videos of impact events activate the auditory cortex [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43205847,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "15aaf247d6d45e93f4a453ac0b20ae671947138c",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical data-driven method for learning intrinsic structures of impact sounds. The method applies principal and independent component analysis to learn low-dimensional representations that model the distribution of both the time-varying spectral and amplitude structure. As a result, the method is able to decompose sounds into a small number of underlying features that characterize acoustic properties such as ringing, resonance, sustain, decay, and onsets. The method is highly flexible and makes no a priori assumptions about the physics, acoustics, or dynamics of the objects. In addition, by modeling the underlying distribution, the method can capture the natural variability of ensembles of related impact sounds."
            },
            "slug": "Statistical-modeling-of-intrinsic-structures-in-Cavaco-Lewicki",
            "title": {
                "fragments": [],
                "text": "Statistical modeling of intrinsic structures in impacts sounds."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By modeling the underlying distribution, the method can capture the natural variability of ensembles of related impact sounds and decompose sounds into a small number of underlying features that characterize acoustic properties such as ringing, resonance, sustain, decay, and onsets."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290352"
                        ],
                        "name": "M. Slaney",
                        "slug": "M.-Slaney",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Slaney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Slaney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "The first is the simple parametric synthesis approach of [42, 32], which iteratively imposes the subband envelopes on a sample of white noise (we used just one iteration)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "Following work in sound synthesis [42, 32], we compute our sound features by decomposing the waveform into subband envelopes \u2013 a simple representation obtained by filtering the waveform and applying a nonlinearity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 489625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1492fdd62030ccd0fd219888d8530b7e842847dc",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Deciding the appropriate representation to use for modeling human auditory processing is a critical issue in auditory science. While engineers have successfully performed many single-speaker tasks with LPC and spectrogram methods, more difficult problems will need a richer representation. This paper describes a powerful auditory representation known as the correlogram and shows how this non-linear representation can be converted back into sound, with no loss of perceptually important information. The correlogram is interesting because it is a neurophysiologically plausible representation of sound. This paper shows improved methods for spectrogram inversion (conventional pattern playback), inversion of a cochlear model, and inversion of the correlogram representation."
            },
            "slug": "Pattern-Playback-in-the-90s-Slaney",
            "title": {
                "fragments": [],
                "text": "Pattern Playback in the 90s"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows improved methods for spectrogram inversion (conventional pattern playback), inversion of a cochlear model, and inversions of the correlogram representation, a non-linear representation of sound."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 74
                            }
                        ],
                        "text": "Other work recognizes objects using audio produced by robotic interaction [41, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 380724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6248cd56230d003b4a380265d28b660deb70808c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we develop a conceptual framework in which acts of manipulation are undertaken for the sake of perceiving material. Within this framework, we disambiguate different materials by actively contacting and probing them, and by sensing the resulting forces, displacements, and sounds. We report experimental results from four separate implementations of this framework using a variety of sensory modalities, including force, vision, and audition. For each implementation, we identify sensor-derived measures that are diagnostic of material properties, and use those measures to categorize objects by their material class. Based on the experimental results, we conclude that the issue of shape-in variance is of critical importance for future work."
            },
            "slug": "Robotic-Perception-of-Material-Krotkov",
            "title": {
                "fragments": [],
                "text": "Robotic Perception of Material"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A conceptual framework in which acts of manipulation are undertaken for the sake of perceiving material, and sensor-derived measures that are diagnostic of material properties are identified and used to categorize objects by their material class."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49935357"
                        ],
                        "name": "A. Davis",
                        "slug": "A.-Davis",
                        "structuredName": {
                            "firstName": "Abe",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18555073"
                        ],
                        "name": "K. Bouman",
                        "slug": "K.-Bouman",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Bouman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108178918"
                        ],
                        "name": "Justin G. Chen",
                        "slug": "Justin-G.-Chen",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Chen",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin G. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144544291"
                        ],
                        "name": "Michael Rubinstein",
                        "slug": "Michael-Rubinstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rubinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Rubinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469877"
                        ],
                        "name": "O. B\u00fcy\u00fck\u00f6zt\u00fcrk",
                        "slug": "O.-B\u00fcy\u00fck\u00f6zt\u00fcrk",
                        "structuredName": {
                            "firstName": "Oral",
                            "lastName": "B\u00fcy\u00fck\u00f6zt\u00fcrk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. B\u00fcy\u00fck\u00f6zt\u00fcrk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145403226"
                        ],
                        "name": "F. Durand",
                        "slug": "F.-Durand",
                        "structuredName": {
                            "firstName": "Fr\u00e9do",
                            "lastName": "Durand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Durand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "Recent work has used these principles to estimate material properties by measuring tiny vibrations in rods and cloth [8], and similar methods have been used to recover sound from high-speed video of a vibrating membrane [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7409091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7505e94b421048b65a832125acf27fd63b2ecef",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "The estimation of material properties is important for scene understanding, with many applications in vision, robotics, and structural engineering. This paper connects fundamentals of vibration mechanics with computer vision techniques in order to infer material properties from small, often imperceptible motions in video. Objects tend to vibrate in a set of preferred modes. The frequencies of these modes depend on the structure and material properties of an object. We show that by extracting these frequencies from video of a vibrating object, we can often make inferences about that object's material properties. We demonstrate our approach by estimating material properties for a variety of objects by observing their motion in high-speed and regular frame rate video."
            },
            "slug": "Visual-Vibrometry:-Estimating-Material-Properties-Davis-Bouman",
            "title": {
                "fragments": [],
                "text": "Visual Vibrometry: Estimating Material Properties from Small Motions in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The fundamentals of vibration mechanics with computer vision techniques are connected in order to infer material properties from small, often imperceptible motions in video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144348441"
                        ],
                        "name": "Dinesh Jayaraman",
                        "slug": "Dinesh-Jayaraman",
                        "structuredName": {
                            "firstName": "Dinesh",
                            "lastName": "Jayaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dinesh Jayaraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 118
                            }
                        ],
                        "text": "For example, [11, 22] learned image representations by predicting the spatial relationship between image patches, and [1, 23] by predicting the egocentric motion between video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1144566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c426ba865e9158a0f7962a86a50575aa943051b1",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance, i.e, they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain."
            },
            "slug": "Learning-Image-Representations-Tied-to-Ego-Motion-Jayaraman-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning Image Representations Tied to Ego-Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video to enforce that the authors' learned features exhibit equivariance, i.e, they respond predictably to transformations associated with distinct ego-motions."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240195"
                        ],
                        "name": "Lavanya Sharan",
                        "slug": "Lavanya-Sharan",
                        "structuredName": {
                            "firstName": "Lavanya",
                            "lastName": "Sharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lavanya Sharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681442"
                        ],
                        "name": "Ce Liu",
                        "slug": "Ce-Liu",
                        "structuredName": {
                            "firstName": "Ce",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ce Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680975"
                        ],
                        "name": "R. Rosenholtz",
                        "slug": "R.-Rosenholtz",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "Rosenholtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenholtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "This task implicitly requires material recognition, but unlike traditional work on this problem [4, 38], we never explicitly tell the algorithm about materials."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1183157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "392e3f9ab733264f938cdf7c4ebbfb4dc94ff919",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Our world consists not only of objects and scenes but also of materials of various kinds. Being able to recognize the materials that surround us (e.g., plastic, glass, concrete) is important for humans as well as for computer vision systems. Unfortunately, materials have received little attention in the visual recognition literature, and very few computer vision systems have been designed specifically to recognize materials. In this paper, we present a system for recognizing material categories from single images. We propose a set of low and mid-level image features that are based on studies of human material recognition, and we combine these features using an SVM classifier. Our system outperforms a state-of-the-art system (Varma and Zisserman, TPAMI 31(11):2032\u20132047, 2009) on a challenging database of real-world material categories (Sharan et al., J Vis 9(8):784\u2013784a, 2009). When the performance of our system is compared directly to that of human observers, humans outperform our system quite easily. However, when we account for the local nature of our image features and the surface properties they measure (e.g., color, texture, local shape), our system rivals human performance. We suggest that future progress in material recognition will come from: (1) a deeper understanding of the role of non-local surface properties (e.g., extended highlights, object identity); and (2) efforts to model such non-local surface properties in images."
            },
            "slug": "Recognizing-Materials-Using-Perceptually-Inspired-Sharan-Liu",
            "title": {
                "fragments": [],
                "text": "Recognizing Materials Using Perceptually Inspired Features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A system for recognizing material categories from single images based on studies of human material recognition is presented, and it is suggested that future progress in material recognition will come from a deeper understanding of the role of non-local surface properties and efforts to model such non- local surface properties in images."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "For example, [11, 22] learned image representations by predicting the spatial relationship between image patches, and [1, 23] by predicting the egocentric motion between video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 98
                            }
                        ],
                        "text": "We note that this evaluation method is different from that of recent unsupervised learning models [11, 1, 47] that train a classifier on the network\u2019s feature activations, rather than on a ground-truth version of the output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9062671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc1b1c9364c58ec406f494dd944b609a6a038ba6",
            "isKey": false,
            "numCitedBy": 1765,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations."
            },
            "slug": "Unsupervised-Visual-Representation-Learning-by-Doersch-Gupta",
            "title": {
                "fragments": [],
                "text": "Unsupervised Visual Representation Learning by Context Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that the feature representation learned using this within-image context indeed captures visual similarity across images and allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8464172"
                        ],
                        "name": "K. V. D. Doel",
                        "slug": "K.-V.-D.-Doel",
                        "structuredName": {
                            "firstName": "Kees",
                            "lastName": "Doel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Doel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1970147"
                        ],
                        "name": "P. Kry",
                        "slug": "P.-Kry",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kry",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694975"
                        ],
                        "name": "D. Pai",
                        "slug": "D.-Pai",
                        "structuredName": {
                            "firstName": "Dinesh",
                            "lastName": "Pai",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Psychophysical study To test whether the sounds produced by our model varied appropriately with different actions and materials, we conducted a psychophysical study on Amazon Mechanical Turk."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2812235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1284a9db84cf8c1a086e11c5dd09ae7480b47272",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe algorithms for real-time synthesis of realistic sound effects for interactive simulations (e.g., games) and animation. These sound effects are produced automatically, from 3D models using dynamic simulation and user interaction. We develop algorithms that are efficient, physically-based, and can be controlled by users in natural ways. We develop effective techniques for producing high quality continuous contact sounds from dynamic simulations running at video rates which are slow relative to audio synthesis. We accomplish this using modal models driven by contact forces modeled at audio rates, which are much higher than the graphics frame rate. The contact forces can be computed from simulations or can be custom designed. We demonstrate the effectiveness with complex realistic simulations."
            },
            "slug": "FoleyAutomatic:-physically-based-sound-effects-for-Doel-Kry",
            "title": {
                "fragments": [],
                "text": "FoleyAutomatic: physically-based sound effects for interactive simulation and animation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Algorithms for real-time synthesis of realistic sound effects for interactive simulations (e.g., games) and animation are described that are efficient, physically-based, and can be controlled by users in natural ways."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34894065"
                        ],
                        "name": "J. Bello",
                        "slug": "J.-Bello",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Bello",
                            "middleNames": [
                                "Pablo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742040"
                        ],
                        "name": "L. Daudet",
                        "slug": "L.-Daudet",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Daudet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Daudet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34212067"
                        ],
                        "name": "S. Abdallah",
                        "slug": "S.-Abdallah",
                        "structuredName": {
                            "firstName": "Samer",
                            "lastName": "Abdallah",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abdallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848576"
                        ],
                        "name": "C. Duxbury",
                        "slug": "C.-Duxbury",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Duxbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Duxbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113976"
                        ],
                        "name": "M. Davies",
                        "slug": "M.-Davies",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Davies",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40764812"
                        ],
                        "name": "M. Sandler",
                        "slug": "M.-Sandler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sandler",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sandler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9539848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe8d91583cf2638f9ff066c99ec75afb407da87f",
            "isKey": false,
            "numCitedBy": 813,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for \"transient\" regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application."
            },
            "slug": "A-tutorial-on-onset-detection-in-music-signals-Bello-Daudet",
            "title": {
                "fragments": [],
                "text": "A tutorial on onset detection in music signals"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations, and methods based on probabilistic signal models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Speech and Audio Processing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144348441"
                        ],
                        "name": "Dinesh Jayaraman",
                        "slug": "Dinesh-Jayaraman",
                        "structuredName": {
                            "firstName": "Dinesh",
                            "lastName": "Jayaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dinesh Jayaraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 106
                            }
                        ],
                        "text": "For example, [9] learned image features by predicting the spatial relationship between image patches, and [1, 20] by predicting the relative camera pose between frames in a video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 565830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3935cabb73d75939ade5fc8839cfd946fbdc8057",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond systematically to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning system significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in a disjoint domain."
            },
            "slug": "Learning-image-representations-equivariant-to-Jayaraman-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning image representations equivariant to ego-motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video to enforce that the authors' learned features exhibit equivariance i.e. they respond systematically to transformations associated with distinct ego-motions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6226925"
                        ],
                        "name": "R. Baillargeon",
                        "slug": "R.-Baillargeon",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Baillargeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baillargeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our neural network and synthesis procedure are shown in Figure 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59036958,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "062eb9ba6ad869599e6c7b6bca9cd3518096a232",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "As adults we possess a great deal of knowledge about the physical world. For example, we realize that an object continues to exist when placed behind a nearer object, that a wide object can be lowered inside a wide but not a narrow container, and that an object typically falls when released in midair. Piaget (1954) was the first researcher to examine whether infants, like adults, hold expectations about physical events. Analyses of infants\u2019 responses in various object-manipulation tasks led him to conclude that, during the first year of life, infants possess very little physical knowledge. For the next several decades, this conclusion was generally accepted (for reviews of this early research, see Bremner, 1985; Gratch, 1976; Harris, 1987; and Schubert, 1983). This state of affairs began to change in the 1980s, however, when evidence obtained with novel, more sensitive tasks revealed that even young infants hold at least limited expectations about physical events (e.g., Baillargeon, 1986; Baillargeon, Spelke, & Wasserman, 1985; Baillargeon & Graber, 1987; Diamond, 1985; Hood & Willatts, 1986; Leslie, 1982, 1984; Pieraut-Le Bonniec, 1985; Spelke & Kestenbaum, 1986). In subsequent years, researchers began to explore many new facets of infants\u2019 physical knowledge, bringing to light new competences and developments (e.g., Arterberry, 1993; Clifton, Rochat, Litovsky, & Perris, 1991; Diamond, 1991; Goubet & Clifton, 1998; Kotovsky & Baillargeon, 1994; Lecuyer, 1993; Needham & Baillargeon, 1993; Oakes & Cohen, 1990; Spelke, Breinlinger, Macomber, & Jacobson, 1992). Today, investigators generally agree (with a few notable exceptions: e.g., Bogartz, Shinskey, & Speaker, 1997;"
            },
            "slug": "The-Acquisition-of-Physical-Knowledge-in-Infancy:-A-Baillargeon",
            "title": {
                "fragments": [],
                "text": "The Acquisition of Physical Knowledge in Infancy: A Summary in Eight Lessons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Investigators generally agree that, during the first year of life, infants possess very little physical knowledge, and began to explore many new facets of infants\u2019 physical knowledge in subsequent years, bringing to light new competences and developments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33932184"
                        ],
                        "name": "Pulkit Agrawal",
                        "slug": "Pulkit-Agrawal",
                        "structuredName": {
                            "firstName": "Pulkit",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pulkit Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 118
                            }
                        ],
                        "text": "For example, [11, 22] learned image representations by predicting the spatial relationship between image patches, and [1, 23] by predicting the egocentric motion between video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 98
                            }
                        ],
                        "text": "We note that this evaluation method is different from that of recent unsupervised learning models [11, 1, 47] that train a classifier on the network\u2019s feature activations, rather than on a ground-truth version of the output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1637703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching."
            },
            "slug": "Learning-to-See-by-Moving-Agrawal-Carreira",
            "title": {
                "fragments": [],
                "text": "Learning to See by Moving"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt with class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730646"
                        ],
                        "name": "Mevlana Gemici",
                        "slug": "Mevlana-Gemici",
                        "structuredName": {
                            "firstName": "Mevlana",
                            "lastName": "Gemici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mevlana Gemici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681995"
                        ],
                        "name": "Ashutosh Saxena",
                        "slug": "Ashutosh-Saxena",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashutosh Saxena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Our dataset is also related to robotic manipulation datasets [41, 35, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16338892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0dcca4fa447a14496f0143c211fd40a54039b98",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Manipulation of complex deformable semi-solids such as food objects is an important skill for personal robots to have. In this work, our goal is to model and learn the physical properties of such objects. We design actions involving use of tools such as forks and knives that obtain haptic data containing information about the physical properties of the object. We then design appropriate features and use supervised learning to map these features to certain physical properties (hardness, plasticity, elasticity, tensile strength, brittleness, adhesiveness). Additionally, we present a method to compactly represent the robot's beliefs about the object's properties using a generative model, which we use to plan appropriate manipulation actions. We extensively evaluate our approach on a dataset including haptic data from 12 categories of food (including categories not seen before by the robot) obtained in 941 experiments. Our robot prepared a salad during 60 sequential robotic experiments where it made a mistake in only 4 instances."
            },
            "slug": "Learning-haptic-representation-for-manipulating-Gemici-Saxena",
            "title": {
                "fragments": [],
                "text": "Learning haptic representation for manipulating deformable food objects"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work designs actions involving use of tools such as forks and knives that obtain haptic data containing information about the physical properties of the object, and presents a method to compactly represent the robot's beliefs about the object's properties using a generative model."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30364688"
                        ],
                        "name": "Max H. Siegel",
                        "slug": "Max-H.-Siegel",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Siegel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max H. Siegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572818"
                        ],
                        "name": "Rachel Magid",
                        "slug": "Rachel-Magid",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Magid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Magid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 116
                            }
                        ],
                        "text": "Recent work suggests that the sounds objects make in response to these interactions may play a role in this process [39, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15760676,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e00b4b9b534aa25c10a3478e07418d56fe9053af",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Black boxes: Hypothesis testing via indirect perceptual evidence Max H. Siegel, Rachel Magid, Joshua B. Tenenbaum, and Laura E. Schulz {maxs, rwmagid, jbt, lschulz}@mit.edu Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139 USA Abstract Studies of children\u2019s causal learning typically provide learners with clear evidence for direct causal relations, e.g., a machine that activates when a toy is placed upon it. But causal systems in the real world often present indirect perceptual evidence generated by interactions between hidden variables: Consider a child trying to figure out what\u2019s inside a box by shaking it. We propose that effective learning and exploration depend on being able to interpret evidence through the lens of intuitive theories \u2013 theories of both the physical world and one\u2019s own perceptual apparatus \u2013 to imagine how one\u2019s actions might change the state of the world and what kinds of changes would be most perceptually discriminable. We present three studies exploring these capacities in young children, and suggest how they could support powerful and sophisticated inferences about hidden causes. In science, much of the evidence we get for causal rela- tionships is indirect: we cannot observe the activation of neu- rons or the presence of microorganisms directly so we de- velop sophisticated technologies to detect neural activation from blood flow, or the presence of microorganisms from the amount of dissolved oxygen. This kind of inference is not re- stricted to scientific practice. Even in everyday life, there are many cases where we have to infer unobserved causes from indirect evidence. We see a curtain move and infer the cat be- hind it, we hear dripping and worry that the faucet is leaking. These are very ordinary examples, ones that might be acces- sible even to small children, but they suggest an extraordinary capacity: the ability to reason backwards from new evidence to probable unobserved causes of the data. While there is a wealth of research on children\u2019s causal reasoning, the majority of studies has focused on childrens inferences about direct causal relationships: observed blocks that do (or do not) light up machines, levers that do (or do not) activate toys, etc (Gopnik, Sobel, Schulz, & Glymour, 2001; Gopnik & Sobel, 2000). Moreover, although there is some work outlining childrens ability to infer unobserved causes from patterns of evidence (Kushnir & Gopnik, 2005; Schulz & Sommerville, 2006; Schulz, Goodman, Tenenbaum, & Jenkins, 2008), in such cases children were asked about arbi- trary causes and what is at stake is simply childrens ability to detect conditions under which a latent variable might explain the pattern of evidence. Here we ask a different question: Do children\u2019s intuitive theories support these inferences in cases where the mapping from causes to effects is unspecified and complex? One way of investigating unfamiliar scenarios is to take actions designed to reduce uncertainty. We can pull back the curtain, we can use more powerful tools as they\u2019re de- veloped. Cook, Goodman, and Schulz (2011) demonstrated that children preferentially select more informative interven- tions when exploring a new causal domain; and there is more generally a rich literature on active learning and information selection in adult human cognition (Oaksford & Chater, 1994; Tsividis, Gershman, Tenenbaum, & Schulz, 2014). We aim to investigate a complementary question. People (scientists or otherwise) are never able to directly observe data that most informs the questions they actually want an- swered, for good reasons: the equipment is too expensive, the cat (or is it?) runs away as we approach. What scientists (and other people too) actually do is more pragmatic. We know our tools, eyes, ears, or electron microscopes; and we know how they sense and transform the objects we\u2019re interested in studying. Answering questions thus requires us to select not only the right tests, but data that would yield informative re- sults given the right test. We are especially interested in what learners decide to in- vestigate in situations where evidence is richly perceptual and interpretation involves intuitive theories of the physical world, because these are closest to the ways young children most naturally explore their surroundings. Our focus here is on physical interactions between objects that generate sound, and on what we need to know, both about these physical events and our abilities to perceive them, in order to learn about objects from the sounds they produce. We take sound as a case study because children have expectations about how objects interact to produce auditory signals, because these in- teractions produce nontrivial evidence (so that there is clearly some work to be done in interpretation), and finally because of the intuitive, everyday quality of perceptual inferences. A rubber ball shaken inside a closed box will sound differ- ent than a glass marble. Two rubber balls will sound different from either; five glass marbles will sound more different still. A novel object that looks pointy or feels hard to the touch will sound different from one that looks rounded or feels soft. How do we know these things? One might think that these examples can be explained as consequences of cross-modal perception and memory. We have experienced seeing a faucet drip while also hearing it. We have all shaken a wrapped present to find out what might be inside. However, we do not always have access to information in multiple modalities, and in these cases we are still able to reason about the causes of various sounds. Indeed, the combinatorial explosion that accompanies real-world perception suggests that reasoning from indirect evidence requires sophisticated and subtle ca- pabilities which are far more powerful than those commonly assumed. We follow in a research tradition that highlights how people use mental models and theories about the world to generate predictions, which are used to interpret new data (Tenenbaum, Kemp, Griffiths, & Goodman, 2011). But we"
            },
            "slug": "Black-boxes:-Hypothesis-testing-via-indirect-Siegel-Magid",
            "title": {
                "fragments": [],
                "text": "Black boxes: Hypothesis testing via indirect perceptual evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is proposed that effective learning and exploration depend on being able to interpret evidence through the lens of intuitive theories \u2013 theories of both the physical world and one's own perceptual apparatus \u2013 to imagine how one\u2019s actions might change the state of the world and what kinds of changes would be most perceptually discriminable."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144892098"
                        ],
                        "name": "P. Hsieh",
                        "slug": "P.-Hsieh",
                        "structuredName": {
                            "firstName": "Po-Jang",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4462940"
                        ],
                        "name": "J. Colas",
                        "slug": "J.-Colas",
                        "structuredName": {
                            "firstName": "Jaron",
                            "lastName": "Colas",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Colas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931482"
                        ],
                        "name": "N. Kanwisher",
                        "slug": "N.-Kanwisher",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Kanwisher",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kanwisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Work in psychology has studied low-dimensional representations for impact sounds [7], and recent work in neuroimaging has shown that silent videos of impact events activate the auditory cortex [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7963100,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "edb7bc689e68d119c9afa7c9275cb97bfab10ecc",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent findings suggest that neural representations in early auditory cortex reflect not only the physical properties of a stimulus, but also high-level, top-down, and even cross-modal information. However, the nature of cross-modal information in auditory cortex remains poorly understood. Here, we used pattern analyses of fMRI data to ask whether early auditory cortex contains information about the visual environment. Our data show that 1) early auditory cortex contained information about a visual stimulus when there was no bottom-up auditory signal, and that 2) no influence of visual stimulation was observed in auditory cortex when visual stimuli did not provide a context relevant to audition. Our findings attest to the capacity of auditory cortex to reflect high-level, top-down, and cross-modal information and indicate that the spatial patterns of activation in auditory cortex reflect contextual/implied auditory information but not visual information per se."
            },
            "slug": "Spatial-pattern-of-BOLD-fMRI-activation-reveals-in-Hsieh-Colas",
            "title": {
                "fragments": [],
                "text": "Spatial pattern of BOLD fMRI activation reveals cross-modal information in auditory cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6903801"
                        ],
                        "name": "B. Glasberg",
                        "slug": "B.-Glasberg",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Glasberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Glasberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145167842"
                        ],
                        "name": "B. Moore",
                        "slug": "B.-Moore",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Moore",
                            "middleNames": [
                                "C.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "We apply a bank of 40 band-pass filters spaced on an equivalent rectangular bandwidth (ERB) scale [16] (plus a low- and high-pass filter) and take the Hilbert envelope of the responses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4772612,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b33c93114e079ea7a93f42dc516ade92aade5390",
            "isKey": false,
            "numCitedBy": 2413,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Derivation-of-auditory-filter-shapes-from-data-Glasberg-Moore",
            "title": {
                "fragments": [],
                "text": "Derivation of auditory filter shapes from notched-noise data"
            },
            "venue": {
                "fragments": [],
                "text": "Hearing Research"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39849136"
                        ],
                        "name": "X. Wang",
                        "slug": "X.-Wang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "At the start of the experiment, we revealed the correct answer to five practice videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5276358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4ff172c2d1820f33c0c72286d52b846ab5a216",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation."
            },
            "slug": "Unsupervised-Learning-of-Visual-Representations-Wang-Gupta",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Visual Representations Using Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A simple yet surprisingly powerful approach for unsupervised learning of CNN that uses hundreds of thousands of unlabeled videos from the web to learn visual representations and designs a Siamese-triplet network with a ranking loss function to train this CNN representation."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2057504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c11626ae08706e6185fceff0a6d05e4bfd6bd06",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a review of unsupervised learning applied to videos with the aim of learning visual representations. We look at different realizations of the notion of temporal coherence across various models. We try to understand the challenges being faced, the strengths and weaknesses of different approaches and identify directions for future work. Unsupervised Learning of Visual Representations using Videos Nitish Srivastava Department of Computer Science, University of Toronto"
            },
            "slug": "Unsupervised-Learning-of-Visual-Representations-Srivastava",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Visual Representations using Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This is a review of unsupervised learning applied to videos with the aim of learning visual representations to understand the challenges being faced, the strengths and weaknesses of different approaches and identify directions for future work."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749989"
                        ],
                        "name": "Zhenhua Ling",
                        "slug": "Zhenhua-Ling",
                        "structuredName": {
                            "firstName": "Zhenhua",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenhua Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157835398"
                        ],
                        "name": "Shiyin Kang",
                        "slug": "Shiyin-Kang",
                        "structuredName": {
                            "firstName": "Shiyin",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiyin Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7565008"
                        ],
                        "name": "Xiaojun Qian",
                        "slug": "Xiaojun-Qian",
                        "structuredName": {
                            "firstName": "Xiaojun",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojun Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145199941"
                        ],
                        "name": "H. Meng",
                        "slug": "H.-Meng",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Meng",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "Sound synthesis Our technical approach resembles speech synthesis methods that use neural networks to predict sound features from pre-tokenized text features and then generate a waveform from those features [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17058256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "849c9a0b3c76c7e65e3e5f0bdfd56921731ea043",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature. This article systematically reviews these emerging speech generation approaches, with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation."
            },
            "slug": "Deep-Learning-for-Acoustic-Modeling-in-Parametric-A-Ling-Kang",
            "title": {
                "fragments": [],
                "text": "Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This article systematically reviews emerging speech generation approaches with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2234342"
                        ],
                        "name": "Lisa Anne Hendricks",
                        "slug": "Lisa-Anne-Hendricks",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Hendricks",
                            "middleNames": [
                                "Anne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lisa Anne Hendricks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811430"
                        ],
                        "name": "Subhashini Venugopalan",
                        "slug": "Subhashini-Venugopalan",
                        "structuredName": {
                            "firstName": "Subhashini",
                            "lastName": "Venugopalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhashini Venugopalan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "Image representation We found it helpful to represent motion information explicitly in our model using a twostream approach [12, 40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "We train the RNN and CNN jointly using stochastic gradient descent with Caffe [25, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5736847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01fc808592ea7c473a69a6e7484040a435f36d9",
            "isKey": false,
            "numCitedBy": 4085,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \u201ctemporally deep\u201d, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
            },
            "slug": "Long-term-recurrent-convolutional-networks-for-and-Donahue-Hendricks",
            "title": {
                "fragments": [],
                "text": "Long-term recurrent convolutional networks for visual recognition and description"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and shows such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390603950"
                        ],
                        "name": "Mingyu Kim",
                        "slug": "Mingyu-Kim",
                        "structuredName": {
                            "firstName": "Mingyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145578392"
                        ],
                        "name": "Juhan Nam",
                        "slug": "Juhan-Nam",
                        "structuredName": {
                            "firstName": "Juhan",
                            "lastName": "Nam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juhan Nam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "For example, [34] learned a joint model of audio and video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 352650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80e9e3fc3670482c1fee16b2542061b779f47c4f",
            "isKey": false,
            "numCitedBy": 2432,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning."
            },
            "slug": "Multimodal-Deep-Learning-Ngiam-Khosla",
            "title": {
                "fragments": [],
                "text": "Multimodal Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work presents a series of tasks for multimodal learning and shows how to train deep networks that learn features to address these tasks, and demonstrates cross modality feature learning, where better features for one modality can be learned if multiple modalities are present at feature learning time."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743600"
                        ],
                        "name": "Shuiwang Ji",
                        "slug": "Shuiwang-Ji",
                        "structuredName": {
                            "firstName": "Shuiwang",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuiwang Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143836295"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41216159"
                        ],
                        "name": "Ming Yang",
                        "slug": "Ming-Yang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1923924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80bfcf1be2bf1b95cc6f36d229665cdf22d76190",
            "isKey": false,
            "numCitedBy": 4218,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods."
            },
            "slug": "3D-Convolutional-Neural-Networks-for-Human-Action-Ji-Xu",
            "title": {
                "fragments": [],
                "text": "3D Convolutional Neural Networks for Human Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel 3D CNN model for action recognition that extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094770"
                        ],
                        "name": "Phillip Isola",
                        "slug": "Phillip-Isola",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Isola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Isola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2944502"
                        ],
                        "name": "Daniel Zoran",
                        "slug": "Daniel-Zoran",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Zoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Zoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707347"
                        ],
                        "name": "Dilip Krishnan",
                        "slug": "Dilip-Krishnan",
                        "structuredName": {
                            "firstName": "Dilip",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dilip Krishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "For example, [11, 22] learned image representations by predicting the spatial relationship between image patches, and [1, 23] by predicting the egocentric motion between video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3009939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5253cf4f3b1eeb63a07982b0e9e42e7d267d767e",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories."
            },
            "slug": "Learning-visual-groups-from-co-occurrences-in-space-Isola-Zoran",
            "title": {
                "fragments": [],
                "text": "Learning visual groups from co-occurrences in space and time"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time is proposed, and it is demonstrated that in each case the learned affinities uncover meaningful semantic groupings."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "1 seconds of it (associating the predicted and ground truth greedily as in [11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11692,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677488"
                        ],
                        "name": "\u00c0. Lapedriza",
                        "slug": "\u00c0.-Lapedriza",
                        "structuredName": {
                            "firstName": "\u00c0gata",
                            "lastName": "Lapedriza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c0. Lapedriza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Unlike traditional object- or scene-centric datasets, such as ImageNet [10] or Places [48], where the focus of the image is a full scene, our dataset contains close-up views of a small number of objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1849990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9667f8264745b626c6173b1310e2ff0298b09cfc",
            "isKey": false,
            "numCitedBy": 2610,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks."
            },
            "slug": "Learning-Deep-Features-for-Scene-Recognition-using-Zhou-Lapedriza",
            "title": {
                "fragments": [],
                "text": "Learning Deep Features for Scene Recognition using Places Database"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new scene-centric database called Places with over 7 million labeled pictures of scenes is introduced with new methods to compare the density and diversity of image datasets and it is shown that Places is as dense as other scene datasets and has more diversity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367683"
                        ],
                        "name": "H. Pirsiavash",
                        "slug": "H.-Pirsiavash",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Pirsiavash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pirsiavash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Several methods have also used temporal proximity as a supervisory signal [33, 17, 47, 46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8596971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fb3b63090f95af97723efe565893eb25ea9188c",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In many computer vision applications, machines will need to reason beyond the present, and predict the future. This task is challenging because it requires leveraging extensive commonsense knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently obtaining this knowledge is through the massive amounts of readily available unlabeled video. In this paper, we present a large scale framework that capitalizes on temporal structure in unlabeled video to learn to anticipate both actions and objects in the future. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. We experimentally validate this idea on two challenging \"in the wild\" video datasets, and our results suggest that learning with unlabeled videos significantly helps forecast actions and anticipate objects."
            },
            "slug": "Anticipating-the-future-by-watching-unlabeled-video-Vondrick-Pirsiavash",
            "title": {
                "fragments": [],
                "text": "Anticipating the future by watching unlabeled video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A large scale framework that capitalizes on temporal structure in unlabeled video to learn to anticipate both actions and objects in the future, and suggests that learning with unlabeling videos significantly helps forecast actions and anticipate objects."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "We found it helpful for convergence to remove dropout [44] and to clip large gradients."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6844431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "isKey": false,
            "numCitedBy": 28158,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "slug": "Dropout:-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton",
            "title": {
                "fragments": [],
                "text": "Dropout: a simple way to prevent neural networks from overfitting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232655"
                        ],
                        "name": "H. Mobahi",
                        "slug": "H.-Mobahi",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Mobahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mobahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Several methods have also used temporal proximity as a supervisory signal [33, 17, 47, 46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1883779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2c477de72bb7718f5304c6f38457fda9c8334b1",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks."
            },
            "slug": "Deep-learning-from-temporal-coherence-in-video-Mobahi-Collobert",
            "title": {
                "fragments": [],
                "text": "Deep learning from temporal coherence in video"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings, and is used to improve the performance on a supervised task of interest."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49389790"
                        ],
                        "name": "J. Kluge",
                        "slug": "J.-Kluge",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Kluge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kluge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "In our case, we predict a signal \u2013 sound \u2013 known to be a useful representation for many tasks [14, 37], and we show that the output (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Material properties, such as stiffness and density [37, 31, 14], can likewise be determined from impact sounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63302595,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "d93f9ac1fbca9d6e557ef0f76374af0f3065cd6b",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Thank you very much for reading theory of vibration an introduction. As you may know, people have look hundreds times for their favorite novels like this theory of vibration an introduction, but end up in harmful downloads. Rather than enjoying a good book with a cup of tea in the afternoon, instead they cope with some malicious virus inside their desktop computer. theory of vibration an introduction is available in our digital library an online access to it is set as public so you can get it instantly. Our digital library spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the theory of vibration an introduction is universally compatible with any devices to read."
            },
            "slug": "Theory-Of-Vibration-An-Introduction-Kluge",
            "title": {
                "fragments": [],
                "text": "Theory Of Vibration An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The theory of vibration an introduction is universally compatible with any devices to read, and is available in the digital library an online access to it is set as public so you can get it instantly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805076"
                        ],
                        "name": "G. Toderici",
                        "slug": "G.-Toderici",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Toderici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Toderici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152821938"
                        ],
                        "name": "Sanketh Shetty",
                        "slug": "Sanketh-Shetty",
                        "structuredName": {
                            "firstName": "Sanketh",
                            "lastName": "Shetty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanketh Shetty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152456068"
                        ],
                        "name": "Thomas Leung",
                        "slug": "Thomas-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "resent the images using a convolutional network, and the time series using a recurrent neural network. We show a subsequence of images corresponding to one impact. is closely related to 3D video CNNs [24,27], as derivatives across channels correspond to temporal derivatives. For each frame t, we construct an input feature vector x t by concatenating CNN features for the spacetime image at frame tand the "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206592218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4c9c923e9f145d1c01a2de2afc38ec23c44253",
            "isKey": false,
            "numCitedBy": 5149,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%)."
            },
            "slug": "Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici",
            "title": {
                "fragments": [],
                "text": "Large-Scale Video Classification with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work studies multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggests a multiresolution, foveated architecture as a promising way of speeding up the training."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "Image representation We found it helpful to represent motion information explicitly in our model using a twostream approach [12, 40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11797475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67dccc9a856b60bdc4d058d83657a089b8ad4486",
            "isKey": false,
            "numCitedBy": 5480,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. \n \nOur contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification."
            },
            "slug": "Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Two-Stream Convolutional Networks for Action Recognition in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a two-stream ConvNet architecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144499674"
                        ],
                        "name": "Sean Bell",
                        "slug": "Sean-Bell",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222840"
                        ],
                        "name": "P. Upchurch",
                        "slug": "P.-Upchurch",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Upchurch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Upchurch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374926"
                        ],
                        "name": "K. Bala",
                        "slug": "K.-Bala",
                        "structuredName": {
                            "firstName": "Kavita",
                            "lastName": "Bala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our neural network and synthesis procedure are shown in Figure 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206593002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0959ef8fefe9e7041f508c2448fc026bc9e08393",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. In this paper, we introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild. MINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled across its 23 categories. Using MINC, we train convolutional neural networks (CNNs) for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classification on MINC we found that the best performing CNN architectures can achieve 85.2% mean class accuracy. We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 73.1% mean class accuracy. Our experiments demonstrate that having a large, well-sampled dataset such as MINC is crucial for real-world material recognition and segmentation."
            },
            "slug": "Material-recognition-in-the-wild-with-the-Materials-Bell-Upchurch",
            "title": {
                "fragments": [],
                "text": "Material recognition in the wild with the Materials in Context Database"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), is introduced, and convolutional neural networks are trained for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "We also use multiple LSTM layers (the number depends on the task; please see Section A1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "For the centered videos, we used two LSTM layers with a 256-dimensional hidden state (and three for the detection model)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "4To simplify the presentation, we have omitted the LSTM\u2019s hidden cell state, which is also updated at each timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "In some sequences, the arm is the dominant feature that is matched between scenes.\nsate for the difference in video and audio sampling rates by upsampling the input to the last LSTM layer (rather than upsampling the CNN features), replicating each input k times (where again k = 3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Sound prediction model We use a recurrent neural network (RNN) with long short-term memory units (LSTM) [18] that takes CNN features as input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "When using multiple LSTM layers, we compen-\nFigure A2: A \u201cwalk\u201d through the dataset using AlexNet fc7 nearest-neighbor matches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "where L is a function that updates the hidden state [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51704,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34026610"
                        ],
                        "name": "Lerrel Pinto",
                        "slug": "Lerrel-Pinto",
                        "structuredName": {
                            "firstName": "Lerrel",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lerrel Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Our dataset is also related to robotic manipulation datasets [41, 35, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3177253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f03b4ff1b4943691cec703b508c0a91f2d97a881",
            "isKey": false,
            "numCitedBy": 867,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Current model free learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping."
            },
            "slug": "Supersizing-self-supervision:-Learning-to-grasp-50K-Pinto-Gupta",
            "title": {
                "fragments": [],
                "text": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper takes the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts, which allows us to train a Convolutional Neural Network for the task of predicting grasp locations without severe overfitting."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46972251"
                        ],
                        "name": "Y. Hu",
                        "slug": "Y.-Hu",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759800"
                        ],
                        "name": "P. Loizou",
                        "slug": "P.-Loizou",
                        "structuredName": {
                            "firstName": "Philipos",
                            "lastName": "Loizou",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Loizou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "We used a separate audio recorder, without auto-gain, and we applied a denoising algorithm [20] to each recording."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15115112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93ff7a1929d1d029e61495c434a73fc715eb44e4",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that the \"musical noise\" encountered in most frequency domain speech enhancement algorithms is partially due to the large variance estimates of the spectra. To address this issue, we propose in this paper the use of low-variance spectral estimators based on wavelet thresholding the multitaper spectra for speech enhancement. A short-time spectral amplitude estimator is derived which incorporates the wavelet-thresholded multitaper spectra. Listening tests showed that the use of multitaper spectrum estimation combined with wavelet thresholding suppressed the musical noise and yielded better quality than the subspace and MMSE algorithms."
            },
            "slug": "Speech-enhancement-based-on-wavelet-thresholding-Hu-Loizou",
            "title": {
                "fragments": [],
                "text": "Speech enhancement based on wavelet thresholding the multitaper spectrum"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A short-time spectral amplitude estimator is derived which incorporates the wavelet-thresholded multitaper spectra for speech enhancement and showed that the use of multitaper spectrum estimation combined with wavelet thresholding suppressed the musical noise and yielded better quality than the subspace and MMSE algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Speech and Audio Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2558463"
                        ],
                        "name": "Ross Goroshin",
                        "slug": "Ross-Goroshin",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Goroshin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Goroshin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143627859"
                        ],
                        "name": "Joan Bruna",
                        "slug": "Joan-Bruna",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bruna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Bruna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704494"
                        ],
                        "name": "Jonathan Tompson",
                        "slug": "Jonathan-Tompson",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Tompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Tompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Several methods have also used temporal proximity as a supervisory signal [33, 17, 47, 46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5919904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "394cbf2d589eadcfbdbdaaed65c77532b9c856af",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric."
            },
            "slug": "Unsupervised-Feature-Learning-from-Temporal-Data-Goroshin-Bruna",
            "title": {
                "fragments": [],
                "text": "Unsupervised Feature Learning from Temporal Data"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work studies unsupervised feature learning in the context of temporally coherent video data and establishes a connection between slow feature learning to metric learning and shows that the trained encoder can be used to define a more temporally and semantically coherent metric."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2740806"
                        ],
                        "name": "M. Gasser",
                        "slug": "M.-Gasser",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gasser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gasser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 116
                            }
                        ],
                        "text": "Recent work suggests that the sounds objects make in response to these interactions may play a role in this process [39, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7107473,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "25f8e9e35cafd7fb686d939f274111bcffeafd6b",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. We offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind."
            },
            "slug": "The-Development-of-Embodied-Cognition:-Six-Lessons-Smith-Gasser",
            "title": {
                "fragments": [],
                "text": "The Development of Embodied Cognition: Six Lessons from Babies"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is argued that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 36
                            }
                        ],
                        "text": "We included models with and without ImageNet pretraining; with and without spacetime images; and with examplebased versus parametric waveform generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 62
                            }
                        ],
                        "text": "Unlike traditional object- or scene-centric datasets, such as ImageNet [10] or Places [48], where the focus of the image is a full scene, our dataset contains close-up views of a small number of objects."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 187
                            }
                        ],
                        "text": "We measured the rate at which participants mistook our model\u2019s result for the ground-truth sound (Figure 5(a)), finding that our full system \u2013 with RGB and spacetime input, RNN connections, ImageNet pretraining, and examplebased waveform generation \u2013 significantly outperformed the image-matching methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "We computed fc7 features from a CNN pretrained on ImageNet [28] for the center frame of each sequence, which by construction is the frame where the impact sound occurs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 178
                            }
                        ],
                        "text": "In our experiments (Section 6), we either initialized the CNN from scratch and trained it jointly with the RNN, or we initialized the CNN with weights from a network trained for ImageNet classification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "In Table 6(b), we trained a classifier using fc7 features \u2013 both those of the model trained from scratch, and of a model trained on ImageNet [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "Both models were pretrained with ImageNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 71
                            }
                        ],
                        "text": "We found that material and reaction recognition accuracy improved with ImageNet pretraining (to 28.8% and to 55.2%, respectively), but that there was a slight decrease for action classification (to 66.5%)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "where \u03c6 are CNN features obtained from layer fc7 of the AlexNet architecture [28] (its penultimate layer), and Ft is the spacetime image at time t."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 80960,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 36
                            }
                        ],
                        "text": "We included models with and without ImageNet pretraining; with and without spacetime images; and with examplebased versus parametric waveform generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Unlike traditional object- or scene-centric datasets, such as ImageNet [10] or Places [48], where the focus of the image is a full scene, our dataset contains close-up views of a small number of objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 187
                            }
                        ],
                        "text": "We measured the rate at which participants mistook our model\u2019s result for the ground-truth sound (Figure 5(a)), finding that our full system \u2013 with RGB and spacetime input, RNN connections, ImageNet pretraining, and examplebased waveform generation \u2013 significantly outperformed the image-matching methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "We computed fc7 features from a CNN pretrained on ImageNet [28] for the center frame of each sequence, which by construction is the frame where the impact sound occurs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 178
                            }
                        ],
                        "text": "In our experiments (Section 6), we either initialized the CNN from scratch and trained it jointly with the RNN, or we initialized the CNN with weights from a network trained for ImageNet classification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 130
                            }
                        ],
                        "text": "In Table 6(b), we trained a classifier using fc7 features \u2013 both those of the model trained from scratch, and of a model trained on ImageNet [28]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "Both models were pretrained with ImageNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 71
                            }
                        ],
                        "text": "We found that material and reaction recognition accuracy improved with ImageNet pretraining (to 28.8% and to 55.2%, respectively), but that there was a slight decrease for action classification (to 66.5%)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": true,
            "numCitedBy": 27420,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 29242,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "We train the RNN and CNN jointly using stochastic gradient descent with Caffe [25, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13757,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 139
                            }
                        ],
                        "text": "We take inspiration from the way infants explore the physical properties of a scene by poking and prodding at the objects in front of them [36, 3], a process that may help them learn an intuitive theory of physics [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17346427,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a5c602afb195c2203b1fe1138c5c6f6184f61da6",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-origins-of-inquiry:-inductive-inference-and-in-Schulz",
            "title": {
                "fragments": [],
                "text": "The origins of inquiry: inductive inference and exploration in early childhood"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2322936"
                        ],
                        "name": "L. Hostetler",
                        "slug": "L.-Hostetler",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Hostetler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hostetler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "4To simplify the presentation, we have omitted the LSTM\u2019s hidden cell state, which is also updated at each timestep."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15299210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98431da7222ee3fe12d277facf5ca1561c56d4f3",
            "isKey": false,
            "numCitedBy": 2994,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonparametric density gradient estimation using a generalized kernel approach is investigated. Conditions on the kernel functions are derived to guarantee asymptotic unbiasedness, consistency, and uniform consistency of the estimates. The results are generalized to obtain a simple mcan-shift estimate that can be extended in a k -nearest-neighbor approach. Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients."
            },
            "slug": "The-estimation-of-the-gradient-of-a-density-with-in-Fukunaga-Hostetler",
            "title": {
                "fragments": [],
                "text": "The estimation of the gradient of a density function, with applications in pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49935357"
                        ],
                        "name": "A. Davis",
                        "slug": "A.-Davis",
                        "structuredName": {
                            "firstName": "Abe",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144544291"
                        ],
                        "name": "Michael Rubinstein",
                        "slug": "Michael-Rubinstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rubinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Rubinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34004812"
                        ],
                        "name": "N. Wadhwa",
                        "slug": "N.-Wadhwa",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Wadhwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wadhwa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781063"
                        ],
                        "name": "G. Mysore",
                        "slug": "G.-Mysore",
                        "structuredName": {
                            "firstName": "Gautham",
                            "lastName": "Mysore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mysore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145403226"
                        ],
                        "name": "F. Durand",
                        "slug": "F.-Durand",
                        "structuredName": {
                            "firstName": "Fr\u00e9do",
                            "lastName": "Durand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Durand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 220
                            }
                        ],
                        "text": "Recent work has used these principles to estimate material properties by measuring tiny vibrations in rods and cloth [8], and similar methods have been used to recover sound from high-speed video of a vibrating membrane [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 250074908,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "1c94f5b708516211ec0330439eec0d9e4d7c42cd",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The input and recovered sounds for all of the experiments in the paper the project web page. Abstract When sound hits an object, it causes small vibrations of the ob- ject\u2019s surface. We show how, using only high-speed video of the object, we can extract those minute vibrations and partially re- cover the sound that produced them, allowing us to turn everyday objects\u2014a glass of water, a potted plant, a box of tissues, or a bag of chips\u2014into visual microphones. We recover sounds from high-speed footage of a variety of objects with different properties, and use both real and simulated data to examine some of the factors that affect our ability to visually recover sound. We evaluate the quality of recovered sounds using intelligibility and SNR metrics and provide input and recovered audio samples for direct comparison. We also explore how to leverage the rolling shutter in regular consumer cameras to recover audio from standard frame-rate videos, and use the spatial resolution of our method to visualize how sound-related vibrations vary over an object\u2019s surface, which we can use to re- cover the vibration modes of an object."
            },
            "slug": "The-Visual-Microphone:-Passive-Recovery-of-Sound-Davis-Rubinstein",
            "title": {
                "fragments": [],
                "text": "The Visual Microphone: Passive Recovery of Sound from Video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103797937"
                        ],
                        "name": "L. Verhoeven",
                        "slug": "L.-Verhoeven",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Verhoeven",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Verhoeven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69365456"
                        ],
                        "name": "W. V. Suijlekom",
                        "slug": "W.-V.-Suijlekom",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Suijlekom",
                            "middleNames": [
                                "D.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. V. Suijlekom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "In order to study the problem of detection \u2013 that is, the task of determining when and whether an action that produces a sound has occurred \u2013 separately from the task of sound prediction, we consider two kinds of videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 218530592,
            "fieldsOfStudy": [],
            "id": "757fb29bd1a6958f6593e708bc4a5ecabcebfba1",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Can-one-Hear-the-Shape-of-a-Drum-Verhoeven-Suijlekom",
            "title": {
                "fragments": [],
                "text": "Can one Hear the Shape of a Drum?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3742713"
                        ],
                        "name": "R. Lutfi",
                        "slug": "R.-Lutfi",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lutfi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lutfi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Material properties, such as stiffness and density [37, 31, 14], can likewise be determined from impact sounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115254036,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "6c092d7173a86c8c04e5ccee393e542b817d9569",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-Sound-Source-Identification-Lutfi",
            "title": {
                "fragments": [],
                "text": "Human Sound Source Identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50552040"
                        ],
                        "name": "M. Kac",
                        "slug": "M.-Kac",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Kac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kac"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " method now known as Foley. Our algorithm performs a kind of automatic Foley, synthesizing plausible sound effects without a human in the loop. Sound and materials In the classic mathematical work of [23], Kac showed that the shape of a drum could be partially recovered from the sound it makes. Material properties, such as stiffness and density [33,27,13], can likewise be determined from impact sounds"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121463094,
            "fieldsOfStudy": [
                "Physics",
                "Art"
            ],
            "id": "a26bb6f1a849f185019dce64529b453e1f3996ff",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Can-One-Hear-the-Shape-of-a-Drum-Kac",
            "title": {
                "fragments": [],
                "text": "Can One Hear the Shape of a Drum"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Free - man . Visual vibrometry : Estimating material properties from small motion in video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "000 ms coco images in 5 minutes"
            },
            "venue": {
                "fragments": [],
                "text": "000 ms coco images in 5 minutes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Freeman. Visual vibrometry: Estimating material properties from small motion in video"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Here we have defined soft materials to be {leaf, grass, cloth, plastic bag, carpet} and hard materials to be {gravel, rock, tile, wood, ceramic, plastic, drywall, glass, metal}."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech enhancement based on wavelet thresholding the multitaper spectrum. Speech and Audio Processing"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Material properties, such as stiffness and density [33, 27, 13], can likewise be determined from impact sounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "In our case, we predict a signal \u2013 sound \u2013 known to be a useful representation for many tasks [13, 33], and we show that the output (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of vibration: an introduction, volume 1"
            },
            "venue": {
                "fragments": [],
                "text": "Springer Science & Business Media,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ioffe and C . Szegedy . Batch normalization : Accelerating deep network training by reducing internal covariate shift 3 d convolutional neural networks for human action recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Analysis and Machine Intelligence , IEEE Transactions on"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 117
                            }
                        ],
                        "text": "Recently there has been quite a bit of interest in the computer vision community on several versions of this problem [1, 2, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "To represent sounds, you may want to consider the cochleograms used in [1], or you could use a spectrogram representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visually indicated sounds"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the IEEE conference on computer vision and pattern recognition,"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 27
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Visually-Indicated-Sounds-Owens-Isola/ac640c2d0f33fb3ab49f37b26982948fc31e3191?sort=total-citations"
}