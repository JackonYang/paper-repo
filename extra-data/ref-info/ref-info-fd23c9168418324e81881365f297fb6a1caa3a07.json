{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39909636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d673dc3200ac1742ee0827535a291eb6e051f8",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated."
            },
            "slug": "Arithmetic-Coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040420"
                        ],
                        "name": "F. Rubin",
                        "slug": "F.-Rubin",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35964389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47d1350e9d5b637a7fc888b52c5b9291985b9ed8",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms are presented for encoding and decoding strings of characters as real binary fractions, using registers of fixed precision. The encoding is left to right and does not require blocking. The algorithms have storage requirements O(N) and computation time O(n \\log_{2}N) for string length n and alphabet size N ."
            },
            "slug": "Arithmetic-stream-coding-using-fixed-precision-Rubin",
            "title": {
                "fragments": [],
                "text": "Arithmetic stream coding using fixed precision registers"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Algorithms are presented for encoding and decoding strings of characters as real binary fractions, using registers of fixed precision, and have storage requirements and computation time O(n \\log_{2}N) for string length n and alphabet size N."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16011297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fcb8d85e3d429f3816861fc7999e1bb68eefd39",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for encoding and decoding finite strings over a finite alphabet are described. The coding operations are arithmetic involving rational numbers li as parameters such that \u03a3i2-l i\u22642-\u2208. This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within \u2208. The coding speed is comparable to that of conventional coding methods."
            },
            "slug": "Generalized-Kraft-Inequality-and-Arithmetic-Coding-Rissanen",
            "title": {
                "fragments": [],
                "text": "Generalized Kraft Inequality and Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within \u2208, which is comparable to that of conventional coding methods."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61646910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "930c9b4a081cd73186ff3d990da1842c6e45f95b",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach for black and white image compression is described, with which the eight CCITT test documents can be compressed in a lossless manner 20-30 percent better than with the best existing compression algorithms. The coding and the modeling aspects are treated separately. The key to these improvements is an efficient binary arithmetic code. The code is relatively simple to implement because it avoids the multiplication operation inherent in some earlier arithmetic codes. Arithmetic coding permits the compression of binary sequences where the statistics change on a bit-to-bit basis. Model statistics are studied from stationary, stationary adaptive, and nonstationary adaptive assumptions."
            },
            "slug": "Compression-of-Black-White-Images-with-Arithmetic-Langdon-Rissanen",
            "title": {
                "fragments": [],
                "text": "Compression of Black-White Images with Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new approach for black and white image compression is described, with which the eight CCITT test documents can be compressed in a lossless manner 20-30 percent better than with the best existing compression algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17097754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50e2733d2a8a9b3929bda278d382c37711f3fa8e",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source."
            },
            "slug": "Data-Compression-Using-Adaptive-Coding-and-Partial-Cleary-Witten",
            "title": {
                "fragments": [],
                "text": "Data Compression Using Adaptive Coding and Partial String Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721050"
                        ],
                        "name": "R. Tarjan",
                        "slug": "R.-Tarjan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tarjan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tarjan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066245870"
                        ],
                        "name": "V. K. Wei",
                        "slug": "V.-K.-Wei",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wei",
                            "middleNames": [
                                "K.-W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. K. Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5854590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d90e9ec4f34e877d58782ab5cfb6b2ddf665050",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decoding, and requires only one pass over the data to be compressed (static Huffman coding takes two passes)."
            },
            "slug": "A-locally-adaptive-data-compression-scheme-Bentley-Sleator",
            "title": {
                "fragments": [],
                "text": "A locally adaptive data compression scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described and proves that it never performs much worse than Huffman coding and can perform substantially better."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12940080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0fa0a73b76e6a1609764ad3a140cd8230b4cdf0",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Arithmetic coding is a data compression technique that encodes data (the data string) by creating a code string which represents a fractional value on the number line between 0 and 1. The coding algorithm is symbolwise recursive; i.e., it operates upon and encodes (decodes) one data symbol per iteration or recursion. On each recursion, the algorithm successively partitions an interval of the number line between 0 and 1, and retains one of the partitions as the new interval. Thus, the algorithm successively deals with smaller intervals, and the code string, viewed as a magnitude, lies in each of the nested intervals. The data string is recovered by using magnitude comparisons on the code string to recreate how the encoder must have successively partitioned and retained each nested subinterval. Arithmetic coding differs considerably from the more familiar compression coding techniques, such as prefix (Huffman) codes. Also, it should not be confused with error control coding, whose object is to detect and correct errors in computer operations. This paper presents the key notions of arithmetic compression coding by means of simple examples."
            },
            "slug": "An-Introduction-to-Arithmetic-Coding-Langdon",
            "title": {
                "fragments": [],
                "text": "An Introduction to Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents the key notions of arithmetic compression coding by means of simple examples and describes how the encoder must have successively partitioned and retained each nested subinterval."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69046804"
                        ],
                        "name": "G. Held",
                        "slug": "G.-Held",
                        "structuredName": {
                            "firstName": "Gil",
                            "lastName": "Held",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Held"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071270941"
                        ],
                        "name": "Thomas R. Marshall",
                        "slug": "Thomas-R.-Marshall",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Marshall",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas R. Marshall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45087594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff5463e4d7b88623d29284093182245dc0c0dc65",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProvides professionals and students with a path to faster data transmission times and reduced transmission costs with its in-depth examination of practical and easy-to-implement data-compression techniques. Retaining all data compression fundamentals from the first two editions, the Third Edition expands to include information on the structure and operation of several popular compression algorithms new to the market, including Microcom Networking Protocol (MNP) Class 5 data compression and MNP Class 7 Enhanced Data Compression. Numerous methods to enhance the efficiency of both character-oriented and statistical compression techniques are included as is a new chapter on character compression that discusses three methods to be used to obtain the special compression indicating character."
            },
            "slug": "Data-compression-techniques-and-applications:-and-Held-Marshall",
            "title": {
                "fragments": [],
                "text": "Data compression - techniques and applications: hardware and software considerations (3. ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28270470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f54bc3a24a1f01808e6e8479a11e5b0244f5523",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems arising in the modeling and coding of strings for compression purposes are discussed. The notion of an information source that simplifies and sharpens the traditional one is axiomatized, and adaptive and nonadaptive models are defined. With a measure of complexity assigned to the models, a fundamental theorem is proved which states that models that use any kind of alphabet extension are inferior to the best models using no alphabet extensions at all. A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models. Because the coding parameters are the probabilities that define the model, their design is easy, and the application of the code is straightforward even with adaptively changing source models."
            },
            "slug": "Universal-modeling-and-coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Universal modeling and coding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "(1) message termination overhead; (2) the use of fixed-length rather than infiniteprecision arithmetic; and (3) scaling of counts so that their total is at most Max- frequency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "(1) adaptive text compression, (2) nonadaptive coding, (3) compressing black/white images, and (4) coding arbitrarily distributed integers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 97814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58135a17632cb70c29883b3100f21dd017f7aeef",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of noiselessly encoding a message when prior statistics are known is considered. The close relationship of arithmetic and enumerative coding for this problem is shown by computing explicit arithmetic coding probabilities for various enumerative coding examples. This enables a comparison to be made of the coding efficiency of Markov models and enumerative codes as well as a new coding scheme intermediate between the two. These codes are then extended to messages whose statistics are not known {\\em a priori} Two adaptive codes are described for this problem whose coding efficiency is upper-bounded by the extended enumerative codes. On some practical examples the adaptive codes perform significantly better than the nonadaptive ones."
            },
            "slug": "A-comparison-of-enumerative-and-adaptive-codes-Cleary-Witten",
            "title": {
                "fragments": [],
                "text": "A comparison of enumerative and adaptive codes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two adaptive codes are described for this problem whose coding efficiency is upper-bounded by the extended enumerative codes and on some practical examples the adaptive codes perform significantly better than the nonadaptive ones."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2576061"
                        ],
                        "name": "T. Welch",
                        "slug": "T.-Welch",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Welch",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such methods have enabled mixed-case English text to be encoded in around 2.2 bits/character with two quite different kinds of model [4, 61. Techniques that do not separate modeling from coding so distinctly, like that of Ziv and Lempel (231, do not seem to show such great potential for compression, although they may be appropriate when the aim is raw speed rather than compression performance [ 22 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2055321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15957c2a8dc85e5066e393da9ab70883876521ea",
            "isKey": true,
            "numCitedBy": 2303,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Data stored on disks and tapes or transferred over communications links in commercial computer systems generally contains significant redundancy. A mechanism or procedure which recodes the data to lessen the redundancy could possibly double or triple the effective data densitites in stored or communicated data. Moreover, if compression is automatic, it can also aid in the rise of software development costs. A transparent compression mechanism could permit the use of \"sloppy\" data structures, in that empty space or sparse encoding of data would not greatly expand the use of storage space or transfer time; however , that requires a good compression procedure. Several problems encountered when common compression methods are integrated into computer systems have prevented the widespread use of automatic data compression. For example (1) poor runtime execution speeds interfere in the attainment of very high data rates; (2) most compression techniques are not flexible enough to process different types of redundancy; (3) blocks of compressed data that have unpredictable lengths present storage space management problems. Each compression ' This article was written while Welch was employed at Sperry Research Center; he is now employed with Digital Equipment Corporation. 8 m, 2 /R4/OflAb l strategy poses a different set of these problems and, consequently , the use of each strategy is restricted to applications where its inherent weaknesses present no critical problems. This article introduces a new compression algorithm that is based on principles not found in existing commercial methods. This algorithm avoids many of the problems associated with older methods in that it dynamically adapts to the redundancy characteristics of the data being compressed. An investigation into possible application of this algorithm yields insight into the compressibility of various types of data and serves to illustrate system problems inherent in using any compression scheme. For readers interested in simple but subtle procedures, some details of this algorithm and its implementations are also described. The focus throughout this article will be on transparent compression in which the computer programmer is not aware of the existence of compression except in system performance. This form of compression is \"noiseless,\" the decompressed data is an exact replica of the input data, and the compression apparatus is given no special program information, such as data type or usage statistics. Transparency is perceived to be important because putting an extra burden on the application programmer would cause"
            },
            "slug": "A-Technique-for-High-Performance-Data-Compression-Welch",
            "title": {
                "fragments": [],
                "text": "A Technique for High-Performance Data Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new compression algorithm is introduced that is based on principles not found in existing commercial methods in that it dynamically adapts to the redundancy characteristics of the data being compressed, and serves to illustrate system problems inherent in using any compression scheme."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720405"
                        ],
                        "name": "J. Ziv",
                        "slug": "J.-Ziv",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50154247"
                        ],
                        "name": "A. Lempel",
                        "slug": "A.-Lempel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Lempel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lempel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20900807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5be4e0eccca2892d31406a03b0c485f7a395fe5a",
            "isKey": false,
            "numCitedBy": 3487,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity \\rho(x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of \\rho(x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences."
            },
            "slug": "Compression-of-individual-sequences-via-coding-Ziv-Lempel",
            "title": {
                "fragments": [],
                "text": "Compression of individual sequences via variable-rate coding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed concept of compressibility is shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3114123"
                        ],
                        "name": "G. Cormack",
                        "slug": "G.-Cormack",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Cormack",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cormack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755188"
                        ],
                        "name": "R. N. Horspool",
                        "slug": "R.-N.-Horspool",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Horspool",
                            "middleNames": [
                                "Nigel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. N. Horspool"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35023825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71568a29d90ca5dca4e2cf45ab235832fc773149",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Adaptive-Huffman-Codes-Cormack-Horspool",
            "title": {
                "fragments": [],
                "text": "Algorithms for Adaptive Huffman Codes"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Lett."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2033521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65e6ef044d951d2c3cbf7512b6bfa83a8b2b0739",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In honor of the twenty-fifth anniversary of Huffman coding, four new results about Huffman codes are presented. The first result shows that a binary prefix condition code is a Huffman code iff the intermediate and terminal nodes in the code tree can be listed by nonincreasing probability so that each node in the list is adjacent to its sibling. The second result upper bounds the redundancy (expected length minus entropy) of a binary Huffman code by P_{1}+ \\log_{2}[2(\\log_{2}e)/e]=P_{1}+0.086 , where P_{1} is the probability of the most likely source letter. The third result shows that one can always leave a codeword of length two unused and still have a redundancy of at most one. The fourth result is a simple algorithm for adapting a Huffman code to slowly varying esthnates of the source probabilities. In essence, one maintains a running count of uses of each node in the code tree and lists the nodes in order of these counts. Whenever the occurrence of a message increases a node count above the count of the next node in the list, the nodes, with their attached subtrees, are interchanged."
            },
            "slug": "Variations-on-a-theme-by-Huffman-Gallager",
            "title": {
                "fragments": [],
                "text": "Variations on a theme by Huffman"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Four new results about Huffman codes are presented and a simple algorithm for adapting a Huffman code to slowly varying esthnates of the source probabilities is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058072623"
                        ],
                        "name": "R. Hunter",
                        "slug": "R.-Hunter",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Hunter",
                            "middleNames": [
                                "Winesanker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31521128"
                        ],
                        "name": "A. H. Robinson",
                        "slug": "A.-H.-Robinson",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Robinson",
                            "middleNames": [
                                "Henry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46403372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c7d6fe0309b24c887db7853097b72f378c5fb69",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently Study Group XIV of CCITT has drafted a new Recommendation (T.4) with the aim of achieving compatibility between digital facsimile apparatus connected to general switched telephone networks. A one-dimensional coding scheme is used in which run lengths are encoded using a modified Huffman code. This allows typical A4 size documents in the form of black and white images scanned at normal resolution (3.85 lines/mm, 1728 pels/line) to be transmitted in an average time of about a minute at a rate of 4800 bit/s. The Recommendation also includes a two-dimensional code, known as the modified relative element address designate (READ) code, which is in the form of an optional extension to the one-dimensional code. This extension allows typical documents scanned at high (twice normal) resolution (with every fourth line one dimensionally coded) to be transmitted in an average time of about 75 s at 4800 bit/s. This paper describes the coding schemes in detail and discusses the factors which led to their choice. In addition, this paper assesses the performance of the codes, particularly in relation to their compression efficiency and vulnerability to transmission errors, making use of 8 CCITT reference documents."
            },
            "slug": "International-digital-facsimile-coding-standards-Hunter-Robinson",
            "title": {
                "fragments": [],
                "text": "International digital facsimile coding standards"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The coding schemes in detail are described in detail and the factors which led to their choice are discussed, and the performance of the codes is assessed, particularly in relation to their compression efficiency and vulnerability to transmission errors."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059600902"
                        ],
                        "name": "T. Lynch",
                        "slug": "T.-Lynch",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lynch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lynch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61014908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fad2c7fbdd839ce0d8a9154e10a0e3496a21b0c",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-compression-techniques-and-applications-Lynch",
            "title": {
                "fragments": [],
                "text": "Data compression techniques and applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125327631,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "faa975eaeb6a45031f77d6d7344ac905f74fb962",
            "isKey": false,
            "numCitedBy": 7189,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientific knowledge grows at a phenomenal pace--but few books have had as lasting an impact or played as important a role in our modern world as The Mathematical Theory of Communication, published originally as a paper on communication theory more than fifty years ago. Republished in book form shortly thereafter, it has since gone through four hardcover and sixteen paperback printings. It is a revolutionary work, astounding in its foresight and contemporaneity. The University of Illinois Press is pleased and honored to issue this commemorative reprinting of a classic."
            },
            "slug": "The-mathematical-theory-of-communication-Shannon",
            "title": {
                "fragments": [],
                "text": "The mathematical theory of communication"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This commemorative reprinting of The Mathematical Theory of Communication, published originally as a paper on communication theory more than fifty years ago, is released."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1948
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144950991"
                        ],
                        "name": "J. H. Hester",
                        "slug": "J.-H.-Hester",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hester",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561382"
                        ],
                        "name": "D. Hirschberg",
                        "slug": "D.-Hirschberg",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Hirschberg",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hirschberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14918332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "833bbe08af5f71ef0e7cdbe887dd81111929dd2f",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms that modify the order of linear search lists are surveyed. First the problem, including assumptions and restrictions, is defined. Next a summary of analysis techniques and measurements that apply to these algorithms is given. The main portion of the survey presents algorithms in the literature with absolute analyses when available. The following section gives relative measures that are applied between two or more algorithms. The final section presents open questions."
            },
            "slug": "Self-organizing-linear-search-Hester-Hirschberg",
            "title": {
                "fragments": [],
                "text": "Self-organizing linear search"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Algorithms that modify the order of linear search lists are surveyed and algorithms in the literature with absolute analyses when available are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55379485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a54194422c56399b2923b2ad706b8175c8c48258",
            "isKey": false,
            "numCitedBy": 34823,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases."
            },
            "slug": "A-mathematical-theory-of-communication-Shannon",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of communication"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This final installment of the paper considers the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now."
            },
            "venue": {
                "fragments": [],
                "text": "MOCO"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144094404"
                        ],
                        "name": "E. N. Adams",
                        "slug": "E.-N.-Adams",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adams",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. N. Adams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41562737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0dca46c2a0669acaf0ea21741db70eff2d4b120",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The implementer of a large, complex software system cannot make it completely defect free, so he must normally provide fixes for defects found after the code is put into service. A system user may do preventive service by installing these fixes before the defects cause him problems. Preventive service can benefit both the software developer and the software user to the extent that it reduces the number of operational problems caused by software errors, but it requires the expenditure of the resources required to prepare, disseminate, and install fixes; and it can be the cause of additional software problems caused by design errors introduced into the code by fixes. The benefit from removing a given defect depends on how many problems it would otherwise cause. Benefits may be estimated by modeling problem occurrence as a random process in execution time governed by a distribution of characteristic rates. It is found that most of the benefit to be realized by preventive service comes from removing a relatively small number of high-rate defects that are found early in the service life of the code. For the typical user corrective service would seem preferable to preventive service as a way of dealing with most defects found after code has had some hundreds of months of usage."
            },
            "slug": "Optimizing-Preventive-Service-of-Software-Products-Adams",
            "title": {
                "fragments": [],
                "text": "Optimizing Preventive Service of Software Products"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is found that most of the benefit to be realized by preventive service comes from removing a relatively small number of high-rate defects that are found early in the service life of the code."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126645"
                        ],
                        "name": "Richard C. Pasco",
                        "slug": "Richard-C.-Pasco",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Pasco",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard C. Pasco"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Practical techniques were first introduced by Rissanen [16] and Pasco [ 15 ], and developed further by Rissanen [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60531818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e9a6af4bc23b4cd139e671f4d1b2ca18228f660",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Source-coding-algorithms-for-fast-data-compression-Pasco",
            "title": {
                "fragments": [],
                "text": "Source coding algorithms for fast data compression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Kraft inequality and arithmmatic coding . IBM 1"
            },
            "venue": {
                "fragments": [],
                "text": "Res . Dev . Acta Polytech . Stand . Math ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "(1) message termination overhead; (2) the use of fixed-length rather than infiniteprecision arithmetic; and (3) scaling of counts so that their total is at most Max- frequency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "(1) adaptive text compression, (2) nonadaptive coding, (3) compressing black/white images, and (4) coding arbitrarily distributed integers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A locally adaptive data compression"
            },
            "venue": {
                "fragments": [],
                "text": "scheme. Commun. ACM 29,"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Arithmetic-coding-for-data-compression-Witten-Neal/fd23c9168418324e81881365f297fb6a1caa3a07?sort=total-citations"
}