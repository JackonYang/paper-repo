{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": ", 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1272090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6a83c4fcc99ba6753109301949c5b7cfa978079",
            "isKey": false,
            "numCitedBy": 1044,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A phrase-based statistical machine translation approach the alignment template approach is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various system components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems."
            },
            "slug": "The-Alignment-Template-Approach-to-Statistical-Och-Ney",
            "title": {
                "fragments": [],
                "text": "The Alignment Template Approach to Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A phrase-based statistical machine translation approach the alignment template approach is described, which allows for general many-to-many relations between words and is easier to extend than classical statistical machinetranslation systems."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48379623"
                        ],
                        "name": "Y. Zhang",
                        "slug": "Y.-Zhang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3322408"
                        ],
                        "name": "Almut Silja Hildebrand",
                        "slug": "Almut-Silja-Hildebrand",
                        "structuredName": {
                            "firstName": "Almut",
                            "lastName": "Hildebrand",
                            "middleNames": [
                                "Silja"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Almut Silja Hildebrand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247319"
                        ],
                        "name": "S. Vogel",
                        "slug": "S.-Vogel",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "The underlying architecture is similar to (Zhang et al., 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "Recently a two-pass approach has been proposed (Zhang et al., 2006), wherein a lowerorder n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7221408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0afe29a9a8871faab23cbf90cf499c735a3b0c51",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a novel distributed language model for N-best list re-ranking. The model is based on the client/server paradigm where each server hosts a portion of the data and provides information to the client. This model allows for using an arbitrarily large corpus in a very efficient way. It also provides a natural platform for relevance weighting and selection. We applied this model on a 2.97 billion-word corpus and re-ranked the N-best list from Hiero, a state-of-the-art phrase-based system. Using BLEU as a metric, the re-ranked translation achieves a relative improvement of 4.8%, significantly better than the model-best translation."
            },
            "slug": "Distributed-Language-Modeling-for-N-best-List-Zhang-Hildebrand",
            "title": {
                "fragments": [],
                "text": "Distributed Language Modeling for N-best List Re-ranking"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A novel distributed language model based on the client/server paradigm for N-best list re-ranking that allows for using an arbitrarily large corpus in a very efficient way and provides a natural platform for relevance weighting and selection."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797272"
                        ],
                        "name": "Ahmad Emami",
                        "slug": "Ahmad-Emami",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Emami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Emami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431938"
                        ],
                        "name": "Jeffrey Scott Sorensen",
                        "slug": "Jeffrey-Scott-Sorensen",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Sorensen",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Scott Sorensen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18193618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "591080c335daba2494f18cc04c9f7071501af0eb",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score."
            },
            "slug": "Large-Scale-Distributed-Language-Modeling-Emami-Papineni",
            "title": {
                "fragments": [],
                "text": "Large-Scale Distributed Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented, which is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 51
                            }
                        ],
                        "text": "The mathematics of the problem were formalized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization\ne\u0302 = arg max e\nM \u2211\nm=1\n\u03bbmhm(e, f) (1)\nwhere {hm(e, f)} is a set of M feature functions and {\u03bbm} a set of weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "The mathematics of the problem were formalized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13259913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab7b5917515c460b90451e67852171a531671ab8",
            "isKey": false,
            "numCitedBy": 4744,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus."
            },
            "slug": "The-Mathematics-of-Statistical-Machine-Translation:-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "The Mathematics of Statistical Machine Translation: Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus, given a set of pairs of sentences that are translations of one another."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830767"
                        ],
                        "name": "P. Placeway",
                        "slug": "P.-Placeway",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Placeway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Placeway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683412"
                        ],
                        "name": "Pascale Fung",
                        "slug": "Pascale-Fung",
                        "structuredName": {
                            "firstName": "Pascale",
                            "lastName": "Fung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascale Fung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150603996"
                        ],
                        "name": "L. Nguyen",
                        "slug": "L.-Nguyen",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Nguyen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18070943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8fcc058607129590fa6ab692a38807efc7d7f1f",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors consider the estimation of powerful statistical language models using a technique that scales from very small to very large amounts of domain-dependent data. They begin with improved modeling of the grammar statistics, based on a combination of the backing-off technique and zero-frequency techniques. These are extended to be more amenable to the particular system considered here. The resulting technique is greatly simplified, more robust, and gives improved recognition performance over either of the previous techniques. The authors also consider the problem of robustness of a model based on a small training corpus by grouping words into obvious semantic classes. This significantly improves the robustness of the resulting statistical grammar. A technique that allows the estimation of a high-order model on modest computation resources is also presented. This makes it possible to run a 4-gram statistical model of a 50-million word corpus on a workstation of only modest capability and cost. Finally, the authors discuss results from applying a 2-gram statistical language model integrated in the HMM (hidden Markov model) search, obtaining a list of the N-best recognition results, and rescoring this list with a higher-order statistical model.<<ETX>>"
            },
            "slug": "The-estimation-of-powerful-language-models-from-and-Placeway-Schwartz",
            "title": {
                "fragments": [],
                "text": "The estimation of powerful language models from small and large corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The authors consider the estimation of powerful statistical language models using a technique that scales from very small to very large amounts of domain-dependent data, and considers the problem of robustness of a model based on a small training corpus by grouping words into obvious semantic classes."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50126864"
                        ],
                        "name": "Joshua Goodman",
                        "slug": "Joshua-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "For this reason, the ML estimate must be modified for use in practice; see (Goodman, 2001) for a discussion of n-gram models and smoothing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12982389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09c76da2361d46689825c4efc37ad862347ca577",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline."
            },
            "slug": "A-bit-of-progress-in-language-modeling-Goodman",
            "title": {
                "fragments": [],
                "text": "A bit of progress in language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A combination of all techniques together to a Katz smoothed trigram model with no count cutoffs achieves perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 183
                            }
                        ],
                        "text": "For each training data size, we report the size of the resulting language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al., 2002) obtained by the machine translation system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16609,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145483154"
                        ],
                        "name": "YU Peng",
                        "slug": "YU-Peng",
                        "structuredName": {
                            "firstName": "YU",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "YU Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39923907"
                        ],
                        "name": "Chengyuan Ma",
                        "slug": "Chengyuan-Ma",
                        "structuredName": {
                            "firstName": "Chengyuan",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengyuan Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708592"
                        ],
                        "name": "Eric Chang",
                        "slug": "Eric-Chang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Chang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17576149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "539b1794fcf455c2c15af375dbcecc18bc64a2b0",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "For efficient organization of speech recordings - meetings, interviews, voice mails, lectures - the ability to search for spoken keywords is an essential capability. Today, most spoken-document retrieval systems use large-vocabulary recognition. For the above scenarios, such systems suffer from both the unpredictable vocabulary/domain and generally high word-error rates (WER). We present a vocabulary-independent system to index and to search rapidly spontaneous speech. A speech recognizer generates lattices of phonetic word fragments, against which keywords are matched phonetically. We first show the need to use recognition alternatives (lattices) in a high-WER context, on a word-based baseline. Then we introduce our new method of phonetic word-fragment lattice generation, which uses longer-span language knowledge than a phoneme recognizer. Last we introduce heuristics to compact the lattices to feasible sizes that can be searched efficiently. On the LDC voice mail corpus, we show that vocabulary/domain-independent phonetic search is as accurate as a vocabulary/domain-dependent word-lattice based baseline system for in-vocabulary keywords (FOMs of 74-75%), but nearly maintains this accuracy also for out-of-vocabulary keywords."
            },
            "slug": "Vocabulary-independent-search-in-spontaneous-speech-Seide-Peng",
            "title": {
                "fragments": [],
                "text": "Vocabulary-independent search in spontaneous speech"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a vocabulary-independent system to index and to search rapidly spontaneous speech, and introduces a new method of phonetic word-fragment lattice generation, which uses longer-span language knowledge than a phoneme recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 36
                            }
                        ],
                        "text": "05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15119437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb826a3899752b796f14df1c50378c64954a6b0a",
            "isKey": false,
            "numCitedBy": 1524,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real."
            },
            "slug": "Statistical-Significance-Tests-for-Machine-Koehn",
            "title": {
                "fragments": [],
                "text": "Statistical Significance Tests for Machine Translation Evaluation"
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182856"
                        ],
                        "name": "S. Ortmanns",
                        "slug": "S.-Ortmanns",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Ortmanns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ortmanns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 68
                            }
                        ],
                        "text": "This may yield better results than n-best list or lattice rescoring (Ney and Ortmanns, 1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2215577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7424d48786b23ffdfe6d938133b89830420afc9b",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors gives a unifying view of the dynamic programming approach to the search problem. They review the search problem from the statistical point-of-view and show how the search space results from the acoustic and language models required by the statistical approach. Starting from the baseline one-pass algorithm using a linear organization of the pronunciation lexicon, they have extended the baseline algorithm toward various dimensions. To handle a large vocabulary, they have shown how the search space can be structured in combination with a lexical prefix tree organization of the pronunciation lexicon. In addition, they have shown how this structure of the search space can be combined with a time-synchronous beam search concept and how the search space can be constructed dynamically during the recognition process. In particular, to increase the efficiency of the beam search concept, they have integrated the language model look-ahead into the pruning operation. To produce sentence alternatives rather than only the single best sentence, they have extended the search strategy to generate a word graph. Finally, they have reported experimental results on a 64 k-word task that demonstrate the efficiency of the various search concepts presented."
            },
            "slug": "Dynamic-programming-search-for-continuous-speech-Ney-Ortmanns",
            "title": {
                "fragments": [],
                "text": "Dynamic programming search for continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A unifying view of the dynamic programming approach to the search problem from the statistical point-of-view is given and how the search space results from the acoustic and language models required by the statistical approach are shown."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Process. Mag."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 29
                            }
                        ],
                        "text": "Goodman (2001) observed that Kneser-Ney Smoothing dominates other schemes over a broad range of conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 34
                            }
                        ],
                        "text": "As examples, Kneser-Ney Smoothing (Kneser and Ney, 1995), Katz Backoff (Katz, 1987) and linear interpolation (Jelinek and Mercer, 1980) can be expressed in this scheme (Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Kneser-Ney Smoothing counts lower-order ngrams differently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 38
                            }
                        ],
                        "text": "The largest models reported here with Kneser-Ney Smoothing were trained on 31 billion tokens."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 16
                            }
                        ],
                        "text": "The gap between Kneser-Ney Smoothing and Stupid Backoff narrows, starting with a difference of 0.85 BP and ending with a not significant difference of 0.24 BP.\nAdding a third language models based on webnews data does not show a jump at the start of the curve."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 33
                            }
                        ],
                        "text": "State-of-the-art techniques like Kneser-Ney Smoothing or Katz Backoff require additional, more expensive steps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 23
                            }
                        ],
                        "text": "Generating models with Kneser-Ney Smoothing takes 6 \u2013 7 times longer than generating models with Stupid Backoff."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "For smaller training sizes, we have also computed test-set perplexity using Kneser-Ney Smoothing, and report it for comparison."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": true,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 57
                            }
                        ],
                        "text": "State-of-the-art techniques like Kneser-Ney Smoothing or Katz Backoff require additional, more expensive steps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 71
                            }
                        ],
                        "text": "As examples, Kneser-Ney Smoothing (Kneser and Ney, 1995), Katz Backoff (Katz, 1987) and linear interpolation (Jelinek and Mercer, 1980) can be expressed in this scheme (Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Katz Backoff requires similar additional steps."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1907,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 116
                            }
                        ],
                        "text": "The most commonly used variant of Kneser-Ney smoothing is interpolated Kneser-Ney smoothing, defined recursively as (Chen and Goodman, 1998):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 168
                            }
                        ],
                        "text": "As examples, Kneser-Ney Smoothing (Kneser and Ney, 1995), Katz Backoff (Katz, 1987) and linear interpolation (Jelinek and Mercer, 1980) can be expressed in this scheme (Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": false,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2139780781"
                        ],
                        "name": "Xiaoyang Yu",
                        "slug": "Xiaoyang-Yu",
                        "structuredName": {
                            "firstName": "Xiaoyang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyang Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61490065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dba4fe110aca0d6c05586b31acaaf51b0a4ae349",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis presents the work of building a large scale distributed ngram language model using a MapReduce platform named Hadoop and a distributed database called Hbase. We propose a method focusing on the time cost and storage size of the model, exploring different Hbase table structures and compression approaches. The method is applied to build up training and testing processes using Hadoop MapReduce framework and Hbase. The experiments evaluate and compare different table structures on training 100 million words for unigram, bigram and trigram models, and the results suggest a table based on half ngram structure is a good choice for distributed language model. The results of this work can be applied and further developed in machine translation and other large scale distributed language processing areas."
            },
            "slug": "Estimating-Language-Models-Using-Hadoop-and-Hbase-Yu",
            "title": {
                "fragments": [],
                "text": "Estimating Language Models Using Hadoop and Hbase"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experiments evaluate and compare different table structures on training 100 million words for unigram, bigram and trigram models, and the results suggest a table based on half ngram structure is a good choice for distributed language model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47994881"
                        ],
                        "name": "S. Buckland",
                        "slug": "S.-Buckland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Buckland",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buckland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70421692"
                        ],
                        "name": "E. Noreen",
                        "slug": "E.-Noreen",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Noreen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Noreen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 36
                            }
                        ],
                        "text": "05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125262802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "189e44489209454d363e6a0e272f3c5c80bbde1b",
            "isKey": false,
            "numCitedBy": 482,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Intensive-Methods-for-Testing-Hypotheses.-Buckland-Noreen",
            "title": {
                "fragments": [],
                "text": "Computer-Intensive Methods for Testing Hypotheses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 109
                            }
                        ],
                        "text": "As examples, Kneser-Ney Smoothing (Kneser and Ney, 1995), Katz Backoff (Katz, 1987) and linear interpolation (Jelinek and Mercer, 1980) can be expressed in this scheme (Chen and Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 10,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d?sort=total-citations"
}