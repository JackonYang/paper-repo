{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285356"
                        ],
                        "name": "Samuele Capobianco",
                        "slug": "Samuele-Capobianco",
                        "structuredName": {
                            "firstName": "Samuele",
                            "lastName": "Capobianco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuele Capobianco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49012494"
                        ],
                        "name": "M. I. Malik",
                        "slug": "M.-I.-Malik",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Malik",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285734"
                        ],
                        "name": "S. Marinai",
                        "slug": "S.-Marinai",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Marinai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marinai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Recently, deep learning methods have also been exploited for document image classification [18]\u2013[20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18], get significant performance boosts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "While the images are visually very different, the visual queues are generic and thus, transfer learning helps to boost the performance of the document image classification [18], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "that it is possible to use transfer learning and the features that are learned from general (daily life) images can be used for the classification of document images [18]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36241428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec65a13db7e387971ec017c721da4c26aeee29c0",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a deep Convolutional Neural Network (CNN) based approach for document image classification. One of the main requirement of deep CNN architecture is that they need huge number of samples for training. To overcome this problem we adopt a deep CNN which is trained using big image dataset containing millions of samples i.e., ImageNet. The proposed work outperforms both the traditional structure similarity methods and the CNN based approaches proposed earlier. The accuracy of the proposed approach with merely 20 images per class outperforms the state-of-the-art by achieving classification accuracy of 68.25%. The best results on Tobbacoo-3428 dataset show that our proposed method outperforms the state-of-the-art method by a significant margin and achieved a median accuracy of 77.6% with 100 samples per class used for training and validation."
            },
            "slug": "Deepdocclassifier:-Document-classification-with-Afzal-Capobianco",
            "title": {
                "fragments": [],
                "text": "Deepdocclassifier: Document classification with deep Convolutional Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A deep Convolutional Neural Network (CNN) based approach for document image classification which is trained using big image dataset containing millions of samples and outperforms both the traditional structure similarity methods and the CNN based approaches proposed earlier."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34939798"
                        ],
                        "name": "Adam W. Harley",
                        "slug": "Adam-W.-Harley",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Harley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam W. Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079687415"
                        ],
                        "name": "Alex Ufkes",
                        "slug": "Alex-Ufkes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ufkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Ufkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150825"
                        ],
                        "name": "K. Derpanis",
                        "slug": "K.-Derpanis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Derpanis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Derpanis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "While the images are visually very different, the visual queues are generic and thus, transfer learning helps to boost the performance of the document image classification [18], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Recently, deep learning methods have also been exploited for document image classification [18]\u2013[20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "First, we train a variety of networks on the Ryerson Vision Lab Complex Document Information Processing (RVL-CDIP) dataset [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] provided a breakthrough when they showed"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] consist of 400, 0000 images that are divided into 16 classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] was that they introduced a dataset consisting of 400, 000 documents divided into 16 classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2760893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd86b4b551b9d3fb498f62008b037e7599365018",
            "isKey": true,
            "numCitedBy": 174,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "slug": "Evaluation-of-deep-convolutional-nets-for-document-Harley-Ufkes",
            "title": {
                "fragments": [],
                "text": "Evaluation of deep convolutional nets for document image classification and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs), and makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "4) Resnet-50: ResNets are a family of very deep CNN architectures which make use of residual connections [30] to overcome the challenge of efficient error backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "2) VGG-16: VGG-16, as the name suggests is a 16-layer CNN [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62220,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "This approach has shown to be effective for real-world image classification [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "1) AlexNet: AlexNet [26] is the eight-layer CNN that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 [21] by a large margin."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80944,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145714522"
                        ],
                        "name": "Le Kang",
                        "slug": "Le-Kang",
                        "structuredName": {
                            "firstName": "Le",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Le Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775793"
                        ],
                        "name": "J. Kumar",
                        "slug": "J.-Kumar",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449660"
                        ],
                        "name": "Peng Ye",
                        "slug": "Peng-Ye",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682487"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": ", they are randomly initialized) are under-performing [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "The pioneering work that performed document classification using CNNs used a rather shallow network for classification [19]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16147742,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "432bbce9609e62f699a7419ea9b243bd486f9acb",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Convolutional Neural Network (CNN) for document image classification. In particular, document image classes are defined by the structural similarity. Previous approaches rely on hand-crafted features for capturing structural information. In contrast, we propose to learn features from raw image pixels using CNN. The use of CNN is motivated by the the hierarchical nature of document layout. Equipped with rectified linear units and trained with dropout, our CNN performs well even when document layouts present large inner-class variations. Experiments on public challenging datasets demonstrate the effectiveness of the proposed approach."
            },
            "slug": "Convolutional-Neural-Networks-for-Document-Image-Kang-Kumar",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Networks for Document Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Equipped with rectified linear units and trained with dropout, this CNN performs well even when document layouts present large inner-class variations, and experiments on public challenging datasets demonstrate the effectiveness of the proposed approach."
            },
            "venue": {
                "fragments": [],
                "text": "2014 22nd International Conference on Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "3) GoogLeNet: GoogLeNet, just like VGG-16, won a category of the ILSVRC 2014, namely the classification category [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29480,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403927620"
                        ],
                        "name": "Joan Pastor-Pellicer",
                        "slug": "Joan-Pastor-Pellicer",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Pastor-Pellicer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Pastor-Pellicer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5913437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdeb574c0847c41dcee7de59e917da9c0dd3f2c1",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose to address the problem of Document Image Binarization (DIB) using Long Short-Term Memory (LSTM) which is specialized in processing very long sequences. Thus, the image is considered as a 2D sequence of pixels and in accordance to this a 2D LSTM is employed for the classification of each pixel as text or background. The proposed approach processes the information using local context and then propagates the information globally in order to achieve better visual coherence. The method is robust against most of the document artifacts. We show that with a very simple network without any feature extraction and with limited amount of data the proposed approach works reasonably well for the DIBCO 2013 dataset. Furthermore a synthetic dataset is considered to measure the performance of the proposed approach with both binarization and OCR groundtruth. The proposed approach significantly outperforms standard binarization approaches both for F-Measure and OCR accuracy with the availability of enough training samples."
            },
            "slug": "Document-Image-Binarization-using-LSTM:-A-Sequence-Afzal-Pastor-Pellicer",
            "title": {
                "fragments": [],
                "text": "Document Image Binarization using LSTM: A Sequence Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed approach significantly outperforms standard binarization approaches both for F-Measure and OCR accuracy with the availability of enough training samples."
            },
            "venue": {
                "fragments": [],
                "text": "HIP@ICDAR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700495"
                        ],
                        "name": "Mathias Seuret",
                        "slug": "Mathias-Seuret",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Seuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Seuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38890619"
                        ],
                        "name": "Michele Alberti",
                        "slug": "Michele-Alberti",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Alberti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Alberti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4702752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "056b27fb8e12c81ebfb66616c199d79b63236923",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel approach for initializing deep neural networks, i.e., by using Principal Component Analysis (PCA) to initialize neural layers. Usually, the initialization of the weights of a deep neural network is done in one of the three following ways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or as auto-encoder, and 3) re-use of layers from another network (transfer learning). Therefore, typically, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn a PCA into an auto-encoder, by generating an encoder layer of the PCA parameters and furthermore adding a decoding layer. We analyze the initialization technique on real documents. First, we show that a PCA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis we investigate the effectiveness of PCAbased initialization and show that it outperforms state-of-the-art random weight initialization methods."
            },
            "slug": "PCA-Initialized-Deep-Neural-Networks-Applied-to-Seuret-Alberti",
            "title": {
                "fragments": [],
                "text": "PCA-Initialized Deep Neural Networks Applied to Document Image Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes how to turn a PCA into an auto-encoder, by generating an encoder layer of the PCA parameters and furthermore adding a decoding layer, and investigates the effectiveness of PCAbased initialization for the task of layout analysis."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "So, in case there is no large document dataset available for pretraining, one can and should always resort to using an ImageNet pretrained model for finetuning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "1) AlexNet: AlexNet [26] is the eight-layer CNN that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 [21] by a large margin."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 54
                            }
                        ],
                        "text": "Fortunately, there are models which are pretrained on ImageNet available online for many architectures, including the four networks used in this work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "We also compare the networks with ImageNet initialization against randomly initialized networks and find that even though the images are substantially different (cf. Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "1 shows the sample images both real and document images from the ImageNet [21] and Tobacco-3482 datasets respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": "Depending on the amount of available training data, AlexNet and VGG-16 are the best choices when finetuning the networks from models that were pretrained on ImageNet (cf. Fig."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": true,
            "numCitedBy": 25491,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115913164"
                        ],
                        "name": "Min Lin",
                        "slug": "Min-Lin",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35370244"
                        ],
                        "name": "Qiang Chen",
                        "slug": "Qiang-Chen",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "It is therefore a Networkin-Network approach [29]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16636683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "isKey": false,
            "numCitedBy": 4208,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
            },
            "slug": "Network-In-Network-Lin-Chen",
            "title": {
                "fragments": [],
                "text": "Network In Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "With enhanced local modeling via the micro network, the proposed deep network structure NIN is able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734717217"
                        ],
                        "name": "Sheraz Ahmed",
                        "slug": "Sheraz-Ahmed",
                        "structuredName": {
                            "firstName": "Sheraz",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheraz Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49012494"
                        ],
                        "name": "M. I. Malik",
                        "slug": "M.-I.-Malik",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Malik",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16177370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3121fa65f75e61834e552a0189962f7fc8806e2a",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The contribution of this paper is fourfold. The first contribution is a novel, generic method for automatic ground truth generation of camera-captured document images (books, magazines, articles, invoices, etc.). It enables us to build large-scale (i.e., millions of images) labeled camera-captured/scanned documents datasets, without any human intervention. The method is generic, language independent and can be used for generation of labeled documents datasets (both scanned and cameracaptured) in any cursive and non-cursive language, e.g., English, Russian, Arabic, Urdu, etc. To assess the effectiveness of the presented method, two different datasets in English and Russian are generated using the presented method. Evaluation of samples from the two datasets shows that 99:98% of the images were correctly labeled. The second contribution is a large dataset (called C3Wi) of camera-captured characters and words images, comprising 1 million word images (10 million character images), captured in a real camera-based acquisition. This dataset can be used for training as well as testing of character recognition systems on camera-captured documents. The third contribution is a novel method for the recognition of cameracaptured document images. The proposed method is based on Long Short-Term Memory and outperforms the state-of-the-art methods for camera based OCRs. As a fourth contribution, various benchmark tests are performed to uncover the behavior of commercial (ABBYY), open source (Tesseract), and the presented camera-based OCR using the presented C3Wi dataset. Evaluation results reveal that the existing OCRs, which already get very high accuracies on scanned documents, have limited performance on camera-captured document images; where ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while the presented character recognition system has an accuracy of 95.10%."
            },
            "slug": "A-Generic-Method-for-Automatic-Ground-Truth-of-Ahmed-Malik",
            "title": {
                "fragments": [],
                "text": "A Generic Method for Automatic Ground Truth Generation of Camera-captured Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed method is based on Long Short-Term Memory and outperforms the state-of-the-art methods for camera based OCRs and can be used for generation of labeled documents datasets (both scanned and cameracaptured) in any cursive and non-cursive language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403927620"
                        ],
                        "name": "Joan Pastor-Pellicer",
                        "slug": "Joan-Pastor-Pellicer",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Pastor-Pellicer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Pastor-Pellicer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532744"
                        ],
                        "name": "Salvador Espa\u00f1a Boquera",
                        "slug": "Salvador-Espa\u00f1a-Boquera",
                        "structuredName": {
                            "firstName": "Salvador",
                            "lastName": "Boquera",
                            "middleNames": [
                                "Espa\u00f1a"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salvador Espa\u00f1a Boquera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398809775"
                        ],
                        "name": "Francisco Zamora-Mart\u00ednez",
                        "slug": "Francisco-Zamora-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Zamora-Mart\u00ednez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francisco Zamora-Mart\u00ednez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145816817"
                        ],
                        "name": "M. J. Bleda",
                        "slug": "M.-J.-Bleda",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Bleda",
                            "middleNames": [
                                "Jos\u00e9",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bleda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27833595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ba6b2b378986cdf708090d713eaf5c341cc30f",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks have systematically shown good performance in Computer Vision and in Handwritten Text Recognition tasks. This paper proposes the use of these models for document image binarization. The main idea is to classify each pixel of the image into foreground and background from a sliding window centered at the pixel to be classified. An experimental analysis on the effect of sensitive parameters and some working topologies are proposed using two different corpora, of very different properties: DIBCO and Santgall."
            },
            "slug": "Insights-on-the-Use-of-Convolutional-Neural-for-Pastor-Pellicer-Boquera",
            "title": {
                "fragments": [],
                "text": "Insights on the Use of Convolutional Neural Networks for Document Image Binarization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes the use of convolutional Neural Networks for document image binarization to classify each pixel of the image into foreground and background from a sliding window centered at the pixel to be classified."
            },
            "venue": {
                "fragments": [],
                "text": "IWANN"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775793"
                        ],
                        "name": "J. Kumar",
                        "slug": "J.-Kumar",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449660"
                        ],
                        "name": "Peng Ye",
                        "slug": "Peng-Ye",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4] propose a method that relies on the patch code words derived from the document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Then, histograms of codewords are created similar to [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 246
                            }
                        ],
                        "text": "Earlier methods that have been dealing with document classification focused mainly on either exploiting the structural similarity constraints [2], [3] or extracting features from the documents that may be able to help for document classification [4]\u2013[6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 391056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31da8cace27a08e6339145f95f190a77197f01b2",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a method for the retrieval of document images with chosen layout characteristics. The proposed method is based on statistics of patch-codewords over different regions of image. We begin with a set of wanted and a random set of unwanted images representative of a large heterogeneous collection. We then use raw-image patches extracted from the unlabeled images to learn a codebook. To model the spatial relationships between patches, the image is recursively partitioned horizontally and vertically, and a histogram of patch-codewords is computed in each partition. The resulting set of features give a high precision and recall for the retrieval of hand-drawn and machine-print table-documents, and unconstrained mixed form-type documents, when trained using a random forest classifier. We compare our method to the spatial-pyramid method, and show that the proposed approach for learning layout characteristics is competitive for document images."
            },
            "slug": "Learning-document-structure-for-retrieval-and-Kumar-Ye",
            "title": {
                "fragments": [],
                "text": "Learning document structure for retrieval and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The proposed method is based on statistics of patch-codewords over different regions of image, which gives a high precision and recall for the retrieval of hand-drawn and machine-print table-documents, and unconstrained mixed form-type documents, when trained using a random forest classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868014"
                        ],
                        "name": "S. Ahmed",
                        "slug": "S.-Ahmed",
                        "structuredName": {
                            "firstName": "Saad",
                            "lastName": "Ahmed",
                            "middleNames": [
                                "Bin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776683"
                        ],
                        "name": "S. Naz",
                        "slug": "S.-Naz",
                        "structuredName": {
                            "firstName": "Saeeda",
                            "lastName": "Naz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Naz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986976"
                        ],
                        "name": "M. I. Razzak",
                        "slug": "M.-I.-Razzak",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Razzak",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Razzak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153789108"
                        ],
                        "name": "Shiekh Faisal Rashid",
                        "slug": "Shiekh-Faisal-Rashid",
                        "structuredName": {
                            "firstName": "Shiekh",
                            "lastName": "Rashid",
                            "middleNames": [
                                "Faisal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiekh Faisal Rashid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16629411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf399bf9c787976de198725f53b3c9a165b32fef",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nCharacter recognition has been widely used since its inception in applications involved processing of scanned or camera-captured documents. There exist multiple scripts in which the languages are written. The scripts could broadly be divided into cursive and non-cursive scripts. The recurrent neural networks have been proved to obtain state-of-the-art results for optical character recognition. We present a thorough investigation of the performance of recurrent neural network (RNN) for cursive and non-cursive scripts. We employ bidirectional long short-term memory (BLSTM) networks, which is a variant of the standard RNN. The output layer of the architecture used to carry out our investigation is a special layer called connectionist temporal classification (CTC) which does the sequence alignment. The CTC layer takes as an input the activations of LSTM and aligns the target labels with the inputs. The results were obtained at the character level for both cursive Urdu and non-cursive English scripts are significant and suggest that the BLSTM technique is potentially more useful than the existing OCR algorithms.\n"
            },
            "slug": "Evaluation-of-cursive-and-non-cursive-scripts-using-Ahmed-Naz",
            "title": {
                "fragments": [],
                "text": "Evaluation of cursive and non-cursive scripts using recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results were obtained at the character level for both cursive Urdu and non-cursive English scripts are significant and suggest that the BLSTM technique is potentially more useful than the existing OCR algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computing and Applications"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403326950"
                        ],
                        "name": "A. Ul-Hasan",
                        "slug": "A.-Ul-Hasan",
                        "structuredName": {
                            "firstName": "Adnan",
                            "lastName": "Ul-Hasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ul-Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3290619"
                        ],
                        "name": "M. A. Azawi",
                        "slug": "M.-A.-Azawi",
                        "structuredName": {
                            "firstName": "Mayce",
                            "lastName": "Azawi",
                            "middleNames": [
                                "Ibrahim",
                                "Ali",
                                "Al"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Azawi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7244356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f366434f68f9c6db2bda2043f94ff043e7d6d329",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15% (Fontane) and 1.47% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques."
            },
            "slug": "High-Performance-OCR-for-Printed-English-and-Using-Breuel-Ul-Hasan",
            "title": {
                "fragments": [],
                "text": "High-Performance OCR for Printed English and Fraktur Using LSTM Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition and these recognition accuracies were found without using any language modelling or any other post-processing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054232968"
                        ],
                        "name": "Riaz Ahmad",
                        "slug": "Riaz-Ahmad",
                        "structuredName": {
                            "firstName": "Riaz",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Riaz Ahmad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885312"
                        ],
                        "name": "Sheikh Faisal Rashid",
                        "slug": "Sheikh-Faisal-Rashid",
                        "structuredName": {
                            "firstName": "Sheikh",
                            "lastName": "Rashid",
                            "middleNames": [
                                "Faisal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheikh Faisal Rashid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10762166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb137f455738b7c872ed19befb29f8834c2f45b0",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical Character Recognition (OCR) of cursive scripts like Pashto and Urdu is difficult due the presence of complex ligatures and connected writing styles. In this paper, we evaluate and compare different approaches for the recognition of such complex ligatures. The approaches include Hidden Markov Model (HMM), Long Short Term Memory (LSTM) network and Scale Invariant Feature Transform (SIFT). Current state of the art in cursive script assumes constant scale without any rotation, while real world data contain rotation and scale variations. This research aims to evaluate the performance of sequence classifiers like HMM and LSTM and compare their performance with descriptor based classifier like SIFT. In addition, we also assess the performance of these methods against the scale and rotation variations in cursive script ligatures. Moreover, we introduce a database of 480,000 images containing 1000 unique ligatures or sub-words of Pashto. In this database, each ligature has 40 scale and 12 rotation variations. The evaluation results show a significantly improved performance of LSTM over HMM and traditional feature extraction technique such as SIFT."
            },
            "slug": "Scale-and-rotation-invariant-OCR-for-Pashto-cursive-Ahmad-Afzal",
            "title": {
                "fragments": [],
                "text": "Scale and rotation invariant OCR for Pashto cursive script using MDLSTM network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This research aims to evaluate the performance of sequence classifiers like HMM and LSTM and compare their performance with descriptor based classifier like SIFT and introduces a database of 480,000 images containing 1000 unique ligatures or sub-words of Pashto."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403434962"
                        ],
                        "name": "K. Collins-Thompson",
                        "slug": "K.-Collins-Thompson",
                        "structuredName": {
                            "firstName": "Kevyn",
                            "lastName": "Collins-Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Collins-Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Some of the methods combine both of the features [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Kevyn and Nickolov [7] used both the layout and the text features for matching the documents for retrieval."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1366361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29febec8e0415fb3ac4b64a269d2c3016f6d9b65",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "For text, audio, video, and still images, a number of projects have addressed the problem of estimating inter-object similarity and the related problem of finding transition, or \u2018segmentation\u2019 points in a stream of objects of the same media type. There has been relatively little work in this area for document images, which are typically text-intensive and contain a mixture of layout, text-based, and image features. Beyond simple partitioning, the problem of clustering related page images is also important, especially for information retrieval problems such as document image searching and browsing. Motivated by this, we describe a model for estimating inter-page similarity in ordered collections of document images, based on a combination of text and layout features. The features are used as input to a discriminative classifier, whose output is used in a constrained clustering criterion. We do a task-based evaluation of our method by applying it the problem of automatic document separation during batch scanning. Using layout and page numbering features, our algorithm achieved a separation accuracy of 95.6% on the test collection."
            },
            "slug": "A-Clustering-Based-Algorithm-for-Automatic-Document-Collins-Thompson",
            "title": {
                "fragments": [],
                "text": "A Clustering-Based Algorithm for Automatic Document Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A model for estimating inter-page similarity in ordered collections of document images, based on a combination of text and layout features is described, which achieves a separation accuracy of 95.6% on the test collection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403927620"
                        ],
                        "name": "Joan Pastor-Pellicer",
                        "slug": "Joan-Pastor-Pellicer",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Pastor-Pellicer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Pastor-Pellicer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145816817"
                        ],
                        "name": "M. J. Bleda",
                        "slug": "M.-J.-Bleda",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Bleda",
                            "middleNames": [
                                "Jos\u00e9",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bleda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2908640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47094349418843db7af76ee457655c5f5b018058",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel Convolutional Neural Network based method for the extraction of text lines, which consists of an initial Layout Analysis followed by the estimation of the Main Body Area (i.e., the text area between the baseline and the corpus line) for each text line. Finally, a region-based method using watershed transform is performed on the map of the Main Body Area for extracting the resulting lines. We have evaluated the new system on the IAM-HisDB, a publicly available dataset containing historical documents, outperforming existing learning-based text line extraction methods, which consider the problem as pixel labelling problem into text and non-text regions."
            },
            "slug": "Complete-System-for-Text-Line-Extraction-Using-and-Pastor-Pellicer-Afzal",
            "title": {
                "fragments": [],
                "text": "Complete System for Text Line Extraction Using Convolutional Neural Networks and Watershed Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel Convolutional Neural Network based method for the extraction of text lines, which consists of an initial Layout Analysis followed by the estimation of the Main Body Area for each text line, outperforming existing learning-based text line extraction methods."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069634816"
                        ],
                        "name": "J. Kumar",
                        "slug": "J.-Kumar",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449660"
                        ],
                        "name": "Peng Ye",
                        "slug": "Peng-Ye",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "Secondly, we use the Tobacco-3482 dataset [6] to evaluate the performance of the deep CNNs and to investigate to which extent transfer learning from the first dataset is applicable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 250
                            }
                        ],
                        "text": "Earlier methods that have been dealing with document classification focused mainly on either exploiting the structural similarity constraints [2], [3] or extracting features from the documents that may be able to help for document classification [4]\u2013[6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] and allows for a fair comparison with their approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] build a codebook of SURF descriptors extracted from training images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207329118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7ae643f8c1f987e6ba61f177a340e879a8644e0",
            "isKey": true,
            "numCitedBy": 71,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Structural-similarity-for-document-image-and-Kumar-Ye",
            "title": {
                "fragments": [],
                "text": "Structural similarity for document image classification and retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144338164"
                        ],
                        "name": "Siyuan Chen",
                        "slug": "Siyuan-Chen",
                        "structuredName": {
                            "firstName": "Siyuan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siyuan Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119287726"
                        ],
                        "name": "Yuan He",
                        "slug": "Yuan-He",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291081"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753831"
                        ],
                        "name": "S. Naoi",
                        "slug": "S.-Naoi",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Naoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Naoi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5] propose a method based on low-level image features to classify documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12462738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "887e5ebacf16eebe47a187ff1b1e45bc0d96e464",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the recent trend in using low level image features in classifying document images, in this paper we present a novel approach for structured document classification by matching the salient feature points between the query image and the reference images. Our method is robust to diverse training data size, image formats and qualities. Through matching the feature points, image registration is available for the query image as well. Although we aimed for the large domain of the structured document images, our method already achieved zero error rates in the tests on the benchmark NIST tax form databases."
            },
            "slug": "Structured-document-classification-by-matching-Chen-He",
            "title": {
                "fragments": [],
                "text": "Structured document classification by matching local salient features"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper presents a novel approach for structured document classification by matching the salient feature points between the query image and the reference images, which is robust to diverse training data size, image formats and qualities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403326950"
                        ],
                        "name": "A. Ul-Hasan",
                        "slug": "A.-Ul-Hasan",
                        "structuredName": {
                            "firstName": "Adnan",
                            "lastName": "Ul-Hasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ul-Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181206"
                        ],
                        "name": "Muhammad Zeshan Afzal",
                        "slug": "Muhammad-Zeshan-Afzal",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Afzal",
                            "middleNames": [
                                "Zeshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Zeshan Afzal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6238671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e76dcb66cdd9b59356d86bbfaf1a32f6e6b6cd9f",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel methodology for multiple script identification using Long Short-Term Memory (LSTM) networks' sequence-learning capabilities. Our method is able to identify multiple scripts at text-line level, where two or more scripts are present in the same text-line. Unlike traditional techniques, where either shape features or bounding boxes of individual characters are extracted, the LSTM-based system learns a particular script in a supervised learning framework. Moreover, this system neither needs specific features nor other preprocessing steps other than text-line extraction and text-line normalization. The proposed method works on text-line level, where it identifies each character as belonging to a particular script. We have developed a database consisting of English and Greek script, and our system achieved a script recognition accuracy of 98.186% on this dataset."
            },
            "slug": "A-sequence-learning-approach-for-multiple-script-Ul-Hasan-Afzal",
            "title": {
                "fragments": [],
                "text": "A sequence learning approach for multiple script identification"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A novel methodology for multiple script identification using Long Short-Term Memory networks' sequence-learning capabilities, which is able to identify multiple scripts at text-line level, where two or more scripts are present in the same text- line."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2187254"
                        ],
                        "name": "K. V. U. Reddy",
                        "slug": "K.-V.-U.-Reddy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Reddy",
                            "middleNames": [
                                "V.",
                                "Umamaheswara"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. U. Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Reddy and Govindaraju [25] used binary images for the classification of the documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26819227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ec134307c11bd35b69d3dfeed692bd85a406fa4",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of form classification is to assign a single-page form image to one of a set of predefined form types or classes. We classify the form images using low level pixel density information from the binary images of the documents. In this paper, we solve the form classification problem with a classifier based on the k-means algorithm, supported by adaptive boosting. Our classification method is tested on the NIST scanned tax forms data bases (special forms databases 2 and 6) which include machine-typed and handwritten documents. Our method improves the performance over published results on the same databases, while still using a simple set of image features."
            },
            "slug": "Form-classification-Reddy-Govindaraju",
            "title": {
                "fragments": [],
                "text": "Form classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper solves the form classification problem with a classifier based on the k-means algorithm, supported by adaptive boosting, and improves the performance over published results on the same databases, while still using a simple set of image features."
            },
            "venue": {
                "fragments": [],
                "text": "DRR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc. Recently, deep learning methods have also been exploited for document image classification [18]\u2013[20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Deep Learning has been used for many document analysis tasks such as binarization [8], [9], layout analysis [10], [11], Optical Character Recognition (OCR) [12]\u2013[17] etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5668166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fa5235e49fa6f16d047c999234d1b93df360b0",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0%, compared with 65.4% using a previously developed HMMbased recognition system."
            },
            "slug": "A-novel-approach-to-on-line-handwriting-recognition-Liwicki-Graves",
            "title": {
                "fragments": [],
                "text": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes using a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749498"
                        ],
                        "name": "Andrew D. Bagdanov",
                        "slug": "Andrew-D.-Bagdanov",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Bagdanov",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew D. Bagdanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Bagdanov and Worring [22] classify machine-printed documents by using the Attributed Relational Graphs (ARGs)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15780873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1546988d9874914a772084aafbb2da3636f08590",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We approach the general problem of classifying machine-printed documents into genres. Layout is a critical factor in recognizing fine-grained genres, as document content features are similar. Document genre is determined from the layout structure detected from scanned binary images of the document pages, using no OCR results and minimal a priori knowledge of document logical structures. Our method uses the attributed relational graphs (ARGs) to represent the layout structure of document instances, and the first order random graphs (FORGs) to represent document genres. In this paper we develop our FORG-based genre classification method and present a comparative evaluation between our technique and a variety of statistical pattern classifiers. FORGs are capable of modeling common layout structure within a document genre and are shown to significantly outperform traditional pattern classification techniques when fine-grained genre distinctions must be drawn."
            },
            "slug": "Fine-grained-document-genre-classification-using-Bagdanov-Worring",
            "title": {
                "fragments": [],
                "text": "Fine-grained document genre classification using first order random graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper develops a FORG-based genre classification method for machine-printed documents and presents a comparative evaluation between the technique and a variety of statistical pattern classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730636"
                        ],
                        "name": "Yungcheol Byun",
                        "slug": "Yungcheol-Byun",
                        "structuredName": {
                            "firstName": "Yungcheol",
                            "lastName": "Byun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yungcheol Byun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710749"
                        ],
                        "name": "Yillbyung Lee",
                        "slug": "Yillbyung-Lee",
                        "structuredName": {
                            "firstName": "Yillbyung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yillbyung Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Byun and Lee [2] used parts of the documents for the recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "Earlier methods that have been dealing with document classification focused mainly on either exploiting the structural similarity constraints [2], [3] or extracting features from the documents that may be able to help for document classification [4]\u2013[6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15262202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e919593a2990ec6020abda6bd658036f221bd5c",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to process many kinds of form document, form classification must be performed in the first place. However, a substantial amount of time is required to analyze and recognize such form documents according to the increase of t, lte number of registered models. To solve this problem, w,' suggest document understanding with a partial matchmg method in which form structure recognition and form (:lassification are performed for only some areas of the input form. In this case, the definition of the matching areas and ibrm classification are performed by using a DP matching algorithm. By using this approach, the recognition of an input form and the extraction of field items can be protessed at a high recognition rate in a reasonable time span. in terms of al)plicability, the experimental recognition rate and processing time seem to be encouraging."
            },
            "slug": "Form-classification-using-DP-matching-Byun-Lee",
            "title": {
                "fragments": [],
                "text": "Form classification using DP matching"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "By using this approach, the recognition of an input form and the extraction of field items can be protessed at a high recognition rate in a reasonable time span, and the experimental recognition rate and processing time seem to be encouraging."
            },
            "venue": {
                "fragments": [],
                "text": "SAC '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1941286"
                        ],
                        "name": "Christian K. Shin",
                        "slug": "Christian-K.-Shin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Shin",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian K. Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Shin and Doermann [3] proposed an approach that used layout structural similarity for full or partial image matching for retrieval."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "Earlier methods that have been dealing with document classification focused mainly on either exploiting the structural similarity constraints [2], [3] or extracting features from the documents that may be able to help for document classification [4]\u2013[6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8015543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67d64b16537107ccb81af1915b7b0b40b147300",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe issues related to the measurement of structural similarity between document images. We define structural similarity, and discuss the benefits of using it as a complement to content similarity for querying document image databases. We present an approach to computing a geometrically invariant structural similarity, and use this measure to search document image databases. Our approach supports both full image matching using query by example (QBE) and sub-image matching using query by sketch (QBS). The similarity measure considers spatial and layout structure, and is computed by aggregating content area overlap measures with respect to their underlying column structures. These techniques are tested within the Intelligent Document Image Retrieval (IDIR) System, and results demonstrating effectiveness and efficiency of structure queries with respect to human relevance judgments are presented."
            },
            "slug": "Document-Image-Retrieval-Based-on-Layout-Structural-Shin-Doermann",
            "title": {
                "fragments": [],
                "text": "Document Image Retrieval Based on Layout Structural Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An approach to computing a geometrically invariant structural similarity, and use this measure to search document image databases, and results demonstrating effectiveness and efficiency of structure queries with respect to human relevance judgments are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IPCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084270570"
                        ],
                        "name": "Guillaume Joutel",
                        "slug": "Guillaume-Joutel",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Joutel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Joutel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721326"
                        ],
                        "name": "V. Eglin",
                        "slug": "V.-Eglin",
                        "structuredName": {
                            "firstName": "V\u00e9ronique",
                            "lastName": "Eglin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Eglin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784847"
                        ],
                        "name": "S. Bres",
                        "slug": "S.-Bres",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Bres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739381"
                        ],
                        "name": "H. Emptoz",
                        "slug": "H.-Emptoz",
                        "structuredName": {
                            "firstName": "Hubert",
                            "lastName": "Emptoz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Emptoz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] proposed a method that used curvelet transformation for indexing and querying the documents at different image scales."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16683602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "443adf054bbf093cd5c90900d7f5594ff3ebb15c",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new use of the curvelet transform as a multiscale method for indexing linear singularities and curved handwritten shapes in documents images. As it belongs to the wavelet family, this representation can be useful at several scales of details. The proposed scheme for handwritten shape characterization targets to detect oriented and curved fragments at different scales so as to compose an unique signature for each handwritten analyzed samples. In this way, curvelets coefficients are used as a representation tool for handwriting when searching in large manuscripts databases by finding similar handwritten samples. Current results of ancient manuscripts retrieval are very promising with very satisfying precisions and recalls."
            },
            "slug": "Curvelets-Based-Queries-for-CBIR-Application-in-Joutel-Eglin",
            "title": {
                "fragments": [],
                "text": "Curvelets Based Queries for CBIR Application in Handwriting Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents a new use of the curvelet transform as a multiscale method for indexing linear singularities and curved handwritten shapes in documents images to compose an unique signature for each handwritten analyzed samples."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642099"
                        ],
                        "name": "F. Dubiel",
                        "slug": "F.-Dubiel",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dubiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dubiel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "An early classification of documents helps to process the subsequent processes in DIPP such as information extraction, text recognition etc [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Dengel and Dubiel [1] used layout structure printed documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19200855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ea4167f9eb9ad893be32ba35147fe4539b6cdd4",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a system which is capable of learning the presentation of document logical structures, exemplarily shown for business letters. Presenting a set of instances to the system, it clusters them into structural concepts and induces a concept hierarchy. This concept hierarchy is taken as a source for classifying future input. The paper introduces the different learning steps, describes how the resulting concept hierarchy is applied for logical labeling and reports on the results."
            },
            "slug": "Clustering-and-classification-of-document-machine-Dengel-Dubiel",
            "title": {
                "fragments": [],
                "text": "Clustering and classification of document structure-a machine learning approach"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A system capable of learning the presentation of document logical structures, exemplarily shown for business letters, is described, which clusters them into structural concepts and induces a concept hierarchy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844557"
                        ],
                        "name": "T. Kochi",
                        "slug": "T.-Kochi",
                        "structuredName": {
                            "firstName": "Tsukasa",
                            "lastName": "Kochi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kochi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066251026"
                        ],
                        "name": "T. Saitoh",
                        "slug": "T.-Saitoh",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Kochi and Saitoh [24] used textual descriptions of document images for information extraction from documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42577487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c37afa374165479c0789a8748bc47f3ce756a0bb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic document entry system is described that identifies the type of document and extracts textual information, such as titles or authors, from semi-formatted document images. The system registers documents, offers easy retrieval of documents used in a daily workflow analyzes the layout structure of documents by using document specific models, and assumes that each type of document is known in advance. In this paper we focus on a method for identifying the type of document."
            },
            "slug": "User-defined-template-for-identifying-document-type-Kochi-Saitoh",
            "title": {
                "fragments": [],
                "text": "User-defined template for identifying document type and extracting information from documents"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An automatic document entry system is described that identifies the type of document and extracts textual information, such as titles or authors, from semi-formatted document images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 21
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Cutting-the-Error-by-Half:-Investigation-of-Very-Afzal-K\u00f6lsch/3526555fa0178c101ee9896252c818f9e03532a5?sort=total-citations"
}