{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805398"
                        ],
                        "name": "Haibin Ling",
                        "slug": "Haibin-Ling",
                        "structuredName": {
                            "firstName": "Haibin",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haibin Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34713849"
                        ],
                        "name": "N. Ramanathan",
                        "slug": "N.-Ramanathan",
                        "structuredName": {
                            "firstName": "Narayanan",
                            "lastName": "Ramanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ramanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34734622"
                        ],
                        "name": "D. Jacobs",
                        "slug": "D.-Jacobs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "Our low-level features are designed following a great deal of work in face recognition (and the larger recognition community) which has identified gradient direction and local descriptors around fiducial features as effective first steps toward dealing with illumination [6, 28, 22, 23, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12287775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f1886482893adf2f61e3b344d5483c86792ab5",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor."
            },
            "slug": "A-Study-of-Face-Recognition-as-People-Age-Ling-Soatto",
            "title": {
                "fragments": [],
                "text": "A Study of Face Recognition as People Age"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes using the gradient orientation pyramid for face verification as a two-class problem and uses a support vector machine as a classifier and finds that the new descriptor yields a robust and discriminative representation."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996078"
                        ],
                        "name": "Neeraj Kumar",
                        "slug": "Neeraj-Kumar",
                        "structuredName": {
                            "firstName": "Neeraj",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neeraj Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We note that although a few are lower than [21], the images used in our system are not limited to only frontal poses (as in theirs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "We thus train several attribute classifiers, using an approach much like [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "More recently, a method for automatically training classifiers for these and many other types of attributes was proposed, for the purpose of searching databases of face images [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "We call these visual traits \u201cattributes,\u201d following the name and method of [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "(To handle the larger variation of pose in our data, we slightly enlarged the regions shown in [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "To extract low-level features, we follow the procedure described in [21], summarized here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 945967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d363d3343cb70ad757d472d80622e1a7c5982fd9",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We have created the first image search engine based entirely on faces. Using simple text queries such as \"smiling men with blond hair and mustaches,\" users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and computes all labels off-line, leading to fast on-line search performance. In addition, we describe how our system can be used for a number of applications, including law enforcement, social networks, and personal photo management. Our search engine will soon be made publicly available."
            },
            "slug": "FaceTracer:-A-Search-Engine-for-Large-Collections-Kumar-Belhumeur",
            "title": {
                "fragments": [],
                "text": "FaceTracer: A Search Engine for Large Collections of Images with Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The first image search engine based entirely on faces, which shows state-of-the-art classification results compared to previous works, and demonstrates the power of the architecture through a functional, large-scale face search engine."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3230391"
                        ],
                        "name": "A. Georghiades",
                        "slug": "A.-Georghiades",
                        "structuredName": {
                            "firstName": "Athinodoros",
                            "lastName": "Georghiades",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Georghiades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9234219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e",
            "isKey": false,
            "numCitedBy": 4696,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions."
            },
            "slug": "From-Few-to-Many:-Illumination-Cone-Models-for-Face-Georghiades-Belhumeur",
            "title": {
                "fragments": [],
                "text": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A generative appearance-based method for recognizing human faces under variation in lighting and viewpoint that exploits the fact that the set of images of an object in fixed pose but under all possible illumination conditions, is a convex cone in the space of images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756099"
                        ],
                        "name": "Tal Hassner",
                        "slug": "Tal-Hassner",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Hassner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Hassner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188620"
                        ],
                        "name": "Yaniv Taigman",
                        "slug": "Yaniv-Taigman",
                        "structuredName": {
                            "firstName": "Yaniv",
                            "lastName": "Taigman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaniv Taigman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "Not surprisingly, LFW has proven difficult for automatic face verification methods [25, 34, 17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Remarkably, both the attribute and simile classifiers give state-of-the-art results, reducing the previous best error rates [34] on LFW [19] by 23."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34], our features are designed to provide information about the identity of an individual in two ways: by recognizing describable attributes (attribute classifiers), and by recognizing similarity to a set of reference people (simile classifiers)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34], which uses a large set of carefully designed binary patch features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14166521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f28dfadba11bd3489d008827d9b1a539b34b50df",
            "isKey": true,
            "numCitedBy": 500,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning similarity between images have presented impressive results in the problem of pair matching (same/notsame classification) of face images. In this paper we explore how well this performance carries over to the related task of multi-option face identification, specifically on the Labeled Faces in the Wild (LFW) image set. In addition, we seek to compare the performance of similarity learning methods to descriptor based methods. We present the following results: (1) Descriptor-Based approaches that efficiently encode the appearance of each face image as a vector outperform the leading similarity based method in the task of multi-option face identification. (2) Straightforward use of Euclidean distance on the descriptor vectors performs somewhat worse than the similarity learning methods on the task of pair matching. (3) Adding a learning stage, the performance of descriptor based methods matches and exceeds that of similarity methods on the pair matching task. (4) A novel patch based descriptor we propose is able to improve the performance of the successful Local Binary Pattern (LBP) descriptor in both multi-option identification and same/not-same classification."
            },
            "slug": "Descriptor-Based-Methods-in-the-Wild-Wolf-Hassner",
            "title": {
                "fragments": [],
                "text": "Descriptor Based Methods in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper explores how well this performance carries over to the related task of multi-option face identification, specifically on the Labeled Faces in the Wild (LFW) image set, and seeks to compare the performance of similarity learning methods to descriptor based methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985062"
                        ],
                        "name": "Marwan A. Mattar",
                        "slug": "Marwan-A.-Mattar",
                        "structuredName": {
                            "firstName": "Marwan",
                            "lastName": "Mattar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marwan A. Mattar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404579703"
                        ],
                        "name": "Eric Learned-Miller",
                        "slug": "Eric-Learned-Miller",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Learned-Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "The Labeled Faces in the Wild (LFW) [19] data set consists of 13,233 images of 5,749 people, which are organized into 2 views \u2013 a development set of 2,200 pairs for training and 1,000 pairs for testing, on which to build models and choose features; and a 10-fold cross-validation set of 6,000 pairs, on which to evaluate final performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "Not surprisingly, LFW has proven difficult for automatic face verification methods [25, 34, 17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Remarkably, both the attribute and simile classifiers give state-of-the-art results, reducing the previous best error rates [34] on LFW [19] by 23."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Recently, there has been significant work on the \u201cLabeled Faces in the Wild\u201d (LFW) data set [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "\u2019s \u201cLabeled Faces in the Wild\u201d (LFW) benchmark data set [19] and similar data sets [2, 10], 3D alignment is difficult and has not (yet) been demonstrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 88166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
            "isKey": true,
            "numCitedBy": 4898,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version."
            },
            "slug": "Labeled-Faces-in-the-Wild:-A-Database-forStudying-Huang-Mattar",
            "title": {
                "fragments": [],
                "text": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life, and exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33731953"
                        ],
                        "name": "R. Gross",
                        "slug": "R.-Gross",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737918"
                        ],
                        "name": "J. Cohn",
                        "slug": "J.-Cohn",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cohn",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 164
                            }
                        ],
                        "text": "The Pose, Illumination, and Expression (PIE) data set and follow-on results [33] showed that sometimes alignment, especially in 3D, can overcome these difficulties [3, 4, 16, 33, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10478476,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "4d15254f6f31356963cc70319ce416d28d8924a3",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the past decade, major advances have occurred in face recognition. With few exceptions, however, most research has been limited to training and testing on frontal views. Little is known about the extent to which face pose, illumination, expression, occlusion, and individual differences, such as those associated with gender, influence recognition accuracy. We systematically varied these factors to test the performance of two leading algorithms, one template based and the other feature based. Image data consisted of over 21000 images from 3 publicly available databases: CMU PIE, Cohn-Kanade, and AR databases. In general, both algorithms were robust to variation in illumination and expression. Recognition accuracy was highly sensitive to variation in pose. For frontal training images, performance was attenuated beginning at about 15 degrees. Beyond about 30 degrees, performance became unacceptable. For non-frontal training images, fall off was more severe. Small but consistent differences were found for individual differences in subjects. These findings suggest direction for future research, including design of experiments and data collection."
            },
            "slug": "Quo-vadis-Face-Recognition-Gross-Shi",
            "title": {
                "fragments": [],
                "text": "Quo vadis Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Recognition accuracy was highly sensitive to variation in pose and fall off was more severe for non-frontal training images, while small but consistent differences were found for individual differences in subjects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5656478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b14d3375cb9c92e04a8bd9176bbaa4f536f29461",
            "isKey": false,
            "numCitedBy": 645,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonlinear support vector machines (SVMs) are investigated for appearance-based gender classification with low-resolution \"thumbnail\" faces processed from 1,755 images from the FERET (FacE REcognition Technology) face database. The performance of SVMs (3.4% error) is shown to be superior to traditional pattern classifiers (linear, quadratic, Fisher linear discriminant, nearest-neighbor) as well as more modern techniques, such as radial basis function (RBF) classifiers and large ensemble-RBF networks. Furthermore, the difference in classification performance with low-resolution \"thumbnails\" (21/spl times/12 pixels) and the corresponding higher-resolution images (84/spl times/48 pixels) was found to be only 1%, thus demonstrating robustness and stability with respect to scale and the degree of facial detail."
            },
            "slug": "Learning-Gender-with-Support-Faces-Moghaddam-Yang",
            "title": {
                "fragments": [],
                "text": "Learning Gender with Support Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Nonlinear support vector machines are investigated for appearance-based gender classification with low-resolution \"thumbnail\" faces processed from the FERET (FacE REcognition Technology) face database, demonstrating robustness and stability with respect to scale and the degree of facial detail."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113288329"
                        ],
                        "name": "A. Ferencz",
                        "slug": "A.-Ferencz",
                        "structuredName": {
                            "firstName": "Andras",
                            "lastName": "Ferencz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ferencz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 164
                            }
                        ],
                        "text": "Various 2D alignment strategies have been applied to LFW \u2013 aligning all faces [17] to each other, or aligning each pair of images to be considered for verification [25, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6162491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcd2e7d42b4c8ad3538205692a5899f3d040fd1b",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nObject identification is a specialized type of recognition in which the category (e.g.\u00a0cars) is known and the goal is to recognize an object\u2019s exact identity (e.g.\u00a0Bob\u2019s BMW). Two special challenges characterize object identification. First, inter-object variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. Second, there may be many different instances of the category but few or just one positive \u201ctraining\u201d examples per object instance. Because variation among object instances may be small, a solution must locate possibly subtle object-specific salient features, like a door handle, while avoiding distracting ones such as specular highlights. With just one training example per object instance, however, standard modeling and feature selection techniques cannot be used. We describe an on-line algorithm that takes one image from a known category and builds an efficient \u201csame\u201d versus \u201cdifferent\u201d classification cascade by predicting the most discriminative features for that object instance. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered sequence of discriminative features specific to the given image. Learned stopping thresholds make the identifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods.\n"
            },
            "slug": "Learning-to-Locate-Informative-Features-for-Visual-Ferencz-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Learning to Locate Informative Features for Visual Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An on-line algorithm is described that takes one image from a known category and builds an efficient \u201csame\u201d versus \u201cdifferent\u201d classification cascade by predicting the most discriminative features for that object instance, outperforms a wide variety of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50716462"
                        ],
                        "name": "Andrew C. Gallagher",
                        "slug": "Andrew-C.-Gallagher",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew C. Gallagher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40894914"
                        ],
                        "name": "Tsuhan Chen",
                        "slug": "Tsuhan-Chen",
                        "structuredName": {
                            "firstName": "Tsuhan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsuhan Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Gallagher and Chen [ 13 ] use estimates of age and gender to compute the likelihood of first names being associated with a particular face, but to our knowledge, no previous work has used attributes as features for face verification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8068500,
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "id": "8a84803e0f9821fb55e561068af2025a8eb6eb6d",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing people in images is one of the foremost challenges in computer vision. It is important to remember that consumer photography has a highly social aspect. The photographer captures images not in a random fashion, but rather to remember or document meaningful events in her life. The culture of the society of which the photographer is a part provides a strong context for recognizing the content of the captured images. We demonstrate one aspect of this cultural context by recognizing people from first names. The distribution of first names chosen for newborn babies evolves with time and is gender-specific. As a result, a first name provides a strong prior for describing the individual. Specifically, we use the U.S. Social Security Administration baby name database to learn priors for gender and age for 6693 first names. Most face recognition methods do not even consider the name of the individual of interest, or the name is treated merely as an identifier that provides no information about appearance. In contrast, we combine image-based gender and age classifiers with the cultural context information provided by first names to recognize people with no labeled examples. Our model uses image-based age and gender estimates for assigning first names to people and in turn, the age and gender estimates are improved."
            },
            "slug": "Estimating-age,-gender,-and-identity-using-first-Gallagher-Chen",
            "title": {
                "fragments": [],
                "text": "Estimating age, gender, and identity using first name priors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model uses image-based gender and age classifiers with the cultural context information provided by first names to recognize people with no labeled examples and in turn, the age and gender estimates are improved."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819046"
                        ],
                        "name": "Xuhui Shao",
                        "slug": "Xuhui-Shao",
                        "structuredName": {
                            "firstName": "Xuhui",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuhui Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "This was later extended to the recognition of ethnicity [32], pose [20], expression [1], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1902304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c4c61440cfceddf1b1521ee7abf04ed31b79f37",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an approach for the problem of face pose discrimination using support vector machines (SVM). Face pose discrimination means that one can label the face image as one of several known poses. Face images are drawn from the standard FERET database. The training set consists of 150 images equally distributed among frontal, approximately 33.75/spl deg/ rotated left and right poses, respectively, and the test set consists of 450 images again equally distributed among the three different types of poses. SVM achieved perfect accuracy-100%-discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or radial basis functions (RBF) as kernel approximation functions."
            },
            "slug": "Face-pose-discrimination-using-support-vector-(SVM)-Huang-Shao",
            "title": {
                "fragments": [],
                "text": "Face pose discrimination using support vector machines (SVM)"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper describes an approach for the problem of face pose discrimination using support vector machines (SVM), which achieved perfect accuracy-100%-discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or radial basis functions (RBF) as kernel approximation functions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064850418"
                        ],
                        "name": "E. Nowak",
                        "slug": "E.-Nowak",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nowak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nowak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "Not surprisingly, LFW has proven difficult for automatic face verification methods [25, 34, 17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 164
                            }
                        ],
                        "text": "Various 2D alignment strategies have been applied to LFW \u2013 aligning all faces [17] to each other, or aligning each pair of images to be considered for verification [25, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 216034155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05b2b57e388b218949628fd52bda33d36762446d",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose and evaluate an algorithm that learns a similarity measure for comparing never seen objects. The measure is learned from pairs of training images labeled \"same\" or \"different\". This is far less informative than the commonly used individual image labels (e.g., \"car model X\"), but it is cheaper to obtain. The proposed algorithm learns the characteristic differences between local descriptors sampled from pairs of \"same\" and \"different\" images. These differences are vector quantized by an ensemble of extremely randomized binary trees, and the similarity measure is computed from the quantized differences. The extremely randomized trees are fast to learn, robust due to the redundant information they carry and they have been proved to be very good clusterers. Furthermore, the trees efficiently combine different feature types (SIFT and geometry). We evaluate our innovative similarity measure on four very different datasets and consistently outperform the state-of-the-art competitive approaches."
            },
            "slug": "Learning-Visual-Similarity-Measures-for-Comparing-Nowak-Jurie",
            "title": {
                "fragments": [],
                "text": "Learning Visual Similarity Measures for Comparing Never Seen Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An algorithm that learns a similarity measure for comparing never seen objects that is fast to learn, robust due to the redundant information they carry and they have been proved to be very good clusterers is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "This was later extended to the recognition of ethnicity [32], pose [20], expression [1], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5425332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef1e122d4f87f48993c18f1d53d650bb3df946d9",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents progress toward an integrated, robust, real-time face detection and demographic analysis system. Faces are detected and extracted using the fast algorithm proposed by P. Viola and M.J. Jones (2001). Detected faces are passed to a demographic (gender and ethnicity) classifier which uses the same architecture as the face detector. This demographic classifier is extremely fast, and delivers error rates slightly better than the best-known classifiers. To counter the unconstrained and noisy sensing environment, demographic information is integrated across time for each individual. Therefore, the final demographic classification combines estimates from many facial detections in order to reduce the error rate. The entire system processes 10 frames per second on an 800-MHz Intel Pentium III."
            },
            "slug": "A-unified-learning-framework-for-real-time-face-and-Shakhnarovich-Viola",
            "title": {
                "fragments": [],
                "text": "A unified learning framework for real time face detection and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper presents progress toward an integrated, robust, real-time face detection and demographic analysis system and combines estimates from many facial detections in order to reduce the error rate."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400601690"
                        ],
                        "name": "A. O'Toole",
                        "slug": "A.-O'Toole",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "O'Toole",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145781156"
                        ],
                        "name": "F. Jiang",
                        "slug": "F.-Jiang",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2997101"
                        ],
                        "name": "Janet H. Ayyad",
                        "slug": "Janet-H.-Ayyad",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Ayyad",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet H. Ayyad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098732379"
                        ],
                        "name": "Nils Penard",
                        "slug": "Nils-Penard",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Penard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nils Penard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144319186"
                        ],
                        "name": "H. Abdi",
                        "slug": "H.-Abdi",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Abdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Abdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "To obtain this data, we followed the procedure of [27], but on Amazon Mechanical Turk, averaging the replies of 10 different users per pair to get smoothed estimates of average human performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7142774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7893c0ac5120b3a14ebfc5a4aad69562ce821c4",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been significant progress in improving the performance of computer-based face recognition algorithms over the last decade. Although algorithms have been tested and compared extensively with each other, there has been remarkably little work comparing the accuracy of computer-based face recognition systems with humans. We compared seven state-of-the-art face recognition algorithms with humans on a face-matching task. Humans and algorithms determined whether pairs of face images, taken under different illumination conditions, were pictures of the same person or of different people. Three algorithms surpassed human performance matching face pairs prescreened to be \"difficult\" and six algorithms surpassed humans on \"easy\" face pairs. Although illumination variation continues to challenge face recognition algorithms, current algorithms compete favorably with humans. The superior performance of the best algorithms over humans, in light of the absolute performance levels of the algorithms, underscores the need to compare algorithms with the best current control-humans."
            },
            "slug": "Face-Recognition-Algorithms-Surpass-Humans-Matching-O'Toole-Phillips",
            "title": {
                "fragments": [],
                "text": "Face Recognition Algorithms Surpass Humans Matching Faces Over Changes in Illumination"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Seven state-of-the-art face recognition algorithms are compared with humans on a face-matching task and three algorithms surpassed human performance matching face pairs prescreened to be \"difficult\" and six algorithms surpassed humans on \"easy\" face pairs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3293655"
                        ],
                        "name": "S. Romdhani",
                        "slug": "S.-Romdhani",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Romdhani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Romdhani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 164
                            }
                        ],
                        "text": "The Pose, Illumination, and Expression (PIE) data set and follow-on results [33] showed that sometimes alignment, especially in 3D, can overcome these difficulties [3, 4, 16, 33, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13175333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23fcdda2d12c982bcc0abb948ff835c370b8d75e",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach for recognizing faces in images taken from different directions and under different illumination. The method is based on a 3D morphable face model that encodes shape and texture in terms of model parameters, and an algorithm that recovers these parameters from a single image of a face. For face identification, we use the shape and texture parameters of the model that are separated from imaging parameters, such as pose and illumination. In addition to the identity, the system provides a measure of confidence. We report experimental results for more than 4000 images from the publicly available CMU-PIE database."
            },
            "slug": "Face-identification-across-different-poses-and-with-Blanz-Romdhani",
            "title": {
                "fragments": [],
                "text": "Face identification across different poses and illuminations with a 3D morphable model"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A novel approach for recognizing faces in images taken from different directions and under different illumination is presented, based on a 3D morphable face model that encodes shape and texture in terms of model parameters, and an algorithm that recovers these parameters from a single image of a face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246870"
                        ],
                        "name": "Vidit Jain",
                        "slug": "Vidit-Jain",
                        "structuredName": {
                            "firstName": "Vidit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vidit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "Not surprisingly, LFW has proven difficult for automatic face verification methods [25, 34, 17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Various 2D alignment strategies have been applied to LFW \u2013 aligning all faces [17] to each other, or aligning each pair of images to be considered for verification [25, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3028943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "592e329fd243f38c4cd1ff393af1f24f6a10e3b8",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Many recognition algorithms depend on careful positioning of an object into a canonical pose, so the position of features relative to a fixed coordinate system can be examined. Currently, this positioning is done either manually or by training a class-specialized learning algorithm with samples of the class that have been hand-labeled with parts or poses. In this paper, we describe a novel method to achieve this positioning using poorly aligned examples of a class with no additional labeling. Given a set of unaligned examplars of a class, such as faces, we automatically build an alignment mechanism, without any additional labeling of parts or poses in the data set. Using this alignment mechanism, new members of the class, such as faces resulting from a face detector, can be precisely aligned for the recognition process. Our alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images. We also demonstrate its use on an entirely different class of objects (cars), again without providing any information about parts or pose to the learning algorithm."
            },
            "slug": "Unsupervised-Joint-Alignment-of-Complex-Images-Huang-Jain",
            "title": {
                "fragments": [],
                "text": "Unsupervised Joint Alignment of Complex Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46548046"
                        ],
                        "name": "G. Littlewort",
                        "slug": "G.-Littlewort",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Littlewort",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Littlewort"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039025"
                        ],
                        "name": "I. Fasel",
                        "slug": "I.-Fasel",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Fasel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Fasel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "This was later extended to the recognition of ethnicity [32], pose [20], expression [1], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7376030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3dc3353c5c916a865eb3477f739cc21142430fd",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM's enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot interaction."
            },
            "slug": "Real-Time-Face-Detection-and-Facial-Expression-and-Bartlett-Littlewort",
            "title": {
                "fragments": [],
                "text": "Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel combination of Adaboost and SVM's enhances performance and the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner."
            },
            "venue": {
                "fragments": [],
                "text": "2003 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98902637"
                        ],
                        "name": "Wen Zhao",
                        "slug": "Wen-Zhao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "It is well understood that variation in pose and expression and, to a lesser extent, lighting cause significant difficulties for recognizing the identity of a person [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12331515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28312c3a47c1be3a67365700744d3d6665b86f22",
            "isKey": false,
            "numCitedBy": 6984,
            "numCiting": 418,
            "paperAbstract": {
                "fragments": [],
                "text": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered."
            },
            "slug": "Face-recognition:-A-literature-survey-Zhao-Chellappa",
            "title": {
                "fragments": [],
                "text": "Face recognition: A literature survey"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper provides an up-to-date critical survey of still- and video-based face recognition research, and categorizes existing recognition techniques but also presents detailed descriptions of representative methods within each category."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "Our low-level features are designed following a great deal of work in face recognition (and the larger recognition community) which has identified gradient direction and local descriptors around fiducial features as effective first steps toward dealing with illumination [6, 28, 22, 23, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586343"
                        ],
                        "name": "Carlos D. Castillo",
                        "slug": "Carlos-D.-Castillo",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Castillo",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos D. Castillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34734622"
                        ],
                        "name": "D. Jacobs",
                        "slug": "D.-Jacobs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 164
                            }
                        ],
                        "text": "The Pose, Illumination, and Expression (PIE) data set and follow-on results [33] showed that sometimes alignment, especially in 3D, can overcome these difficulties [3, 4, 16, 33, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5550493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1032c2c6a95f85d6465ffac7c1dfd2842a457f10",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose using stereo matching for 2-D face recognition across pose. We match one 2-D query image to one 2-D gallery image without performing 3-D reconstruction. Then the cost of this matching is used to evaluate the similarity of the two images. We show that this cost is robust to pose variations. To illustrate this idea we built a face recognition system on top of a dynamic programming stereo matching algorithm. The method works well even when the epipolar lines we use do not exactly fit the viewpoints. We have tested our approach on the PIE dataset. In all the experiments, our method demonstrates effective performance compared with other algorithms."
            },
            "slug": "Using-Stereo-Matching-for-2-D-Face-Recognition-Pose-Castillo-Jacobs",
            "title": {
                "fragments": [],
                "text": "Using Stereo Matching for 2-D Face Recognition Across Pose"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work built a face recognition system on top of a dynamic programming stereo matching algorithm and showed that the method works well even when the epipolar lines the authors use do not exactly fit the viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40756494"
                        ],
                        "name": "J. Metcalfe",
                        "slug": "J.-Metcalfe",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Metcalfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Metcalfe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 45129728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39fab4b94954d457d4491966d7e7970aa301b307",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The dimensionality of a set of 160 face images of 10 male and 10 female subjects is reduced from 4096 to 40 via an autoencoder network. The extracted features do not correspond to the features used in previous face recognition systems (Kanade, 1973), such as ratios of distances between facial elements. Rather, they are whole-face features we call holons. The holons are given to 1 and 2 layer back propagation networks that are trained to classify the input features for identity, feigned emotional state and gender. The automatically extracted holons provide a sufficient basis for all of the gender discriminations, 99% of the identity discriminations and several of the emotion discriminations among the training set. Network and human judgements of the emotions are compared, and it is found that the networks tend to confuse more distant emotions than humans do."
            },
            "slug": "EMPATH:-Face,-Emotion,-and-Gender-Recognition-Using-Cottrell-Metcalfe",
            "title": {
                "fragments": [],
                "text": "EMPATH: Face, Emotion, and Gender Recognition Using Holons"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The dimensionality of a set of 160 face images of 10 male and 10 female subjects is reduced from 4096 to 40 via an autoencoder network, and it is found that the networks tend to confuse more distant emotions than humans do."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118025322"
                        ],
                        "name": "Hansen F. Chen",
                        "slug": "Hansen-F.-Chen",
                        "structuredName": {
                            "firstName": "Hansen",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hansen F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34734622"
                        ],
                        "name": "D. Jacobs",
                        "slug": "D.-Jacobs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "Our low-level features are designed following a great deal of work in face recognition (and the larger recognition community) which has identified gradient direction and local descriptors around fiducial features as effective first steps toward dealing with illumination [6, 28, 22, 23, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9980512,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9cab8e79ee34b346fae514310ad0402cf5e3119c",
            "isKey": false,
            "numCitedBy": 360,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of determining functions of an image of an object that are insensitive to illumination changes. We first show that for an object with Lambertian reflectance there are no discriminative functions that are invariant to illumination. This result leads as to adopt a probabilistic approach in which we analytically determine a probability distribution for the image gradient as a function of the surface's geometry and reflectance. Our distribution reveals that the direction of the image gradient is insensitive to changes in illumination direction. We verify this empirically by constructing a distribution for the image gradient from more than 20 million samples of gradients in a database of 1,280 images of 20 inanimate objects taken under varying lighting condition. Using this distribution we develop an illumination insensitive measure of image comparison and test it on the problem of face recognition."
            },
            "slug": "In-search-of-illumination-invariants-Chen-Belhumeur",
            "title": {
                "fragments": [],
                "text": "In search of illumination invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An illumination insensitive measure of image comparison is developed and tested on the problem of face recognition by analytically determining a probability distribution for the image gradient as a function of the surface's geometry and reflectance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067071261"
                        ],
                        "name": "Terence Sim",
                        "slug": "Terence-Sim",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terence Sim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240967"
                        ],
                        "name": "Maan Bsat",
                        "slug": "Maan-Bsat",
                        "structuredName": {
                            "firstName": "Maan",
                            "lastName": "Bsat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maan Bsat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Making matters worse \u2013 at least for researchers in computer vision \u2013 is that the illumination direction, camera type, focus, resolution, and image compression are all almost certain to differ as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2091854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b89f0e4f43570688dd983813c9a3efa2fa7e7ebc",
            "isKey": true,
            "numCitedBy": 1630,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database."
            },
            "slug": "The-CMU-Pose,-Illumination,-and-Expression-(PIE)-Sim-Baker",
            "title": {
                "fragments": [],
                "text": "The CMU Pose, Illumination, and Expression (PIE) database"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Between October 2000 and December 2000, a database of over 40,000 facial images of 68 people was collected, using the CMU 3D Room to imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621438"
                        ],
                        "name": "B. Golomb",
                        "slug": "B.-Golomb",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Golomb",
                            "middleNames": [
                                "Alexandra"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Golomb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057547636"
                        ],
                        "name": "D. T. Lawrence",
                        "slug": "D.-T.-Lawrence",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. T. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "Automatically determining the gender of a face has been an active area of research since at least 1990 [15, 9], and includes more recent work [24] using Support Vector Machines (SVMs) [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11962383,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb",
            "isKey": false,
            "numCitedBy": 547,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30\u00d730 were compressed using a 900\u00d740\u00d7900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation \"SexNet\" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans."
            },
            "slug": "SEXNET:-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence",
            "title": {
                "fragments": [],
                "text": "SEXNET: A Neural Network Identifies Sex From Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars, and some SexNet errors mimicked those of humans."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Training requires a set of positive and negative example images for each attribute, and uses a simplified version of adaboost [12] to choose from the set of low-level features described in the previous section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067993"
                        ],
                        "name": "W. T. Scruggs",
                        "slug": "W.-T.-Scruggs",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Scruggs",
                            "middleNames": [
                                "Todd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Scruggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061326"
                        ],
                        "name": "W. Worek",
                        "slug": "W.-Worek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Worek",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Worek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1697087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd47a8ce453ad80f6c5c0857fe58caffc3502c7c",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of the face recognition grand challenge (FRGC) is to improve the performance of face recognition algorithms by an order of magnitude over the best results in face recognition vendor test (FRVT) 2002. The FRGC is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with a data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper presents preliminary results of the FRGC for all six experiments. The preliminary results indicate that significant progress has been made towards achieving the stated goals"
            },
            "slug": "Preliminary-Face-Recognition-Grand-Challenge-Phillips-Flynn",
            "title": {
                "fragments": [],
                "text": "Preliminary Face Recognition Grand Challenge Results"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Preliminary results of the face recognition grand challenge indicate that significant progress has been made towards achieving the stated goals."
            },
            "venue": {
                "fragments": [],
                "text": "7th International Conference on Automatic Face and Gesture Recognition (FGR06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143920486"
                        ],
                        "name": "F. Samaria",
                        "slug": "F.-Samaria",
                        "structuredName": {
                            "firstName": "Ferdinando",
                            "lastName": "Samaria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Samaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144024405"
                        ],
                        "name": "A. Harter",
                        "slug": "A.-Harter",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Harter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Harter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2153469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "294adb9b8dca839832836bee0f06e15be550aaaa",
            "isKey": false,
            "numCitedBy": 2542,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on face identification using continuous density Hidden Markov Models (HMMs) has shown that stochastic modelling can be used successfully to encode feature information. When frontal images of faces are sampled using top-bottom scanning, there is a natural order in which the features appear and this can be conveniently modelled using a top-bottom HMM. However, a top-bottom HMM is characterised by different parameters, the choice of which has so far been based on subjective intuition. This paper presents a set of experimental results in which various HMM parameterisations are analysed.<<ETX>>"
            },
            "slug": "Parameterisation-of-a-stochastic-model-for-human-Samaria-Harter",
            "title": {
                "fragments": [],
                "text": "Parameterisation of a stochastic model for human face identification"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents a set of experimental results in which various HMM parameterisations are analysed and shows that stochastic modelling can be used successfully to encode feature information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Applications of Computer Vision"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8300220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4aba56927d7841c0aaedf5c73d42ccfadd75124",
            "isKey": false,
            "numCitedBy": 649,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of automatically labelling appearances of characters in TV or film material. This is tremendously challenging due to the huge variation in imaged appearance of each character and the weakness and ambiguity of available annotation. However, we demonstrate that high precision can be achieved by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) automatic generation of time stamped character annotation by aligning subtitles and transcripts; (ii) strengthening the supervisory information by identifying when characters are speaking; (iii) using complementary cues of face matching and clothing matching to propose common annotations for face tracks. Results are presented on episodes of the TV series \u201cBuffy the Vampire Slayer\u201d."
            },
            "slug": "Hello!-My-name-is...-Buffy''-Automatic-Naming-of-in-Everingham-Sivic",
            "title": {
                "fragments": [],
                "text": "Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that high precision can be achieved by combining multiple sources of information, both visual and textual, by automatic generation of time stamped character annotation by aligning subtitles and transcripts."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059876008"
                        ],
                        "name": "Matthijs C. Dorst",
                        "slug": "Matthijs-C.-Dorst",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Dorst",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthijs C. Dorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "Our low-level features are designed following a great deal of work in face recognition (and the larger recognition community) which has identified gradient direction and local descriptors around fiducial features as effective first steps toward dealing with illumination [6, 28, 22, 23, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130535382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcae70dce393c1796d4f15c7b8bbf0ed6f468be1",
            "isKey": false,
            "numCitedBy": 15903,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images. These features can then be used to reliably match objects in diering images. The algorithm was rst proposed by Lowe [12] and further developed to increase performance resulting in the classic paper [13] that served as foundation for SIFT which has played an important role in robotic and machine vision in the past decade."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-Dorst",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The Scale-Invariant Feature Transform (or SIFT) algorithm is a highly robust method to extract and consequently match distinctive invariant features from images that can then be used to reliably match objects in diering images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 184
                            }
                        ],
                        "text": "Automatically determining the gender of a face has been an active area of research since at least 1990 [15, 9], and includes more recent work [24] using Support Vector Machines (SVMs) [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33434,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723059"
                        ],
                        "name": "H. Christensen",
                        "slug": "H.-Christensen",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "Christensen",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Making matters worse \u2013 at least for researchers in computer vision \u2013 is that the illumination direction, camera type, focus, resolution, and image compression are all almost certain to differ as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60199183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71400436e9731778e9fb3fc726c62cac80e92f7b",
            "isKey": true,
            "numCitedBy": 149,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated performance evaluation of range image segmentation algorithms training/test data partitioning for empirical performance evaluation analyzing PCA-based face recognition algorithms -eigenvector selection and distance measures design of a visual system for detecting natural events by the use of an independent visual estimate - a human fall detector task-based evaluation of image filtering within a class of geometry-driven-diffusion algorithms a comparative analysis of cross-correlation matching algorithms using a pyramidal resolution approach performance evaluation of medical image processing algorithms."
            },
            "slug": "Empirical-Evaluation-Methods-in-Computer-Vision-Christensen-Phillips",
            "title": {
                "fragments": [],
                "text": "Empirical Evaluation Methods in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Automated performance evaluation of range image segmentation algorithms training/test data partitioning for empirical performance evaluation analyzing PCA-based face recognition algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "Each attribute classifier is an SVM with an RBF kernel, trained using libsvm [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 961425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "isKey": false,
            "numCitedBy": 40077,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "slug": "LIBSVM:-A-library-for-support-vector-machines-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "LIBSVM: A library for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Gallagher and Chen [13] use estimates of age and gender to compute the likelihood of first names being associated with a particular face, but to our knowledge, no previous work has used attributes as features for face verification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating age"
            },
            "venue": {
                "fragments": [],
                "text": "gender, and identity using first name priors. CVPR"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "The Pose, Illumination, and Expression (PIE) data set and follow-on results [33] showed that sometimes alignment, especially in 3D, can overcome these difficulties [3, 4, 16, 33, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 252
                            }
                        ],
                        "text": "These manifold differences in images of the same person have confounded methods for automatic face recognition and verification, often limiting the reliability of automatic algorithms to the domain of more controlled settings with cooperative subjects [33, 3, 29, 16, 30, 31, 14]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CMU Pose"
            },
            "venue": {
                "fragments": [],
                "text": "Illumination, and Expression (PIE) database. FGR"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "Automatically determining the gender of a face has been an active area of research since at least 1990 [15, 9], and includes more recent work [24] using Support Vector Machines (SVMs) [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empath: face"
            },
            "venue": {
                "fragments": [],
                "text": "emotion, and gender recognition using holons. In NIPS, pages 564\u2013571"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Automatically determining the gender of a face has been an active area of research since at least 1990 [15, 9], and includes more recent work [24] using Support Vector Machines (SVMs) [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning gender with support"
            },
            "venue": {
                "fragments": [],
                "text": "faces. PAMI,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "Not surprisingly, LFW has proven difficult for automatic face verification methods [25, 34, 17, 18, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LFW results using a combined Nowak plus MERL recognizer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "For further testing across pose, illumination, and expression, we introduce a new data set \u2013 termed PubFig \u2013 of real-world images of public figures (celebrities and politicians) acquired from the internet."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learned-Miller. LFW results using a combined Nowak plus MERL recognizer"
            },
            "venue": {
                "fragments": [],
                "text": "Learned-Miller. LFW results using a combined Nowak plus MERL recognizer"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "In this paper, we attempt to advance the state-of-theart for face verification in uncontrolled settings with noncooperative subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "search of illumination invariants. CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "search of illumination invariants. CVPR"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Attribute-and-simile-classifiers-for-face-Kumar-Berg/d3046251ec5d6e7f90ef5ef2b0ac885c01138555?sort=total-citations"
}