{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Those results are further generalized and presented with an empirical convergence theorem in the parallel COLT paper [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2789515,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7edde67273c2ad09458d73328628f3385d0df837",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In the machine learning community it is generally believed that graph Laplacians corresponding to a finite sample of data points converge to a continuous Laplace operator if the sample size increases. Even though this assertion serves as a justification for many Laplacian-based algorithms, so far only some aspects of this claim have been rigorously proved. In this paper we close this gap by establishing the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator. Our investigation also includes the important case where the data lies on a submanifold of R d ."
            },
            "slug": "From-Graphs-to-Manifolds-Weak-and-Strong-Pointwise-Hein-Audibert",
            "title": {
                "fragments": [],
                "text": "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper establishes the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805915"
                        ],
                        "name": "A. Singer",
                        "slug": "A.-Singer",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "Further generalization and an empirical convergence result was presented in [16] and an improved rate in [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6065268,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d0c7de463bfaaacaecc39c13644041c011f4beac",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-graph-to-manifold-Laplacian:-The-convergence-Singer",
            "title": {
                "fragments": [],
                "text": "From graph to manifold Laplacian: The convergence rate"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118947079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0b74dd2397001588673891771de6c221fb91a894",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis discusses the general problem of learning a function on a manifold given by data points. The space of functions on a Riemannian manifold has a family of smoothness functionals and a canonical basis associated to the Laplace-Beltrami operator. Moreover, the Laplace-Beltrami operator can be reconstructed with certain convergence guarantees when the manifold is only known through the sampled data points. This allows the techniques of regularization and Fourier analysis to be applied to functions defined on data. A convergence result is proved for the case when data is sampled from a compact submanifold of R\u2227k . Several applications are considered."
            },
            "slug": "Problems-of-learning-on-manifolds-Niyogi-Belkin",
            "title": {
                "fragments": [],
                "text": "Problems of learning on manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 103
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14879317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88816ae492956f3004daa41357166f1181c0c1bf",
            "isKey": false,
            "numCitedBy": 7047,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed."
            },
            "slug": "Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a geometrically motivated algorithm for representing the high-dimensional data that provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684296"
                        ],
                        "name": "C. Grimes",
                        "slug": "C.-Grimes",
                        "structuredName": {
                            "firstName": "Carrie",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Grimes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103664548"
                        ],
                        "name": "Hessian Eigenmaps",
                        "slug": "Hessian-Eigenmaps",
                        "structuredName": {
                            "firstName": "Hessian",
                            "lastName": "Eigenmaps",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hessian Eigenmaps"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6618760,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ac43625772e994b5265e5c6c1961de58ac3d6014",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method to recover the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based Locally Linear Embedding (HLLE), derives from a conceptual framework of Local Isometry in which the manifold M , viewed as a Riemannian submanifold of the ambient Euclidean space Rn, is locally isometric to an open, connected subset \u0398 of Euclidean space Rd. Since \u0398 does not have to be convex, this framework is able to handle a significantly wider class of situations than the original Isomap algorithm. The theoretical framework revolves around a quadratic form H(f) = \u222b M ||Hf (m)|| 2 F dm defined on functions f : M 7\u2192 R. Here Hf denotes the Hessian of f , and H(f) averages the Frobenius norm of the Hessian over M . To define the Hessian, we use orthogonal coordinates on the tangent planes of M . The key observation is that, if M truly is locally isometric to an open connected subset of Rd, then H(f) has a (d+1)-dimensional nullspace, consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of the Locally Linear Embedding and our theoretical framework as a modification of the Laplacian Eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian."
            },
            "slug": "Hessian-Eigenmaps-:-new-locally-linear-embedding-Donoho-Grimes",
            "title": {
                "fragments": [],
                "text": "Hessian Eigenmaps : new locally linear embedding techniques for high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Hessian-based Locally Linear Embedding (HLLE) derives from a conceptual framework of Local Isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space Rn, is locally isometric to an open, connected subset \u0398 of Euclidan space Rd."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144243543"
                        ],
                        "name": "S. Rosenberg",
                        "slug": "S.-Rosenberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rosenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118250915,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "32a00effe852d527067648d82f35b58c1ecf6de8",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter we will generalize the Laplacian on Euclidean space to an operator on differential forms on a Riemannian manifold. By a Riemannian manifold, we roughly mean a manifold equipped with a method for measuring lengths of tangent vectors, and hence of curves. Throughout this text, we will concentrate on studying the heat flow associated to these Laplacians. The main result of this chapter, the Hodge theorem, states that the long time behavior of the heat flow is controlled by the topology of the manifold. In \u00a71.1, the basic examples of heat flow on the one dimensional manifolds S 1 and R are studied. The heat flow on the circle already contains the basic features of heat flow on a compact manifold, although the circle is too simple topologically and geometrically to really reveal the information contained in the heat flow. In contrast, heat flow on R is more difficult to study, which indicates why we will restrict attention to compact manifolds. In \u00a71.2, we introduce the notion of a Riemannian metric on a manifold, define the spaces of L 2 functions and forms on a manifold with a Riemannian metric, and introduce the Laplacian associated to the metric. The Hodge theorem is proved in \u00a71.3 by heat equation methods. The kernel of the Laplacian on forms is isomorphic to the de Rham cohomology groups, and hence is a topological invariant. The de Rham cohomology groups are discussed in \u00a71.4, and the isomorphism between the kernel of the Laplacian and de Rham cohomology is shown in \u00a71.5."
            },
            "slug": "The-Laplacian-on-a-Riemannian-Manifold:-The-on-a-Rosenberg",
            "title": {
                "fragments": [],
                "text": "The Laplacian on a Riemannian Manifold: The Laplacian on a Riemannian Manifold"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 224
                            }
                        ],
                        "text": "Manifold methods have become increasingly important and popular in machine learning and have seen numerous recent applications in data analysis including dimensionality reduction, visualization, clustering and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6789724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a49f2d906b48a36ab4baca448298666a9ec259",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the sub-manifold in question rather than the total ambient space. Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once a basis is obtained, training can be performed using the labeled data set. Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian. Practical applications to image and text classification are considered."
            },
            "slug": "Using-manifold-structure-for-partially-labelled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using manifold structure for partially labelled classification"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner under the assumption that the data lie on a submanifold in a high dimensional space is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34911188"
                        ],
                        "name": "S. Smale",
                        "slug": "S.-Smale",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48760197"
                        ],
                        "name": "S. Weinberger",
                        "slug": "S.-Weinberger",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Weinberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "However in almost all modeling situations, one does not have access to the underlying manifold but instead approximates it from a point cloud."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1788129,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "345d254795b1aa41c9cb38bae12a580ebec7e8e1",
            "isKey": false,
            "numCitedBy": 589,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high-dimensional spaces. We consider the case where data are drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. We show how to \u201clearn\u201d the homology of the submanifold with high confidence. We discuss an algorithm to do this and provide learning-theoretic complexity bounds. Our bounds are obtained in terms of a condition number that limits the curvature and nearness to self-intersection of the submanifold. We are also able to treat the situation where the data are \u201cnoisy\u201d and lie near rather than on the submanifold in question."
            },
            "slug": "Finding-the-Homology-of-Submanifolds-with-High-Niyogi-Smale",
            "title": {
                "fragments": [],
                "text": "Finding the Homology of Submanifolds with High Confidence from\u00a0Random\u00a0Samples"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work considers the case where data are drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space and shows how to \u201clearn\u201d the homology of the sub manifold with high confidence."
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Comput. Geom."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5945304"
                        ],
                        "name": "I. Holopainen",
                        "slug": "I.-Holopainen",
                        "structuredName": {
                            "firstName": "Ilkka",
                            "lastName": "Holopainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Holopainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4067798,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "727db858887077fba3a710626e1ff50d679f799c",
            "isKey": false,
            "numCitedBy": 6085,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "THE recent physical interpretation of intrinsic differential geometry of spaces has stimulated the study of this subject. Riemann proposed the generalisation, to spaces of any order, of Gauss's theory of surfaces, and introduced certain fundamental ideas in this general theory. Bianchi, Beltrami, and others made substantial contributions to the subject, which was extended by Ricci with the use of tensor analysis and his absolute calculus. Recently there has been an extensive study and development of Riemannian geometry, and the book before us aims at presenting the existing theory.Riemannian Geometry.By Prof. L. P. Eisenhart. Pp. vii + 262. (Princeton: Princeton University Press; London: Oxford University Press, 1926.) 13s. 6d. net."
            },
            "slug": "Riemannian-Geometry-Holopainen",
            "title": {
                "fragments": [],
                "text": "Riemannian Geometry"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1927
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "Manifold methods have become increasingly important and popular in machine learning and have seen numerous recent applications in data analysis including dimensionality reduction, visualization, clustering and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5525836,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6320770fe216ebbba769b9f0a006669b616a03d0",
            "isKey": false,
            "numCitedBy": 889,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-Kernels-on-Graphs-and-Other-Discrete-Kondor-Lafferty",
            "title": {
                "fragments": [],
                "text": "Diffusion Kernels on Graphs and Other Discrete Input Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea, and focuses on generating kernels on graphs, for which a special class of exponential kernels called diffusion kernels are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684296"
                        ],
                        "name": "C. Grimes",
                        "slug": "C.-Grimes",
                        "structuredName": {
                            "firstName": "Carrie",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Grimes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1810410,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "57a66ac4a4e0a00d2cdee8711ce0a18b49e9f7a2",
            "isKey": false,
            "numCitedBy": 1589,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space \u211dn, is locally isometric to an open, connected subset \u0398 of Euclidean space \u211dd. Because \u0398 does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form \u210b(f) = \u222bM\u2009\u2225Hf(m)\u2225\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{equation*}{\\mathrm{_{{\\mathit{F}}}^{2}}}\\end{equation*}\\end{document}dm defined on functions f : M \u21a6 \u211d. Here Hf denotes the Hessian of f, and \u210b(f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of \u211dd, then \u210b(f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian."
            },
            "slug": "Hessian-eigenmaps:-Locally-linear-embedding-for-Donoho-Grimes",
            "title": {
                "fragments": [],
                "text": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Hessian-based locally linear embedding method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space is described, where the isometric coordinates can be recovered up to a linear isometry."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13983,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37805393"
                        ],
                        "name": "S. Lafon",
                        "slug": "S.-Lafon",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Lafon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lafon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107734495"
                        ],
                        "name": "A. B. Lee",
                        "slug": "A.-B.-Lee",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Lee",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. B. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34207023"
                        ],
                        "name": "M. Maggioni",
                        "slug": "M.-Maggioni",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Maggioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maggioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786884"
                        ],
                        "name": "B. Nadler",
                        "slug": "B.-Nadler",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Nadler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Nadler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49818480"
                        ],
                        "name": "F. Warner",
                        "slug": "F.-Warner",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Warner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Warner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15926341,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01b24de15cf337c55b9866c4b534596ca3d93abe",
            "isKey": false,
            "numCitedBy": 1370,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis."
            },
            "slug": "Geometric-diffusions-as-a-tool-for-harmonic-and-of-Coifman-Lafon",
            "title": {
                "fragments": [],
                "text": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 162
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3711,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this paper we take the first steps towards a theoretical foundation for manifold-based methods in learning, by showing that under certain conditions the graph Laplacian is directly related to the manifold Laplace-Beltrami operator and converges to it as the amount of data goes to infinity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88517984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6214fc0672de4ccfce1cef8b2d1875e6ea7a3db7",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Consistency is a key property of all statistical procedures analyzing randomly sampled data. Surprisingly, despite decades of work, little is known about consistency of most clustering algorithms. In this paper we investigate consistency of the popular family of spectral clustering algorithms, which clusters the data with the help of eigenvectors of graph Laplacian matrices. We develop new methods to establish that, for increasing sample size, those eigenvectors converge to the eigenvectors of certain limit operators. As a result, we can prove that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other (unnormalized clustering) is only consistent under strong additional assumptions, which are not always satisfied in real data. We conclude that our analysis provides strong evidence for the superiority of normalized spectral clustering."
            },
            "slug": "Consistency-of-spectral-clustering-Luxburg-Belkin",
            "title": {
                "fragments": [],
                "text": "Consistency of spectral clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other is only consistent under strong additional assumptions, which are not always satisfied in real data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 232
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18764978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c02dfd94b11933093c797c362e2f8f6a3b9b8012",
            "isKey": false,
            "numCitedBy": 8412,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite many empirical successes of spectral clustering methods\u2014 algorithms that cluster points using eigenvectors of matrices derived from the data\u2014there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems."
            },
            "slug": "On-Spectral-Clustering:-Analysis-and-an-algorithm-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Spectral Clustering: Analysis and an algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A simple spectral clustering algorithm that can be implemented using a few lines of Matlab is presented, and tools from matrix perturbation theory are used to analyze the algorithm, and give conditions under which it can be expected to do well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 141
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed for various problems and applications, including [26,32,3,14,12] for visualization and data representation, [38,34,37,10,27,5,2,31] for partially supervised classification and [30,33,28,21,17], among others, for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12184,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "Manifold methods have become increasingly important and popular in machine learning and have seen numerous recent applications in data analysis including dimensionality reduction, visualization, clustering and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7326173,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "60de4b6068407defa3c88f5feeb8b74d8e55fe9c",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators."
            },
            "slug": "Kernels-and-Regularization-on-Graphs-Smola-Kondor",
            "title": {
                "fragments": [],
                "text": "Kernels and Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators and can be found as a special case of the reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39651651"
                        ],
                        "name": "Jean-Fran\u00e7ois Paiement",
                        "slug": "Jean-Fran\u00e7ois-Paiement",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Paiement",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Paiement"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "A discussion of various spectral methods and their out-of-sample extensions is given in [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6894357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f39fe2659603f4194fd638d1a2e17985415c3bb",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data."
            },
            "slug": "Out-of-Sample-Extensions-for-LLE,-Isomap,-MDS,-and-Bengio-Paiement",
            "title": {
                "fragments": [],
                "text": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for extending Local Linear Embedding, Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling as well as for Spectral Clustering is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461956"
                        ],
                        "name": "S. Teng",
                        "slug": "S.-Teng",
                        "structuredName": {
                            "firstName": "Shang-Hua",
                            "lastName": "Teng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14861472,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3868293dee5b1dff83650bea83054b35c2e2ade8",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral partitioning methods use the Fiedler vector-the eigenvector of the second-smallest eigenvalue of the Laplacian matrix-to find a small separator of a graph. These methods are important components of many scientific numerical algorithms and have been demonstrated by experiment to work extremely well. In this paper, we show that spectral partitioning methods work well on bounded-degree planar graphs and finite element meshes-the classes of graphs to which they are usually applied. While active spectral bisection does not necessarily work, we prove that spectral partitioning techniques can be used to produce separators whose ratio of vertices removed to edges cut is O(/spl radic/n) for bounded-degree planar graphs and two-dimensional meshes and O(n/sup 1/d/) for well-shaped d-dimensional meshes. The heart of our analysis is an upper bound on the second-smallest eigenvalues of the Laplacian matrices of these graphs: we prove a bound of O(1/n) for bounded-degree planar graphs and O(1/n/sup 2/d/) for well-shaped d-dimensional meshes."
            },
            "slug": "Spectral-partitioning-works:-planar-graphs-and-Spielman-Teng",
            "title": {
                "fragments": [],
                "text": "Spectral partitioning works: planar graphs and finite element meshes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proved that spectral partitioning techniques can be used to produce separators whose ratio of vertices removed to edges cut is O(/spl radic/n) for bounded-degree planar graphs and two-dimensional meshes and O(n/sup 1/d/) for well-shaped d-dimensional mesh."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 37th Conference on Foundations of Computer Science"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144110793"
                        ],
                        "name": "O. Smolyanov",
                        "slug": "O.-Smolyanov",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Smolyanov",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Smolyanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101120550"
                        ],
                        "name": "H. Weizs\u00e4cker",
                        "slug": "H.-Weizs\u00e4cker",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Weizs\u00e4cker",
                            "middleNames": [
                                "v."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weizs\u00e4cker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737208"
                        ],
                        "name": "O. Wittich",
                        "slug": "O.-Wittich",
                        "structuredName": {
                            "firstName": "Olaf",
                            "lastName": "Wittich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Wittich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14801435,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b04616f663f4b393a6381ee8b6cfbccd94217e79",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $(S(t))_{t \\ge 0}$ be a one-parameter family of positive integral operators on a locally compact space $L$. For a possibly non-uniform partition of $[0,1]$ define a finite measure on the path space $C_L[0,1]$ by using a) $S(\\Delta t)$ for the transition between any two consecutive partition times of distance $\\Delta t$ and b) a suitable continuous interpolation scheme (e.g. Brownian bridges or geodesics). If necessary normalize the result to get a probability measure. We prove a version of Chernoff's theorem of semigroup theory and tightness results which yield convergence in law of such measures as the partition gets finer. In particular let $L$ be a closed smooth submanifold of a manifold $M$. We prove convergence of Brownian motion on $M$, conditioned to visit $L$ at all partition times, to a process on $L$ whose law has a density with respect to Brownian motion on $L$ which contains scalar, mean and sectional curvatures terms. Various approximation schemes for Brownian motion on $L$ are also given."
            },
            "slug": "Chernoff's-Theorem-and-Discrete-Time-Approximations-Smolyanov-Weizs\u00e4cker",
            "title": {
                "fragments": [],
                "text": "Chernoff's Theorem and Discrete Time Approximations of Brownian Motion on Manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3118640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e0d11533c411e3c0559762e7cfc6790c28ccf2b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations."
            },
            "slug": "Measure-Based-Regularization-Bousquet-Chapelle",
            "title": {
                "fragments": [],
                "text": "Measure Based Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes three theoretical methods for taking into account this distribution P(x) for regularization and provides links to existing graph-based semi-supervised learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841861"
                        ],
                        "name": "F. M\u00e9moli",
                        "slug": "F.-M\u00e9moli",
                        "structuredName": {
                            "firstName": "Facundo",
                            "lastName": "M\u00e9moli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. M\u00e9moli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699339"
                        ],
                        "name": "G. Sapiro",
                        "slug": "G.-Sapiro",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Sapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "Some of the recent work includes estimating geometric invariants of the manifold, such as homology [31, 20], geodesic distances [6], and comparing point clouds using Gromov-Hausdorff distance [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207156533,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c7f1b341b729b33e2bbc6f169176c331c2646fd2",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Point clouds are one of the most primitive and fundamental surface representations. A popular source of point clouds are three dimensional shape acquisition devices such as laser range scanners. Another important field where point clouds are found is in the representation of high-dimensional manifolds by samples. With the increasing popularity and very broad applications of this source of data, it is natural and important to work directly with this representation, without having to go to the intermediate and sometimes impossible and distorting steps of surface reconstruction. A geometric framework for comparing manifolds given by point clouds is presented in this paper. The underlying theory is based on Gromov-Hausdorff distances, leading to isometry invariant and completely geometric comparisons. This theory is embedded in a probabilistic setting as derived from random sampling of manifolds, and then combined with results on matrices of pairwise geodesic distances to lead to a computational implementation of the framework. The theoretical and computational results here presented are complemented with experiments for real three dimensional shapes."
            },
            "slug": "Comparing-point-clouds-M\u00e9moli-Sapiro",
            "title": {
                "fragments": [],
                "text": "Comparing point clouds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A geometric framework for comparing manifolds given by point clouds is presented in this paper and the underlying theory is based on Gromov-Hausdorff distances, leading to isometry invariant and completely geometric comparisons."
            },
            "venue": {
                "fragments": [],
                "text": "SGP '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 232
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 864066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "023625e7a9a6ab7ba6fc12e3414a18e96bdf0386",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features."
            },
            "slug": "Learning-Segmentation-by-Random-Walks-Meil\u0103-Shi",
            "title": {
                "fragments": [],
                "text": "Learning Segmentation by Random Walks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation and proves that the Normalized Cut method arises naturally from the framework."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 162
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805915"
                        ],
                        "name": "A. Singer",
                        "slug": "A.-Singer",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12282327,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9aaf09b316740122149f9a615a882d98b71246e1",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spectral-independent-component-analysis-Singer",
            "title": {
                "fragments": [],
                "text": "Spectral independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143683314"
                        ],
                        "name": "A. Zomorodian",
                        "slug": "A.-Zomorodian",
                        "structuredName": {
                            "firstName": "Afra",
                            "lastName": "Zomorodian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zomorodian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40119133"
                        ],
                        "name": "G. Carlsson",
                        "slug": "G.-Carlsson",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "Carlsson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "Some of the recent work includes estimating geometric invariants of the manifold, such as homology [31, 20], geodesic distances [6], and comparing point clouds using Gromov-Hausdorff distance [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 187674,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c11e55c089deaefb1fad4b0936921dc94862bab7",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe show that the persistent homology of a filtered d-dimensional\nsimplicial complex is simply the standard homology of a \nparticular graded module over a polynomial ring.\nOur analysis establishes the existence of a simple description of\npersistent homology groups over arbitrary fields.\nIt also enables us to derive a natural\nalgorithm for computing persistent homology of spaces in \narbitrary dimension over any field.\nThis result generalizes and extends the previously known\nalgorithm that was restricted to subcomplexes of S3 and\nZ2 coefficients.\nFinally, our study implies the lack of a simple \nclassification over non-fields.\nInstead, we give an algorithm for computing individual\npersistent homology groups over an arbitrary principal ideal domain \nin any dimension.\n\n"
            },
            "slug": "Computing-Persistent-Homology-Zomorodian-Carlsson",
            "title": {
                "fragments": [],
                "text": "Computing Persistent Homology"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The analysis establishes the existence of a simple description of persistent homology groups over arbitrary fields and derives an algorithm for computing individual persistent homological groups over an arbitrary principal ideal domain in any dimension."
            },
            "venue": {
                "fragments": [],
                "text": "SCG '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082836538"
                        ],
                        "name": "Evarist Gin'e",
                        "slug": "Evarist-Gin'e",
                        "structuredName": {
                            "firstName": "Evarist",
                            "lastName": "Gin'e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evarist Gin'e"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this paper we take the first steps towards a theoretical foundation for manifold-based methods in learning, by showing that under certain conditions the graph Laplacian is directly related to the manifold Laplace-Beltrami operator and converges to it as the amount of data goes to infinity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10336721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4668cdf5dcf787e37c4bfa02755ac633229c7c8c",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Let ${M}$ be a compact Riemannian submanifold of ${{\\bf R}^m}$ of dimension $\\scriptstyle{d}$ and let ${X_1,...,X_n}$ be a sample of i.i.d. points in ${M}$ with uniform distribution. We study the random operators $$ \\Delta_{h_n,n}f(p):=\\frac{1}{nh_n^{d+2}}\\sum_{i=1}^n K(\\frac{p-X_i}{h_n})(f(X_i)-f(p)), p\\in M $$ where ${K(u):={\\frac{1}{(4\\pi)^{d/2}}}e^{-\\|u\\|^2/4}}$ is the Gaussian kernel and ${h_n\\to 0}$ as ${n\\to\\infty.}$ Such operators can be viewed as graph laplacians (for a weighted graph with vertices at data points) and they have been used in the machine learning literature to approximate the Laplace-Beltrami operator of ${M,}$ ${\\Delta_Mf}$ (divided by the Riemannian volume of the manifold). We prove several results on a.s. and distributional convergence of the deviations ${\\Delta_{h_n,n}f(p)-{\\frac{1}{|\\mu|}}\\Delta_Mf(p)}$ for smooth functions ${f}$ both pointwise and uniformly in ${f}$ and ${p}$ (here ${|\\mu|=\\mu(M)}$ and ${\\mu}$ is the Riemannian volume measure). In particular, we show that for any class ${{\\cal F}}$ of three times differentiable functions on ${M}$ with uniformly bounded derivatives $$ \\sup_{p\\in M}\\sup_{f\\in F}\\Big|\\Delta_{h_n,p}f(p)-\\frac{1}{|\\mu|}\\Delta_Mf(p)\\Big|= O\\Big(\\sqrt{\\frac{\\log(1/h_n)}{nh_n^{d+2}}}\\Big) a.s. $$ as soon as $$ nh_n^{d+2}/\\log h_n^{-1}\\to \\infty and nh^{d+4}_n/\\log h_n^{-1}\\to 0, $$ and also prove asymptotic normality of ${\\Delta_{h_n,p}f(p)-{\\frac{1}{|\\mu|}}\\Delta_Mf(p)}$ (functional CLT) for a fixed ${p\\in M}$ and uniformly in ${f}.$"
            },
            "slug": "Empirical-graph-Laplacian-approximation-of-Large-Gin'e-Koltchinskii",
            "title": {
                "fragments": [],
                "text": "Empirical graph Laplacian approximation of Laplace\u2013Beltrami operators: Large sample results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 232
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14848918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173",
            "isKey": false,
            "numCitedBy": 12819,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging."
            },
            "slug": "Normalized-cuts-and-image-segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized cuts and image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "Manifold methods have become increasingly important and popular in machine learning and have seen numerous recent applications in data analysis including dimensionality reduction, visualization, clustering and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 331378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bbc0c752570c46a772f2982728f9ad4191f25dd",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach."
            },
            "slug": "Cluster-Kernels-for-Semi-Supervised-Learning-Chapelle-Weston",
            "title": {
                "fragments": [],
                "text": "Cluster Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label is proposed by modifying the eigenspectrum of the kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144632403"
                        ],
                        "name": "R. Kannan",
                        "slug": "R.-Kannan",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776865"
                        ],
                        "name": "A. Vetta",
                        "slug": "A.-Vetta",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Vetta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vetta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 232
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed, including [22, 27, 3, 12] for visualization and data representation, [30, 29, 9, 23, 4, 2, 26] for partially supervised classification and [25, 28, 24, 18, 14] among others for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61731027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f36bfb04f8ff4d10aaa204b5338d2234f05c97d2",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new measure for assessing the quality of a clustering. A simple heuristic is shown to give worst-case guarantees under the new measure. Then we present two results regarding the quality of the clustering found by a popular spectral algorithm. One proffers worst case guarantees whilst the other shows that if there exists a \"good\" clustering then the spectral algorithm will find one close to it."
            },
            "slug": "On-clusterings-good,-bad-and-spectral-Kannan-Vempala",
            "title": {
                "fragments": [],
                "text": "On clusterings-good, bad and spectral"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Two results regarding the quality of the clustering found by a popular spectral algorithm are presented, one proffers worst case guarantees whilst the other shows that if there exists a \"good\" clustering then the spectral algorithm will find one close to it."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 41st Annual Symposium on Foundations of Computer Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "thesis of Stefan Lafon [19] (see also [12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 149
                            }
                        ],
                        "text": "The normalized Laplacian and a generalization to a family of normalized Laplacians was first pointed out in the current manifold learning context by [19,12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 77
                            }
                        ],
                        "text": "This direction was systematically pursued by Lafon, Coifman, and others (see [19,12])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 141
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed for various problems and applications, including [26,32,3,14,12] for visualization and data representation, [38,34,37,10,27,5,2,31] for partially supervised classification and [30,33,28,21,17], among others, for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric diffusions as a tool for harmonic analysis and structure definition of data (Parts I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Nat. Acad. Sci.,"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144110793"
                        ],
                        "name": "O. Smolyanov",
                        "slug": "O.-Smolyanov",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Smolyanov",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Smolyanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101120550"
                        ],
                        "name": "H. Weizs\u00e4cker",
                        "slug": "H.-Weizs\u00e4cker",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Weizs\u00e4cker",
                            "middleNames": [
                                "v."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weizs\u00e4cker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737208"
                        ],
                        "name": "O. Wittich",
                        "slug": "O.-Wittich",
                        "structuredName": {
                            "firstName": "Olaf",
                            "lastName": "Wittich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Wittich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "We also note the closely related work [35], where similar functional objects were studied in a different context."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 33
                            }
                        ],
                        "text": "Now we make use of the fact (see [35,36] and references therein) that the metric tensor has an asymptotic expansion in exponential coordinates given by det(gij) = 1 \u2212 1 6 x Rx + O(\u2016x\u2016(3)) where R is the Ricci curvature tensor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115706783,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8044a250ed34db8ab5e171f28946f6d3df28622f",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Brownian-motion-on-a-manifold-as-a-limit-of-motions-Smolyanov-Weizs\u00e4cker",
            "title": {
                "fragments": [],
                "text": "Brownian motion on a manifold as a limit of stepwise conditioned standard Brownian motions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12671141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11b324fe84f6aba94af668086812a83b19494c3b",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Manifold-Stucture-for-Partially-Labeled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using Manifold Stucture for Partially Labeled Classification"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057372949"
                        ],
                        "name": "M. Bernstein",
                        "slug": "M.-Bernstein",
                        "structuredName": {
                            "firstName": "Mira",
                            "lastName": "Bernstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39453972"
                        ],
                        "name": "Vin de Silva",
                        "slug": "Vin-de-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "Silva",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vin de Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Some of the recent work includes estimating geometric invariants of the manifold, such as homology [31, 20], geodesic distances [6], and comparing point clouds using Gromov-Hausdorff distance [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10751942,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2dbc325d7014114242e63c93709ec820d8caf7b0",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "0 Introduction In [1] Tenenbaum, de Silva and Langford consider the problem of non-linear dimensionality reduction: discovering intrinsically low-dimensional structures embedded in high-dimensional data sets. They describe an algorithm, called Isomap, and demonstrate its successful application to several real and synthetic data sets. In this paper, we discuss some of the theoretical claims for Isomap made in [1]. In particular, we give a full proof of the asymptotic convergence theorem referred to in that paper. Isomap deals with finite data sets of points in R n which are assumed to lie on a smooth submanifold M d of low dimension d < n. The algorithm attempts to recover M given only the data points. A crucial stage in the algorithm involves estimating the unknown geodesic distance in M between data points in terms of the graph distance with respect to some graph G constructed on the data points. We show that the two distance metrics approximate each other arbitrarily closely, as the density of data points tends to infinity. Main Theorem A * The authors are listed alphabetically."
            },
            "slug": "Graph-approximations-to-geodesics-on-embedded-Bernstein-Silva",
            "title": {
                "fragments": [],
                "text": "Graph approximations to geodesics on embedded manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A full proof of the asymptotic convergence theorem of Isomap is given, showing that the two distance metrics approximate each other arbitrarily closely, as the density of data points tends to infinity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Outof-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering, NIPS 2003"
            },
            "venue": {
                "fragments": [],
                "text": "Outof-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering, NIPS 2003"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "We also note [20], where convergence of spectral properties of graph Laplacians, such as their eigenvectors and eigenvalues, was demonstrated in a non-geometric setting for a fixed kernel bandwidth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Heat Kernels on Weighted Manifolds and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Cont. Math"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "thesis of Lafon, [16], which generalized the convergence results from [1] to the important case of an arbitrary probability distribution on a manifold."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "This direction was systematically pursued by Lafon, Coifman, and others (see [19,12])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Diffusion Maps and Geodesic Harmonics"
            },
            "venue": {
                "fragments": [],
                "text": "Diffusion Maps and Geodesic Harmonics"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 141
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed for various problems and applications, including [26,32,3,14,12] for visualization and data representation, [38,34,37,10,27,5,2,31] for partially supervised classification and [30,33,28,21,17], among others, for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Dimensionality Reduction by Locally"
            },
            "venue": {
                "fragments": [],
                "text": "Linear Embedding, Science,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 141
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed for various problems and applications, including [26,32,3,14,12] for visualization and data representation, [38,34,37,10,27,5,2,31] for partially supervised classification and [30,33,28,21,17], among others, for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Dimensionality Reduction by Locally"
            },
            "venue": {
                "fragments": [],
                "text": "Linear Embedding, Science,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "This paper presents and extends the unpublished results obtained in [1]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "thesis of Lafon, [16], which generalized the convergence results from [1] to the important case of an arbitrary probability distribution on a manifold."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Problems of Learning on Manifolds, The University of Chicago"
            },
            "venue": {
                "fragments": [],
                "text": "Problems of Learning on Manifolds, The University of Chicago"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Manifold methods have become increasingly important and popular in machine learning and have seen numerous recent applications in data analysis including dimensionality reduction, visualization, clustering and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this paper we take the first steps towards a theoretical foundation for manifold-based methods in learning, by showing that under certain conditions the graph Laplacian is directly related to the manifold Laplace-Beltrami operator and converges to it as the amount of data goes to infinity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric diffusions as a tool for harmonic analysis and structure definition of data (Parts I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Nat. Acad. Sci"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "We also note [20], where convergence of spectral properties of graph Laplacians, such as their eigenvectors and eigenvalues, was demonstrated in a non-geometric setting for a fixed kernel bandwidth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistency of Spectral Clustering, Max Planck Institute for Biological Cybernetics"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report TR 134,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 200
                            }
                        ],
                        "text": "1 Prior Work Many manifold and graph-motivated learning methods have been recently proposed for various problems and applications, including [26,32,3,14,12] for visualization and data representation, [38,34,37,10,27,5,2,31] for partially supervised classification and [30,33,28,21,17], among others, for spectral clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using Manifold Structure for Partially Labeled Classification, NIPS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 13,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Towards-a-theoretical-foundation-for-manifold-Belkin-Niyogi/40cd779cb417c9e665ec29fdccc73a6499c5ae5e?sort=total-citations"
}