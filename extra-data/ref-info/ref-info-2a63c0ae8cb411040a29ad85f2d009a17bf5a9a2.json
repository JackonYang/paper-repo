{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "Employing special hardware or o { line learning, several researchers have developed successful systems to recognize general hand gestures [22, 14, 6, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7097815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6f2df5d02d226d75d7c47b203d7054f7bcf3dbd",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing human-hand gestures using a model-based approach. A nite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for nding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for"
            },
            "slug": "Gesture-Recognition-Davis-Shah",
            "title": {
                "fragments": [],
                "text": "Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents a method for recognizing human-hand gestures using a model-based approach and results are presented showing recognition of seven gestures using images sampled at 4Hz on a SPARC-1 without any special hardware."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "A low-level approach, such as was taken by [6], would process data at a level not much higher than that of pixel intensities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "Employing special hardware or o { line learning, several researchers have developed successful systems to recognize general hand gestures [22, 14, 6, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5344867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1807058512ae2934b2be0b43f395d8583ef67303",
            "isKey": false,
            "numCitedBy": 442,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for learning, tracking, and recognizing human gestures using a view-based approach to model articulated objects is presented. Objects are represented using sets of view models, rather than single templates. Stereotypical space-time patterns, i.e., gestures, are then matched to stored gesture patterns using dynamic time warping. Real-time performance is achieved by using special purpose correlation hardware and view prediction to prune as much of the search space as possible. Both view models and view predictions are learned from examples. Results showing tracking and recognition of human hand gestures at over 10 Hz are presented.<<ETX>>"
            },
            "slug": "Space-time-gestures-Darrell-Pentland",
            "title": {
                "fragments": [],
                "text": "Space-time gestures"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A method for learning, tracking, and recognizing human gestures using a view-based approach to model articulated objects is presented and results showing tracking and recognition of human hand gestures at over 10 Hz are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22560441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1218c31305b30cb102cc6fb2483b4ba26ba12f0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for detecting orientation in texture is developed and compared with results of humans detecting orientation in the same textures. The algorithm is based on the steerable filters of Freeman and Adelson (IEEE Trans. PAMI 13, 891-906, 1991), orientation-selective filters derived from derivatives of Gaussians. The filters are applied over multiple scales and their outputs non-linearly contrast-normalized. The data for humans were collected from forty subjects who were asked to identify 'the minimum number of dominant orientations' they perceived, and the 'strength' with which they perceived each orientation. Test data consisted of 111 grey-level images of natural textures taken from the Brodatz album, a standard collection used in computer vision and image processing. Results show that the computer and humans chose at least one of the same dominant orientations on 95 of the natural textures. Of these textures, 74 were also in 100% agreement on the location of all the dominant orientations chosen by both humans and computer. Disagreements are analyzed and possible causes are discussed. Some apparent limitations in the current filter shapes and sizes are illustrated, as well as some (surprisingly small) effects believed to be caused by semantic recognition and gestalt grouping."
            },
            "slug": "Finding-perceptually-dominant-orientations-in-Picard-Gorkani",
            "title": {
                "fragments": [],
                "text": "Finding perceptually dominant orientations in natural textures."
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An algorithm for detecting orientation in texture is developed and compared with results of humans detecting Orientation in the same textures, which show that the computer and humans chose at least one of the same dominant orientations on 95 of the natural textures."
            },
            "venue": {
                "fragments": [],
                "text": "Spatial vision"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "Employing special hardware or o { line learning, several researchers have developed successful systems to recognize general hand gestures [22, 14, 6, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14133354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaf6f7a14ba984b296b524474823e425ab3dd61e",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from gray scale images at speeds of up to 10 Hz. We employ kinematic and geometric hand models, along with a high temporal sampling rate, to decompose global image patterns into incremental, local motions of simple shapes. Hand pose and joint angles are estimated from line and point features extracted from images of unmarked, unadorned hands, taken from one or more viewpoints. We present some preliminary results on a 3D mouse interface based on the DigitEyes sensor."
            },
            "slug": "DigitEyes:-Vision-Based-Human-Hand-Tracking-Rehg-Kanade",
            "title": {
                "fragments": [],
                "text": "DigitEyes: Vision-Based Human Hand Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model-based hand tracking system that can recover the state of a 27 DOF hand model from gray scale images at speeds of up to 10 Hz is described, and some preliminary results on a 3D mouse interface based on the DigitEyes sensor are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Blake and Isard [4] have developed a fast contour-based tracker which they applied to hands, but the discrimination of di erent hand poses is limited."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10214317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "706c7bd758f933bffd60b5153a4498e83a880a38",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in video-tracking allow the outlines of moving, natural objects in a video-camera input stream to be tracked live, at full video-rate. Previous systems have been available to do this for specially illuminated objects or for naturally illuminated but polyhedral objects. Other systems have been able to track nonpolyhedral objects in motion, in some cases from live video, but following only centroids or key-points rather than tracking whole curves. The system described here can track accurately the curved silhouettes of moving non-polyhedral objects at frame-rate, for example hands, lips, legs, vehicles, fruit, and without any special hardware beyond a desktop workstation and a video-camera and framestore. The new algorithms are a synthesis of methods in deformable models, B-splines curve representation and control theory. This paper shows how such a facility can be used to turn parts of the body\u2014for instance, hands and lips\u2014into input devices. Rigid motion of a hand can be used as a 3D mouse with non-rigid gestures signalling a button press or the \u201clifting\u201d of the mouse. Both rigid and non-rigid motions of lips can be tracked independently and used as inputs, for example to animate a computer-generated face."
            },
            "slug": "3D-position,-attitude-and-shape-input-using-video-Blake-Isard",
            "title": {
                "fragments": [],
                "text": "3D position, attitude and shape input using video tracking of hands and lips"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The system described here can track accurately the curved silhouettes of moving non-polyhedral objects at frame-rate, for example hands, lips, legs, vehicles, fruit, and without any special hardware beyond a desktop workstation and a video-camera and framestore."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29530521"
                        ],
                        "name": "Y. Okamoto",
                        "slug": "Y.-Okamoto",
                        "structuredName": {
                            "firstName": "Yasukazu",
                            "lastName": "Okamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Okamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737913"
                        ],
                        "name": "Y. Kuno",
                        "slug": "Y.-Kuno",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Kuno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kuno"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 114
                            }
                        ],
                        "text": "Relying on visual markings on the hands, previous researchers have recognized sign language and pointing gestures [24, 5, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14445046,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "15c4cca0d98f2ec7537d15d9f722041995ad7db7",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient and geometrically intuitive algorithm for reliably interpreting the image velocities of moving objects in 3-D is presented. It is well known that under a weak perspective the image motion of points on a plane can be characterized by an affine transformation. It is shown that the relative image motion of a nearby non-coplanar point and its projection on the plane is equivalent to motion parallax, and because it is independent of view rotations it is a reliable geometric cue to 3-D shape and viewer/object motion. The authors summarize why structure from motion algorithms are often very sensitive to errors in the measured image velocities and then show how to efficiently and reliably extract an incomplete qualitative solution. They also show how to augment this into a complete solution if additional constraints or views are available. A real-time example is presented in which the 3-D visual interpretation of hand gestures or a hand-held object is used as part of a man-machine interface. This is an alternative to the Polhemus coil instrumented Dataglove commonly used in sensing manual gestures.<<ETX>>"
            },
            "slug": "Robust-structure-from-motion-using-motion-parallax-Cipolla-Okamoto",
            "title": {
                "fragments": [],
                "text": "Robust structure from motion using motion parallax"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the relative image motion of a nearby non-coplanar point and its projection on the plane is equivalent to motion parallax, and because it is independent of view rotations it is a reliable geometric cue to 3-D shape and viewer/object motion."
            },
            "venue": {
                "fragments": [],
                "text": "1993 (4th) International Conference on Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150111027"
                        ],
                        "name": "G. Granlund",
                        "slug": "G.-Granlund",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "Granlund",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Granlund"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Gorkani and Picard [18] used orientation histograms to compute dominant texture orientations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15757543,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "39cf0efb61b9b0650f919cf0367a09a25efdab81",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "In-search-of-a-general-picture-processing-operator-Granlund",
            "title": {
                "fragments": [],
                "text": "In search of a general picture processing operator"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3239730"
                        ],
                        "name": "M. Landy",
                        "slug": "M.-Landy",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Landy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Landy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Values of k between 1.2 and 2.7 worked well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8208105,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "5e2d6a4646bde6d0dc7add71922d246750c90c6d",
            "isKey": true,
            "numCitedBy": 230,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual texture segregation is the perceptual phenomenonillustrated in figure 17.1. In this image a rectangular areacomposed of small X-shaped patterns appears as a distinctregion against a background of L-shaped patterns. Thissegregation occurs rapidly and unconsciously, without effort or search. The rectangular area composed of T- shaped patterns, however, does not appear as a distinct region in this sense. It is very easy to discriminate amongall three pattems, but only the X and L regions displayspontaneous segregation. This distinction between pat- tern discrimination and perceptual segregation gives par-ticular interest to the study of this aspect of visual textureperception. Other aspects of texture perception include the role of texture in the perception of surface orientation and shape and the influence of surface texture in de-termining color appearance. In this paper we limit con-sideration to the phenomenon of segregation.Pure texture-based segregation is not a very importantphenomenon in everyday visual experience. Objects are not usually distinguished from their backgrounds purely by textural differences. In this respect, the study of puretexture differences (in the absence of differences in bright-ness, color, depth, or other properties) is analogous to thestudy of isoluminant color differences, which also are notvery common in natural scenes. The relative rarity ofisoluminant color discrimination in the real world does not imply that color perception is an unimportant compo-nent of seeing. Similarly, the rarity of pure texture differ-ences does not reduce the potential importance of textureperception, especially in the visual processing of complexscenes."
            },
            "slug": "Computational-Modeling-of-Visual-Texture-Bergen-Landy",
            "title": {
                "fragments": [],
                "text": "Computational Modeling of Visual Texture Segregation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272433"
                        ],
                        "name": "V. Tartter",
                        "slug": "V.-Tartter",
                        "structuredName": {
                            "firstName": "Vivien",
                            "lastName": "Tartter",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tartter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33094997"
                        ],
                        "name": "K. Knowlton",
                        "slug": "K.-Knowlton",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Knowlton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Knowlton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 114
                            }
                        ],
                        "text": "Relying on visual markings on the hands, previous researchers have recognized sign language and pointing gestures [24, 5, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4312352,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "813836b88a143a17fc6877ebd96ca1ce32090dae",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Many deaf people in the USA communicate in American sign language (ASL), which has an expressive capacity equivalent to that of a spoken language, although structurally independent of spoken languages1,2. It comprises hand and arm movements often combined with particular facial gestures; together these are sufficiently precise to transmit all the complexities and innuendoes of a language. Here we demonstrate that fluent ASL users can communicate easily when all they see of each other is an array of 27 light spots strategically placed on the hands and face. The results indicate the salient locations in normal sign perception, and suggest that it is feasible to transmit signs using the bandwidth of one telephone line rather than a much more expensive TV line."
            },
            "slug": "Perception-of-sign-language-from-an-array-of-27-Tartter-Knowlton",
            "title": {
                "fragments": [],
                "text": "Perception of sign language from an array of 27 moving spots"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that fluent ASL users can communicate easily when all they see of each other is an array of 27 light spots strategically placed on the hands and face, and suggested that it is feasible to transmit signs using the bandwidth of one telephone line rather than a much more expensive TV line."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065554001"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Values of k between 1.2 and 2.7 worked well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5601682,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "29cb9c230d999a2175c31969f0d90fcae3fb4efe",
            "isKey": true,
            "numCitedBy": 1083,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model of human preattentive texture perception. This model consists of three stages: (1) convolution of the image with a bank of even-symmetric linear filters followed by half-wave rectification to give a set of responses modeling outputs of V1 simple cells, (2) inhibition, localized in space, within and among the neural-response profiles that results in the suppression of weak responses when there are strong responses at the same or nearby locations, and (3) texture-boundary detection by using wide odd-symmetric mechanisms. Our model can predict the salience of texture boundaries in any arbitrary gray-scale image. A computer implementation of this model has been tested on many of the classic stimuli from psychophysical literature. Quantitative predictions of the degree of discriminability of different texture pairs match well with experimental measurements of discriminability in human observers."
            },
            "slug": "Preattentive-texture-discrimination-with-early-Malik-Perona",
            "title": {
                "fragments": [],
                "text": "Preattentive texture discrimination with early vision mechanisms."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A model of human preattentive texture perception that can predict the salience of texture boundaries in any arbitrary gray-scale image and Quantitative predictions of the degree of discriminability of different texture pairs match well with experimental measurements of discriminateability in human observers."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23111666"
                        ],
                        "name": "S. Fels",
                        "slug": "S.-Fels",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Fels",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Devices such as the Dataglove [1] can be worn which sense hand and nger positions [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41170404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "703b3e9dc9ad014cb1baa840f5a03b33c67021d8",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single ;speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user."
            },
            "slug": "Glove-Talk:-a-neural-network-interface-between-a-a-Fels-Hinton",
            "title": {
                "fragments": [],
                "text": "Glove-Talk: a neural network interface between a data-glove and a speech synthesizer"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system, demonstrating that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2778348"
                        ],
                        "name": "D. Rubine",
                        "slug": "D.-Rubine",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Rubine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2854307"
                        ],
                        "name": "P. McAvinney",
                        "slug": "P.-McAvinney",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "McAvinney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. McAvinney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 28
                            }
                        ],
                        "text": "The marking-free systems of [12, 21] can recognize speci c nger or pointing events, but not general gestures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62621870,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "6fff179cae9a31f23e2f84e4989b9f9aacb2aebe",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, the controls of a musical instrument have been mechanically connected to its sound generators. It is well known that electrical couplings may be used instead of mechanical ones, and that digital instead of analog connections may be used. The freedom of not having the controls mechanically constrained to the means of sound production brings a corresponding burden. What form should the controls now take? Knowledge of the practice of music, psychoacoustics, ergonomics, sensing technologies, design, computer science, and economics can all be brought to bear on the question; common sense and personal taste also play a large role. Our interest in instrument control has led us to"
            },
            "slug": "Programmable-Finger-Tracking-Instrument-Controllers-Rubine-McAvinney",
            "title": {
                "fragments": [],
                "text": "Programmable Finger-Tracking Instrument Controllers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Then interpolation, for example, by radial basis functions [19], allows the machine to interpolate arbitrary angles from the user's hand input."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "McConnell [16] proposed this pattern recognition method, although he used a more complicated histogram comparison scheme than the squared error measure we use here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "nition technique developed by McConnell [16] employing histograms of local"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 111
                            }
                        ],
                        "text": "Abstract\nWe present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 102
                            }
                        ],
                        "text": "We present a method to recognize hand gestures, based on a pattern recog-\nnition technique developed by McConnell [16] employing histograms of local\norientation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Method of and apparatus for pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "U. S. Patent No"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893129"
                        ],
                        "name": "M. Bichsel",
                        "slug": "M.-Bichsel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bichsel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bichsel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "If dx and dy are the outputs of the x and y derivative operators, then the gradient direction is arctan(dx; dy), and the contrast is p\ndx2 + dy2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "However, others have observed [3] that local orientation measurments are less sensitive\nrecognition system\nto lighting changes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142804056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9373c1a890f9d4d1847d602d6e14df00e1bd0115",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strategies-of-robust-object-recognition-for-the-of-Bichsel",
            "title": {
                "fragments": [],
                "text": "Strategies of robust object recognition for the automatic identification of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "(The orientation maps were computed using steerable lters [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "(d) and (e) show the local direction of dominant orientation (as computed in [10]) for the images (a) and (b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design and use of steerable lters"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Pat. Anal. Mach. Intell.,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Devices such as the Dataglove [1] can be worn which sense hand and nger positions [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dataglove model 2 operating manual"
            },
            "venue": {
                "fragments": [],
                "text": "Dataglove model 2 operating manual"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "Employing special hardware or o { line learning, several researchers have developed successful systems to recognize general hand gestures [22, 14, 6, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real time hand shape recognition using pipeline image processor"
            },
            "venue": {
                "fragments": [],
                "text": "In IEEE Intl. Workshop on robot and human communication,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "(see [23] for a related system, not visually controlled)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Whole hand input"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Massachusetts Institute of Technology,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "Employing special hardware or o { line learning, several researchers have developed successful systems to recognize general hand gestures [22, 14, 6, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gest: a learning computer vision system that recognizes gestures"
            },
            "venue": {
                "fragments": [],
                "text": "In Machine Learning IV. Morgan Kau man,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "However, these methods require the placement of markers on the hands."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hand shape identication and tracking for sign language interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "Looking at people workshop"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "We have demonstrated a simple one parameter version of such orientation interpolation [11] The ideas above can be extended in a straightforward manner to temporal gestures [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Orientation histograms for hand gesture recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Mitsubishi Electric Research Labs.,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Isard . 3 D position , attitude and shape input using video tracking of hands and lips"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of SIGGRAPH"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Orientation histograms for hand gesture recognition, Intl. Workshop on Automatic Face-and Gesture-Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Orientation histograms for hand gesture recognition, Intl. Workshop on Automatic Face-and Gesture-Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 28
                            }
                        ],
                        "text": "The marking-free systems of [12, 21] can recognize speci c nger or pointing events, but not general gestures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Realtime detection of pointing actions for a glovefree interface"
            },
            "venue": {
                "fragments": [],
                "text": "In Workshop on Machine Vision Applications,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 114
                            }
                        ],
                        "text": "Relying on visual markings on the hands, previous researchers have recognized sign language and pointing gestures [24, 5, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hand shape identi cation and tracking for sign language interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "In Looking at people workshop, Chambery, France,"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth/2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2?sort=total-citations"
}