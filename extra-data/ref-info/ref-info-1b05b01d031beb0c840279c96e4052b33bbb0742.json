{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "\u2026component co-occurrence vectors), which are obtained from structure difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Text information plays a significant role in many applications including assistive navigation, image-based search, object recognition, scene understanding, and geocoding, etc., because it provides more descriptive and abstract information beyond intuitive perception of other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206724376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cb107a5b3b6539a9b9a758d91871f8b2519c79d",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Text information in natural scene images serves as important clues for many image-based applications such as scene understanding, content-based image retrieval, assistive navigation, and automatic geocoding. However, locating text from a complex background with multiple colors is a challenging task. In this paper, we explore a new framework to detect text strings with arbitrary orientations in complex natural scene images. Our proposed framework of text string detection consists of two steps: 1) image partition to find text character candidates based on local gradient features and color uniformity of character components and 2) character candidate grouping to detect text strings based on joint structural features of text characters in each text string such as character size differences, distances between neighboring characters, and character alignment. By assuming that a text string has at least three characters, we propose two algorithms of text string detection: 1) adjacent character grouping method and 2) text line grouping method. The adjacent character grouping method calculates the sibling groups of each character candidate as string segments and then merges the intersecting sibling groups into text string. The text line grouping method performs Hough transform to fit text line among the centroids of text candidates. Each fitted text line describes the orientation of a potential text string. The detected text string is presented by a rectangle region covering all characters whose centroids are cascaded in its text line. To improve efficiency and accuracy, our algorithms are carried out in multi-scales. The proposed methods outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation. Furthermore, the effectiveness of our methods to detect text strings with arbitrary orientations is evaluated on the Oriented Scene Text Dataset collected by ourselves containing text strings in nonhorizontal orientations."
            },
            "slug": "Text-String-Detection-From-Natural-Scenes-by-and-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text String Detection From Natural Scenes by Structure-Based Partition and Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new framework to detect text strings with arbitrary orientations in complex natural scene images with outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "These image patches are defined as character candidates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6109448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5295b6770ebbbc27a4651ed44b4b7e184d884f8e",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection. We have applied the approach in developing a Chinese sign translation system, which can automatically detect and recognize Chinese signs as input from a camera, and translate the recognized text into English."
            },
            "slug": "Automatic-detection-and-recognition-of-signs-from-Chen-Yang",
            "title": {
                "fragments": [],
                "text": "Automatic detection and recognition of signs from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152526333"
                        ],
                        "name": "Ming Zhao",
                        "slug": "Ming-Zhao",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116066317"
                        ],
                        "name": "Shutao Li",
                        "slug": "Shutao-Li",
                        "structuredName": {
                            "firstName": "Shutao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shutao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145193332"
                        ],
                        "name": "J. Kwok",
                        "slug": "J.-Kwok",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kwok",
                            "middleNames": [
                                "Tin-Yau"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kwok"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5f802bf01c8632dda4586b05bc86fc4878326f",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection-in-images-using-sparse-with-Zhao-Li",
            "title": {
                "fragments": [],
                "text": "Text detection in images using sparse representation with discriminative dictionaries"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11631790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b552f05038ad3f93dca68d044b0f093d95e42c9",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel algorithm, based on stroke components and descriptive Gabor filters, to detect text regions in natural scene images. Text characters and strings are constructed by stroke components as basic units. Gabor filters are used to describe and analyze the stroke components in text characters or strings. We define a suitability measurement to analyze the confidence of Gabor filters in describing stroke component and the suitability of Gabor filters on an image window. From the training set, we compute a set of Gabor filters that can describe principle stroke components of text by their parameters. Then a K-means algorithm is applied to cluster the descriptive Gabor filters. The clustering centers are defined as Stroke Gabor Words (SGWs) to provide a universal description of stroke components. By suitability evaluation on positive and negative training samples respectively, each SGW generates a pair of characteristic distributions of suitability measurements. On a testing natural scene image, heuristic layout analysis is applied first to extract candidate image windows. Then we compute the principle SGWs for each image window to describe its principle stroke components. Characteristic distributions generated by principle SGWs are used to classify text or non-text windows. Experimental results on benchmark datasets demonstrate that our algorithm can handle complex backgrounds and variant text patterns (font, color, scale, etc.)."
            },
            "slug": "Text-Detection-in-Natural-Scene-Images-by-Stroke-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text Detection in Natural Scene Images by Stroke Gabor Words"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A novel algorithm, based on stroke components and descriptive Gabor filters, to detect text regions in natural scene images and can handle complex backgrounds and variant text patterns (font, color, scale, etc.)."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390869207"
                        ],
                        "name": "Xu Zhao",
                        "slug": "Xu-Zhao",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148774182"
                        ],
                        "name": "Kai-Hsiang Lin",
                        "slug": "Kai-Hsiang-Lin",
                        "structuredName": {
                            "firstName": "Kai-Hsiang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Hsiang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145692782"
                        ],
                        "name": "Yun Fu",
                        "slug": "Yun-Fu",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yun Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689532"
                        ],
                        "name": "Yuxiao Hu",
                        "slug": "Yuxiao-Hu",
                        "structuredName": {
                            "firstName": "Yuxiao",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxiao Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117416965"
                        ],
                        "name": "Yuncai Liu",
                        "slug": "Yuncai-Liu",
                        "structuredName": {
                            "firstName": "Yuncai",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuncai Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "E-mail: ytian@ccny.cuny.edu.\nimages are still open research topics."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11816081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcbdd4ea38c2617f7a52e05b647f547d8a7dc60e",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text and caption from videos is important and in great demand for video retrieval, annotation, indexing, and content analysis. In this paper, we present a corner based approach to detect text and caption from videos. This approach is inspired by the observation that there exist dense and orderly presences of corner points in characters, especially in text and caption. We use several discriminative features to describe the text regions formed by the corner points. The usage of these features is in a flexible manner, thus, can be adapted to different applications. Language independence is an important advantage of the proposed method. Moreover, based upon the text features, we further develop a novel algorithm to detect moving captions in videos. In the algorithm, the motion features, extracted by optical flow, are combined with text features to detect the moving caption patterns. The decision tree is adopted to learn the classification criteria. Experiments conducted on a large volume of real video shots demonstrate the efficiency and robustness of our proposed approaches and the real-world system. Our text and caption detection system was recently highlighted in a worldwide multimedia retrieval competition, Star Challenge, by achieving the superior performance with the top ranking."
            },
            "slug": "Text-From-Corners:-A-Novel-Approach-to-Detect-Text-Zhao-Lin",
            "title": {
                "fragments": [],
                "text": "Text From Corners: A Novel Approach to Detect Text and Caption in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A corner based approach to detect text and caption from videos inspired by the observation that there exist dense and orderly presences of corner points in characters, especially in text and title, which was recently highlighted in a worldwide multimedia retrieval competition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3186240"
                        ],
                        "name": "Weihua Huang",
                        "slug": "Weihua-Huang",
                        "structuredName": {
                            "firstName": "Weihua",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weihua Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "In this paper we focus on total 62 text characters including 10 digit numbers and 26 English letters with both upper and lower cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7578182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d563d7598d50a68ab1ddcece79a0c9a043c19d",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Both graphic text and scene text detection in video images with complex background and low resolution is still a challenging and interesting problem for researchers in the field of image processing and computer vision. In this paper, we present a novel technique for detecting both graphic text and scene text in video images by finding segments containing text in an input image and then using statistical features such as vertical and horizontal bars for edges in the segments for detecting true text blocks efficiently. To identify a segment containing text, heuristic rules are formed based on combination of filters and edge analysis. Furthermore, the same rules are extended to grow the boundaries of a candidate segment in order to include complete text in the input image. The experimental results of the proposed method show that the technique performs better than existing methods in terms of a number of metrics."
            },
            "slug": "An-Efficient-Edge-Based-Technique-for-Text-in-Video-Shivakumara-Huang",
            "title": {
                "fragments": [],
                "text": "An Efficient Edge Based Technique for Text Detection in Video Frames"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A novel technique for detecting both graphic text and scene text in video images by finding segments containing text in an input image and then using statistical features such as vertical and horizontal bars for edges in the segments for detecting true text blocks efficiently is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Text information plays a significant role in many applications including assistive navigation, image-based search, object recognition, scene understanding, and geocoding, etc., because it provides more descriptive and abstract information beyond intuitive perception of other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064132"
                        ],
                        "name": "V. Dinh",
                        "slug": "V.-Dinh",
                        "structuredName": {
                            "firstName": "Viet",
                            "lastName": "Dinh",
                            "middleNames": [
                                "Cuong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Dinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844363"
                        ],
                        "name": "S. Chun",
                        "slug": "S.-Chun",
                        "structuredName": {
                            "firstName": "Seong",
                            "lastName": "Chun",
                            "middleNames": [
                                "Soo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330406"
                        ],
                        "name": "Seungwook Cha",
                        "slug": "Seungwook-Cha",
                        "structuredName": {
                            "firstName": "Seungwook",
                            "lastName": "Cha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungwook Cha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2013547"
                        ],
                        "name": "Hanjin Ryu",
                        "slug": "Hanjin-Ryu",
                        "structuredName": {
                            "firstName": "Hanjin",
                            "lastName": "Ryu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanjin Ryu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153528"
                        ],
                        "name": "S. Sull",
                        "slug": "S.-Sull",
                        "structuredName": {
                            "firstName": "Sanghoon",
                            "lastName": "Sull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42134356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395a3e9674fcc5686dcdd943ed94175272f7fb00",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Text appearing in video provides semantic knowledge and significant information for video indexing and retrieval system. This paper proposes an effective method for text detection in video based on the similarity in stroke width of text (which is defined as the distance between two edges of a stroke). From the observation that text regions can be characterized by a dominant fixed stroke width, edge detection with local adaptive thresholds is first devised to keep text- while reducing background-regions. Second, morphological dilation operator with adaptive structuring element size determined by stroke width value is exploited to roughly localize text regions. Finally, to reduce false alarm and refine text location, a new multi-frame refinement method is applied. Experimental results show that the proposed method is not only robust to different levels of background complexity, but also effective to different fonts (size, color) and languages of text."
            },
            "slug": "An-Efficient-Method-for-Text-Detection-in-Video-on-Dinh-Chun",
            "title": {
                "fragments": [],
                "text": "An Efficient Method for Text Detection in Video Based on Stroke Width Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Experimental results show that the proposed method for text detection in video based on the similarity in stroke width of text is not only robust to different levels of background complexity, but also effective to different fonts (size, color) and languages of text."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109703899"
                        ],
                        "name": "Long Ma",
                        "slug": "Long-Ma",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16060943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aa89b80a4a394e6abde41938a7e5fd3448c90e2",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a robust method for text detection in color scene image. The algorithm is based on edge detection and connected-component. In our framework, firstly, multi-scale edge detection is achieved by Canny operator and an adaptive thresholding binary method. Secondly, the filtered edges are classified by the classifier trained by SVM combing HOG, LBP and several statistical features, including mean, standard deviation, energy, entropy, inertia, local homogeneity and correlation. Thirdly, k-means clustering algorithm and the binary gradient image are used to filter the candidate regions and re-detect the regions around the candidate text candidates. Finally, the texts are relocated accurately by projection analysis. Experiments on 2003 ICDAR text location competition test database show the effectiveness of the proposed method."
            },
            "slug": "Text-detection-in-natural-images-based-on-edge-and-Ma-Wang",
            "title": {
                "fragments": [],
                "text": "Text detection in natural images based on multi-scale edge detetion and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A robust method for text detection in color scene image based on edge detection and connected-component and based on k-means clustering algorithm and binary gradient image is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2010 3rd International Congress on Image and Signal Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "These image patches are defined as character candidates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 759760,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d881266612513db360b794f2c7cbb6aa8b638e6",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural scene images brought new challenges for a few years and one of them is text understanding over images or videos. Text extraction which consists to segment textual foreground from the background succeeds using color information. Faced to the large diversity of text information in daily life and artistic ways of display, we are convinced that this only information is no more enough and we present a color segmentation algorithm using spatial information. Moreover, a new method is proposed in this paper to handle uneven lighting, blur and complex backgrounds which are inherent degradations to natural scene images. To merge text pixels together, complementary clustering distances are used to support simultaneously clear and well-contrasted images with complex and degraded images. Tests on a public database show finally efficiency of the whole proposed method."
            },
            "slug": "Spatial-and-Color-Spaces-Combination-for-Natural-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Spatial and Color Spaces Combination for Natural Scene Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new method is proposed in this paper to handle uneven lighting, blur and complex backgrounds which are inherent degradations to natural scene images and to merge text pixels together, complementary clustering distances are used."
            },
            "venue": {
                "fragments": [],
                "text": "2006 International Conference on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Further, they are often captured under variant illuminations and view angles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0c4294d517de3243fd4f8a09359e265fbf2f1ea",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an efficient text detection method based on the Laplacian operator. The maximum gradient difference value is computed for each pixel in the Laplacian-filtered image. K-means is then used to classify all the pixels into two clusters: text and non-text. For each candidate text region, the corresponding region in the Sobel edge map of the input image undergoes projection profile analysis to determine the boundary of the text blocks. Finally, we employ empirical rules to eliminate false positives based on geometrical properties. Experimental results show that the proposed method is able to detect text of different fonts, contrast and backgrounds. Moreover, it outperforms three existing methods in terms of detection and false positive rates."
            },
            "slug": "A-Laplacian-Method-for-Video-Text-Detection-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "A Laplacian Method for Video Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed text detection method outperforms three existing methods in terms of detection and false positive rates and employs empirical rules to eliminate false positives based on geometrical properties."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109862994"
                        ],
                        "name": "Sunil Kumar",
                        "slug": "Sunil-Kumar",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunil Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110344379"
                        ],
                        "name": "Rajat Gupta",
                        "slug": "Rajat-Gupta",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajat Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48676526"
                        ],
                        "name": "N. Khanna",
                        "slug": "N.-Khanna",
                        "structuredName": {
                            "firstName": "Nitin",
                            "lastName": "Khanna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Khanna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144725842"
                        ],
                        "name": "S. Chaudhury",
                        "slug": "S.-Chaudhury",
                        "structuredName": {
                            "firstName": "Santanu",
                            "lastName": "Chaudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705669"
                        ],
                        "name": "S. Joshi",
                        "slug": "S.-Joshi",
                        "structuredName": {
                            "firstName": "Shiv",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Dutt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1223283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909f2c6dec43e702d425b6e5166043d878e42996",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we have proposed a novel scheme for the extraction of textual areas of an image using globally matched wavelet filters. A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images. We have extended our text extraction scheme for the segmentation of document images into text, background, and picture components (which include graphics and continuous tone images). Multiple, two-class Fisher classifiers have been used for this purpose. We also exploit contextual information by using a Markov random field formulation-based pixel labeling scheme for refinement of the segmentation results. Experimental results have established effectiveness of our approach."
            },
            "slug": "Text-Extraction-and-Document-Image-Segmentation-and-Kumar-Gupta",
            "title": {
                "fragments": [],
                "text": "Text Extraction and Document Image Segmentation Using Matched Wavelets and MRF Model"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images and a text extraction scheme for the segmentation of document images into text, background, and picture components is extended."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14136313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b8f58a038df83138435b12a499c8bf0de13811",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition."
            },
            "slug": "End-to-end-scene-text-recognition-Wang-Babenko",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "While scene text recognition has generally been treated with highly domain-specific methods, the results demonstrate the suitability of applying generic computer vision methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111216375"
                        ],
                        "name": "Dacheng Wang",
                        "slug": "Dacheng-Wang",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dacheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026a new structure correlation based character appearance model to obtain discriminative appearance features of text characters; (2) a novel text descriptor calculation by structons (e.g. structure difference modeling) and correlatons (e.g. the clustering centers of structure component\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 26685555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "873fa0ca7454bd1ca4de25128c522e088b635ddc",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classification-of-newspaper-image-blocks-using-Wang-Srihari",
            "title": {
                "fragments": [],
                "text": "Classification of newspaper image blocks using texture analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093164350"
                        ],
                        "name": "Nikos A. Nikolaou",
                        "slug": "Nikos-A.-Nikolaou",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Nikolaou",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikos A. Nikolaou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "We adopt color decomposition, character contour refinement, and string line alignment to crop out a set of image patches may contain text from scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11797188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7931dd44065b546c2e91c7ff8039406313f74961",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A new technique for color reduction of complex document images is presented in this article. It reduces significantly the number of colors of the document image (less than 15 colors in most of the cases) so as to have solid characters and uniform local backgrounds. Therefore, this technique can be used as a preprocessing step by text information extraction applications. Specifically, using the edge map of the document image, a representative set of samples is chosen that constructs a 3D color histogram. Based on these samples in the 3D color space, a relatively large number of colors (usually no more than 100 colors) are obtained by using a simple clustering procedure. The final colors are obtained by applying a mean\u2010shift based procedure. Also, an edge preserving smoothing filter is used as a preprocessing stage that enhances significantly the quality of the initial image. Experimental results prove the method's capability of producing correctly segmented complex color documents where the character elements can be easily extracted as connected components. \u00a9 2009 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 19, 14\u201326, 2009"
            },
            "slug": "Color-reduction-for-complex-document-images-Nikolaou-Papamarkos",
            "title": {
                "fragments": [],
                "text": "Color reduction for complex document images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results prove the method's capability of producing correctly segmented complex color documents where the character elements can be easily extracted as connected components."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Imaging Syst. Technol."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49177577"
                        ],
                        "name": "Huizhong Chen",
                        "slug": "Huizhong-Chen",
                        "structuredName": {
                            "firstName": "Huizhong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizhong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701294"
                        ],
                        "name": "Georg Schroth",
                        "slug": "Georg-Schroth",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Schroth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Schroth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Unlike face which is constantly composed of eyes, nose, cheeks, and chin, and human body which is constantly composed of head, torso, and limbs, text consists of all different words with a set of character combinations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11311196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cb3153e5773053916a27bf3ab4530705a6bcf80",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text in natural images is an important prerequisite. In this paper, we propose a novel text detection algorithm, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates. These candidates are then filtered using geometric and stroke width information to exclude non-text objects. Letters are paired to identify text lines, which are subsequently separated into words. We evaluate our system using the ICDAR competition dataset and our mobile document database. The experimental results demonstrate the excellent performance of the proposed method."
            },
            "slug": "Robust-text-detection-in-natural-images-with-Stable-Chen-Tsai",
            "title": {
                "fragments": [],
                "text": "Robust text detection in natural images with edge-enhanced Maximally Stable Extremal Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel text detection algorithm is proposed, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates and Letters are paired to identify text lines, which are subsequently separated into words."
            },
            "venue": {
                "fragments": [],
                "text": "2011 18th IEEE International Conference on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144228782"
                        ],
                        "name": "T. Campos",
                        "slug": "T.-Campos",
                        "structuredName": {
                            "firstName": "Te\u00f3filo",
                            "lastName": "Campos",
                            "middleNames": [
                                "Em\u00eddio",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Campos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6516879"
                        ],
                        "name": "Bodla Rakesh Babu",
                        "slug": "Bodla-Rakesh-Babu",
                        "structuredName": {
                            "firstName": "Bodla",
                            "lastName": "Babu",
                            "middleNames": [
                                "Rakesh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodla Rakesh Babu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4826173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbbd5fdc09349bbfdee7aa7365a9d37716852b32",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper tackles the problem of recognizing characters in images of natural scenes. In particular, we focus on recognizing characters in situations that would traditionally not be handled well by OCR techniques. We present an annotated database of images containing English and Kannada characters. The database comprises of images of street scenes taken in Bangalore, India using a standard camera. The problem is addressed in an object cateogorization framework based on a bag-of-visual-words representation. We assess the performance of various features based on nearest neighbour and SVM classification. It is demonstrated that the performance of the proposed method, using as few as 15 training images, can be far superior to that of commercial OCR systems. Furthermore, the method can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "slug": "Character-Recognition-in-Natural-Images-Campos-Babu",
            "title": {
                "fragments": [],
                "text": "Character Recognition in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the performance of the proposed method can be far superior to that of commercial OCR systems, and can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5416971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b2a523d48cee04c09c327e14fb8928c5feff03c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition (STR) is the recognition of text anywhere in the environment, such as signs and storefronts. Relative to document recognition, it is challenging because of font variability, minimal language context, and uncontrolled conditions. Much information available to solve this problem is frequently ignored or used sequentially. Similarity between character images is often overlooked as useful information. Because of language priors, a recognizer may assign different labels to identical characters. Directly comparing characters to each other, rather than only a model, helps ensure that similar instances receive the same label. Lexicons improve recognition accuracy but are used post hoc. We introduce a probabilistic model for STR that integrates similarity, language properties, and lexical decision. Inference is accelerated with sparse belief propagation, a bottom-up method for shortening messages by reducing the dependency between weakly supported hypotheses. By fusing information sources in one model, we eliminate unrecoverable errors that result from sequential processing, improving accuracy. In experimental results recognizing text from images of signs in outdoor scenes, incorporating similarity reduces character recognition error by 19 percent, the lexicon reduces word recognition error by 35 percent, and sparse belief propagation reduces the lexicon words considered by 99.9 percent with a 12X speedup and no loss in accuracy."
            },
            "slug": "Scene-Text-Recognition-Using-Similarity-and-a-with-Weinman-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition Using Similarity and a Lexicon with Sparse Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model for scene text recognition is introduced that integrates similarity, language properties, and lexical decision and is fusing information sources in one model to eliminate unrecoverable errors that result from sequential processing, improving accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624076"
                        ],
                        "name": "Q. Luong",
                        "slug": "Q.-Luong",
                        "structuredName": {
                            "firstName": "Quang-Tuan",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48804780"
                        ],
                        "name": "James A. Herson",
                        "slug": "James-A.-Herson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Herson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Herson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29394851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4599b80a96821ed9276476edd17c6d70380f150c",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Real-world text on street signs, nameplates, etc. often lies in an oblique plane and hence cannot be recognized by traditional OCR systems due to perspective distortion. Furthermore, such text often comprises only one or two lines, preventing the use of existing perspective rectification methods that were primarily designed for images of document pages. We propose an approach that reliably rectifies and subsequently recognizes individual lines of text. Our system, which includes novel algorithms for extraction of text from real-world scenery, perspective rectification, and binarization, has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "slug": "Rectification-and-recognition-of-text-in-3-D-scenes-Myers-Bolles",
            "title": {
                "fragments": [],
                "text": "Rectification and recognition of text in 3-D scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an approach that reliably rectifies and subsequently recognizes individual lines of text in real-world text that has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Text information plays a significant role in many applications including assistive navigation, image-based search, object recognition, scene understanding, and geocoding, etc., because it provides more descriptive and abstract information beyond intuitive perception of other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1457124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "327c63c3e78e5d80dc762d73eb0128898e4c64af",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new model of object classes which incorporates appearance and shape information jointly. Modeling objects appearance by distributions of visual words has recently proven successful. Here appearancebased models are augmented by capturing the spatial arrangement of visual words. Compact spatial modeling without loss of discrimination is achieved through the introduction of adaptive vector quantized correlograms, which we call correlatons. Efficiency is further improved by means of integral images. The robustness of our new models to geometric transformations, severe occlusions and missing information is also demonstrated. The accuracy of discrimination of the proposed models is assessed with respect to existing databases with large numbers of object classes viewed under general conditions, and shown to outperform appearance-only models."
            },
            "slug": "Discriminative-Object-Class-Models-of-Appearance-by-Savarese-Winn",
            "title": {
                "fragments": [],
                "text": "Discriminative Object Class Models of Appearance and Shape by Correlatons"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A new model of object classes which incorporates appearance and shape information jointly is presented which is shown to outperform appearance-only models and to be robust to geometric transformations, severe occlusions and missing information."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264779"
                        ],
                        "name": "T. Kadir",
                        "slug": "T.-Kadir",
                        "structuredName": {
                            "firstName": "Timor",
                            "lastName": "Kadir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kadir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431498"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Chen et al. employed maximally stable extreme regions which are obtained from enhanced edge map to find out character candidates in scene image [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 825395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "254be2055a84c4d80c4c8eb8e6090b3977cc6fb6",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent.This paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliency; scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated."
            },
            "slug": "Saliency,-Scale-and-Image-Description-Kadir-Brady",
            "title": {
                "fragments": [],
                "text": "Saliency, Scale and Image Description"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818547"
                        ],
                        "name": "Mingcheng Wan",
                        "slug": "Mingcheng-Wan",
                        "structuredName": {
                            "firstName": "Mingcheng",
                            "lastName": "Wan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingcheng Wan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735996"
                        ],
                        "name": "Fengli Zhang",
                        "slug": "Fengli-Zhang",
                        "structuredName": {
                            "firstName": "Fengli",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengli Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151056"
                        ],
                        "name": "Hongrong Cheng",
                        "slug": "Hongrong-Cheng",
                        "structuredName": {
                            "firstName": "Hongrong",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongrong Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143673083"
                        ],
                        "name": "Qiao Liu",
                        "slug": "Qiao-Liu",
                        "structuredName": {
                            "firstName": "Qiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiao Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Although many optical character recognition (OCR) systems [38, 39, 40] were well developed to identify text characters in document images, text classification and detection in scene\n* Corresponding author."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21302840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34ba1bc6d833e32404f8fa1f142172fb9957adbb",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays more and more spam emails convey spam messages in a human readable image instead of text, making detection by conventional content filters difficult. However, the text information contained in spam images can be very useful for spam detection. Our goal in this paper is to propose an effective algorithm for text localization in spam images, the basic idea is to discriminate the non-text edges with some selected features of edges. Furthermore, we construct a corner detection algorithm based on a circular template to predict the corner points of the text in an image, which is crucial for text localization. Our evaluation shows that this algorithm can identify 96% of texts contained in spam images and the precision can reach up to 97.6% on real world data (spam image samples come from the SpamArchive public dataset)."
            },
            "slug": "Text-localization-in-spam-image-using-edge-features-Wan-Zhang",
            "title": {
                "fragments": [],
                "text": "Text localization in spam image using edge features"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper constructs a corner detection algorithm based on a circular template to predict the corner points of the text in an image, which is crucial for text localization in spam images."
            },
            "venue": {
                "fragments": [],
                "text": "2008 International Conference on Communications, Circuits and Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155699044"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords \u2014 Text detection, Scene image, Character appearance, Structure modeling, Structure difference, Structure component co-occurrence, Character identification"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28161769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0567609da19ae90f1742800f1ff873b9f1bd411",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research."
            },
            "slug": "Extraction-of-Text-Objects-in-Video-Documents:-Zhang-Kasturi",
            "title": {
                "fragments": [],
                "text": "Extraction of Text Objects in Video Documents: Recent Progress"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The progress made in text extraction in video documents is introduced and promising directions for future research are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36058334"
                        ],
                        "name": "H. Kronenberg",
                        "slug": "H.-Kronenberg",
                        "structuredName": {
                            "firstName": "Heino",
                            "lastName": "Kronenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kronenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151235999"
                        ],
                        "name": "T. Perroud",
                        "slug": "T.-Perroud",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Perroud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Perroud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "These image patches are defined as character candidates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11116581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfddae4d3b41f168f89d16d4b32833fc0d52813d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The automatic retrieval of indexing information from colored paper documents is a challenging problem. In order to build up bibliographic databases, editing by humans is usually necessary to provide information about title, authors and keywords. For automating the indexing process, the identification of text elements is essential. In this article an approach to automatic text extraction from colored book and journal covers is proposed. Two methods have been developed for extracting text hypotheses. The results of both methods are combined to robustly distinguish between text and non-text elements."
            },
            "slug": "Text-extraction-from-colored-book-and-journal-Sobottka-Kronenberg",
            "title": {
                "fragments": [],
                "text": "Text extraction from colored book and journal covers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An approach to automatic text extraction from colored book and journal covers is proposed and two methods have been developed for extracting text hypotheses to robustly distinguish between text and non-text elements."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49025046"
                        ],
                        "name": "Jing Huang",
                        "slug": "Jing-Huang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683442"
                        ],
                        "name": "Ravi Kumar",
                        "slug": "Ravi-Kumar",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ravi Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798723"
                        ],
                        "name": "Mandar Mitra",
                        "slug": "Mandar-Mitra",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152348371"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "We adopt color decomposition, character contour refinement, and string line alignment to crop out a set of image patches may contain text from scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9317935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fb01cd85317f84fb4def982a81f19da2571bae1",
            "isKey": false,
            "numCitedBy": 1923,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a new image feature called the color correlogram and use it for image indexing and comparison. This feature distills the spatial correlation of colors, and is both effective and inexpensive for content-based image retrieval. The correlogram robustly tolerates large changes in appearance and shape caused by changes in viewing positions, camera zooms, etc. Experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval."
            },
            "slug": "Image-indexing-using-color-correlograms-Huang-Kumar",
            "title": {
                "fragments": [],
                "text": "Image indexing using color correlograms"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Experimental evidence suggests that this new image feature called the color correlogram outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "In [33], an adjacent character grouping method was proposed to search for sibling characters at each contour and then group them into string lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29259,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14911813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs - text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines - one open source and one proprietary - with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16% on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem."
            },
            "slug": "Word-Spotting-in-the-Wild-Wang-Belongie",
            "title": {
                "fragments": [],
                "text": "Word Spotting in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the appearance of words in the wild spans this range of difficulties and a new word recognition approach based on state-of-the-art methods from generic object recognition is proposed, in which object categories are considered to be the words themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2811600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611ce9373302455dd8b33a37eedbb97f7f14e5a6",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Retrieval-of-machine-printed-Latin-documents-Word-Lu-Tan",
            "title": {
                "fragments": [],
                "text": "Retrieval of machine-printed Latin documents through Word Shape Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[16] S."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Robust Reading Dataset (RRD2003) was collected for text detection competition in the conference ICDAR 2003 [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We compare our results with several previous algorithms in [8, 16, 17]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": true,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "In [33], an adjacent character grouping method was proposed to search for sibling characters at each contour and then group them into string lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40309692"
                        ],
                        "name": "C. G. Harris",
                        "slug": "C.-G.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40365651"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Stephens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "Chen et al. employed maximally stable extreme regions which are obtained from enhanced edge map to find out character candidates in scene image [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1694378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "isKey": false,
            "numCitedBy": 14111,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed."
            },
            "slug": "A-Combined-Corner-and-Edge-Detector-Harris-Stephens",
            "title": {
                "fragments": [],
                "text": "A Combined Corner and Edge Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
            },
            "venue": {
                "fragments": [],
                "text": "Alvey Vision Conference"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522420"
                        ],
                        "name": "T. Ojala",
                        "slug": "T.-Ojala",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ojala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ojala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26881819,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5985014dda6d502469614aae17349b4d08f9f74c",
            "isKey": false,
            "numCitedBy": 6552,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparative-study-of-texture-measures-with-based-Ojala-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "A comparative study of texture measures with classification based on featured distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111090524"
                        ],
                        "name": "Jason V. Davis",
                        "slug": "Jason-V.-Davis",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Davis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason V. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692670"
                        ],
                        "name": "B. Kulis",
                        "slug": "B.-Kulis",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kulis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kulis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48964143"
                        ],
                        "name": "Prateek Jain",
                        "slug": "Prateek-Jain",
                        "structuredName": {
                            "firstName": "Prateek",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prateek Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072326"
                        ],
                        "name": "S. Sra",
                        "slug": "S.-Sra",
                        "structuredName": {
                            "firstName": "Suvrit",
                            "lastName": "Sra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2261947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b81381d17baf6750c09bd58e96f4660d25be9225",
            "isKey": false,
            "numCitedBy": 1864,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets."
            },
            "slug": "Information-theoretic-metric-learning-Davis-Kulis",
            "title": {
                "fragments": [],
                "text": "Information-theoretic metric learning"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "An information-theoretic approach to learning a Mahalanobis distance function that can handle a wide variety of constraints and can optionally incorporate a prior on the distance function and derive regret bounds for the resulting algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 Robust Reading Competitions Proceedings of the International Conference on Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR 2003 Robust Reading Competitions Proceedings of the International Conference on Document Analysis and Recognition"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing a new text descriptor based on structure difference and structure component cooccurrence of scene character"
            },
            "venue": {
                "fragments": [],
                "text": "Designing a new text descriptor based on structure difference and structure component cooccurrence of scene character"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining a group of layout rules involved in color uniformity and string alignment to localize candidate image patches that probably contain text"
            },
            "venue": {
                "fragments": [],
                "text": "Combining a group of layout rules involved in color uniformity and string alignment to localize candidate image patches that probably contain text"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026difference modeling and structure component co-occurrences respectively; (3) a group of spatial layout analysis that combines color uniformity and character contour refinement to localize candidate text characters from scene image and adopts string line alignment to refine detected text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text detection in natural scenes by stroke Gabor words"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Keywords \u2014 Text detection, Scene image, Character appearance, Structure modeling, Structure difference, Structure component co-occurrence, Character identification"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCRopus: http://code.google.com/p/ocropus"
            },
            "venue": {
                "fragments": [],
                "text": "OCRopus: http://code.google.com/p/ocropus"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proposing structure correlation to model appearance of scene character"
            },
            "venue": {
                "fragments": [],
                "text": "Proposing structure correlation to model appearance of scene character"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 5,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 43,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-extraction-from-scene-images-by-character-and-Yi-Tian/1b05b01d031beb0c840279c96e4052b33bbb0742?sort=total-citations"
}