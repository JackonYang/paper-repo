{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6299466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a3c74c7b11ad5635570932577cdde2a3f7a6a5c",
            "isKey": false,
            "numCitedBy": 1179,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random \u201cdropout\u201d procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code."
            },
            "slug": "Improving-deep-neural-networks-for-LVCSR-using-and-Dahl-Sainath",
            "title": {
                "fragments": [],
                "text": "Improving deep neural networks for LVCSR using rectified linear units and dropout"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Modelling deep neural networks with rectified linear unit (ReLU) non-linearities with minimal human hyper-parameter tuning on a 50-hour English Broadcast News task shows an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improved over a strong GMM/HMM system."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7253297"
                        ],
                        "name": "T. Munich",
                        "slug": "T.-Munich",
                        "structuredName": {
                            "firstName": "Tu",
                            "lastName": "Munich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Munich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88741922"
                        ],
                        "name": "H Germany",
                        "slug": "H-Germany",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Germany",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H Germany"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "This architecture was proposed in [22], but we have adapted the filter sizes for input images at 300 dpi."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 639755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Offline handwriting recognition\u2014the automatic transcription of images of handwritten text\u2014is a challenging task that combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks\u2014multidimensional recurrent neural networks and connectionist temporal classification\u2014this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic."
            },
            "slug": "Offline-Handwriting-Recognition-with-Recurrent-Munich-Germany",
            "title": {
                "fragments": [],
                "text": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input and does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2250860"
                        ],
                        "name": "M. Kozielski",
                        "slug": "M.-Kozielski",
                        "structuredName": {
                            "firstName": "Micha\u0142",
                            "lastName": "Kozielski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kozielski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145460901"
                        ],
                        "name": "P. Doetsch",
                        "slug": "P.-Doetsch",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Doetsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doetsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48432010"
                        ],
                        "name": "M. Hamdani",
                        "slug": "M.-Hamdani",
                        "structuredName": {
                            "firstName": "Mahdi",
                            "lastName": "Hamdani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hamdani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14288490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50d55c543fa9acbb71480e1b5b2c32c7ebfec908",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a state-of-the-art system for recognizing real-world handwritten images exposing a huge degree of noise and a high out-of-vocabulary rate. We describe methods for successful image demising, line removal, deskewing, deslanting, and text line segmentation. We demonstrate how to use a HMM-based recognition system to obtain competitive results, and how to further improve it using LSTM neural networks in the tandem approach. The final system outperforms other approaches on a new dataset for English and French handwriting. The presented framework scales well across other standard datasets."
            },
            "slug": "Multilingual-Off-Line-Handwriting-Recognition-in-Kozielski-Doetsch",
            "title": {
                "fragments": [],
                "text": "Multilingual Off-Line Handwriting Recognition in Real-World Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated how to use a HMM-based recognition system to obtain competitive results, and how to further improve it using LSTM neural networks in the tandem approach, which outperforms other approaches on a new dataset for English and French handwriting."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "The objective function is the Negative Log-Likelihood (NLL) computed by CTC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The output of softmax is processed by Connectionist Temporal Classification (CTC) [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "There is a one-state HMM for each label (character, whitespace, and the blank symbol of CTC [21]), which has a transition to itself and an outgoing transition with the same probability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "\u2022 CTC is an elegant approach for computing the Negative Log-likelihood for sequences, so the whole architecture is trainable without having to explicitly align each input image with the corresponding target sequence."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9901844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96494e722f58705fa20302fe6179d483f52705b4",
            "isKey": true,
            "numCitedBy": 3476,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN."
            },
            "slug": "Connectionist-temporal-classification:-labelling-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems of sequence learning and post-processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935910"
                        ],
                        "name": "G. Mesnil",
                        "slug": "G.-Mesnil",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Mesnil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mesnil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [24], dropout is used to regularize a bi-directional RNN, but the network has only one hidden layer, there are no LSTM cells involved, and there is no detail on how to apply dropout to the RNN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c782f5a99254725517e5bd526dcc63fb59210589",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the key problems in spoken language understanding (SLU) is the task of slot filling. In light of the recent success of applying deep neural network technologies in domain detection and intent identification, we carried out an in-depth investigation on the use of recurrent neural networks for the more difficult task of slot filling involving sequence discrimination. In this work, we implemented and compared several important recurrent-neural-network architectures, including the Elman-type and Jordan-type recurrent networks and their variants. To make the results easy to reproduce and compare, we implemented these networks on the common Theano neural network toolkit, and evaluated them on the ATIS benchmark. We also compared our results to a conditional random fields (CRF) baseline. Our results show that on this task, both types of recurrent networks outperform the CRF baseline substantially, and a bi-directional Jordantype network that takes into account both past and future dependencies among slots works best, outperforming a CRFbased baseline by 14% in relative error reduction."
            },
            "slug": "Investigation-of-recurrent-neural-network-and-for-Mesnil-He",
            "title": {
                "fragments": [],
                "text": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results show that on this task, both types of recurrent networks outperform the CRF baseline substantially, and a bi-directional Jordantype network that takes into account both past and future dependencies among slots works best, outperforming a CRFbased baseline by 14% in relative error reduction."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "The burden of exploding and vanishing gradient was the reason for the lack of practical applications of RNNs until recently [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6144,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393667185"
                        ],
                        "name": "Ossama Abdel-Hamid",
                        "slug": "Ossama-Abdel-Hamid",
                        "structuredName": {
                            "firstName": "Ossama",
                            "lastName": "Abdel-Hamid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ossama Abdel-Hamid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [14], dropout is used in a convolutional neural network but with a smaller dropout rate because the typical value p = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2032998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ec87a97d202d5e432ca96490e409d931bbead7f",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and present a novel deep convolutional neural network architecture, where heterogeneous pooling is used to provide constrained frequency-shift invariance in the speech spectrogram while minimizing speech-class confusion induced by such invariance. The design of the pooling layer is guided by domain knowledge about how speech classes would change when formant frequencies are modified. The convolution and heterogeneous-pooling layers are followed by a fully connected multi-layer neural network to form a deep architecture interfaced to an HMM for continuous speech recognition. During training, all layers of this entire deep net are regularized using a variant of the \u201cdropout\u201d technique. Experimental evaluation demonstrates the effectiveness of both heterogeneous pooling and dropout regularization. On the TIMIT phonetic recognition task, we have achieved an 18.7% phone error rate, lowest on this standard task reported in the literature with a single system and with no use of information about speaker identity. Preliminary experiments on large vocabulary speech recognition in a voice search task also show error rate reduction using heterogeneous pooling in the deep convolutional neural network."
            },
            "slug": "A-deep-convolutional-neural-network-using-pooling-Deng-Abdel-Hamid",
            "title": {
                "fragments": [],
                "text": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel deep convolutional neural network architecture is developed, where heterogeneous pooling is used to provide constrained frequency-shift invariance in the speech spectrogram while minimizing speech-class confusion induced by such invariance."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727524"
                        ],
                        "name": "M. Seltzer",
                        "slug": "M.-Seltzer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Seltzer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seltzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49416592"
                        ],
                        "name": "Yongqiang Wang",
                        "slug": "Yongqiang-Wang",
                        "structuredName": {
                            "firstName": "Yongqiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongqiang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10310847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "223e9b53fc271d451801fb63869945a0d8c2ed61",
            "isKey": false,
            "numCitedBy": 610,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a new acoustic model based on deep neural networks (DNN) has been introduced. While the DNN has generated significant improvements over GMM-based systems on several tasks, there has been no evaluation of the robustness of such systems to environmental distortion. In this paper, we investigate the noise robustness of DNN-based acoustic models and find that they can match state-of-the-art performance on the Aurora 4 task without any explicit noise compensation. This performance can be further improved by incorporating information about the environment into DNN training using a new method called noise-aware training. When combined with the recently proposed dropout training technique, a 7.5% relative improvement over the previously best published result on this task is achieved using only a single decoding pass and no additional decoding complexity compared to a standard DNN."
            },
            "slug": "An-investigation-of-deep-neural-networks-for-noise-Seltzer-Yu",
            "title": {
                "fragments": [],
                "text": "An investigation of deep neural networks for noise robust speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The noise robustness of DNN-based acoustic models can match state-of-the-art performance on the Aurora 4 task without any explicit noise compensation and can be further improved by incorporating information about the environment into DNN training using a new method called noise-aware training."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Moreover, dropout was typically applied only at fully-connected layers [12], [18], even in convolutional networks [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Note that previous works about dropout seem to favor rectified linear units (ReLU) [13] over tanh or sigmoid for the network nonlinearity since it provides better covergence rate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80950,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Moreover, dropout was typically applied only at fully-connected layers [12], [18], even in convolutional networks [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Originally proposed in [12], dropout involves randomly removing some hidden units in a neural network during training but keeping all of them during testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "In this work, we only consider the original idea of dropout [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": true,
            "numCitedBy": 6191,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721096"
                        ],
                        "name": "S. Marukatat",
                        "slug": "S.-Marukatat",
                        "structuredName": {
                            "firstName": "Sanparith",
                            "lastName": "Marukatat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marukatat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7364123"
                        ],
                        "name": "T. Arti\u00e8res",
                        "slug": "T.-Arti\u00e8res",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Arti\u00e8res",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Arti\u00e8res"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3006660"
                        ],
                        "name": "B. Dorizzi",
                        "slug": "B.-Dorizzi",
                        "structuredName": {
                            "firstName": "Bernadette",
                            "lastName": "Dorizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorizzi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Early works typically use a Hidden Markov Model (HMM) [2] or an HMM-neural network hybrid system [3], [4] for the recognizer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16277635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33d25b10b93ec965a62154598fdefb0879d934cf",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on designing a handwriting recognition system dealing with on-line signal, i.e. temporal handwriting signal captured through an electronic pen or a digitalized tablet. We present here some new results concerning a hybrid on-line handwriting recognition system based on Hidden Markov Models (HMMs) and Neural Networks (NNs), which has already been presented in several contributions. In our approach, a letter-model is a Left-Right HMM, whose emission probability densities are approximated with mixtures of predictive multilayer perceptrons. The basic letter models are cascaded in order to build models for words and sentences. At the word level, recognition is performed thanks to a dictionary organized with a tree-structure. At the sentence level, a word-predecessor conditioned frame synchronous beam search algorithm allows to perform simultaneously segmentation into words and word recognition. It processes through the building of a word graph from which a set of candidate sentences may be extracted. Word and sentence recognition performances are evaluated on parts of the UNIPEN international database."
            },
            "slug": "Sentence-recognition-through-hybrid-neuro-Markovian-Marukatat-Arti\u00e8res",
            "title": {
                "fragments": [],
                "text": "Sentence recognition through hybrid neuro-Markovian modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Some new results are presented concerning a hybrid on-line handwriting recognition system based on Hidden Markov Models (HMMs) and Neural Networks (NNs), which has already been presented in several contributions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "The burden of exploding and vanishing gradient was the reason for the lack of practical applications of RNNs until recently [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18452318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9fac1091d9a1646314b1b91e58f40dae3a750cd",
            "isKey": false,
            "numCitedBy": 1464,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time."
            },
            "slug": "The-Vanishing-Gradient-Problem-During-Learning-Nets-Hochreiter",
            "title": {
                "fragments": [],
                "text": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The de-caying error flow is theoretically analyzed, methods trying to overcome vanishing gradients are briefly discussed, and experiments comparing conventional algorithms and alternative methods are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Uncertain. Fuzziness Knowl. Based Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053671127"
                        ],
                        "name": "Li Wan",
                        "slug": "Li-Wan",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Wan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Wan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33551113"
                        ],
                        "name": "Sixin Zhang",
                        "slug": "Sixin-Zhang",
                        "structuredName": {
                            "firstName": "Sixin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sixin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [18], a theoretical generalization bound of dropout was also derived."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 3
                            }
                        ],
                        "text": "If DropConnect is applied at a convolutional layer with k weights, it can sample at most 2k different models during training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Moreover, dropout was typically applied only at fully-connected layers [12], [18], even in convolutional networks [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Due to the impressive performance of dropout, some extensions of this technique were proposed, including DropConnect [18], Maxout networks [19], and an approximate approach for fast training with dropout [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 8
                            }
                        ],
                        "text": "However DropConnect was designed for fullyconnected layers, where it makes sense to drop the entries of the weight matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Another appealing method similar to dropout is DropConnect [18], which drops the connections, instead of the hidden units values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2936324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
            "isKey": true,
            "numCitedBy": 2093,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models."
            },
            "slug": "Regularization-of-Neural-Networks-using-DropConnect-Wan-Zeiler",
            "title": {
                "fragments": [],
                "text": "Regularization of Neural Networks using DropConnect"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work introduces DropConnect, a generalization of Dropout, for regularizing large fully-connected layers within neural networks, and derives a bound on the generalization performance of both Dropout and DropConnect."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532744"
                        ],
                        "name": "Salvador Espa\u00f1a Boquera",
                        "slug": "Salvador-Espa\u00f1a-Boquera",
                        "structuredName": {
                            "firstName": "Salvador",
                            "lastName": "Boquera",
                            "middleNames": [
                                "Espa\u00f1a"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salvador Espa\u00f1a Boquera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145816817"
                        ],
                        "name": "M. J. Bleda",
                        "slug": "M.-J.-Bleda",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Bleda",
                            "middleNames": [
                                "Jos\u00e9",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bleda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403812963"
                        ],
                        "name": "J. Gorbe-Moya",
                        "slug": "J.-Gorbe-Moya",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Gorbe-Moya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gorbe-Moya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398809775"
                        ],
                        "name": "Francisco Zamora-Mart\u00ednez",
                        "slug": "Francisco-Zamora-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Zamora-Mart\u00ednez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francisco Zamora-Mart\u00ednez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2258370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b098fb3fa3f270f8dbec444612e2e2acc9607d",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes the use of hybrid Hidden Markov Model (HMM)/Artificial Neural Network (ANN) models for recognizing unconstrained offline handwritten texts. The structural part of the optical models has been modeled with Markov chains, and a Multilayer Perceptron is used to estimate the emission probabilities. This paper also presents new techniques to remove slope and slant from handwritten text and to normalize the size of text images with supervised learning methods. Slope correction and size normalization are achieved by classifying local extrema of text contours with Multilayer Perceptrons. Slant is also removed in a nonuniform way by using Artificial Neural Networks. Experiments have been conducted on offline handwritten text lines from the IAM database, and the recognition rates achieved, in comparison to the ones reported in the literature, are among the best for the same task."
            },
            "slug": "Improving-Offline-Handwritten-Text-Recognition-with-Boquera-Bleda",
            "title": {
                "fragments": [],
                "text": "Improving Offline Handwritten Text Recognition with Hybrid HMM/ANN Models"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The use of hybrid Hidden Markov Model (HMM)/Artificial Neural Network (ANN) models for recognizing unconstrained offline handwritten texts and new techniques to remove slope and slant from handwritten text and to normalize the size of text images with supervised learning methods are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3387810"
                        ],
                        "name": "Th\u00e9odore Bluche",
                        "slug": "Th\u00e9odore-Bluche",
                        "structuredName": {
                            "firstName": "Th\u00e9odore",
                            "lastName": "Bluche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Th\u00e9odore Bluche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373952"
                        ],
                        "name": "J. Louradour",
                        "slug": "J.-Louradour",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Louradour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Louradour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27345660"
                        ],
                        "name": "Maxime Knibbe",
                        "slug": "Maxime-Knibbe",
                        "structuredName": {
                            "firstName": "Maxime",
                            "lastName": "Knibbe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maxime Knibbe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344452"
                        ],
                        "name": "Bastien Moysset",
                        "slug": "Bastien-Moysset",
                        "structuredName": {
                            "firstName": "Bastien",
                            "lastName": "Moysset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bastien Moysset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2590729"
                        ],
                        "name": "Mohamed Benzeghiba",
                        "slug": "Mohamed-Benzeghiba",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Benzeghiba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Benzeghiba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156685"
                        ],
                        "name": "Christopher Kermorvant",
                        "slug": "Christopher-Kermorvant",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kermorvant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kermorvant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "In fact, this architecture was featured in our winning entry of the Arabic handwriting recognition competition OpenHaRT 2013 [11], where such a RNN was used as the optical model in the recognition system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Note that the results presented in Table III can not be directly compared to state-of-the-art results previously published on the same databases [29], [11], since the RNNs only output unconstrained sequences of characters."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "In fact, RNNs enhanced by LSTM cells [8] won several important contests [9], [10], [11] and currently hold the best known results in handwriting recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "The method to create an FST that is compatible with the RNN outputs is described in [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17468371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19d8b1dd85080d25c0670ecaab58c52fdc41d54c",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Arabic handwriting recognition systems proposed by A2iA to the NIST OpenHaRT2013 evaluation. These systems were based on an optical model using Long Short-Term Memory (LSTM) recurrent neural networks, trained to recognize the different forms of the Arabic characters directly from the image, without explicit feature extraction nor segmentation.Large vocabulary selection techniques and n-gram language modeling were used to provide a full paragraph recognition, without explicit word segmentation. Several recognition systems were also combined with the ROVER combination algorithm. The best system exceeded 80% of recognition rate."
            },
            "slug": "The-A2iA-Arabic-Handwritten-Text-Recognition-System-Bluche-Louradour",
            "title": {
                "fragments": [],
                "text": "The A2iA Arabic Handwritten Text Recognition System at the Open HaRT2013 Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper describes the Arabic handwriting recognition systems proposed by A2iA to the NIST OpenHaRT2013 evaluation, based on an optical model using Long Short-Term Memory recurrent neural networks trained to recognize the different forms of the Arabic characters directly from the image, without explicit feature extraction nor segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155871822"
                        ],
                        "name": "Jie Li",
                        "slug": "Jie-Li",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108159566"
                        ],
                        "name": "Xiaorui Wang",
                        "slug": "Xiaorui-Wang",
                        "structuredName": {
                            "firstName": "Xiaorui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaorui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49821282"
                        ],
                        "name": "Bo Xu",
                        "slug": "Bo-Xu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "It is shown to be effective with deep convolutional networks [12], [13], [14], feed-forward networks [15], [16], [17] but, to the best of our knowledge, has never been applied to RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17998005,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "8cb022cb7af25ac5adef26de1b031013a9d1dbd8",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The work by Hinton et al shows that the dropout strategy can greatly improve the performance of neural networks as well as reducing the influence of over-fitting. Nevertheless, there is still not a more detailed study on this strategy. In addition, the effectiveness of dropout on the task of LVCSR has not been analyzed. In this paper, we attempt to make a further discussion on the dropout strategy. The impacts on performance of different dropout probabilities for phone recognition task are experimented on TIMIT. To get an in-depth understanding of dropout, experiments of dropout testing are designed from the perspective of model averaging. The effectiveness of dropout is analyzed on a LVCSR task. Results show that the method of dropout fine-tuning combined with standard back-propagation gives significant performance improvements."
            },
            "slug": "Understanding-the-dropout-strategy-and-analyzing-on-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Understanding the dropout strategy and analyzing its effectiveness on LVCSR"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper attempts to make a further discussion on the dropout strategy, and shows that the method of dropout fine-tuning combined with standard back-propagation gives significant performance improvements."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51693,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8729431"
                        ],
                        "name": "Sida I. Wang",
                        "slug": "Sida-I.-Wang",
                        "structuredName": {
                            "firstName": "Sida",
                            "lastName": "Wang",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sida I. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Due to the impressive performance of dropout, some extensions of this technique were proposed, including DropConnect [18], Maxout networks [19], and an approximate approach for fast training with dropout [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10357959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec92efde21707ddf4b81f301cd58e2051c1a2443",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations."
            },
            "slug": "Fast-dropout-training-Wang-Manning",
            "title": {
                "fragments": [],
                "text": "Fast dropout training"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective, which gives an order of magnitude speedup and more stability."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126725"
                        ],
                        "name": "F. Menasri",
                        "slug": "F.-Menasri",
                        "structuredName": {
                            "firstName": "Far\u00e8s",
                            "lastName": "Menasri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menasri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373952"
                        ],
                        "name": "J. Louradour",
                        "slug": "J.-Louradour",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Louradour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Louradour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403835840"
                        ],
                        "name": "Anne-Laure Bianne-Bernard",
                        "slug": "Anne-Laure-Bianne-Bernard",
                        "structuredName": {
                            "firstName": "Anne-Laure",
                            "lastName": "Bianne-Bernard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anne-Laure Bianne-Bernard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156685"
                        ],
                        "name": "Christopher Kermorvant",
                        "slug": "Christopher-Kermorvant",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kermorvant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kermorvant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "In fact, RNNs enhanced by LSTM cells [8] won several important contests [9], [10], [11] and currently hold the best known results in handwriting recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18334832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a80412b2a3ab7a9f8d6de68e6d180160c0ffa127",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the system for the recognition of French handwriting submitted by A2iA to the competition organized at ICDAR2011 using the Rimes database. This system is composed of several recognizers based on three different recognition technologies, combined using a novel combination method. A framework multi-word recognition based on weighted finite state transducers is presented, using an explicit word segmentation, a combination of isolated word recognizers and a language model. The system was tested both for isolated word recognition and for multi-word line recognition and submitted to the RIMES-ICDAR2011 competition. This system outperformed all previously proposed systems on these tasks."
            },
            "slug": "The-A2iA-French-handwriting-recognition-system-at-Menasri-Louradour",
            "title": {
                "fragments": [],
                "text": "The A2iA French handwriting recognition system at the Rimes-ICDAR2011 competition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper describes the system for the recognition of French handwriting submitted by A2iA to the competition organized at ICDAR2011 using the Rimes database, which outperformed all previously proposed systems on these tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Due to the impressive performance of dropout, some extensions of this technique were proposed, including DropConnect [18], Maxout networks [19], and an approximate approach for fast training with dropout [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10600578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "isKey": false,
            "numCitedBy": 1822,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN."
            },
            "slug": "Maxout-Networks-Goodfellow-Warde-Farley",
            "title": {
                "fragments": [],
                "text": "Maxout Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A simple new model called maxout is defined designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Early works typically use a Hidden Markov Model (HMM) [2] or an HMM-neural network hybrid system [3], [4] for the recognizer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46516403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a56aa5ce50e6756206ec61c29c7bc1e1b579d8c",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a complete system for the recognition of off-line handwriting. Preprocessing techniques are described, including segmentation and normalization of word images to give invariance to scale, slant, slope and stroke thickness. Representation of the image is discussed and the skeleton and stroke features used are described. A recurrent neural network is used to estimate probabilities for the characters represented in the skeleton. The operation of the hidden Markov model that calculates the best word in the lexicon is also described. Issues of vocabulary choice, rejection, and out-of-vocabulary word recognition are discussed."
            },
            "slug": "An-Off-Line-Cursive-Handwriting-Recognition-System-Senior-Robinson",
            "title": {
                "fragments": [],
                "text": "An Off-Line Cursive Handwriting Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Describes a complete system for the recognition of off-line handwriting, including segmentation and normalization of word images to give invariance to scale, slant, slope and stroke thickness."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967060"
                        ],
                        "name": "P. Dreuw",
                        "slug": "P.-Dreuw",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Dreuw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dreuw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145460901"
                        ],
                        "name": "P. Doetsch",
                        "slug": "P.-Doetsch",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Doetsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doetsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758476"
                        ],
                        "name": "Christian Plahl",
                        "slug": "Christian-Plahl",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Plahl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Plahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14994309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c2b6657d7e1d40d7ef1a158f9f855df86310be2",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We use neural network based features extracted by a hierarchical multilayer-perceptron (MLP) network either in a hybrid MLP/HMM approach or to discriminatively retrain a Gaussian hidden Markov model (GHMM) system in a tandem approach. MLP networks have been successfully used to model long-term and non-linear features dependencies in automatic speech and optical character recognition. In offline handwriting recognition, MLPs have been mostly used for isolated character and word recognition in hybrid approaches. Here we analyze MLPs within an LVCSR framework for continuous handwriting recognition using discriminative MMI/MPE training. Especially hybrid MLP/HMM and discriminatively retrained MLP-GHMM tandem approaches are evaluated. Significant improvements and competitive results are reported for a closed-vocabulary task on the IfN/ENIT Arabic handwriting database and for a large-vocabulary task using the IAM English handwriting database."
            },
            "slug": "Hierarchical-hybrid-MLP/HMM-or-rather-MLP-features-Dreuw-Doetsch",
            "title": {
                "fragments": [],
                "text": "Hierarchical hybrid MLP/HMM or rather MLP features for a discriminatively trained Gaussian HMM: A comparison for offline handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work analyzes neural network based features extracted by a hierarchical multilayer-perceptron (MLP) network within an LVCSR framework for continuous handwriting recognition using discriminative MMI/MPE training and evaluates hybrid MLP/HMM and discriminatively retrained MLP-GHMM tandem approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2011 18th IEEE International Conference on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586498"
                        ],
                        "name": "R. Plamondon",
                        "slug": "R.-Plamondon",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Plamondon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Plamondon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Readers are referred to [1] for an extensive review of handwriting recognition systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15782139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d12864a8acbab1830be755bfb9cb177e31ca5e20",
            "isKey": false,
            "numCitedBy": 2744,
            "numCiting": 719,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered."
            },
            "slug": "On-Line-and-Off-Line-Handwriting-Recognition:-A-Plamondon-Srihari",
            "title": {
                "fragments": [],
                "text": "On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms are described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2250860"
                        ],
                        "name": "M. Kozielski",
                        "slug": "M.-Kozielski",
                        "structuredName": {
                            "firstName": "Micha\u0142",
                            "lastName": "Kozielski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kozielski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145460901"
                        ],
                        "name": "P. Doetsch",
                        "slug": "P.-Doetsch",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Doetsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doetsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Note that on the 5th line of Table V, the system presented in [29] adopts an open-vocabulary approach and can recognize out-of-vocabulary words, which can not be directly compared to our models."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Note that the results presented in Table III can not be directly compared to state-of-the-art results previously published on the same databases [29], [11], since the RNNs only output unconstrained sequences of characters."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7114586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcef47a4ad5169276b23e9aba2a14b062ff8fe27",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a novel HMM-based system for off-line handwriting recognition. We adapt successful techniques from the domains of large vocabulary speech recognition and image object recognition: moment-based image normalization, writer adaptation, discriminative feature extraction and training, and open-vocabulary recognition. We evaluate those methods and examine their cumulative effect on the recognition performance. The final system outperforms current state-of-the-art approaches on two standard evaluation corpora for English and French handwriting."
            },
            "slug": "Improvements-in-RWTH's-System-for-Off-Line-Kozielski-Doetsch",
            "title": {
                "fragments": [],
                "text": "Improvements in RWTH's System for Off-Line Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A novel HMM-based system for off-line handwriting recognition that outperforms current state-of-the-art approaches on two standard evaluation corpora for English and French handwriting."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "There is a one-state HMM for each label (character, whitespace, and the blank symbol of CTC [21]), which has a transition to itself and an outgoing transition with the same probability."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The HMM is also represented as an FST H and is composed with the lexicon FST L, and the language model G."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Early works typically use a Hidden Markov Model (HMM) [2] or an HMM-neural network hybrid system [3], [4] for the recognizer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "However, the hidden states of HMMs follow a first-order Markov chain, hence they cannot handle longterm dependencies in sequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Concretely, we build a hybrid HMM/RNN model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Specifically, the posteriors p(s|x) are divided by the priors p(s), scaled by some factor \u03ba :\np(s|x) p(s)\u03ba , where s is the HMM state, i.e. a\ncharacter, a blank, or a whitespace, and x is the input."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Moreover, at each time step, HMMs can only select one hidden state, hence an HMM with n hidden states can typically carry only log (n) bits of information about its dynamics [5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10207300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15725948c2ea8b190b825a0887e430dc4898428",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a system for the reading of totally unconstrained handwritten text is presented. The kernel of the system is a hidden Markov model (HMM) for handwriting recognition. The HMM is enhanced by a statistical language model. Thus linguistic knowledge beyond the lexicon level is incorporated in the recognition process. Another novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided. A number of experiments with various language models and large vocabularies have been conducted. The language models used in the system were also analytically compared based on their perplexity."
            },
            "slug": "Using-a-Statistical-Language-Model-to-Improve-the-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "Using a Statistical Language Model to Improve the Performance of an HMM-Based Cursive Handwriting Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided and linguistic knowledge beyond the lexicon level is incorporated in the recognition process."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3191442"
                        ],
                        "name": "E. Grosicki",
                        "slug": "E.-Grosicki",
                        "structuredName": {
                            "firstName": "Emmanu\u00e8le",
                            "lastName": "Grosicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Grosicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801094"
                        ],
                        "name": "H. E. Abed",
                        "slug": "H.-E.-Abed",
                        "structuredName": {
                            "firstName": "Haikal",
                            "lastName": "Abed",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. E. Abed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "We report the best known results on Rimes and OpenHaRT databases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "It can be seen that dropout works very well on IAM and Rimes where it significantly improves the performance by 10 \u2212 20% regardless of the number of topmost hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "The results are presented in Tables IV (Rimes), V (IAM) and VI (OpenHaRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 25
                            }
                        ],
                        "text": "This makes sense because Rimes is the smallest of the three datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 94
                            }
                        ],
                        "text": "On the 4th row, when dropout and lexical constraints are both enabled, dropout achieves 5.7% (Rimes), 19.0% (IAM) and 4.1% (OpenHaRT) relative improvement in CER, and 2.4% (Rimes), 14.5% (IAM) and 3.2% (OpenHaRT) relative improvement in WER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 93
                            }
                        ],
                        "text": "Especially for OpenHaRT, since its training and validation sets are much larger than IAM and Rimes, 30 hidden units are inadequate and training takes a long time to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "For Rimes, we used a vocabulary made of 12k words from the training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "However we observed overfitting on Rimes when we use 4 and 20 features at the lowest LSTM layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Three handwriting datasets are used to evaluate our system: Rimes [25], IAM [26] and OpenHaRT [27] containing handwritten French, English and Arabic text, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7536616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbcdab5afc0553b85325fe18cd346bdbf0fbb74c",
            "isKey": true,
            "numCitedBy": 110,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the handwriting recognition competitionheld at ICDAR 2009. This competition is based onthe RIMES-database, with French written text documents.These document are classified in three different categories,complete text pages, words, and isolated characters. Thisyear 10 systems were submitted for the handwritten recognitioncompetition on snippets of French words. The systemswere evaluated in three subtask depending of the sizes ofthe used dictionary. A comparison between different classificationand recognition systems show interesting results. Ashort description of the participating groups, their systems,and the results achieved are presented."
            },
            "slug": "ICDAR-2009-Handwriting-Recognition-Competition-Grosicki-Abed",
            "title": {
                "fragments": [],
                "text": "ICDAR 2009 Handwriting Recognition Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper describes the handwriting recognition competition held at ICDAR 2009, based on the RIMES-database, with French written text documents, which shows interesting results."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168488"
                        ],
                        "name": "Roman Bertolami",
                        "slug": "Roman-Bertolami",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bertolami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Bertolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54db5331b787c2223a0c5f7a28c50bfbe9a8b801",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 179,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hidden-Markov-model-based-ensemble-methods-for-text-Bertolami-Bunke",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model-based ensemble methods for offline handwritten text line recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "It can be seen that dropout works very well on IAM and Rimes where it significantly improves the performance by 10 \u2212 20% regardless of the number of topmost hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "The results are presented in Tables IV (Rimes), V (IAM) and VI (OpenHaRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 277
                            }
                        ],
                        "text": "WER CER WER CER\nMDLSTM-RNN 32.6 8.1 35.4 8.9 + dropout 25.4 5.9 28.5 6.8 + Vocab&LM 14.0 3.7 12.6 3.5\n+ dropout 13.1 3.3 12.3 3.3 Messina et al. [30] - - 13.3 - Kozielski et al. [29] - - 13.7 4.6 Messina et al. [30] - - 14.6 -\nMenasri et al. [9] - - 15.2 7.2\nTABLE V. RESULTS ON IAM\nValid."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "On the 4th row, when dropout and lexical constraints are both enabled, dropout achieves 5.7% (Rimes), 19.0% (IAM) and 4.1% (OpenHaRT) relative improvement in CER, and 2.4% (Rimes), 14.5% (IAM) and 3.2% (OpenHaRT) relative improvement in WER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Especially for OpenHaRT, since its training and validation sets are much larger than IAM and Rimes, 30 hidden units are inadequate and training takes a long time to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "For IAM, we applied a 3-gram language model trained on the LOB, Brown and Wellington corpora."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Three handwriting datasets are used to evaluate our system: Rimes [25], IAM [26] and OpenHaRT [27] containing handwritten French, English and Arabic text, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29622813,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59",
            "isKey": true,
            "numCitedBy": 1060,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. In this paper we describe a database that consists of handwritten English sentences. It is based on the Lancaster-Oslo/Bergen (LOB) corpus. This corpus is a collection of texts that comprise about one million word instances. The database includes 1,066 forms produced by approximately 400 different writers. A total of 82,227 word instances out of a vocabulary of 10,841 words occur in the collection. The database consists of full English sentences. It can serve as a basis for a variety of handwriting recognition tasks. However, it is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used, because this knowledge can be automatically derived from the underlying corpus. The database also includes a few image-processing procedures for extracting the handwritten text from the forms and the segmentation of the text into lines and words."
            },
            "slug": "The-IAM-database:-an-English-sentence-database-for-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "The IAM-database: an English sentence database for offline handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A database that consists of handwritten English sentences based on the Lancaster-Oslo/Bergen corpus, which is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2929158"
                        ],
                        "name": "T. Nion",
                        "slug": "T.-Nion",
                        "structuredName": {
                            "firstName": "Thibauld",
                            "lastName": "Nion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126725"
                        ],
                        "name": "F. Menasri",
                        "slug": "F.-Menasri",
                        "structuredName": {
                            "firstName": "Far\u00e8s",
                            "lastName": "Menasri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menasri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373952"
                        ],
                        "name": "J. Louradour",
                        "slug": "J.-Louradour",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Louradour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Louradour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3361041"
                        ],
                        "name": "C\u00e9dric Sibade",
                        "slug": "C\u00e9dric-Sibade",
                        "structuredName": {
                            "firstName": "C\u00e9dric",
                            "lastName": "Sibade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C\u00e9dric Sibade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552681"
                        ],
                        "name": "T. Retornaz",
                        "slug": "T.-Retornaz",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Retornaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Retornaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821698"
                        ],
                        "name": "Pierre-Yves Metaireau",
                        "slug": "Pierre-Yves-Metaireau",
                        "structuredName": {
                            "firstName": "Pierre-Yves",
                            "lastName": "Metaireau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Yves Metaireau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156685"
                        ],
                        "name": "Christopher Kermorvant",
                        "slug": "Christopher-Kermorvant",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kermorvant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kermorvant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "In fact, RNNs enhanced by LSTM cells [8] won several important contests [9], [10], [11] and currently hold the best known results in handwriting recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10723882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20398d3016325d009bb57950bc323a92bf6ae06a",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a complete system for hand-written information extraction in historical documents. The system was evaluated in real conditions and at a large scale (8 millions of snippets) on the tables of the 1930 US Census. The location of the table position was based on a registration algorithm using printed word anchors. The rows and columns were extracted for nine different fields. For each field, a recognizer based either on convolutional neural networks for small lexicon fields or recurrent neural networks for large lexicon fields were trained. This system yields very high results for data extraction, allowing to achieve more than 70% of automation rate at a error rate similar to human keyers for a complete identity field."
            },
            "slug": "Handwritten-Information-Extraction-from-Historical-Nion-Menasri",
            "title": {
                "fragments": [],
                "text": "Handwritten Information Extraction from Historical Census Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A complete system for hand-written information extraction in historical documents, allowing to achieve more than 70% of automation rate at a error rate similar to human keyers for a complete identity field is described."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3480666"
                        ],
                        "name": "Ronaldo O. Messina",
                        "slug": "Ronaldo-O.-Messina",
                        "structuredName": {
                            "firstName": "Ronaldo",
                            "lastName": "Messina",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronaldo O. Messina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156685"
                        ],
                        "name": "Christopher Kermorvant",
                        "slug": "Christopher-Kermorvant",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kermorvant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kermorvant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15955585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2ae9a97f21beea249fc8e07e07704188734e87f",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Hybrid statistical grammars both at word and character levels can be used to perform open-vocabulary recognition. This is usually done by allowing the special symbol for unknown-word in the word-level grammar and dynamically replacing it by a (long) n-gramat character-level, as the full transducer does not fit in the memory of most current computers. We present a modification of a finite-state-transducer (fst) n-gram that enables the creation of a static transducer, i.e. when it is not possible to perform on-demand composition. By combining paths in the \"LG\" transducer (composition of lexicon and n-gram)making it over-generative with respect to the n-grams observed in the corpus, it is possible to reduce the number of actual occurrences of the character-level grammar, the resulting transducer fits the memory of practical machines. We evaluate this model for handwriting recognition using the RIMES and the IAM dabases. We study its effect on the vocabulary size and show that this model is competitive with state-of-the-art solutions."
            },
            "slug": "Over-Generative-Finite-State-Transducer-N-Gram-for-Messina-Kermorvant",
            "title": {
                "fragments": [],
                "text": "Over-Generative Finite State Transducer N-Gram for Out-of-Vocabulary Word Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A modification of a finite-state-transducer (fst) n-gram that enables the creation of a static transducer, i.e. when it is not possible to perform on-demand composition, is presented and it is shown that this model is competitive with state-of-the-art solutions."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 174
                            }
                        ],
                        "text": "Moreover, at each time step, HMMs can only select one hidden state, hence an HMM with n hidden states can typically carry only log (n) bits of information about its dynamics [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 519313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e510627d3f28601e370212bf063bbfa539ebed",
            "isKey": false,
            "numCitedBy": 1200,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot."
            },
            "slug": "Factorial-Hidden-Markov-Models-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Factorial Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner, and a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 46
                            }
                        ],
                        "text": "We report the best known results on Rimes and OpenHaRT databases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "For OpenHaRT, we selected a 95k words vocabulary containing all the words of the training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 64
                            }
                        ],
                        "text": "The results are presented in Tables IV (Rimes), V (IAM) and VI (OpenHaRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "On the 4th row, when dropout and lexical constraints are both enabled, dropout achieves 5.7% (Rimes), 19.0% (IAM) and 4.1% (OpenHaRT) relative improvement in CER, and 2.4% (Rimes), 14.5% (IAM) and 3.2% (OpenHaRT) relative improvement in WER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "Especially for OpenHaRT, since its training and validation sets are much larger than IAM and Rimes, 30 hidden units are inadequate and training takes a long time to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 111
                            }
                        ],
                        "text": "In fact, this architecture was featured in our winning entry of the Arabic handwriting recognition competition OpenHaRT 2013 [11], where such a RNN was used as the optical model in the recognition system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Three handwriting datasets are used to evaluate our system: Rimes [25], IAM [26] and OpenHaRT [27] containing handwritten French, English and Arabic text, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Convergence Curves on OpenHaRT."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "On OpenHaRT, dropout also helps with 50, 100 or 200 units, but hurts the performance with 30 units, most likely because the model with 30 units is underfitted."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NIST 2013 Open Handwriting Recognition and Translation Evaluation Plan"
            },
            "venue": {
                "fragments": [],
                "text": "2013. [Online]. Available: http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013 EvalPlan v1- 7.pdf"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 46
                            }
                        ],
                        "text": "We report the best known results on Rimes and OpenHaRT databases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "For OpenHaRT, we selected a 95k words vocabulary containing all the words of the training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 64
                            }
                        ],
                        "text": "The results are presented in Tables IV (Rimes), V (IAM) and VI (OpenHaRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "On the 4th row, when dropout and lexical constraints are both enabled, dropout achieves 5.7% (Rimes), 19.0% (IAM) and 4.1% (OpenHaRT) relative improvement in CER, and 2.4% (Rimes), 14.5% (IAM) and 3.2% (OpenHaRT) relative improvement in WER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "Especially for OpenHaRT, since its training and validation sets are much larger than IAM and Rimes, 30 hidden units are inadequate and training takes a long time to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 111
                            }
                        ],
                        "text": "In fact, this architecture was featured in our winning entry of the Arabic handwriting recognition competition OpenHaRT 2013 [11], where such a RNN was used as the optical model in the recognition system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Three handwriting datasets are used to evaluate our system: Rimes [25], IAM [26] and OpenHaRT [27] containing handwritten French, English and Arabic text, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Convergence Curves on OpenHaRT."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "On OpenHaRT, dropout also helps with 50, 100 or 200 units, but hurts the performance with 30 units, most likely because the model with 30 units is underfitted."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NIST 2013 Open Handwriting Recognition and Translation Evaluation Plan"
            },
            "venue": {
                "fragments": [],
                "text": "NIST 2013 Open Handwriting Recognition and Translation Evaluation Plan"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168488"
                        ],
                        "name": "Roman Bertolami",
                        "slug": "Roman-Bertolami",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bertolami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Bertolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14635907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "375214ac340226e23ec428e92ec499fb89f508b8",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance."
            },
            "slug": "A-Novel-Connectionist-System-for-Unconstrained-Graves-Liwicki",
            "title": {
                "fragments": [],
                "text": "A Novel Connectionist System for Unconstrained Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies, significantly outperforming a state-of-the-art HMM-based system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the other hand, tanh activations (in [-1,1]) are sharper: More \"helpful\" features learned by \"preventing co-adaptation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxout networks Fast dropout training"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classif weights -Baseline Classif weights -Dropout OpenHaRT (Arabic)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classif weights -Baseline Classif weights -Dropout Outgoing weights are smaller: L1 and L2 norms are greatly reduced Better than L1/L2 Weight Decay (and also simple to implement) Data-driven approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classif weights -Baseline Classif weights -Dropout Rimes (French)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [24], dropout is used to regularize a bi-directional RNN, but the network has only one hidden layer, there are no LSTM cells involved, and there is no detail on how to apply dropout to the RNN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Investigation of recurrentneural-network architectures and learning methods for spoken language understanding"
            },
            "venue": {
                "fragments": [],
                "text": "Interspeech, 2013."
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Previous work [28] suggests that dropout is most helpful when the size of the model is relatively big, and the network suffers from overfitting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dropout: A simple and effective way to improve neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems, 2012. [Online]. Available: http://videolectures.net/nips2012 hinton networks/"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 16,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 41,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Dropout-Improves-Recurrent-Neural-Networks-for-Pham-Kermorvant/c0b624c46b51920dfec5aa02cc86323c0beb0df5?sort=total-citations"
}