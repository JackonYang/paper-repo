{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920864"
                        ],
                        "name": "Yangyan Li",
                        "slug": "Yangyan-Li",
                        "structuredName": {
                            "firstName": "Yangyan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangyan Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2604835"
                        ],
                        "name": "S. Pirk",
                        "slug": "S.-Pirk",
                        "structuredName": {
                            "firstName": "S\u00f6ren",
                            "lastName": "Pirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329939"
                        ],
                        "name": "C. Qi",
                        "slug": "C.-Qi",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "FPNN [12] and Vote3D [23] proposed special methods to deal with the sparsity problem; however, their operations are still on sparse volumes, it\u2019s challenging for them to process very large point clouds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "FPNN [14] and Vote3D [27] proposed special methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it\u2019s challenging for them to process very large point clouds."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14543802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15ca7adccf5cd4dc309cdcaa6328f4c429ead337",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points --- sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space \"intelligently\", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets."
            },
            "slug": "FPNN:-Field-Probing-Neural-Networks-for-3D-Data-Li-Pirk",
            "title": {
                "fragments": [],
                "text": "FPNN: Field Probing Neural Networks for 3D Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work represents 3D spaces as volumetric fields, and proposes a novel design that employs field probing filters to efficiently extract features from them, showing that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2043965"
                        ],
                        "name": "Daniel Maturana",
                        "slug": "Daniel-Maturana",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Maturana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Maturana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32634992"
                        ],
                        "name": "S. Scherer",
                        "slug": "S.-Scherer",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Scherer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Scherer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "Multiview CNNs: [24, 19] have tried to render 3D point cloud or shapes into 2D images and then apply 2D conv nets to classify them."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "Volumetric CNNs: [25, 15, 16] are the pioneers applying 3D convolutional neural networks on voxelized shapes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "Volumetric CNNs: [29, 18, 19] are the pioneers applying 3D convolutional neural networks on voxelized shapes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Spectral CNNs: Some latest works [5, 17] use spectral CNNs on meshes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14620252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f1c6749edfaf4f89bac38b2d15a0493bd9aa253",
            "isKey": true,
            "numCitedBy": 2071,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second."
            },
            "slug": "VoxNet:-A-3D-Convolutional-Neural-Network-for-Maturana-Scherer",
            "title": {
                "fragments": [],
                "text": "VoxNet: A 3D Convolutional Neural Network for real-time object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "VoxNet is proposed, an architecture to tackle the problem of robust object recognition by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152247501"
                        ],
                        "name": "Zhirong Wu",
                        "slug": "Zhirong-Wu",
                        "structuredName": {
                            "firstName": "Zhirong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhirong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340170"
                        ],
                        "name": "S. Song",
                        "slug": "S.-Song",
                        "structuredName": {
                            "firstName": "Shuran",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064662"
                        ],
                        "name": "Linguang Zhang",
                        "slug": "Linguang-Zhang",
                        "structuredName": {
                            "firstName": "Linguang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linguang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 29
                            }
                        ],
                        "text": "We evaluate our model on the ModelNet40 [29] shape classification benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 11
                            }
                        ],
                        "text": "We use the ModelNet40 shape classification problem as a test bed for comparisons\nof those options, the following two control experiments will be on this task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Spectral CNNs: Some latest works [5, 17] use spectral CNNs on meshes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 81
                            }
                        ],
                        "text": "5 0.875 59.2 13.3 0.9375 33.2 10.2\nMetric is overall classification accurcacy on ModelNet40 test set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "Volumetric CNNs: [25, 15, 16] are the pioneers applying 3D convolutional neural networks on voxelized shapes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "We evaluate our model on the ModelNet40 [25] shape classification benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "Multiview CNNs: [24, 19] have tried to render 3D point cloud or shapes into 2D images and then apply 2D conv nets to classify them."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "Volumetric CNNs: [29, 18, 19] are the pioneers applying 3D convolutional neural networks on voxelized shapes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206592833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c8a51d04522496c43db68f2582efd45eaf59fea",
            "isKey": true,
            "numCitedBy": 3086,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks."
            },
            "slug": "3D-ShapeNets:-A-deep-representation-for-volumetric-Wu-Song",
            "title": {
                "fragments": [],
                "text": "3D ShapeNets: A deep representation for volumetric shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network, and shows that this 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111212729"
                        ],
                        "name": "Dominic Zeng Wang",
                        "slug": "Dominic-Zeng-Wang",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Wang",
                            "middleNames": [
                                "Zeng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominic Zeng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1834086"
                        ],
                        "name": "I. Posner",
                        "slug": "I.-Posner",
                        "structuredName": {
                            "firstName": "Ingmar",
                            "lastName": "Posner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Posner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "FPNN [12] and Vote3D [23] proposed special methods to deal with the sparsity problem; however, their operations are still on sparse volumes, it\u2019s challenging for them to process very large point clouds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "FPNN [14] and Vote3D [27] proposed special methods to deal with the sparsity problem; however,\ntheir operations are still on sparse volumes, it\u2019s challenging for them to process very large point clouds."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15568286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8154027ed2e0c1772b54e79c40d30ae9ee468331",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an efficient and effective scheme to applying the sliding window approach popular in computer vision to 3D data. Specifically, the sparse nature of the problem is exploited via a voting scheme to enable a search through all putative object locations at any orientation. We prove that this voting scheme is mathematically equivalent to a convolution on a sparse feature grid and thus enables the processing, in full 3D, of any point cloud irrespective of the number of vantage points required to construct it. As such it is versatile enough to operate on data from popular 3D laser scanners such as a Velodyne as well as on 3D data obtained from increasingly popular push-broom configurations. Our approach is \u201cembarrassingly parallelisable\u201d and capable of processing a point cloud containing over 100K points at eight orientations in less than 0.5s. For the object classes car, pedestrian and bicyclist the resulting detector achieves best-in-class detection and timing performance relative to prior art on the KITTI dataset as well as compared to another existing 3D object detection approach."
            },
            "slug": "Voting-for-Voting-in-Online-Point-Cloud-Object-Wang-Posner",
            "title": {
                "fragments": [],
                "text": "Voting for Voting in Online Point Cloud Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that this voting scheme is mathematically equivalent to a convolution on a sparse feature grid and thus enables the processing, in full 3D, of any point cloud irrespective of the number of vantage points required to construct it."
            },
            "venue": {
                "fragments": [],
                "text": "Robotics: Science and Systems"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689311"
                        ],
                        "name": "R. Rusu",
                        "slug": "R.-Rusu",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Rusu",
                            "middleNames": [
                                "Bogdan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rusu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810221"
                        ],
                        "name": "Nico Blodow",
                        "slug": "Nico-Blodow",
                        "structuredName": {
                            "firstName": "Nico",
                            "lastName": "Blodow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nico Blodow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709715"
                        ],
                        "name": "Zolt\u00e1n-Csaba M\u00e1rton",
                        "slug": "Zolt\u00e1n-Csaba-M\u00e1rton",
                        "structuredName": {
                            "firstName": "Zolt\u00e1n-Csaba",
                            "lastName": "M\u00e1rton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zolt\u00e1n-Csaba M\u00e1rton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746229"
                        ],
                        "name": "M. Beetz",
                        "slug": "M.-Beetz",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Beetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Beetz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1038604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aef7f3c512cd9f96a2108b96a717d6d64882e187",
            "isKey": false,
            "numCitedBy": 679,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate the usage of persistent point feature histograms for the problem of aligning point cloud data views into a consistent global model. Given a collection of noisy point clouds, our algorithm estimates a set of robust 16D features which describe the geometry of each point locally. By analyzing the persistence of the features at different scales, we extract an optimal set which best characterizes a given point cloud. The resulted persistent features are used in an initial alignment algorithm to estimate a rigid transformation that approximately registers the input datasets. The algorithm provides good starting points for iterative registration algorithms such as ICP (Iterative Closest Point), by transforming the datasets to its convergence basin. We show that our approach is invariant to pose and sampling density, and can cope well with noisy data coming from both indoor and outdoor laser scans."
            },
            "slug": "Aligning-point-cloud-views-using-persistent-feature-Rusu-Blodow",
            "title": {
                "fragments": [],
                "text": "Aligning point cloud views using persistent feature histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper investigates the usage of persistent point feature histograms for the problem of aligning point cloud data views into a consistent global model, and estimates a set of robust 16D features which describe the geometry of each point locally."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329939"
                        ],
                        "name": "C. Qi",
                        "slug": "C.-Qi",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209612"
                        ],
                        "name": "M. Nie\u00dfner",
                        "slug": "M.-Nie\u00dfner",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Nie\u00dfner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nie\u00dfner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2208531"
                        ],
                        "name": "Angela Dai",
                        "slug": "Angela-Dai",
                        "structuredName": {
                            "firstName": "Angela",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angela Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235234"
                        ],
                        "name": "Mengyuan Yan",
                        "slug": "Mengyuan-Yan",
                        "structuredName": {
                            "firstName": "Mengyuan",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengyuan Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "Volumetric CNNs: [25, 15, 16] are the pioneers applying 3D convolutional neural networks on voxelized shapes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "Multiview CNNs: [20, 16] have tried to render 3D point cloud or shapes into 2D images and then apply 2D conv nets to classify them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "While MVCNN [20] and Subvolume (3D CNN) [16] achieve high performance, PointNet is orders more efficient in computational cost (measured in FLOPs/sample: 141x and 8x more efficient, respectively)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1009127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cdcf2ae5e1fafebd9b3613247a7b1962584da34",
            "isKey": false,
            "numCitedBy": 1191,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-theart methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multiresolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data."
            },
            "slug": "Volumetric-and-Multi-view-CNNs-for-Object-on-3D-Qi-Su",
            "title": {
                "fragments": [],
                "text": "Volumetric and Multi-view CNNs for Object Classification on 3D Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces two distinct network architectures of volumetric CNNs and examines multi-view CNNs, providing a better understanding of the space of methods available for object classification on 3D data."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144904233"
                        ],
                        "name": "Hang Su",
                        "slug": "Hang-Su",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808670"
                        ],
                        "name": "E. Kalogerakis",
                        "slug": "E.-Kalogerakis",
                        "structuredName": {
                            "firstName": "Evangelos",
                            "lastName": "Kalogerakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kalogerakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Therefore, the model needs to be able to capture local structures from nearby points, and the combinatorial interactions among local structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2407217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ba8376f416e90fe434977ae9f300997667498d2",
            "isKey": false,
            "numCitedBy": 2015,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives."
            },
            "slug": "Multi-view-Convolutional-Neural-Networks-for-3D-Su-Maji",
            "title": {
                "fragments": [],
                "text": "Multi-view Convolutional Neural Networks for 3D Shape Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and shows that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art3D shape descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47782132"
                        ],
                        "name": "L. Yi",
                        "slug": "L.-Yi",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3082383"
                        ],
                        "name": "Vladimir G. Kim",
                        "slug": "Vladimir-G.-Kim",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kim",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir G. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39686979"
                        ],
                        "name": "Duygu Ceylan",
                        "slug": "Duygu-Ceylan",
                        "structuredName": {
                            "firstName": "Duygu",
                            "lastName": "Ceylan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duygu Ceylan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34023939"
                        ],
                        "name": "I-Chao Shen",
                        "slug": "I-Chao-Shen",
                        "structuredName": {
                            "firstName": "I-Chao",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I-Chao Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890549"
                        ],
                        "name": "Mengyan Yan",
                        "slug": "Mengyan-Yan",
                        "structuredName": {
                            "firstName": "Mengyan",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengyan Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830034"
                        ],
                        "name": "Cewu Lu",
                        "slug": "Cewu-Lu",
                        "structuredName": {
                            "firstName": "Cewu",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cewu Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32798502"
                        ],
                        "name": "Qixing Huang",
                        "slug": "Qixing-Huang",
                        "structuredName": {
                            "firstName": "Qixing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixing Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778286"
                        ],
                        "name": "A. Sheffer",
                        "slug": "A.-Sheffer",
                        "structuredName": {
                            "firstName": "Alla",
                            "lastName": "Sheffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sheffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We evaluate on ShapeNet part data set from [26], which contains 16,881 shapes from 16 categories, annotated with 50 parts in total."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "In this section, we compare our segmentation version PointNet (a modified version of Fig 2, Segmentation Network) with two traditional methods [24] and [26] that both take advantage of point-wise geometry features and correspondences between shapes, as well as our own 3D CNN baseline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "We compare with two traditional methods [24] and [26] and a 3D fully convolutional network baseline proposed by us."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2880712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "729d5c7dc6bfb32e47b5bd24cdb01ccaaf62bba5",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Large repositories of 3D shapes provide valuable input for data-driven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a user-specified region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the verification results to improve the automatic propagation algorithm. We use a unified utility function that explicitly models the time cost of human input across all steps of our method. This allows us to jointly optimize for the set of models to annotate and for the set of models to verify based on the predicted impact of these actions on the human efficiency. We demonstrate that incorporating verification of all produced labelings within this unified objective improves both accuracy and efficiency of the active learning procedure. We automatically propagate human labels across a dynamic shape network using a conditional random field (CRF) framework, taking advantage of global shape-to-shape similarities, local feature similarities, and point-to-point correspondences. By combining these diverse cues we achieve higher accuracy than existing alternatives. We validate our framework on existing benchmarks demonstrating it to be significantly more efficient at using human input compared to previous techniques. We further validate its efficiency and robustness by annotating a massive shape dataset, labeling over 93,000 shape parts, across multiple model classes, and providing a labeled part collection more than one order of magnitude larger than existing ones."
            },
            "slug": "A-scalable-active-framework-for-region-annotation-Yi-Kim",
            "title": {
                "fragments": [],
                "text": "A scalable active framework for region annotation in 3D shape collections"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations, and demonstrates that incorporating verification of all produced labelings within this unified objective improves both accuracy and efficiency of the active learning procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689311"
                        ],
                        "name": "R. Rusu",
                        "slug": "R.-Rusu",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Rusu",
                            "middleNames": [
                                "Bogdan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rusu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810221"
                        ],
                        "name": "Nico Blodow",
                        "slug": "Nico-Blodow",
                        "structuredName": {
                            "firstName": "Nico",
                            "lastName": "Blodow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nico Blodow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746229"
                        ],
                        "name": "M. Beetz",
                        "slug": "M.-Beetz",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Beetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Beetz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15022990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "940dd2fa074ad97d5e8efa7e867b1f4460cfb8d5",
            "isKey": false,
            "numCitedBy": 2389,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi-dimensional features which describe the local geometry around a point p for 3D point cloud datasets. In this paper, we modify their mathematical expressions and perform a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views. More concretely, we present several optimizations that reduce their computation times drastically by either caching previously computed values or by revising their theoretical formulations. The latter results in a new type of local features, called Fast Point Feature Histograms (FPFH), which retain most of the discriminative power of the PFH. Moreover, we propose an algorithm for the online computation of FPFH features for realtime applications. To validate our results we demonstrate their efficiency for 3D registration and propose a new sample consensus based method for bringing two datasets into the convergence basin of a local non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment)."
            },
            "slug": "Fast-Point-Feature-Histograms-(FPFH)-for-3D-Rusu-Blodow",
            "title": {
                "fragments": [],
                "text": "Fast Point Feature Histograms (FPFH) for 3D registration"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper modifications their mathematical expressions and performs a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views, and proposes an algorithm for the online computation of FPFH features for realtime applications."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Robotics and Automation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049156"
                        ],
                        "name": "Yi Fang",
                        "slug": "Yi-Fang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152539938"
                        ],
                        "name": "J. Xie",
                        "slug": "J.-Xie",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824268"
                        ],
                        "name": "Guoxian Dai",
                        "slug": "Guoxian-Dai",
                        "structuredName": {
                            "firstName": "Guoxian",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoxian Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39872583"
                        ],
                        "name": "M. Wang",
                        "slug": "M.-Wang",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152506137"
                        ],
                        "name": "F. Zhu",
                        "slug": "F.-Zhu",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118717996"
                        ],
                        "name": "Tiantian Xu",
                        "slug": "Tiantian-Xu",
                        "structuredName": {
                            "firstName": "Tiantian",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tiantian Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "Feature-based DNNs: [6, 8] firstly convert the 3D data into a vector, by extracting traditional shape features and then use a fully connected net to classify the shape."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "Feature-based DNNs: [7, 9] firstly convert the 3D data into a vector, by extracting traditional shape features and then use a fully connected net to classify the shape."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15872657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "170cfe612cc064d8b07626df307e546498c2dc94",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Shape descriptor is a concise yet informative representation that provides a 3D object with an identification as a member of some category. We have developed a concise deep shape descriptor to address challenging issues from ever-growing 3D datasets in areas as diverse as engineering, medicine, and biology. Specifically, in this paper, we developed novel techniques to extract concise but geometrically informative shape descriptor and new methods of defining Eigen-shape descriptor and Fisher-shape descriptor to guide the training of a deep neural network. Our deep shape descriptor tends to maximize the inter-class margin while minimize the intra-class variance. Our new shape descriptor addresses the challenges posed by the high complexity of 3D model and data representation, and the structural variations and noise present in 3D models. Experimental results on 3D shape retrieval demonstrate the superior performance of deep shape descriptor over other state-of-the-art techniques in handling noise, incompleteness and structural variations."
            },
            "slug": "3D-deep-shape-descriptor-Fang-Xie",
            "title": {
                "fragments": [],
                "text": "3D deep shape descriptor"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Novel techniques to extract concise but geometrically informative shape descriptor and new methods of defining Eigen-shape descriptor and Fisher-shape descriptors to guide the training of a deep neural network are developed."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798372"
                        ],
                        "name": "Iro Armeni",
                        "slug": "Iro-Armeni",
                        "structuredName": {
                            "firstName": "Iro",
                            "lastName": "Armeni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iro Armeni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3114252"
                        ],
                        "name": "Ozan Sener",
                        "slug": "Ozan-Sener",
                        "structuredName": {
                            "firstName": "Ozan",
                            "lastName": "Sener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ozan Sener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40029556"
                        ],
                        "name": "A. Zamir",
                        "slug": "A.-Zamir",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Zamir",
                            "middleNames": [
                                "Roshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4910251"
                        ],
                        "name": "Helen Jiang",
                        "slug": "Helen-Jiang",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770886"
                        ],
                        "name": "I. Brilakis",
                        "slug": "I.-Brilakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Brilakis",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Brilakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113447583"
                        ],
                        "name": "Martin Fischer",
                        "slug": "Martin-Fischer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "We experiment on the Stanford 3D semantic parsing data set [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "We follow the same protocol as [1] to use k-fold strategy for train and test."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9649070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc3f8c8513441915408ab0549e9ac5f2f2f31eec",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for discovering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geometric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m2 and over 215 million points, demonstrating robust results readily useful for practical applications."
            },
            "slug": "3D-Semantic-Parsing-of-Large-Scale-Indoor-Spaces-Armeni-Sener",
            "title": {
                "fragments": [],
                "text": "3D Semantic Parsing of Large-Scale Indoor Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper argues that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used, and proposes a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061501761"
                        ],
                        "name": "Kan Guo",
                        "slug": "Kan-Guo",
                        "structuredName": {
                            "firstName": "Kan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780914"
                        ],
                        "name": "Dongqing Zou",
                        "slug": "Dongqing-Zou",
                        "structuredName": {
                            "firstName": "Dongqing",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongqing Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425091532"
                        ],
                        "name": "Xiaowu Chen",
                        "slug": "Xiaowu-Chen",
                        "structuredName": {
                            "firstName": "Xiaowu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaowu Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Feature-based DNNs: [6, 8] firstly convert the 3D data into a vector, by extracting traditional shape features and then use a fully connected net to classify the shape."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18675333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "681bfa1afd80b8210938706cd88a6a0dc02d238d",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a novel approach for 3D mesh labeling by using deep Convolutional Neural Networks (CNNs). Many previous methods on 3D mesh labeling achieve impressive performances by using predefined geometric features. However, the generalization abilities of such low-level features, which are heuristically designed to process specific meshes, are often insufficient to handle all types of meshes. To address this problem, we propose to learn a robust mesh representation that can adapt to various 3D meshes by using CNNs. In our approach, CNNs are first trained in a supervised manner by using a large pool of classical geometric features. In the training process, these low-level features are nonlinearly combined and hierarchically compressed to generate a compact and effective representation for each triangle on the mesh. Based on the trained CNNs and the mesh representations, a label vector is initialized for each triangle to indicate its probabilities of belonging to various object parts. Eventually, a graph-based mesh-labeling algorithm is adopted to optimize the labels of triangles by considering the label consistencies. Experimental results on several public benchmarks show that the proposed approach is robust for various 3D meshes, and outperforms state-of-the-art approaches as well as classic learning algorithms in recognizing mesh labels."
            },
            "slug": "3D-Mesh-Labeling-via-Deep-Convolutional-Neural-Guo-Zou",
            "title": {
                "fragments": [],
                "text": "3D Mesh Labeling via Deep Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on several public benchmarks show that the proposed approach is robust for various 3D meshes, and outperforms state-of-the-art approaches as well as classic learning algorithms in recognizing mesh labels."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] introduces the idea of spatial transformer to align 2D images through sampling and interpolation, achieved by a specifically tailored layer implemented on GPU."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Our input form of point clouds allows us to achieve this goal in a much simpler way compared with [9]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6099034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe87ea16d5eb1c7509da9a0314bbf4c7b0676506",
            "isKey": false,
            "numCitedBy": 4581,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
            },
            "slug": "Spatial-Transformer-Networks-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Spatial Transformer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426718"
                        ],
                        "name": "Jonathan Masci",
                        "slug": "Jonathan-Masci",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Masci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Masci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804261"
                        ],
                        "name": "D. Boscaini",
                        "slug": "D.-Boscaini",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boscaini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boscaini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732570"
                        ],
                        "name": "M. Bronstein",
                        "slug": "M.-Bronstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bronstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bronstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697397"
                        ],
                        "name": "P. Vandergheynst",
                        "slug": "P.-Vandergheynst",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Vandergheynst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vandergheynst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 33
                            }
                        ],
                        "text": "Spectral CNNs: Some latest works [4, 14] use spectral CNNs on meshes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11170942,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a743115795375666593919cbd48590213c229ae9",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional neural networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract \"patches\", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use ShapeNet to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence."
            },
            "slug": "Geodesic-Convolutional-Neural-Networks-on-Manifolds-Masci-Boscaini",
            "title": {
                "fragments": [],
                "text": "Geodesic Convolutional Neural Networks on Riemannian Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional neural networks (CNN) paradigm to non-Euclidean manifolds is introduced, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision Workshop (ICCVW)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143627859"
                        ],
                        "name": "Joan Bruna",
                        "slug": "Joan-Bruna",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bruna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Bruna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563432"
                        ],
                        "name": "Wojciech Zaremba",
                        "slug": "Wojciech-Zaremba",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Zaremba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Zaremba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149531"
                        ],
                        "name": "Arthur D. Szlam",
                        "slug": "Arthur-D.-Szlam",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Szlam",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arthur D. Szlam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 33
                            }
                        ],
                        "text": "Spectral CNNs: Some latest works [4, 14] use spectral CNNs on meshes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17682909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e925a9f1e20df61d1e860a7aa71894b35a1c186",
            "isKey": false,
            "numCitedBy": 2788,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures."
            },
            "slug": "Spectral-Networks-and-Locally-Connected-Networks-on-Bruna-Zaremba",
            "title": {
                "fragments": [],
                "text": "Spectral Networks and Locally Connected Networks on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper considers possible generalizations of CNNs to signals defined on more general domains without the action of a translation group, and proposes two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69424391"
                        ],
                        "name": "Ieee Xplore",
                        "slug": "Ieee-Xplore",
                        "structuredName": {
                            "firstName": "Ieee",
                            "lastName": "Xplore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ieee Xplore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The architecture of our network (Sec 4.2) is inspired by the properties of point sets in Rn (Sec 4.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60546322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8ed6678758f9200bd23fcf11dd733c8f4d9d71c",
            "isKey": true,
            "numCitedBy": 1832,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the real world, a realistic setting for computer vision or multimedia recognition problems is that we have some classes containing lots of training data and many classes contain a small amount of training data. Therefore, how to use frequent classes to help learning rare classes for which it is harder to collect the training data is an open question. Learning with Shared Information is an emerging topic in machine learning, computer vision and multimedia analysis. There are different level of components that can be shared during concept modeling and machine learning stages, such as sharing generic object parts, sharing attributes, sharing transformations, sharing regularization parameters and sharing training examples, etc. Regarding the specific methods, multi-task learning, transfer learning and deep learning can be seen as using different strategies to share information. These learning with shared information methods are very effective in solving real-world large-scale problems. This special issue aims at gathering the recent advances in learning with shared information methods and their applications in computer vision and multimedia analysis. Both state-of-the-art works, as well as literature reviews, are welcome for submission. Papers addressing interesting real-world computer vision and multimedia applications are especially encouraged. Topics of interest include, but are not limited to:  \u2022 Multi-task learning or transfer learning for large-scale computer vision and multimedia analysis \u2022 Deep learning for large-scale computer vision and multimedia analysis \u2022 Multi-modal approach for large-scale computer vision and multimedia analysis \u2022 Different sharing strategies, e.g., sharing generic object parts, sharing attributes, sharing transformations, sharing regularization parameters and sharing training examples, \u2022 Real-world computer vision and multimedia applications based on learning with shared information, e.g., event detection, object recognition, object detection, action recognition, human head pose estimation, object tracking, location-based services, semantic indexing. \u2022 New datasets and metrics to evaluate the benefit of the proposed sharing ability for the specific computer vision or multimedia problem. \u2022 Survey papers regarding the topic of learning with shared information.  Authors who are unsure whether their planned submission is in scope may contact the guest editors prior to the submission deadline with an abstract, in order to receive feedback."
            },
            "slug": "IEEE-transactions-on-pattern-analysis-and-machine-Xplore",
            "title": {
                "fragments": [],
                "text": "IEEE transactions on pattern analysis and machine intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This special issue aims at gathering the recent advances in learning with shared information methods and their applications in computer vision and multimedia analysis and addressing interesting real-world computer Vision and multimedia applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1848882"
                        ],
                        "name": "Zizhao Wu",
                        "slug": "Zizhao-Wu",
                        "structuredName": {
                            "firstName": "Zizhao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zizhao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3218604"
                        ],
                        "name": "Ruyang Shou",
                        "slug": "Ruyang-Shou",
                        "structuredName": {
                            "firstName": "Ruyang",
                            "lastName": "Shou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruyang Shou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144039495"
                        ],
                        "name": "Yunhai Wang",
                        "slug": "Yunhai-Wang",
                        "structuredName": {
                            "firstName": "Yunhai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunhai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49544028"
                        ],
                        "name": "Xinguo Liu",
                        "slug": "Xinguo-Liu",
                        "structuredName": {
                            "firstName": "Xinguo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinguo Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5509050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff5d25c1d8541f7773829dedfaab1084b4eb0a1c",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interactive-shape-co-segmentation-via-label-Wu-Shou",
            "title": {
                "fragments": [],
                "text": "Interactive shape co-segmentation via label propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2576037"
                        ],
                        "name": "Ding-Yun Chen",
                        "slug": "Ding-Yun-Chen",
                        "structuredName": {
                            "firstName": "Ding-Yun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ding-Yun Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2176227"
                        ],
                        "name": "Xiao-Pei Tian",
                        "slug": "Xiao-Pei-Tian",
                        "structuredName": {
                            "firstName": "Xiao-Pei",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Pei Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32833446"
                        ],
                        "name": "E. Shen",
                        "slug": "E.-Shen",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Shen",
                            "middleNames": [
                                "Yu-Te"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744863"
                        ],
                        "name": "Ming Ouhyoung",
                        "slug": "Ming-Ouhyoung",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Ouhyoung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Ouhyoung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 395534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc01194dca94f0cde2f98caa93f9740d7cbd1642",
            "isKey": false,
            "numCitedBy": 1385,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of 3D models are created and available on the Web, since more and more 3D modelling anddigitizing tools are developed for ever increasing applications. The techniques for content\u2010based 3D model retrievalthen become necessary. In this paper, a visual similarity\u2010based 3D model retrieval system is proposed.This approach measures the similarity among 3D models by visual similarity, and the main idea is that if two 3Dmodels are similar, they also look similar from all viewing angles. Therefore, one hundred orthogonal projectionsof an object, excluding symmetry, are encoded both by Zernike moments and Fourier descriptors as features forlater retrieval. The visual similarity\u2010based approach is robust against similarity transformation, noise, model degeneracyetc., and provides 42%, 94% and 25% better performance (precision\u2010recall evaluation diagram) thanthree other competing approaches: (1) the spherical harmonics approach developed by Funkhouser et al., (2) theMPEG\u20107 Shape 3D descriptors, and (3) the MPEG\u20107 Multiple View Descriptor. The proposed system is on the Webfor practical trial use (http://3d.csie.ntu.edu.tw), and the database contains more than 10,000 publicly available3D models collected from WWW pages. Furthermore, a user friendly interface is provided to retrieve 3D modelsby drawing 2D shapes. The retrieval is fast enough on a server with Pentium IV 2.4 GHz CPU, and it takes about2 seconds and 0.1 seconds for querying directly by a 3D model and by hand drawn 2D shapes, respectively."
            },
            "slug": "On-Visual-Similarity-Based-3D-Model-Retrieval-Chen-Tian",
            "title": {
                "fragments": [],
                "text": "On Visual Similarity Based 3D Model Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A visual similarity\u2010based 3D model retrieval system that is robust against similarity transformation, noise, model degeneracy, and provides 42%, 94% and 25% better performance than three other competing approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "More Visualizations Classification Visualization We use t-SNE[2] to embed point cloud global signature (1024-dim) from our classification PointNet into a 2D space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5855042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c46943103bd7b7a2c7be86859995a4144d1938b",
            "isKey": false,
            "numCitedBy": 22357,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets."
            },
            "slug": "Visualizing-Data-using-t-SNE-Maaten-Hinton",
            "title": {
                "fragments": [],
                "text": "Visualizing Data using t-SNE"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new technique called t-SNE that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map, a variation of Stochastic Neighbor Embedding that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3037691"
                        ],
                        "name": "A. Johnson",
                        "slug": "A.-Johnson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Johnson",
                            "middleNames": [
                                "Edie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1377132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df6c0c55864252090b4099237aa821a6c75b52c2",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a 3D shape-based object recognition system for simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching surfaces by matching points using the spin image representation. The spin image is a data level shape descriptor that is used to match surfaces represented as surface meshes. We present a compression scheme for spin images that results in efficient multiple object recognition which we verify with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trials on 100 scenes."
            },
            "slug": "Using-Spin-Images-for-Efficient-Object-Recognition-Johnson-Hebert",
            "title": {
                "fragments": [],
                "text": "Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A compression scheme for spin images that results in efficient multiple object recognition which is verified with results showing the simultaneous recognition of multiple objects from a library of 20 models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690653"
                        ],
                        "name": "M. Kazhdan",
                        "slug": "M.-Kazhdan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kazhdan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kazhdan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7723706"
                        ],
                        "name": "S. Rusinkiewicz",
                        "slug": "S.-Rusinkiewicz",
                        "structuredName": {
                            "firstName": "Szymon",
                            "lastName": "Rusinkiewicz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rusinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The test results for all six areas are aggregated for the PR curve generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8166368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da275dcffd835ddfd41fd73ea147c767c605d3f0",
            "isKey": false,
            "numCitedBy": 1421,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the challenges in 3D shape matching arises from the fact that in many applications, models should be considered to be the same if they differ by a rotation. Consequently, when comparing two models, a similarity metric implicitly provides the measure of similarity at the optimal alignment. Explicitly solving for the optimal alignment is usually impractical. So, two general methods have been proposed for addressing this issue: (1) Every model is represented using rotation invariant descriptors. (2) Every model is described by a rotation dependent descriptor that is aligned into a canonical coordinate system defined by the model. In this paper, we describe the limitations of canonical alignment and discuss an alternate method, based on spherical harmonics, for obtaining rotation invariant representations. We describe the properties of this tool and show how it can be applied to a number of existing, orientation dependent descriptors to improve their matching performance. The advantages of this tool are two-fold: First, it improves the matching performance of many descriptors. Second, it reduces the dimensionality of the descriptor, providing a more compact representation, which in turn makes comparing two models more efficient."
            },
            "slug": "Rotation-Invariant-Spherical-Harmonic-of-3D-Shape-Kazhdan-Funkhouser",
            "title": {
                "fragments": [],
                "text": "Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The limitations of canonical alignment are described and an alternate method, based on spherical harmonics, for obtaining rotation invariant representations is discussed, which reduces the dimensionality of the descriptor, providing a more compact representation, which in turn makes comparing two models more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Geometry Processing"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805398"
                        ],
                        "name": "Haibin Ling",
                        "slug": "Haibin-Ling",
                        "structuredName": {
                            "firstName": "Haibin",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haibin Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34734622"
                        ],
                        "name": "D. Jacobs",
                        "slug": "D.-Jacobs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17848433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9bb27a60b6c2555a4c01c4c0b8808f1e3625403",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Part structure and articulation are of fundamental importance in computer and human vision. We propose using the inner-distance to build shape descriptors that are robust to articulation and capture part structure. The inner-distance is defined as the length of the shortest path between landmark points within the shape silhouette. We show that it is articulation insensitive and more effective at capturing part structures than the Euclidean distance. This suggests that the inner-distance can be used as a replacement for the Euclidean distance to build more accurate descriptors for complex shapes, especially for those with articulated parts. In addition, texture information along the shortest path can be used to further improve shape classification. With this idea, we propose three approaches to using the inner-distance. The first method combines the inner-distance and multidimensional scaling (MDS) to build articulation invariant signatures for articulated shapes. The second method uses the inner-distance to build a new shape descriptor based on shape contexts. The third one extends the second one by considering the texture information along shortest paths. The proposed approaches have been tested on a variety of shape databases, including an articulated shape data set, MPEG7 CE-Shape-1, Kimia silhouettes, the ETH-80 data set, two leaf data sets, and a human motion silhouette data set. In all the experiments, our methods demonstrate effective performance compared with other algorithms"
            },
            "slug": "Shape-Classification-Using-the-Inner-Distance-Ling-Jacobs",
            "title": {
                "fragments": [],
                "text": "Shape Classification Using the Inner-Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is suggested that the inner-distance can be used as a replacement for the Euclidean distance to build more accurate descriptors for complex shapes, especially for those with articulated parts."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732570"
                        ],
                        "name": "M. Bronstein",
                        "slug": "M.-Bronstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bronstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bronstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 174
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9117881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ec6dd508e48231fcbe9c11c7e18790b0dee8b26",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the biggest challenges in non-rigid shape retrieval and comparison is the design of a shape descriptor that would maintain invariance under a wide class of transformations the shape can undergo. Recently, heat kernel signature was introduced as an intrinsic local shape descriptor based on diffusion scale-space analysis. In this paper, we develop a scale-invariant version of the heat kernel descriptor. Our construction is based on a logarithmically sampled scale-space in which shape scaling corresponds, up to a multiplicative constant, to a translation. This translation is undone using the magnitude of the Fourier transform. The proposed scale-invariant local descriptors can be used in the bag-of-features framework for shape retrieval in the presence of transformations such as isometric deformations, missing data, topological noise, and global and local scaling. We get significant performance improvement over state-of-the-art algorithms on recently established non-rigid shape retrieval benchmarks."
            },
            "slug": "Scale-invariant-heat-kernel-signatures-for-shape-Bronstein-Kokkinos",
            "title": {
                "fragments": [],
                "text": "Scale-invariant heat kernel signatures for non-rigid shape recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A scale-invariant version of the heat kernel descriptor that can be used in the bag-of-features framework for shape retrieval in the presence of transformations such as isometric deformations, missing data, topological noise, and global and local scaling."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48581305"
                        ],
                        "name": "Michael Gschwandtner",
                        "slug": "Michael-Gschwandtner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gschwandtner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Gschwandtner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2132917"
                        ],
                        "name": "R. Kwitt",
                        "slug": "R.-Kwitt",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kwitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kwitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30134366"
                        ],
                        "name": "A. Uhl",
                        "slug": "A.-Uhl",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Uhl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Uhl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684717"
                        ],
                        "name": "W. Pree",
                        "slug": "W.-Pree",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Pree",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pree"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26434726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a31f548fde2f1622bb5040048c160c93417fb0dd",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel software package for the simulation of various types of range scanners. The goal is to provide researchers in the fields of obstacle detection, range data segmentation, obstacle tracking or surface reconstruction with a versatile and powerful software package that is easy to use and allows to focus on algorithmic improvements rather than on building the software framework around it. The simulation environment and the actual simulations can be efficiently distributed with a single compact file. Our proposed approach facilitates easy regeneration of published results, hereby highlighting the value of reproducible research."
            },
            "slug": "BlenSor:-Blender-Sensor-Simulation-Toolbox-Gschwandtner-Kwitt",
            "title": {
                "fragments": [],
                "text": "BlenSor: Blender Sensor Simulation Toolbox"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel software package for the simulation of various types of range scanners that is easy to use and allows to focus on algorithmic improvements rather than on building the software framework around it."
            },
            "venue": {
                "fragments": [],
                "text": "ISVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38767254"
                        ],
                        "name": "David Steinkraus",
                        "slug": "David-Steinkraus",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinkraus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steinkraus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "input error (%) Multi-layer perceptron [4] vector 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4659176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5562a56da3a96dae82add7de705e2bd841eb00fc",
            "isKey": false,
            "numCitedBy": 2432,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages."
            },
            "slug": "Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus",
            "title": {
                "fragments": [],
                "text": "Best practices for convolutional neural networks applied to visual document analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A set of concrete bestpractices that document analysis researchers can use to get good results with neural networks, including a simple \"do-it-yourself\" implementation of convolution with a flexible architecture suitable for many visual document problems."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057642721"
                        ],
                        "name": "Mart\u00edn Abadi",
                        "slug": "Mart\u00edn-Abadi",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Abadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Abadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078528337"
                        ],
                        "name": "Ashish Agarwal",
                        "slug": "Ashish-Agarwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144758007"
                        ],
                        "name": "P. Barham",
                        "slug": "P.-Barham",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Barham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Barham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445241"
                        ],
                        "name": "E. Brevdo",
                        "slug": "E.-Brevdo",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Brevdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brevdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48738717"
                        ],
                        "name": "C. Citro",
                        "slug": "C.-Citro",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Citro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Citro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36347083"
                        ],
                        "name": "Andy Davis",
                        "slug": "Andy-Davis",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780892"
                        ],
                        "name": "S. Ghemawat",
                        "slug": "S.-Ghemawat",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Ghemawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghemawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064102917"
                        ],
                        "name": "A. Harp",
                        "slug": "A.-Harp",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Harp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Harp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060655766"
                        ],
                        "name": "Geoffrey Irving",
                        "slug": "Geoffrey-Irving",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Irving",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey Irving"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942300"
                        ],
                        "name": "M. Kudlur",
                        "slug": "M.-Kudlur",
                        "structuredName": {
                            "firstName": "Manjunath",
                            "lastName": "Kudlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kudlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3369421"
                        ],
                        "name": "J. Levenberg",
                        "slug": "J.-Levenberg",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Levenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30415265"
                        ],
                        "name": "Dandelion Man\u00e9",
                        "slug": "Dandelion-Man\u00e9",
                        "structuredName": {
                            "firstName": "Dandelion",
                            "lastName": "Man\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dandelion Man\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375552"
                        ],
                        "name": "Sherry Moore",
                        "slug": "Sherry-Moore",
                        "structuredName": {
                            "firstName": "Sherry",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherry Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20154699"
                        ],
                        "name": "D. Murray",
                        "slug": "D.-Murray",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Murray",
                            "middleNames": [
                                "Gordon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37232298"
                        ],
                        "name": "C. Olah",
                        "slug": "C.-Olah",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Olah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Olah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32163737"
                        ],
                        "name": "Benoit Steiner",
                        "slug": "Benoit-Steiner",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Steiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benoit Steiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35210462"
                        ],
                        "name": "Kunal Talwar",
                        "slug": "Kunal-Talwar",
                        "structuredName": {
                            "firstName": "Kunal",
                            "lastName": "Talwar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kunal Talwar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765169"
                        ],
                        "name": "F. Vi\u00e9gas",
                        "slug": "F.-Vi\u00e9gas",
                        "structuredName": {
                            "firstName": "Fernanda",
                            "lastName": "Vi\u00e9gas",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vi\u00e9gas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47941411"
                        ],
                        "name": "P. Warden",
                        "slug": "P.-Warden",
                        "structuredName": {
                            "firstName": "Pete",
                            "lastName": "Warden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Warden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145233583"
                        ],
                        "name": "M. Wattenberg",
                        "slug": "M.-Wattenberg",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wattenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wattenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35078078"
                        ],
                        "name": "M. Wicke",
                        "slug": "M.-Wicke",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117163698"
                        ],
                        "name": "Yuan Yu",
                        "slug": "Yuan-Yu",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152198093"
                        ],
                        "name": "Xiaoqiang Zheng",
                        "slug": "Xiaoqiang-Zheng",
                        "structuredName": {
                            "firstName": "Xiaoqiang",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqiang Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "To be more specific, for every given query shape from ModelNet test split, we compute its global signature (output of the layer before the score prediction layer) given by our classification PointNet and retrieve similar shapes in the train split by nearest neighbor search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5707386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "isKey": false,
            "numCitedBy": 9182,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
            },
            "slug": "TensorFlow:-Large-Scale-Machine-Learning-on-Systems-Abadi-Agarwal",
            "title": {
                "fragments": [],
                "text": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The TensorFlow interface and an implementation of that interface that is built at Google are described, which has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152147554"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690177"
                        ],
                        "name": "M. Ovsjanikov",
                        "slug": "M.-Ovsjanikov",
                        "structuredName": {
                            "firstName": "Maks",
                            "lastName": "Ovsjanikov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ovsjanikov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 174
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12701882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e847a57817dd4a24eb0d843ac62a2f602058808",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel point signature based on the properties of the heat diffusion process on a shape. Our signature, called the Heat Kernel Signature (or HKS), is obtained by restricting the well\u2010known heat kernel to the temporal domain. Remarkably we show that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry. This means that the restriction to the temporal domain, on the one hand, makes HKS much more concise and easily commensurable, while on the other hand, it preserves all of the information about the intrinsic geometry of the shape. In addition, HKS inherits many useful properties from the heat kernel, which means, in particular, that it is stable under perturbations of the shape. Our signature also provides a natural and efficiently computable multi\u2010scale way to capture information about neighborhoods of a given point, which can be extremely useful in many applications. To demonstrate the practical relevance of our signature, we present several methods for non\u2010rigid multi\u2010scale matching based on the HKS and use it to detect repeated structure within the same shape and across a collection of shapes."
            },
            "slug": "A-Concise-and-Provably-Informative-Multi\u2010Scale-on-Sun-Ovsjanikov",
            "title": {
                "fragments": [],
                "text": "A Concise and Provably Informative Multi\u2010Scale Signature Based on Heat Diffusion"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The Heat Kernel Signature, called the HKS, is obtained by restricting the well\u2010known heat kernel to the temporal domain and shows that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942300"
                        ],
                        "name": "M. Kudlur",
                        "slug": "M.-Kudlur",
                        "structuredName": {
                            "firstName": "Manjunath",
                            "lastName": "Kudlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kudlur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "The attention method is similar to that in [22], where a scalar score is predicted from each point feature, then the score is normalized across points by computing a softmax."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "One recent work from Oriol Vinyals et al [22] looks into this problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "However in \u201cOrderMatters\u201d [22] the authors have shown that order does matter and cannot be totally omitted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 21
                            }
                        ],
                        "text": "One recent work from Oriol Vinyals et al [26] looks into this problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 11
                            }
                        ],
                        "text": "However in \u201cOrderMatters\u201d [26] the authors have shown that order does matter and cannot be totally omitted."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13948549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d01379ebb53c66a4ccf5f4959d904dcf9e161e41",
            "isKey": true,
            "numCitedBy": 656,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models."
            },
            "slug": "Order-Matters:-Sequence-to-sequence-for-sets-Vinyals-Bengio",
            "title": {
                "fragments": [],
                "text": "Order Matters: Sequence to sequence for sets"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way is discussed and a loss is proposed which, by searching over possible orders during training, deals with the lack of structure of output sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35263,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48582897"
                        ],
                        "name": "Mathieu Aubry",
                        "slug": "Mathieu-Aubry",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Aubry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Aubry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123320"
                        ],
                        "name": "Ulrich Schlickewei",
                        "slug": "Ulrich-Schlickewei",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Schlickewei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrich Schlickewei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 174
                            }
                        ],
                        "text": "Point features often encode certain statistical properties of points and are designed to be invariant to certain transformations, which are typically classified as intrinsic [2, 21, 3] or extrinsic [18, 17, 13, 10, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17847588,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "955635d7179484f106f584c3d5766dee0cfa45db",
            "isKey": false,
            "numCitedBy": 641,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the Wave Kernel Signature (WKS) for characterizing points on non-rigid three-dimensional shapes. The WKS represents the average probability of measuring a quantum mechanical particle at a specific location. By letting vary the energy of the particle, the WKS encodes and separates information from various different Laplace eigenfrequencies. This clear scale separation makes the WKS well suited for a large variety of applications. Both theoretically and in quantitative experiments we demonstrate that the WKS is substantially more discriminative and therefore allows for better feature matching than the commonly used Heat Kernel Signature (HKS). As an application of the WKS in shape analysis we show results on shape matching."
            },
            "slug": "The-wave-kernel-signature:-A-quantum-mechanical-to-Aubry-Schlickewei",
            "title": {
                "fragments": [],
                "text": "The wave kernel signature: A quantum mechanical approach to shape analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Both theoretically and in quantitative experiments it is demonstrated that the WKS is substantially more discriminative and therefore allows for better feature matching than the commonly used Heat Kernel Signature."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, rotating and translating points all together should not modify the global point cloud category nor the segmentation of the points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9773222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b89e9f0cef5ace08946a7c07bf7284854c418445",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "When a vision system creates an interpretation of some input data, it assigns truth values or probabilities to internal hypotheses about the world. \\Ve present a non-deterministic method for assigning truth values that avoids many of the problems encountered by existing relaxation methods. Instead of representing probabilities with realnumbers. we use a more direct encoding in which the probability \\ associated with a hypothesis is represented by the probability that it is in one of two states. true or false. We give a particular nondeterministic operator. based on statistlcal mechanics. for updating the truth values of hypotheses. The operator ensures that the probability of discovering a particular combination of hypotheses is a simple function of how good that combination is. We show that there is a simple relationship between this operato.r and Bayesian inference. and we describe a learning rule which allows a parallel system to converge on a set\"of weights that optimizes its perceptual inferences."
            },
            "slug": "Proceedings-of-the-IEEE-Conference-on-Computer-and-Hinton",
            "title": {
                "fragments": [],
                "text": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition-' Washington , D . C . , June , 1983 OPTIMAL PERCEPTUAL INFERENCE"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A learning rule is described which allows a parallel system to converge on a set of weights that optimizes its perceptual inferences and it is shown that there is a simple relationship between this operato.r and Bayesian inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The architecture of our network (Sec 4.2) is inspired by the properties of point sets in Rn (Sec 4.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shrec16 track largescale 3d shape retrieval from shapenet core55"
            },
            "venue": {
                "fragments": [],
                "text": "Shrec16 track largescale 3d shape retrieval from shapenet core55"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic segmentation is not that accurate, with only 47% mIoU, where the ones nowadays can achieve over 80%"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "It provides a unified approach to a number of 3D recognition tasks including object classification, part segmentation and semantic segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "\u2022 PointNet is a novel deep neural network that directly consumes point cloud"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": "For every CAD model in the ShapeNet part data set, we use Blensor Kinect Simulator [8] to generate incomplete point clouds from six random viewpoints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "For every CAD model in the ShapeNet part data set, we use Blensor Kinect Simulator [7] to generate incomplete point clouds from six random viewpoints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BlenSor: Blender Sensor Simulation Toolbox Advances in Visual Computing"
            },
            "venue": {
                "fragments": [],
                "text": "volume 6939 of Lecture Notes in Computer Science, chapter 20, pages 199\u2013208. Springer Berlin / Heidelberg, Berlin, Heidelberg,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": "For every CAD model in the ShapeNet part data set, we use Blensor Kinect Simulator [8] to generate incomplete point clouds from six random viewpoints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "For every CAD model in the ShapeNet part data set, we use Blensor Kinect Simulator [7] to generate incomplete point clouds from six random viewpoints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BlenSor: Blender Sensor Simulation Toolbox Advances in Visual Computing. volume 6939 of Lecture Notes in Computer Science, chapter 20, pages 199\u2013208"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Voxnet : A 3 d convolutional neural network for real - time object recognition Volumetric and multiview cnns for object classification on 3 d data"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Computer Vision and Pattern Recognition ( CVPR ) , IEEE"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The test results for all six areas are aggregated for the PR curve generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatial transformer networks. In NIPS 2015"
            },
            "venue": {
                "fragments": [],
                "text": "Spatial transformer networks. In NIPS 2015"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interactive shape cosegmentation via label propagation. Computers Graphics"
            },
            "venue": {
                "fragments": [],
                "text": "Interactive shape cosegmentation via label propagation. Computers Graphics"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/PointNet:-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba?sort=total-citations"
}