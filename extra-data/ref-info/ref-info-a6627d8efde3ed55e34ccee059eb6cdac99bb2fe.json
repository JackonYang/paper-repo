{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119559365,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5b163877b1c9e19b9d1bfa11ad667564a2e9678f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The objects listed in the title have proven to be useful and practical modeling tools in continuous speech recognition work and elsewhere. Nevertheless, there are natural and simple situations in which the forward-backward algorithm will be inadequate for its intended purpose of finding useful maximum likelihood estimates of the parameters of the distribution of a probabilistic function of a Markov chain (a \"hidden Markov model\" or \"Markov source model\"). We observe some difficulties that arise in the case of common (e.g., Gaussian) families of conditional distributions for the observables. These difficulties are due not to the algorithm itself, but to modeling assumptions which introduce singularities into the likelihood function. We also comment on the fact that the parameters of a hidden Markov model cannot, in general, be determined, even if the distribution of the observables is completely known. We close with remarks about some effects of these modeling and estimating difficulties on practical speech recognition, and about the role of initial statistics."
            },
            "slug": "Hidden-Markov-chains,-the-forward-backward-and-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "Hidden Markov chains, the forward-backward algorithm, and initial statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145778742"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Juang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11358505,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d076613d7c36dbda4a6ff42fbdd076604b96630",
            "isKey": false,
            "numCitedBy": 2944,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."
            },
            "slug": "An-introduction-to-hidden-Markov-models-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "An introduction to hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The purpose of this tutorial paper is to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787242"
                        ],
                        "name": "Y. Ephraim",
                        "slug": "Y.-Ephraim",
                        "structuredName": {
                            "firstName": "Yariv",
                            "lastName": "Ephraim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ephraim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768934"
                        ],
                        "name": "A. Dembo",
                        "slug": "A.-Dembo",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Dembo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dembo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of studies have been aimed at finding an alternate criterion to maximum likelihood in selecting the parameters of the model: these include Mercer\u2019s mavimum mutual-information [3], [53] and the work of Ephraim et a/ on minimum cross-entropy [ 24 ] (see also [32], [68])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1312345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f352b1770357f931807c6232ac879f2845980413",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A new iterative approach for hidden Markov modeling of information sources which aims at minimizing the discrimination information (or the cross-entropy) between the source and the model is proposed. This approach does not require the commonly used assumption that the source to be modeled is a hidden Markov process. The algorithm is started from the model estimated by the traditional maximum likelihood (ML) approach and alternatively decreases the discrimination information over all probability distributions of the source which agree with the given measurements and all hidden Markov models. The proposed procedure generalizes the Baum algorithm for ML hidden Markov modeling. The procedure is shown to be a descent algorithm for the discrimination information measure and its local convergence is proved."
            },
            "slug": "A-minimum-discrimination-information-approach-for-Ephraim-Dembo",
            "title": {
                "fragments": [],
                "text": "A minimum discrimination information approach for hidden Markov modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new iterative approach for hidden Markov modeling of information sources which aims at minimizing the discrimination information (or the cross-entropy) between the source and the model is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46254718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call \u201cleft-to-right\u201d models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper."
            },
            "slug": "An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner",
            "title": {
                "fragments": [],
                "text": "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain, and focuses on a particular class of Markov models, which are especially appropriate for isolated word recognition."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423281"
                        ],
                        "name": "L. A. Liporace",
                        "slug": "L.-A.-Liporace",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Liporace",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. A. Liporace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30026295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "664eb4fb59f2ce8f2e019a77653f9ed2cc5df591",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Parameter estimation for multivariate functions of Markov chains, a class of versatile statistical models for vector random processes, is discussed. The model regards an ordered sequence of vectors as noisy multivariate observations of a Markov chain. Mixture distributions are a special case. The foundations of the theory presented here were established by Baum, Petrie, Soules, and Weiss. A powerful representation theorem by Fan is employed to generalize the analysis of Baum, {\\em et al.} to a larger class of distributions."
            },
            "slug": "Maximum-likelihood-estimation-for-multivariate-of-Liporace",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation for multivariate observations of Markov sources"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Parameter estimation for multivariate functions of Markov chains, a class of versatile statistical models for vector random processes, is discussed, and a powerful representation theorem by Fan is employed to generalize the analysis of Baum, et al. to a larger class of distributions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121831295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7c3f98c68b6609599771b1161c0d94200eae03dc",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Continuously-variable-duration-hidden-Markov-models-Levinson",
            "title": {
                "fragments": [],
                "text": "Continuously variable duration hidden Markov models for automatic speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102981024"
                        ],
                        "name": "A. Poritz",
                        "slug": "A.-Poritz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Poritz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poritz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73321613"
                        ],
                        "name": "A. Richter",
                        "slug": "A.-Richter",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Richter",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Richter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61065603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "489983d52c982aa8c00fae6b9dc8d16afbc8c7f0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov modeling has become an increasingly popular technique in automatic speech recognition. Recently, attention has been focused on the application of these models to talker-independent, isolated-word recognition. Initial results using models with discrete output densities for isolated-digit recognition were later improved using models based on continuous output densities. In a series of experiments on isolated-word recognition, we applied hidden Markov models with multivariate Gaussian output densities to the problem. Speech data was represented by feature vectors consisting of eight log area ratios and the log LPC error. A weak measure of vocal-tract dynamics was included in the observations by appending to the feature vector observed at time t, the vector observed at time t-\u03b4, for some fixed offset \u03b4. The best models were obtained with offsets of 75 or 90 msecs. When a comparison is made on a common data base, the resulting error rate of 0.2% for isolated-digit recognition improves on previous algorithms."
            },
            "slug": "On-hidden-Markov-models-in-isolated-word-Poritz-Richter",
            "title": {
                "fragments": [],
                "text": "On hidden Markov models in isolated word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In a series of experiments on isolated-word recognition, hidden Markov models with multivariate Gaussian output densities with best models obtained with offsets of 75 or 90 msecs improved on previous algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The time series dynamics may carry Markov structure; improvement in performance has been obt,ained by concatenating items from nearby times into a single jointly-distributed poly-observation [59],[ 3 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of studies have been aimed at finding an alternate criterion to maximum likelihood in selecting the parameters of the model: these include Mercer\u2019s mavimum mutual-information [ 3 ], [53] and the work of Ephraim et a/ on minimum cross-entropy [24] (see also [32], [68])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "11 [63],[59],[44],[ 3 ] also NQ Given a signal, Y = (y-~+ as a time series generated by a hz Poritz [58] . . , y~), we can consider it er hidden Markov model; see"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60753901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c41868f69d265783b7540094946ee902571c5cd",
            "isKey": true,
            "numCitedBy": 50,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The acoustic-modelling problem in automatic speech recognition is examined from an information theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is factored into two steps: a signal-processing step which converts a speech waveform into a sequence of informative acoustic feature vectors, and a step which models such a sequence. The authors are primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space. They explore the trade-off between packing information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous-parameter sequences is addressed by investigating a method of parameter estimation which is designed to cope with inaccurate modeling assumptions.<<ETX>>"
            },
            "slug": "Speech-recognition-with-continuous-parameter-hidden-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "Speech recognition with continuous-parameter hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors explore the trade-off between packing information into sequences of feature vectors and being able to model them accurately and investigate a method of parameter estimation which is designed to cope with inaccurate modeling assumptions."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15553242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "436f38dc28ca25af965b202ebe0e27c747888da6",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a signal modeling technique based upon finite mixture autoregressive probabilistic functions of Markov chains is developed and applied to the problem of speech recognition, particularly speaker-independent recognition of isolated digits. Two types of mixture probability densities are investigated: finite mixtures of Gaussian autoregressive densities (GAM) and nearest-neighbor partitioned finite mixtures of Gaussian autoregressive densities (PGAM). In the former (GAM), the observation density in each Markov state is simply a (stochastically constrained) weighted sum of Gaussian autoregressive densities, while in the latter (PGAM) it involves nearest-neighbor decoding which in effect, defines a set of partitions on the observation space. In this paper we discuss the signal modeling methodology and give experimental results on speaker independent recognition of isolated digits. We also discuss the potential use of the modeling technique for other applications."
            },
            "slug": "Mixture-autoregressive-hidden-Markov-models-for-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Mixture autoregressive hidden Markov models for speech signals"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The signal modeling methodology is discussed and experimental results on speaker independent recognition of isolated digits are given and the potential use of the modeling technique for other applications are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102981024"
                        ],
                        "name": "A. Poritz",
                        "slug": "A.-Poritz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Poritz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poritz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These models approximate (for example, a vocal tract, [ 58 ] time-dependent, noisy articulation of"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Given a signal, Y = (y-~+ as a time series generated by a hz Poritz [ 58 ] . . , y~), we can consider it er hidden Markov model; see"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "They include a number of generalizations of both the spatial and temporal components of the models, for example variable-duration hidden Markov models [30], continuous multivariate hidden Markov models [47], hidden-filter hidden Markov models [ 58 ], and trainable finite-state (hidden) grammars [8] A special case of the results in [13] has been designated by Dempster et alas the EM al,gorithm, see 1231, especially pp. 28-29 and 1621 NJ 08540"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The idea has been used to model text (Cave and Neuwirth [17]), phonetics (Neuburg [54]) and speech (Poritz [ 58 ])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32413326,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1e6f96bee0a7b78402866e1461d00a72612dcc69",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for modelling time series is presented and then applied to the analysis of the speech signal. A time series is represented as a sample sequence generated by a finite state hidden Markov model with output densities parameterized by linear prediction polynomials and error variances. These objects are defined and their properties developed. The theory culminates in a theorem that provides a computationally efficient iterative scheme to improve the model. The theorem has been used to create models from speech signals of considerable length. One such model is examined with emphasis on the relationship between states of the model and traditional classes of speech events. A use of the method is illustrated by an application to the talker verification problem."
            },
            "slug": "Linear-predictive-hidden-Markov-models-and-the-Poritz",
            "title": {
                "fragments": [],
                "text": "Linear predictive hidden Markov models and the speech signal"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method for modelling time series is presented and then applied to the analysis of the speech signal, resulting in a theorem that provides a computationally efficient iterative scheme to improve the model."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829859"
                        ],
                        "name": "E. Neuburg",
                        "slug": "E.-Neuburg",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Neuburg",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Neuburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119813540,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3a22351902820cb877ac4c611c1d5a320fcf393c",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical statistical models for phonetic text view phonemes as states of a Markov chain of some low order. There is a model\u2010building method which produces models that lie between these classical models and the combinatorial model of O'Connor and Trimm. To produce the new models, one hypothesizes the existence of an underlying Markov chain of low order with some small number n of states, and assumes that the probability distribution of phonetic symbols is a function of the state of the underlying chain. Given n, the model\u2010building method involves a computer hill\u2010climbing procedure on a corpus of phonetic text to find the maximum\u2010likelihood model for that text and that n. This paper describes the method and the results of a large number of ascents on a certain corpus of text. The models developed show strong linguistic features; for example, for text segmented only by silences between phoneme strings, the vowels, consonants, and boundary symbols are statistically identified with 15% errors. With text s..."
            },
            "slug": "Markov-Models-for-Phonetic-Text-Neuburg",
            "title": {
                "fragments": [],
                "text": "Markov Models for Phonetic Text"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 25179305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8acf7cb1d476ba09b401b0c13abe81d4b96d128e",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an approach to speaker-independent, isolated word recognition in which the well-known techniques of vector quantization and hidden Markov modeling are combined with a linear predictive coding analysis front end. This is done in the framework of a standard statistical pattern recognition model. Both the vector quantizer and the hidden Markov models need to be trained for the vocabulary being recognized. Such training results in a distinct hidden Markov model for each word of the vocabulary. Classification consists of computing the probability of generating the test word with each word model and choosing the word model that gives the highest probability. There are several factors, in both the vector quantizer and the hidden Markov modeling, that affect the performance of the overall word recognition system, including the size of the vector quantizer, the structure of the hidden Markov model, the ways of handling insufficient training data, etc. The effects, on recognition accuracy, of many of these factors are discussed in this paper. The entire recognizer (training and testing) has been evaluated on a 10-word digits vocabulary. For training, a set of 100 talkers spoke each of the digits one time. For testing, an independent set of 100 tokens of each of the digits was obtained. The overall recognition accuracy was found to be 96.5 percent for the 100-talker test set. These results are comparable to those obtained in earlier work, using a dynamic time-warping recognition algorithm with multiple templates per digit. It is also shown that the computation and storage requirements of the new recognizer were an order of magnitude less than that required for a conventional pattern recognition system using linear prediction with dynamic time warping."
            },
            "slug": "On-the-application-of-vector-quantization-and-to-Rabiner-Levinson",
            "title": {
                "fragments": [],
                "text": "On the application of vector quantization and hidden Markov models to speaker-independent, isolated word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper presents an approach to speaker-independent, isolated word recognition in which the well-known techniques of vector quantization and hidden Markov modeling are combined with a linear predictive coding analysis front end in the framework of a standard statistical pattern recognition model."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20391216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f90d340bbf9854a3c4e9436ba51f5fd090571fe",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 123,
            "paperAbstract": {
                "fragments": [],
                "text": "The past decade has witnessed substantial progress toward the goal of constructing a machine capable of understanding colloquial discourse. Central to this progress has been the development and application of mathematical methods that permit modeling the speech signal as a complex code with several coexisting levels of structure. The most successful of these are \"template matching,\" stochastic modeling, and probabilistic parsing. The manifestation of common themes such as dynamic programming and finite-state descriptions accentuates a superficial likeness amongst the methods which is often mistaken for the deeper similarity arising from their shared Bayesian foundation. In this paper, we outline the mathematical bases of these methods, invariant metrics, hidden Markov chains, and formal grammars, respectively. We then recount and briefly interpret the results of experiments in speech recognition to which the various methods were applied. Since these mathematical principles seem to bear little resemblance to traditional linguistic characterizations of speech, the success of the experiments is occasionally attributed, even by their authors, merely to excellent engineering. We conclude by speculating that, quite to the contrary, these methods actually constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers."
            },
            "slug": "Structural-methods-in-automatic-speech-recognition-Levinson",
            "title": {
                "fragments": [],
                "text": "Structural methods in automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is speculated that these mathematical principles constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92533445"
                        ],
                        "name": "R. L. Stratonovich",
                        "slug": "R.-L.-Stratonovich",
                        "structuredName": {
                            "firstName": "Rouslan",
                            "lastName": "Stratonovich",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Stratonovich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121247114,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "41362d63f57e60f24d5060edc79181fb286de442",
            "isKey": false,
            "numCitedBy": 337,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Relationships are given between the probabilities of conditional Markov chains for neighboring tests. The conditional probabilities at the end of the observation interval (the final probabilities) are satisfied by equations of the first kind corresponding to an increase in the observation interval. The equations of the second kind for the conditional probabilities within the observation interval are written in terms of these final probabilities.The following special cases are considered. Gaussian noise with independent values which becomes a delta-correlational process when the moments of time are compacted, and a continuous Markov process.The related problem of the time reversal of ordinary (a priori) Markov processes is treated as a side issue."
            },
            "slug": "CONDITIONAL-MARKOV-PROCESSES-Stratonovich",
            "title": {
                "fragments": [],
                "text": "CONDITIONAL MARKOV PROCESSES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They include a number of generalizations of both the spatial and temporal components of the models, for example variable-duration hidden Markov models [30], continuous multivariate hidden Markov models [47], hidden-filter hidden Markov models [58], and trainable finite-state (hidden) grammars [ 8 ] A special case of the results in [13] has been designated by Dempster et alas the EM al,gorithm, see 1231, especially pp. 28-29 and 1621 NJ 08540"
                    },
                    "intents": []
                }
            ],
            "corpusId": 121084921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c79a9bb8f885050cad70b4c69e016b186ffa538",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms which are based on modeling speech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component."
            },
            "slug": "Trainable-grammars-for-speech-recognition-Baker",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes that permits automatic training of the stochastic analog of an arbitrary context free grammar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31590556"
                        ],
                        "name": "H. Landau",
                        "slug": "H.-Landau",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Landau",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Landau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31677760,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "24e233b8933b1e0cf0ac1c20e0e1a5bd6390288a",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. The trigonometric moment problem stands at the source of several major streams in analysis. From it flow developments in function theory, in spectral representation of operators, in probability, in approximation, and in the study of inverse problems. Here we connect it also with a group of questions centering on entropy and prediction. In turn, this will suggest a simple approach, by way of orthogonal decomposition, to the moment problem itself. In statistical estimation, one often wants to guess an unknown probability distribution, given certain observations based on it. There are generally infinitely many distributions consistent with the data, and the question of which of these to select is an important one. The notion of entropy has been proposed here as the basis of a principle of salience which has received considerable attention. We will show that, in the context of spectral analysis, this idea is linked to a certain question of prediction by the trigonometric moment problem, and that all three strongly illuminate one another. The phenomena we describe are known, but our object is to unify them conceptually and to reduce the analytic intricacy of the arguments. To this end, we give a completely elementary discussion, virtually free of calculation, which shows that all the facts, including those concerning the moment problem, can be understood as direct consequences of orthogonal decomposition in a finite-dimensional space. We then describe how, in its continuous version, this leads to a view of second-order Sturm-Liouville differential equations, and conclude with some questions concerning the connection between combinatorial ideas and orthogonality in this problem."
            },
            "slug": "Maximum-entropy-and-the-moment-problem-Landau",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and the moment problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32588087"
                        ],
                        "name": "R. Redner",
                        "slug": "R.-Redner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Redner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Redner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145221576"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Walker",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2611600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54323bf565cea5d2aaee88a03ec9d1d3444a9bfd",
            "isKey": false,
            "numCitedBy": 2829,
            "numCiting": 158,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating the parameters which determine a mixture density has been the subject of a large, diverse body of literature spanning nearly ninety years. During the last two decades, the method of maximum likelihood has become the most widely followed approach to this problem, thanks primarily to the advent of high speed electronic computers. Here, we first offer a brief survey of the literature directed toward this problem and review maximum-likelihood estimation for it. We then turn to the subject of ultimate interest, which is a particular iterative procedure for numerically approximating maximum-likelihood estimates for mixture density problems. This procedure, known as the EM algorithm, is a specialization to the mixture density context of a general algorithm of the same name used to approximate maximum-likelihood estimates for incomplete data problems. We discuss the formulation and theoretical and practical properties of the EM algorithm for mixture densities, focussing in particular on ..."
            },
            "slug": "Mixture-densities,-maximum-likelihood,-and-the-EM-Redner-Walker",
            "title": {
                "fragments": [],
                "text": "Mixture densities, maximum likelihood, and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work discusses the formulation and theoretical and practical properties of the EM algorithm, a specialization to the mixture density context of a general algorithm used to approximate maximum-likelihood estimates for incomplete data problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61001580,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4062986d268d936d714c3c7580d3d6ec19e3c3fc",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an experimental continuous speech recognition system comprising procedures for acoustic/phonetic classification, lexical access and sentence retrieval. Speech is assumed to be composed of a small number of phonetic units which may be identified with the states of a hidden Markov model. The acoustic correlates of the phonetic units are then characterized by the observable Gaussian process associated with the corresponding state of the underlying Markov chain. Once the parameters of such a model are determined, a phonetic transcription of an utterance can be obtained by means of a Viterbi-like algorithm. Given a lexicon in which each entry is orthographically represented in terms of the chosen phonetic units, a word lattice is produced by a lexical access procedure. Lexical items whose orthography matches subsequences of the phonetic transcription are sought by means of a hash coding technique and their likelihoods are computed directly from the corresponding interval of acoustic measurements. The recognition process is completed by recovering from the word lattice, the string of words of maximum likelihood conditioned on the measurements. The desired string is derived by a best-first search algorithm. In an experimental evaluation of the system, the parameters of an acoustic/phonetic model were estimated from fluent utterances of 37 seven-digit numbers. A digit recognition rate of 96% was then observed on an independent test set of 59 utterances of the same form from the same speaker. Half of the observed errors resulted from insertions while deletions and substitutions accounted equally for the other half."
            },
            "slug": "Continuous-speech-recognition-by-means-of-acoustic/-Levinson",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition by means of acoustic/ Phonetic classification obtained from a hidden Markov model"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An experimental continuous speech recognition system comprising procedures for acoustic/phonetic classification, lexical access and sentence retrieval and an experimental evaluation of the system, the parameters of an acoustic/Phonetic model were estimated from fluent utterances of 37 seven-digit numbers."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103072546"
                        ],
                        "name": "J. Eagon",
                        "slug": "J.-Eagon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Eagon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eagon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14153120,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69fc8c03d21e22e30d6642824c37158b314f36c3",
            "isKey": false,
            "numCitedBy": 1122,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Summary. The object of this note is to prove the theorem below and sketch two applications, one to statistical estimation for (proba-bilistic) functions of Markov processes [l] and one to Blakley's model for ecology [4]. 2. Result. THEOREM. Let P(x)=P({xij}) be a polynomial with nonnegative coefficients homogeneous of degree d in its variables {##}. Let x= {##} be any point of the domain D: ## \u00a7:(), ]pLi ## = 1, i = l, \u2022 \u2022 \u2022 , p, j=l, \u2022 \u2022 \u2022 , q%. For x= {xij} \u00a3\u00a3> let 3(#) = 3{##} denote the point of D whose i, j coordinate is (dP\\ \\ f \u00ab dP 3(*)<i = (Xij 7\u2014) / 2* *<i \u2014 \\ dXij\\(X)// ,-i dXij (\u00bb> Then P(3(x))>P(x) unless 3(x)=x. Notation, fi will denote a doubly indexed array of nonnegative integers: fx= {M#}> i = l> \u2022 \u2022 \u2022 > <lu i=l, \u2022 \u2022 \u2022 , A #* then denotes Ilf-iH\u00ee-i^* Similarly, c M is an abbreviation for C[ MiJ }. The polynomial P({xij}) is then written P(x) = ]CM V^-In our notation : (1) 3(&)*i = (Z) \u00abWnys*) / JLH CpiiijX\u00bb."
            },
            "slug": "An-inequality-with-applications-to-statistical-for-Baum-Eagon",
            "title": {
                "fragments": [],
                "text": "An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122661322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "226a6dff9ccc1c2db9f09db644b13eb9d04322e7",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The language model probabilities are estimated by an empirical Bayes approach in which a prior distribution for the unknown probabilities is itself estimated through a novel choice of data. The predictive power of the model thus fitted is compared by means of its experimental perplexity [1] to the model as fitted by the Jelinek-Mercer deleted estimator and as fitted by the Turing-Good formulas for probabilities of unseen or rarely seen events."
            },
            "slug": "Estimation-of-probabilities-in-the-language-model-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities in the language model of the IBM speech recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The predictive power of the model thus fitted is compared by means of its experimental perplexity to the model as fitted by the Jelinek-Mercer deleted estimator and by the Turing-Good formulas for probabilities of unseen or rarely seen events."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981002"
                        ],
                        "name": "J. Spohrer",
                        "slug": "J.-Spohrer",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Spohrer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Spohrer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143860536"
                        ],
                        "name": "P. Hochschild",
                        "slug": "P.-Hochschild",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hochschild",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hochschild"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "During tlie recovery of the hidden state sequence with a dynamic program it is frequently sufficient to save the pointers for only a limited time back from the current time; see Brown e2 a/ [ 16 ]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206726708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b7e49e3ee16965d4bfdeeac03fbebb3308ebbec",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic programming is used in speech recognition to search efficiently for word sequences whose templates best match acoustic data. The search is constrained by finite-state networks embodying grammatical rules. Typically, dynamic programming is implemented in two steps: the first calculates, for each state in a network and for each time, the best way of arriving at that state at that time; the second traces back from the final state at the final time to the initial state at the initial time to determine the best path through the network. This second step cannot be initiated before the determination (usually from the detection of silence) that the final state has been reached. Such a determination is difficult in the recognition of truly continuous speech; there are often no reliable anchor points. Further, it is often desirable to be able to recognize at least part of an utterance before a speaker has stopped talking. In this paper we introduce a technique for discovering the initial section of the optimal path through a network before the traversal of the network is complete. It can be used to report a system's interpretation of acoustic data from the not-too-distant past without relying on or making any decisions which may degrade recognition accuracy."
            },
            "slug": "Partial-traceback-and-dynamic-programming-Brown-Spohrer",
            "title": {
                "fragments": [],
                "text": "Partial traceback and dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A technique for discovering the initial section of the optimal path through a network before the traversal of the network is complete is introduced and can be used to report a system's interpretation of acoustic data from the not-too-distant past without relying on or making any decisions which may degrade recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235286"
                        ],
                        "name": "M. Russell",
                        "slug": "M.-Russell",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107422095"
                        ],
                        "name": "R. Moore",
                        "slug": "R.-Moore",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Moore",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15720949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "703c96e539b4c797dc42833b06268ba331ac6532",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-Markov models have been proposed as a mechanism for overcoming some of the limitations inherent in first-order Markov modelling of speech signals. Results have been presented which show that these models provide an appropriate framework for modelling durational structure and can lead to significant improvements in recognition accuracy."
            },
            "slug": "Explicit-modelling-of-state-occupancy-in-hidden-for-Russell-Moore",
            "title": {
                "fragments": [],
                "text": "Explicit modelling of state occupancy in hidden Markov models for automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results have been presented which show that these semi-Markov models provide an appropriate framework for modelling durational structure and can lead to significant improvements in recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '85. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The entire information hierarchy can be assembled into one grand hidden Markov model or integrated network, [6], [ 5 ], [4], with a sparse transition matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60864895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0802c70eba26b9798e1e8cbb6e285bce842fd5c",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Automatic recognition of continuous speech involves estimation of a sequence X(1), X(2), X(3), ..., X(T) which is not directly observed (such as the words of a spoken utterance), based on a sequence Y(1), Y(2), Y(3), ..., Y(T) of related observations (such as the sequence of acoustic parameter values) and a variety of sources of knowledge. Formally the author wishes to find the sequence x(1:T) which maximizes the a posteriori probability Pr(x(1:T))=(1:T) Y(1:T) =y(1:T),A,L.P,S), where A,L,P,S represent the acoustic-phonetic, lexical, phonological, and syntactic-semantic knowledge. A speech recognition system must attempt to approximate a solution to this problem, whether or not the system uses a formal stochastic model. The DRAGON speech recognition system models the knowledge sources as probalistic functions of Markov processes. The assumption of the Markov property allows the use of an optimal search strategy. A simplified implementation of the DRAGON system has been developed using knowledge A and L, and some of the knowledge from S."
            },
            "slug": "Stochastic-modeling-as-a-means-of-automatic-speech-Baker",
            "title": {
                "fragments": [],
                "text": "Stochastic modeling as a means of automatic speech recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A simplified implementation of the DRAGON speech recognition system has been developed using knowledge A and L, and some of the knowledge from S, where A,L,P,S represent the acoustic-phonetic, lexical, phonological, and syntactic-semantic knowledge."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24158615,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a4361a4bd93e207fb4cf263a63c24ead39cc2076",
            "isKey": false,
            "numCitedBy": 7712,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a \"theory.\" What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology. The more we study the information processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them. In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?"
            },
            "slug": "Dynamic-Programming-Bellman",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The more the authors study the information processing aspects of the mind, the more perplexed and impressed they become, and it will be a very long time before they understand these processes sufficiently to reproduce them."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The deleted interpolation technique of Jelinek and Mercer is summarized in [ 4 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To handle large state spaces or to restrict the state sequences to respect additional constraints, there are approximate methods; the use of a stack algorithm for example, [72], [36], [ 4 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The entire information hierarchy can be assembled into one grand hidden Markov model or integrated network, [6], [5], [ 4 ], with a sparse transition matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": true,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33275295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "039900eaeeddd13752aa8d6c61759f0b0e54f0de",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Training methods for designing better decoders are compared. The training problem is considered as a statistical parameter estimation problem. In particular, the conditional maximum likelihood estimate (CMLE), which estimates the parameter values that maximize the conditional probability of words given acoustics during training, is compared to the maximum-likelihood estimate, which is obtained by maximizing the joint probability of the words and acoustics. For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect. In this sense, the CMLE/MMIE appears more robust than the MLE. >"
            },
            "slug": "On-a-model-robust-training-method-for-speech-N\u00e1das-Nahamoo",
            "title": {
                "fragments": [],
                "text": "On a model-robust training method for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145767634"
                        ],
                        "name": "B. Musicus",
                        "slug": "B.-Musicus",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Musicus",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Musicus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2593390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf300a251f9d1f4807ffcd94c79b43eeecc36ffa",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to the problem of estimating multiple signal and parameter unknowns given noisy and incomplete data. Using cross-entropy, we fit a separable density to the given model density, then use this separable density to estimate each unknown independently. Not only does this method include all the various MAP methods as degenerate cases, but it also directly leads to a simple iterative algorithm which can solve either the cross-entropy method or any of the MAP methods. This algorithm is particularly effective for exponential families of densities. Applications include estimation using grouped or quantized data, and a wide variety of reconstruction, smoothing, interpolation, extrapolation and modeling problems involving linear Gaussian systems."
            },
            "slug": "Iterative-algorithms-for-optimal-signal-and-given-Musicus",
            "title": {
                "fragments": [],
                "text": "Iterative algorithms for optimal signal reconstruction and parameter identification given noisy and incomplete data"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new approach to the problem of estimating multiple signal and parameter unknowns given noisy and incomplete data is presented, using cross-entropy to fit a separable density to the given model density, then use this separabledensity to estimate each unknown independently."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The entire information hierarchy can be assembled into one grand hidden Markov model or integrated network, [ 6 ], [5], [4], with a sparse transition matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61904772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c180f387357d9302a558bcd643209831744c639b",
            "isKey": false,
            "numCitedBy": 604,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model--that of a probabilistic function of a Markov process--is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities."
            },
            "slug": "The-DRAGON-system--An-overview-Baker",
            "title": {
                "fragments": [],
                "text": "The DRAGON system--An overview"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper briefly describes the major features of the DRAGON speech understanding system, which makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92032001"
                        ],
                        "name": "R. Chang",
                        "slug": "R.-Chang",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Chang",
                            "middleNames": [
                                "Pang",
                                "Heng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058321160"
                        ],
                        "name": "J. Hancock",
                        "slug": "J.-Hancock",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hancock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hancock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "the areas of research I, WI, P41, D91, WI, ding theory [ 18 ], [2], 61, financial modeling"
                    },
                    "intents": []
                }
            ],
            "corpusId": 206729000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c55a5f24234c131c37dd3c9c7560699175edf084",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with m -ary communication channels (m \\geq 2) having intersymbol interference between L time periods (L \\geq 2) . Receiver structures are developed for making jointly optimum (minimum probability of error) decisions about L consecutive symbols on the basis of the complete message received. The decision statistics are computed by a sequential procedure, and the number of computations increases only linearly with the message length. The method can be applied to the general problem of making decisions about the states of a discrete-state Markov information source which is observable only through a channel with additive Gaussian or non-Gaussian noise."
            },
            "slug": "On-receiver-structures-for-channels-having-memory-Chang-Hancock",
            "title": {
                "fragments": [],
                "text": "On receiver structures for channels having memory"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Receiver structures are developed for making jointly optimum decisions about L consecutive symbols on the basis of the complete message received and the decision statistics are computed by a sequential procedure, and the number of computations increases only linearly with the message length."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144928357"
                        ],
                        "name": "Y. Vardi",
                        "slug": "Y.-Vardi",
                        "structuredName": {
                            "firstName": "Yehuda",
                            "lastName": "Vardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Vardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108020"
                        ],
                        "name": "L. Shepp",
                        "slug": "L.-Shepp",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Shepp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17836207,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "57f9830c30bb32c324266e51b82b70de62352334",
            "isKey": false,
            "numCitedBy": 891,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Positron emission tomography (PET)\u2014still in its research stages\u2014is a technique that promises to open new medical frontiers by enabling physicians to study the metabolic activity of the body in a pictorial manner. Much as in X-ray transmission tomography and other modes of computerized tomography, the quality of the reconstructed image in PET is very sensitive to the mathematical algorithm to be used for reconstruction. In this article, we tailor a mathematical model to the physics of positron emissions, and we use the model to describe the basic image reconstruction problem of PET as a standard problem in statistical estimation from incomplete data. We describe various estimation procedures, such as the maximum likelihood (ML) method (using the EM algorithm), the method of moments, and the least squares method. A computer simulation of a PET experiment is then used to demonstrate the ML and the least squares reconstructions. The main purposes of this article are to report on what we believe is an..."
            },
            "slug": "A-Statistical-Model-for-Positron-Emission-Vardi-Shepp",
            "title": {
                "fragments": [],
                "text": "A Statistical Model for Positron Emission Tomography"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67046059"
                        ],
                        "name": "J. Ott",
                        "slug": "J.-Ott",
                        "structuredName": {
                            "firstName": "Jurg",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30858516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6000d4f505b6a552bf5c4193cfcf219fb1b370ca",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The likelihood of human pedigree data can be written in such a form as to allow the computation of derivatives. This is done for various parameters in linkage and segregation analysis. The equations for the maximum likelihood estimates are represented in a particularly appealing form which allows iterative solutions. This process is an extension to pedigrees of Smith's (1957) counting methods. All these procedures belong to a general class of MLE methods for incomplete data called EM algorithms (Dempster et al. 1976)."
            },
            "slug": "Counting-methods-(EM-algorithm)-in-human-pedigree-Ott",
            "title": {
                "fragments": [],
                "text": "Counting methods (EM algorithm) in human pedigree analysis: Linkage and segregation analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The likelihood of human pedigree data can be written in such a form as to allow the computation of derivatives for various parameters in linkage and segregation analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of human genetics"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762024"
                        ],
                        "name": "R. Bakis",
                        "slug": "R.-Bakis",
                        "structuredName": {
                            "firstName": "Raimo",
                            "lastName": "Bakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A commonly used elementary model was introduced by Bakis [ 9 ]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 119929465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8291be2289154cf1dcd5a4009222c1899533e253",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous speech was treated as if produced by a finite\u2010state machine making a transition every centisecond. The observable output from state transitions was considered to be a power spectrum\u2014a probabilistic function of the target state of each transition. Using this model, observed sequences of power spectra from real speech were decoded as sequences of acoustic states by means of the Viterbi trellis algorithm. The finite\u2010state machine used as a representation of the speech source was composed of machines representing words, combined according to a \u201clanguage model.\u201d When trained to the voice of a particular speaker, the decoder recognized seven\u2010digit telephone numbers correctly 96% of the time, with a better than 99% per\u2010digit accuracy. Results for other tests of the system, including syllable and phoneme recognition, will also be given."
            },
            "slug": "Continuous-speech-recognition-via-centisecond-Bakis",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition via centisecond acoustic states"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "When trained to the voice of a particular speaker, the decoder recognized seven\u2010digit telephone numbers correctly 96% of the time, with a better than 99% per\u2010digit accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727234"
                        ],
                        "name": "Y. Chow",
                        "slug": "Y.-Chow",
                        "structuredName": {
                            "firstName": "Yen-lu",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1941347"
                        ],
                        "name": "M. O. Dunham",
                        "slug": "M.-O.-Dunham",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Dunham",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O. Dunham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353221"
                        ],
                        "name": "O. Kimball",
                        "slug": "O.-Kimball",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Kimball",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kimball"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4953636"
                        ],
                        "name": "M. Krasner",
                        "slug": "M.-Krasner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Krasner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krasner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084322707"
                        ],
                        "name": "G. Kubala",
                        "slug": "G.-Kubala",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kubala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kubala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48258694"
                        ],
                        "name": "P. Price",
                        "slug": "P.-Price",
                        "structuredName": {
                            "firstName": "Patti",
                            "lastName": "Price",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46924970"
                        ],
                        "name": "Salim Roukos",
                        "slug": "Salim-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salim Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152901373"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "Evan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61608679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8fe93d3e5205a450fdd8a9fb94cea0ab73b067f",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe BYBLOS, the BBN continuous speech recognition system. The system, designed for large vocabulary applications, integrates acoustic, phonetic, lexical, and linguistic knowledge sources to achieve high recognition performance. The basic approach, as described in previous papers [1, 2], makes extensive use of robust context-dependent models of phonetic coarticulation using Hidden Markov Models (HMM). We describe the components of the BYBLOS system, including: signal processing frontend, dictionary, phonetic model training system, word model generator, grammar and decoder. In recognition experiments, we demonstrate consistently high word recognition performance on continuous speech across: speakers, task domains, and grammars of varying complexity. In speaker-dependent mode, where 15 minutes of speech is required for training to a speaker, 98.5% word accuracy has been achieved in continuous speech for a 350-word task, using grammars with perplexity ranging from 30 to 60. With only 15 seconds of training speech we demonstrate performance of 97% using a grammar."
            },
            "slug": "BYBLOS:-The-BBN-continuous-speech-recognition-Chow-Dunham",
            "title": {
                "fragments": [],
                "text": "BYBLOS: The BBN continuous speech recognition system"
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35201459"
                        ],
                        "name": "J. Shore",
                        "slug": "J.-Shore",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shore",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122686756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be831a4a0a5820e1a3a79691505a91fd6f51d279",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The principle of minimum cross-entropy (minimum directed divergence, minimum discrimination information, minimum relative entropy) is summarized, discussed, and applied to the classical problem of estimating power spectra given values of the autocorrelation function. This new method differs from previous methods in its explicit inclusion of a prior estimate of the power spectrum, and it reduces to maximum entropy spectral analysis as a special case. The prior estimate can be viewed as a means of shaping the spectral estimator. Cross-entropy minimization yields a family of shaped spectral estimators consistent with known autocorrelations. Results are derived in two equivalent ways: once by minimizing the cross-entropy of underlying probability densities, and once by arguments concerning the cross-entropy between the input and output of linear filters. Several example minimum cross-entropy spectra are included."
            },
            "slug": "Minimum-cross-entropy-spectral-analysis-Shore",
            "title": {
                "fragments": [],
                "text": "Minimum cross-entropy spectral analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This new method differs from previous methods in its explicit inclusion of a prior estimate of the power spectrum, and it reduces to maximum entropy spectral analysis as a special case."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9101213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687",
            "isKey": false,
            "numCitedBy": 2530,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed."
            },
            "slug": "Prediction-and-entropy-of-printed-English-Shannon",
            "title": {
                "fragments": [],
                "text": "Prediction and entropy of printed English"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A new method of estimating the entropy and redundancy of a language is described, which exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16512130"
                        ],
                        "name": "J. Raviv",
                        "slug": "J.-Raviv",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Raviv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Raviv"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "the areas of research I, WI, P41, D91, WI, ding theory [18], [ 2 ], 61, financial modeling"
                    },
                    "intents": []
                }
            ],
            "corpusId": 28594190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b51c6a5610be2c5648d1476b6f70e8037e0e8cb8",
            "isKey": false,
            "numCitedBy": 6485,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived."
            },
            "slug": "Optimal-decoding-of-linear-codes-for-minimizing-Bahl-Cocke",
            "title": {
                "fragments": [],
                "text": "Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered and an optimal decoding algorithm is derived."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34886891"
                        ],
                        "name": "A. Gray",
                        "slug": "A.-Gray",
                        "structuredName": {
                            "firstName": "Augustine",
                            "lastName": "Gray",
                            "middleNames": [
                                "H."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349348"
                        ],
                        "name": "G. Rebolledo",
                        "slug": "G.-Rebolledo",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Rebolledo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rebolledo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35201459"
                        ],
                        "name": "J. Shore",
                        "slug": "J.-Shore",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shore",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40272879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e16afced04247c02fb99036b7e00a1256fdbe066",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "An information theory approach to the theory and practice of linear predictive coded (LPC) speech compression systems is developed. It is shown that a traditional LPC system can be viewed as a minimum distortion or nearest-neighbor system where the distortion measure is a minimum discrimination information between a speech process model and an observed frame of actual speech. This distortion measure is used in an algorithm for computer-aided design of block source codes subject to a fidelity criterion to obtain a 750-bits/s speech compression system that resembles an LPC system but has a much lower rate, a larger memory requirement, and requires no on-line LPC analysis. Quantitative and informal subjective comparisons are made among our system and LPC systems."
            },
            "slug": "Rate-distortion-speech-coding-with-a-minimum-Gray-Gray",
            "title": {
                "fragments": [],
                "text": "Rate-distortion speech coding with a minimum discrimination information distortion measure"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "An information theory approach to the theory and practice of linear predictive coded speech compression systems is developed and it is shown that a traditional LPC system can be viewed as a minimum distortion or nearest-neighbor system where the distortion measure is a minimum discrimination information between a speech process model and an observed frame of actual speech."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120638127,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "79eb272eaf061cf4e65b8e61c9f02c027b3b6933",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of method for training a speech recognizer is posed as an optimization problem. The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "slug": "A-decision-theorectic-formulation-of-a-training-in-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "A decision theorectic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In some of the literature, e.g , [7], [ 37 ], 1, output probabilities depend on a pair of states (1 e, bs,7(k)) WO variants have parallel theories."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31408841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods."
            },
            "slug": "Continuous-speech-recognition-by-statistical-Jelinek",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition by statistical methods"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Experimental results are presented that indicate the power of the methods and concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703304"
                        ],
                        "name": "A. Averbuch",
                        "slug": "A.-Averbuch",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Averbuch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Averbuch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762024"
                        ],
                        "name": "R. Bakis",
                        "slug": "R.-Bakis",
                        "structuredName": {
                            "firstName": "Raimo",
                            "lastName": "Bakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1878312"
                        ],
                        "name": "G. Daggett",
                        "slug": "G.-Daggett",
                        "structuredName": {
                            "firstName": "Gregg",
                            "lastName": "Daggett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Daggett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144750187"
                        ],
                        "name": "S. Das",
                        "slug": "S.-Das",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48710731"
                        ],
                        "name": "K. Davies",
                        "slug": "K.-Davies",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Davies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054592358"
                        ],
                        "name": "S. V. Gennaro",
                        "slug": "S.-V.-Gennaro",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gennaro",
                            "middleNames": [
                                "V.",
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Gennaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8138376"
                        ],
                        "name": "E. Epstein",
                        "slug": "E.-Epstein",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Epstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Epstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69893463"
                        ],
                        "name": "D. Fraleigh",
                        "slug": "D.-Fraleigh",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Fraleigh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fraleigh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143613831"
                        ],
                        "name": "B. Lewis",
                        "slug": "B.-Lewis",
                        "structuredName": {
                            "firstName": "Burn",
                            "lastName": "Lewis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143754167"
                        ],
                        "name": "J. Moorhead",
                        "slug": "J.-Moorhead",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Moorhead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moorhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66648677"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801588"
                        ],
                        "name": "G. Shichman",
                        "slug": "G.-Shichman",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Shichman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Shichman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153068955"
                        ],
                        "name": "P. Spinelli",
                        "slug": "P.-Spinelli",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Spinelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spinelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722640158"
                        ],
                        "name": "D. V. Compernolle",
                        "slug": "D.-V.-Compernolle",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Compernolle",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. V. Compernolle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36088321"
                        ],
                        "name": "H. Wilkens",
                        "slug": "H.-Wilkens",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Wilkens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wilkens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60970721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ae4756313dc6e8e33ab69593a6f7475025a64fd",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Speech Recognition Group at IBM Research in Yorktown Heights has developed a real-time, isolated-utterance speech recognizer for natural language based on the IBM Personal Computer AT and IBM Signal Processors. The system has recently been enhanced by expanding the vocabulary from 5,000 words to 20,000 words and by the addition of a speech workstation to support usability studies on document creation by voice. The system supports spelling and interactive personalization to augment the vocabularies. This paper describes the implementation, user interface, and comparative performance of the recognizer."
            },
            "slug": "Experiments-with-the-Tangora-20,000-word-speech-Averbuch-Bahl",
            "title": {
                "fragments": [],
                "text": "Experiments with the Tangora 20,000 word speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The implementation, user interface, and comparative performance of the recognizer is described, which supports spelling and interactive personalization to augment the vocabularies."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7183102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f51017714534dc649c8b53c90c80aef4a3481d32",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Let the random (stock market) vector X \\geq 0 be drawn according to a known distribution function F(x), x \\in R^{m} . A log-optimal portfolio b^{\\ast} is any portfolio b achieving maximal expected \\log return W^{\\ast}=\\sup_{b} E \\ln b^{t}X , where the supremum is over the simplex b \\geq 0, \\sum_{i=1}^{m} b_{i} = 1 . An algorithm is presented for finding b^{\\ast} . The algorithm consists of replacing the portfolio b by the expected portfolio b^{'}, b_{i}^{'} = E(b_{i}X_{i}/b^{t}X) , corresponding to the expected proportion of holdings in each stock after one market period. The improvement in W(b) after each iteration is lower-bounded by the Kullback-Leibler information number D(b^{'}\\|b) between the current and updated portfolios. Thus the algorithm monotonically improves the return W . An upper bound on W^{\\ast} is given in terms of the current portfolio and the gradient, and the convergence of the algorithm is established."
            },
            "slug": "An-algorithm-for-maximizing-expected-log-investment-Cover",
            "title": {
                "fragments": [],
                "text": "An algorithm for maximizing expected log investment return"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An upper bound on W^{\\ast} is given in terms of the current portfolio and the gradient, and the convergence of the algorithm is established."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62562997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae9443b39a5abfbf3cc9776173c1ae4f94732408",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a new sequential decoding algorithm is introduced that uses stack storage at the receiver. It is much simpler to describe and analyze than the Fano algorithm, and is about six times faster than the latter at transmission rates equal to Rcomp the rate below which the average number of decoding steps is bounded by a constant. Practical problems connected with implementing the stack algorithm are discussed and a scheme is described that facilitates satisfactory performance even with limited stack storage capacity. Preliminary simulation results estimating the decoding effort and the needed stack siazree presented."
            },
            "slug": "Fast-sequential-decoding-algorithm-using-a-stack-Jelinek",
            "title": {
                "fragments": [],
                "text": "Fast sequential decoding algorithm using a stack"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new sequential decoding algorithm is introduced that uses stack storage at the receiver that is much simpler to describe and analyze than the Fano algorithm, and is about six times faster than the latter at transmission rates equal to Rcomp."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144519810"
                        ],
                        "name": "M. Feder",
                        "slug": "M.-Feder",
                        "structuredName": {
                            "firstName": "Meir",
                            "lastName": "Feder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Feder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33340475"
                        ],
                        "name": "A. Oppenheim",
                        "slug": "A.-Oppenheim",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Oppenheim",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oppenheim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659357"
                        ],
                        "name": "E. Weinstein",
                        "slug": "E.-Weinstein",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Weinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Weinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32424306,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9912f64439f2d564311088016582e2b319fdf01f",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Single microphone speech enhancement systems have typically shown limited performance, while multiple microphone systems based on a least-squares error criterion have shown encouraging results in some contexts. In this paper we formulate a new approach to multiple microphone speech enhancement. Specifically, we formulate a maximum likelihood (ML) problem for estimating the parameters needed for canceling the noise in a two microphone speech enhancement system. This ML problem is solved via the iterative EM (Estimate-Maximize) technique. The resulting algorithm shows encouraging results when applied to the speech enhancement problem."
            },
            "slug": "Methods-for-noise-cancellation-based-on-the-EM-Feder-Oppenheim",
            "title": {
                "fragments": [],
                "text": "Methods for noise cancellation based on the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A maximum likelihood problem for estimating the parameters needed for canceling the noise in a two microphone speech enhancement system is formulated via the iterative EM (Estimate-Maximize) technique and shows encouraging results when applied to the speech enhancement problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66544391"
                        ],
                        "name": "A. House",
                        "slug": "A.-House",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "House",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. House"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829859"
                        ],
                        "name": "E. Neuburg",
                        "slug": "E.-Neuburg",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Neuburg",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Neuburg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60773907,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "dffd8aca9cacee3d4f452d0f44797476e6988906",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of preparation of a ternary or quaternary alloy composed of the elements A1 A2 A3 (ternary alloy) or the elements A1 A2 A3 A4 (quaternary alloy) consists of the successive steps of preparation of one or a number of binary alloys such as A1A2, purification of the binary alloys by zone melting, placing of the binary alloy or alloys which are in the solid state together with the other constituent elements of the alloy within a container in the presence of a solvent, crystallization of the ternary or quaternary alloy by melting all the elements within the container and recrystallization in the presence of the solvent."
            },
            "slug": "Toward-automatic-identification-of-the-language-of-House-Neuburg",
            "title": {
                "fragments": [],
                "text": "Toward automatic identification of the language of an utterance. I. Preliminary methodological con"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102329511"
                        ],
                        "name": "George W. Soules",
                        "slug": "George-W.-Soules",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Soules",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George W. Soules"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063108982"
                        ],
                        "name": "Norman Weiss",
                        "slug": "Norman-Weiss",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norman Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The class of models covered in [ 13 ] includes the general distribution and the binomial distribution in the finite alphabet case, the Poissoii distribution in the case of countable outputs and both the univariate normal and Gamma distributions among the continuous densities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "They include a number of generalizations of both the spatial and temporal components of the models, for example variable-duration hidden Markov models [30], continuous multivariate hidden Markov models [47], hidden-filter hidden Markov models [58], and trainable finite-state (hidden) grammars [8] A special case of the results in [ 13 ] has been designated by Dempster et alas the EM al,gorithm, see 1231, especially pp. 28-29 and 1621 NJ 08540"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A yet more fruitful technique based on the Kullback-Leibler number [41] was presented by Baum, Petrie, Soules and Weiss in [ 13 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "theory (1970) was the maximization f Baum, Petrie, Soules and Weiss [ 13 ] that extended coverage to many of the classical distributions This work has itself lead to a wide range of theoretical outgrowths."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122568650,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4",
            "isKey": true,
            "numCitedBy": 4551,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Maximization-Technique-Occurring-in-the-Analysis-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The time series dynamics may carry Markov structure; improvement in performance has been obt,ained by concatenating items from nearby times into a single jointly-distributed poly-observation [59],[ 3 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of studies have been aimed at finding an alternate criterion to maximum likelihood in selecting the parameters of the model: these include Mercer\u2019s mavimum mutual-information [ 3 ], [53] and the work of Ephraim et a/ on minimum cross-entropy [24] (see also [32], [68])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "11 [63],[59],[44],[ 3 ] also NQ Given a signal, Y = (y-~+ as a time series generated by a hz Poritz [58] . . , y~), we can consider it er hidden Markov model; see"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62589537,
            "fieldsOfStudy": [],
            "id": "d34e23ec879c3a69b55e16dc7bdc2ad112a2dfa2",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition with continuous-parameter hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143628300"
                        ],
                        "name": "H. Hartley",
                        "slug": "H.-Hartley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Hartley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hartley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123908692,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "54fbfb0276023ac5595d21d98215837a01efcbc5",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Maximum-Likelihood-Estimation-from-Incomplete-Data-Hartley",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Estimation from Incomplete Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52112354"
                        ],
                        "name": "M. Shlezinger",
                        "slug": "M.-Shlezinger",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Shlezinger",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shlezinger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122552231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e5a9834c417aab171ae96c059a7ac9dcb0ae346",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-interaction-of-learning-and-self-organization-Shlezinger",
            "title": {
                "fragments": [],
                "text": "The interaction of learning and self-organization in pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the effort to lift the veil, a substantial body of theory has been developed over the past twenty-five years The initial work dealt with finite probability spaces and addressed the problems of tractability of probability computation, the recovery of the hidden e maximum-likelihood estimation of model parameters proof of consistency of the estimates; [ll], Baum and Petrie [ 12 ]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120208815,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "603bdbb17ba1f909280405a076455ac4f878fbf3",
            "isKey": false,
            "numCitedBy": 2773,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Inference-for-Probabilistic-Functions-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "Statistical Inference for Probabilistic Functions of Finite State Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101681617"
                        ],
                        "name": "P. Billingsley",
                        "slug": "P.-Billingsley",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Billingsley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Billingsley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119400345,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5126c863bb169292050dca9f090ae7d043ff0664",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-inference-for-Markov-processes-Billingsley",
            "title": {
                "fragments": [],
                "text": "Statistical inference for Markov processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102909471"
                        ],
                        "name": "R. A. Leibler",
                        "slug": "R.-A.-Leibler",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Leibler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Leibler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120349231,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c054360ec3ccadae977fdd0d77694c9655478a41",
            "isKey": false,
            "numCitedBy": 10535,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Information-and-Sufficiency-Kullback-Leibler",
            "title": {
                "fragments": [],
                "text": "On Information and Sufficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 86832357,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6919ee826b24146ee4551c9ea183dfb6b030ee3f",
            "isKey": false,
            "numCitedBy": 2057,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mathematical-Theory-of-Communications-Shannon",
            "title": {
                "fragments": [],
                "text": "A Mathematical Theory of Communications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1948
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62282319,
            "fieldsOfStudy": [],
            "id": "d87a423334afb20747c367b2d907069d7f3b4ed2",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The viterbi algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": false,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69047585"
                        ],
                        "name": "N. A. Esin",
                        "slug": "N.-A.-Esin",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Esin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. A. Esin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52112354"
                        ],
                        "name": "M. Shlezinger",
                        "slug": "M.-Shlezinger",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Shlezinger",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shlezinger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59828780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b8717fd5719de43bdb76f8fee90443721a3552b",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Synthesis-of-a-probabilistic-finite-state-grammar-a-Esin-Shlezinger",
            "title": {
                "fragments": [],
                "text": "Synthesis of a probabilistic finite-state grammar describing a given set of sequences"
            },
            "venue": {
                "fragments": [],
                "text": "Cybernetics"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 43148643,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "afb8908aff1d2c5917a40bd3d6d4c6f4f2f401bf",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "These papers* are statistically motivated; the content is mathematical. The motivation is this: Given is an s X s stochastic matrix A = ((aij)) and an s X r stochastic matrix B = ((bj0)) where A generates a stationary Mlarkov process IXt} according to a1j = P[X,\u00b11 = A Xr = i] and B generates a process I Y1} described by P[Yt = k|Xt = j] = bj,, so if R is the set of integers 1, 2 ... .r and R' = H' 1R, R, = R (a point Y E R' has coordinates Y,), then the matrices A and B define a measure P(AB) on R', for kf ie R P(A, B){ Y1 = ki, 1 2 = k2 n =n. V 2 in = laj0aj01bik1alb22* ain-linbin where {ai0} is the stationary absolute distribution for A. The resulting process Y0} is called a probabilistic function of the MIarkov process {Xt}. Let Al be the space of s X s ergodic stochastic matrices, A2 the space of s X r stochastic matrices and H = Al X A2. The above associates to r = (A, B) E H and a stationary vector a for A a measure P7r on Re. The Problem.-Fix 7ro E H and let a sample Y1, Y2... Y. be generated according to the distribution PR0. From the sample Y... Y,, obtain an estimate HI(Y) of 7ro so that HI,(Y) Tro a.e., P7.0. Throughout this paper ro is fixed and 7r varies in HI. The Mathematics.--Part I (classification of equivalent processes) demonstrates that the problem has a solution in the following sense: Let AIM [ro] = { TELIIPr = P7.0 as measures on Ro }. Clearly the points of M[ro] cannot be distinguished by any finite or infinite sample. The description of M[7ro] is crucial in our study. Let (E be the symmetric group of degree s operating on the integers 1 through s. (is acts on H by -(A,B) = (aA,o-B), (oA)Xj = aa(i),7(j) (aB) j = b,(j)k for ue G5 Observe that Porr = P7. as measures on R'. The main result of part I is THEOREM 1. There is an open subset Ho of H of Euclidean measure 1 such that for To E 1oM [7ro] = GSro, i.e., 7rO is distinguishable up to permutation by the measure P0(rSTro = {I ToIa eU~} ). Part II (limit theorems and statistical analysis) extends and generalizes the results of reference 1. For each n and each Y E R there is a function H. [Tr, Y] on H 1 defined by H, [T, Y] = log Pal Y1, Y2... Y4}; thus, each H, [T ] is a random n variable on the probability space (R',P7.0). The value Hn [7r, Y] is a function on I. These random variables hold the solution to our problem, as the following shows. THEOREM 2. lim Hn[,Y] = H7 (7r) exists a.e., P7r., nf'a THEOREM 3. H7r (T) < Hro(To) and H7.(T) = H710(To) iff T E M [To] Define H,,(Y) = I 7r' e I 7r' maximizes H,,[,Y]}. THEOREM 4. Hn(Y) M[Tro] a.e., P,. Theorems 1 through 4 theoretically solve our problem. Note in particular the importance of the function H.0(7r) in view of Theorems 1 and 3. 1'art III (AMorse theory) makes a further study of the function H,,(r) for r e11 ="
            },
            "slug": "Probabilistic-functions-of-finite-state-markov-Petrie",
            "title": {
                "fragments": [],
                "text": "Probabilistic functions of finite-state markov chains."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1967
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 58,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Hidden-Markov-models:-a-guided-tour-Poritz/a6627d8efde3ed55e34ccee059eb6cdac99bb2fe?sort=total-citations"
}