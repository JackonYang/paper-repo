{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144437467"
                        ],
                        "name": "H. Lum",
                        "slug": "H.-Lum",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Lum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 47
                            }
                        ],
                        "text": "This is not discussed here, but is covered in (Buntine, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "A graphical model can be developed to represent the basic prediction done by linearregression, a Bayesian network for an expert system, a hidden Markov model, or a con-nectionist feed-forward network (Buntine, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "Graphical models are used in domains such as diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57513905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bbd287ca0d99b3bbc1bfe8fd2406a2a3e8292ae",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic graphical models are being used widely in artificial intelligence, for instance, in diagnosis and expert systems, as a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. Their development and use spans several fields including artificial intelligence, decision theory and statistics, and provides an important bridge between these communities. This paper shows by way of example that these models can be extended to machine learning, neural networks and knowledge discovery by representing the notion of a sample on the graphical model. Not only does this allow a flexible variety of learning problems to be represented, it also provides the means for representing the goal of learning and opens the way for the automatic development of learning algorithms from specifications."
            },
            "slug": "Representing-Learning-With-Graphical-Models-Buntine-Lum",
            "title": {
                "fragments": [],
                "text": "Representing Learning With Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "These models can be extended to machine learning, neural networks and knowledge discovery by representing the notion of a sample on the graphical model, which provides the means for representing the goal of learning and opens the way for the automatic development of learning algorithms from specifications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852530"
                        ],
                        "name": "C. Robert Kenley",
                        "slug": "C.-Robert-Kenley",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Kenley",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert Kenley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "Other specialclasses of inference algorithms include the cases where the model is a multivariate Gaussian(Shachter & Kenley, 1989; Whittaker, 1990), or corresponds to some speci c diagnosticstructure, such as two-level believe networks with a level of symptoms connected to a levelof diseases\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119913293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5e1c09bad43cd9bffa3f88dbc99b668a6694385",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An influence diagram is a network representation of probabilistic inference and decision analysis models. The nodes correspond to variables that can be either constants, uncertain quantities, decisions, or objectives. The arcs reveal probabilistic dependence of the uncertain quantities and information available at the time of the decisions. The influence diagram focuses attention on relationships among the variables. As a result, it is increasingly popular for eliciting and communicating the structure of a decision or probabilistic model. \n \nThis paper develops the framework for assessment and analysis of linear-quadratic-Gaussian models within the influence diagram representation. The \"Gaussian influence diagram\" exploits conditional independence in a model to simplify elicitation of parameters for the multivariate normal distribution. It is straightforward to assess and maintain a positive semi-definite covariance matrix. Problems of inference and decision making can be analyzed using simple transformations to the assessed model, and these procedures have attractive numerical properties. Algorithms are also provided to translate between the Gaussian influence diagram and covariance matrix representations for the normal distribution."
            },
            "slug": "Gaussian-influence-diagrams-Shachter-Kenley",
            "title": {
                "fragments": [],
                "text": "Gaussian influence diagrams"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper develops the framework for assessment and analysis of linear-quadratic-Gaussian models within the influence diagram representation, and provides algorithms to translate between the Gaussian influence diagram and covariance matrix representations for the normal distribution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 236
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known in closed\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 86367536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6acd45949c2b167928211c562c8d9c445651f1ef",
            "isKey": true,
            "numCitedBy": 777,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "We review recent developments in applying Bayesian probabilistic and statistical ideas to expert systems. Using a real, moderately complex, medical example we illustrate how qualitative and quantitative knowledge can be represented within a directed graphical model, generally known as a belief network in this context. Exact probabilistic inference on individual cases is possible using a general propagation procedure. When data on a series of cases are available, Bayesian statistical techniques can be used for updating the original subjective quantitative inputs, and we present a sets of diagnostics for identifying conflicts between the data and the prior specification. A model comparison procedure is explored, and a number of links made with mainstream statistical methods. Details are given on the use of Dirichlet prior distributions for learning about parameters and the process of transforming the original graphical model to a junction tree as the basis for efficient computation."
            },
            "slug": "Bayesian-analysis-in-expert-systems-Spiegelhalter-Dawid",
            "title": {
                "fragments": [],
                "text": "Bayesian analysis in expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Using a real, moderately complex, medical example, it is illustrated how qualitative and quantitative knowledge can be represented within a directed graphical model, generally known as a belief network in this context."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 60
                            }
                        ],
                        "text": "This would be the method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992) covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15883988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f2433c088591d265891231f1c22424047f1bc1",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for deletion of weights; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the e ective number of well{determined parameters in a model; (5) quanti ed estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian `evidence' automatically embodies `Occam's razor,' penalising over{ exible and over{complex architectures. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well{ matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backprop-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backprop Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks and a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3184408"
                        ],
                        "name": "N. Wermuth",
                        "slug": "N.-Wermuth",
                        "structuredName": {
                            "firstName": "Nanny",
                            "lastName": "Wermuth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wermuth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 43
                            }
                        ],
                        "text": "These mixed graphs arecalled chain graphs (Wermuth & Lauritzen, 1989; Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124985704,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d9646d48c8e8032f06d67108a780e96af186297a",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Graphs consisting of points, and lines or arrows as connections between selected pairs of points, are used to formulate hypotheses about relations between variables. Points stand for variables, connections represent associations. When a missing connection is interpreted as a conditional independence, the graph characterizes a conditional independence structure as well. Statistical models, called graphical chain models, correspond to special types of graphs which are interpreted in this fashion. Examples are used to illustrate how conditional independences are reflected in summary statistics derived from the models and how the graphs help to identify analogies and equivalences between different models. Graphical chain models are shown to provide a unifying concept for many statistical techniques that in the past have proven to be useful in analyses of data. They also provide tools for new types of analysis."
            },
            "slug": "On-Substantive-Research-Hypotheses,-Conditional-and-Wermuth-Lauritzen",
            "title": {
                "fragments": [],
                "text": "On Substantive Research Hypotheses, Conditional Independence Graphs and Graphical Chain Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710305"
                        ],
                        "name": "D. Madigan",
                        "slug": "D.-Madigan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Madigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Madigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18709953,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cbb7afa526e1f947520ffeec8f734794802ee3c7",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1..."
            },
            "slug": "Model-Selection-and-Accounting-for-Model-in-Models-Madigan-Raftery",
            "title": {
                "fragments": [],
                "text": "Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624867"
                        ],
                        "name": "John Binder",
                        "slug": "John-Binder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Binder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Binder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 262
                            }
                        ],
                        "text": "\u2026;var2;var3; ; (1class=d;var2=true)= 1 2;51var2=true p(class = 5jvar1; var2; var3; ; )+ 11 2;5 1var2=false p(class = 5jvar1; var2; var3; ; ) :Notice that the derivative is computed by doing rst-order inference to nd p(class =5jvar1; var2; var3; ; ), as noted by Russell, Binder, and Koller (1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14007529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad9849aa429a601a093c8bded9e53a1e10526087",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief networks (or probabilistic networks) and neural networks are two forms of network representation that have been used in the development of intelligent systems in the field of artificial intelligence. Belief networks provide a concise representation of general probability distributions over a set of random variables. Neural net- works, which represent parameterized algebraic combinations of nonlinear activation functions, have found widespread use as models of real neural systems and as function approximators because of their amenability to simple training algorithms. Furthermore, the simple, local nature of most neural network training algorithms provides a certain biological plausibility and allows for a massively parallel implementation. In this paper, we show that similar local learning algorithms can be derived for belief networks, and that these learning algorithms can operate using only information that is directly available from the normal, inferential processes of the networks. This removes the main obstacle preventing belief networks from competing with neural networks on the above-mentioned tasks. The precise, local, probabilistic interpretation of belief networks also allows them to be partially or wholly constructed by humans; allows the results of learning to be easily understood; and allows them to contribute to rational decision- making in a well-defined way."
            },
            "slug": "Adaptive-Probabilistic-Networks-Russell-Binder",
            "title": {
                "fragments": [],
                "text": "Adaptive Probabilistic Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that similar local learning algorithms can be derived for belief networks, and that theselearning algorithms can operate using only information that is directly available from the normal, inferential processes of the networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 61
                            }
                        ],
                        "text": "This meta-level use of graphical models was rst suggested by Spiegelhalter and Lauritzen (1990) inthe context of learning probabilities for Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10739577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcce2a3564685657c23d1afa00155c03560e76ac",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A directed acyclic graph or influence diagram is frequently used as a representation for qualitative knowledge in some domains in which expert system techniques have been applied, and conditional probability tables on appropriate sets of variables form the quantitative part of the accumulated experience. It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates. By exploiting the graphical structure, the updating can be performed locally, either approximately or exactly, and the setup makes it possible to take advantage of a range of well-established statistical techniques. As examples we discuss discrete models, models based on Dirichlet distributions and models of the logistic regression type."
            },
            "slug": "Sequential-updating-of-conditional-probabilities-on-Spiegelhalter-Lauritzen",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional probabilities on directed graphical structures"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates and how to take advantage of a range of well-established statistical techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 55
                            }
                        ],
                        "text": "Results for Gaussians are presented, for instance, in (Geiger & Heckerman,1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5521191,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "640e79936e620fa3436b4e9f16dbba00b9a26c2a",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Gaussian-Networks-Geiger-Heckerman",
            "title": {
                "fragments": [],
                "text": "Learning Gaussian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096463740"
                        ],
                        "name": "Wray",
                        "slug": "Wray",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Wray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080610618"
                        ],
                        "name": "BuntineRIACSyArti",
                        "slug": "BuntineRIACSyArti",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "BuntineRIACSyArti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "BuntineRIACSyArti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 218
                            }
                        ],
                        "text": "This allows commonalities between seemingly diverse pairs of algorithms|such as k-means clustering versus approximate methods for learning hidden Markovmodels, learning decision trees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus the expectation maximization algorithm in Section 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15409203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4356e9cba1e24306d9ab9bce65fef5ae52eefe2f",
            "isKey": true,
            "numCitedBy": 9,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes how a competitive tree learning algorithm can be derived from rst principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed."
            },
            "slug": "Classi-ers-:-A-Theoretical-and-Empirical-Study-Wray-BuntineRIACSyArti",
            "title": {
                "fragments": [],
                "text": "Classi ers : A Theoretical and Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Comparative experiments show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost, than the mature AI and statistical families of tree learning algorithms currently in use."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11654278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66f02ad6395a3883c2fba7d851fdac91c796148a",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed."
            },
            "slug": "Classifiers:-A-Theoretical-and-Empirical-Study-Buntine",
            "title": {
                "fragments": [],
                "text": "Classifiers: A Theoretical and Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Comparative experiments show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost, than the several mature AI and statistical families of tree learning algorithms currently in use."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46558286"
                        ],
                        "name": "S. K. Andersen",
                        "slug": "S.-K.-Andersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Kj\u00e6r"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34760449"
                        ],
                        "name": "K. Olesen",
                        "slug": "K.-Olesen",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Olesen",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Olesen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191477"
                        ],
                        "name": "F. V. Jensen",
                        "slug": "F.-V.-Jensen",
                        "structuredName": {
                            "firstName": "Finn",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Verner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. V. Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120177012"
                        ],
                        "name": "Frank Jensen",
                        "slug": "Frank-Jensen",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Jensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16250237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "181b3b89260b8859d86bf641b7e2b2a4c2663e98",
            "isKey": false,
            "numCitedBy": 466,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Causal probabilistic networks have proved to be a useful knowledge representation tool for modelling domains where causal relations in a broad sense are a natural way of relating domain objects and where uncertainty is inherited in these relations. This paper outlines an implementation the HUGIN shell - for handling a domain model expressed by a causal probabilistic network. The only topological restriction imposed on the network is that, it must not contain any directed loops. The approach is illustrated step by step by solving a genetic breeding problem. A graph representation of the domain model is interactively created by using instances of the basic network components-- nodes and arcs--as building blocks. This structure, together with the quantitative relations between nodes and their immediate causes expressed as conditional probabilities, are automatically transformed into a tree structure, a junction tree. Here a computationally efficient and conceptually simple algebra of Bayesian belief universes supports incorporation of new evidence, propagation of information, and calculation of revised beliefs in the states of the nodes in the network. Finally, as an exam ple of a real world application, MUNIN an expert system for electromyography is discussed."
            },
            "slug": "HUGIN-A-Shell-for-Building-Bayesian-Belief-for-Andersen-Olesen",
            "title": {
                "fragments": [],
                "text": "HUGIN - A Shell for Building Bayesian Belief Universes for Expert Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper outlines an implementation the HUGIN shell - for handling a domain model expressed by a causal probabilistic network, and a computationally efficient and conceptually simple algebra of Bayesian belief universes supports incorporation of new evidence, propagation of information, and calculation of revised beliefs in the states of the nodes in the network."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93273692"
                        ],
                        "name": "Leland Stewart",
                        "slug": "Leland-Stewart",
                        "structuredName": {
                            "firstName": "Leland",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leland Stewart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 252
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 128
                            }
                        ],
                        "text": "Learning with Graphical Models Exact Bayes factors: Model selection and averaging methods are used to deal with multiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118128864,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5471001490c16ca6f439e7916c804b54e1a1893e",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian approach allows the statistician to compute the posterior probability for each model in a set of possible models and therefore to retain consideration of several or many models throughout the analysis rather than to restrict attention to just one 'best' model. This paper illustrates the use of Monte Carlo integration in hierarchical Bayesian analysis when there are many possible models of different dimensions. A hierarchical approach can facilitate the choice of a prior distribution as well as the Monte Carlo computation. The methodology is illustrated by an example in multiple logistic regression involving 256 possible models."
            },
            "slug": "Hierarchical-Bayesian-Analysis-using-Monte-Carlo-Stewart",
            "title": {
                "fragments": [],
                "text": "Hierarchical Bayesian Analysis using Monte Carlo Integration: Computing Posterior Distributions when"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The use of Monte Carlo integration in hierarchical Bayesian analysis when there are many possible models of different dimensions is illustrated by an example in multiple logistic regression involving 256 possible models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406721516"
                        ],
                        "name": "Adriano Azevedo-Filho",
                        "slug": "Adriano-Azevedo-Filho",
                        "structuredName": {
                            "firstName": "Adriano",
                            "lastName": "Azevedo-Filho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adriano Azevedo-Filho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9903027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d7a296a0b0210be84934120396a41b7f7b2c25f",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Laplace's-Method-Approximations-for-Probabilistic-Azevedo-Filho-Shachter",
            "title": {
                "fragments": [],
                "text": "Laplace's Method Approximations for Probabilistic Inference in Belief Networks with Continuous Variables"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706719"
                        ],
                        "name": "Wayne Iba",
                        "slug": "Wayne-Iba",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Iba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Iba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123928894"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15383317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1925bacaa10b4ec83a0509132091bb79243b41b6",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an average-case analysis of the Bayesian classiier, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, Boolean attributes that are independent of each other and that follow a single distribution, and the absence of attribute noise. We rst calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions ; we then use this expression to compute the probability of correct classiication over the space of instances. The analysis takes into account the number of training instances, the number of relevant and irrelevant attributes, the distribution of these attributes, and the level of class noise. In addition, we explore the behavioral implications of the analysis by presenting predicted learning curves for a number of artiicial domains. We also give experimental results on these domains as a check on our reasoning. Finally, we discuss some unresolved questions about the behavior of Bayesian classiiers and outline directions for future research. we nd the current format more desirable. We have not submitted the paper to any other conference or journal."
            },
            "slug": "An-Analysis-of-Bayesian-Classiiers-Langley-Iba",
            "title": {
                "fragments": [],
                "text": "An Analysis of Bayesian Classiiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An average-case analysis of the Bayesian classiier is presented, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks and explores the behavioral implications by presenting predicted learning curves for a number of artiicial domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 249
                            }
                        ],
                        "text": "\u2026; 1; 2; 3jM) d( ; 1; 2; 3) : (10)This term is called the evidence for model M , or model likelihood, and is the basis formost Bayesian model selection, model averaging methods, and Bayesian hypothesis testing171\nBuntinemethods using Bayes factors (Smith & Spiegelhalter, 1980; Kass & Raftery, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115306388,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "92501b3408eee32f4c7df8e98b71bc37f0933934",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Global and local Bayes factors are defined and their respective roles examined as choice criteria among alternative linear models. The global Bayes factor is seen to function, in appropriate contexts, as a fully automatic Occam's razor and to be closely related to the Schwarz model choice criterion. The local Bayes factor is shown to have a close relationship with the Akaike Information Criterion. THE problem of choosing between alternative models continues to attract a good deal of theoretical attention, much of which has been stimulated by the appearance of the Akaike Information Criterion (Akaike, 1973). The AIC and its variants (see, for example, Bhansali and Downham, 1977) essentially adjust the likelihood ratio test statistic by a constant multiple of the difference in dimensionalities of the two models under consideration. Schwarz (1978) has recently proposed a fundamentally different criterion which replaces the constant multiplier of the AIC by the logarithm of the sample size. Stone (1979) has compared and contrasted these two approaches in terms of certain of their asymptotic properties, and a further contribution in this area is provided by Hannan and Quinn (1979). In this paper, which deals with the important special case of choice between alternative nested linear models, we shall also be concerned with comparing and contrasting these two forms of model choice criteria, but from a rather different, non-asymptotic, perspective. Our starting point is not a consideration of the criteria themselves, but, instead, a discussion of the Bayesian approach to comparing alternative nested linear models on the basis of their posterior probabilities, or, equivalently, on the basis of ratios of posterior to prior odds. Stone has, in effect, argued that the comparison of choice criteria for linear models on the basis of their asymptotic properties is rather arbitrarily dependent on the assumptions made about the embedded sequence of design matrices. Our approach avoids the arbitrary asymptotics and concentrates, instead, on the way in which the dependence of the prior specification on the design matrix influences the forms of model choice criteria which arise from consideration of posterior probabilities. It will be shown that, depending on the nature of the prior specification adopted for model parameters, two fundamentally different forms of odds ratio, or Bayesfactor, arise. The first of these, which we shall call the global Bayes factor, will be discussed in Section 2, and will be shown to lead, essentially, to the Schwarz-type of criterion. The relationship of the global Bayes factor to the so-called Lindley Paradox (Lindley, 1957) will also be examined and, motivated by this, in Section 3 we derive what we shall call the local Bayes factor. This will be shown to lead to a variant of the Akaike criterion, and a comparison will then be made with various Akaike-type procedures, including those of Bhansali and Downham (1977)."
            },
            "slug": "Bayes-Factors-and-Choice-Criteria-for-Linear-Models-Smith-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Bayes Factors and Choice Criteria for Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107218881"
                        ],
                        "name": "A. Thomas",
                        "slug": "A.-Thomas",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 98
                            }
                        ],
                        "text": "A version of this toolkit alreadyexists using Gibbs sampling as the general computational scheme (Gilks et al., 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 40
                            }
                        ],
                        "text": "Thomas, Spiegelhalter, and Gilks (1992)(Gilks et al., 1993b) have taken advantage ofthis general applicability of sampling to create a compiler that converts a graphical represen-tation of a data analysis problem, with plates, into a matching Gibbs sampling algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 95
                            }
                        ],
                        "text": "It follows that Gibbs sampling is readily applied to learning as ageneral inference algorithm (Gilks et al., 1993a, 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 183
                            }
                        ],
                        "text": "\u2026main families of these approximate methods.7.1 Gibbs samplingGibbs sampling is the basic tool of simulation and can be applied to most probabilitydistributions (Geman & Geman, 1984; Gilks et al., 1993a; Ripley, 1987) as long as the fulljoint has no zeros (all variable instantiations are possible)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 245
                            }
                        ],
                        "text": "\u2026diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et al., 1993a) and statistics (Whittaker, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41819931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a1e584f9a91472d6e15184f1648f57256216198",
            "isKey": true,
            "numCitedBy": 718,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler; third, a sampling module containing numerical routines to perform the sampling. S objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX."
            },
            "slug": "A-Language-and-Program-for-Complex-Bayesian-Gilks-Thomas",
            "title": {
                "fragments": [],
                "text": "A Language and Program for Complex Bayesian Modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes some general purpose software that is currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling), written in Modula-2 and runs under both DOS and UNIX."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14437598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c403081c41dce4a1dff1a31a468b4e55aadef3d8",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "I address practical issues concerning the construction of normative expert systems--expert systems that encode knowledge within a decision-theoretic framework. In particular, I examine the similarity network and partition, two extensions to the influence diagram. A similarity network is a tool for building an influence diagram, whereas a partition is a tool for assessing the probabilities associated with an influence diagram. Both representations encode asymmetric forms of conditional independence that are not represented conveniently in an ordinary influence diagram. Similarity networks and partitions exploit these forms of conditional independence to facilitate the construction and assessment of influence diagrams for problems of diagnosis. \nThe representations aided considerably the construction of Pathfinder, a large normative expert system for the diagnosis of lymph-node diseases (the domain contains approximately 60 diseases and 110 disease findings). In an early version of the system, I encoded the knowledge of the expert using an erroneous assumption that all disease findings were conditionally independent, given each disease. When the expert and I attempted to build an influence diagram for the domain to capture the dependencies among the disease findings, we failed. Using a similarity network, however, we were able to construct the influence diagram for the entire domain in approximately 40 hours. Furthermore, using the partition representation, the expert was able to decrease the time required to assess a probability--on average--by almost one order of magnitude. Most important, through a comparison procedure based in decision theory, I found that the improvements in diagnostic accuracy afforded by the more sophisticated model of the domain were well worth the additional effort that we had invested to build the revised version of the system. \nIn this work, I examine in detail the theoretical properties of similarity networks and partitions, and discuss the application of these representations to the construction of Pathfinder. This research suggests strongly that, by identifying specific forms of conditional independence, and by developing representations that exploit these forms of independence for knowledge acquisition, knowledge engineers can construct normative expert systems for domains of larger scope and greater complexity than the domains previously through to be amenable to the decision-theoretic approach."
            },
            "slug": "Probabilistic-similarity-networks-Heckerman",
            "title": {
                "fragments": [],
                "text": "Probabilistic similarity networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This research suggests strongly that, by identifying specific forms of conditional independence, and by developing representations that exploit these forms of independence for knowledge acquisition, knowledge engineers can construct normative expert systems for domains of larger scope and greater complexity than the domains previously through to be amenable to the decision-theoretic approach."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2124212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011fc271a69a3aa4cf2683099a5abcdc03317e26",
            "isKey": false,
            "numCitedBy": 756,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-Refinement-on-Bayesian-Networks-Buntine",
            "title": {
                "fragments": [],
                "text": "Theory Refinement on Bayesian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46558286"
                        ],
                        "name": "S. K. Andersen",
                        "slug": "S.-K.-Andersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Kj\u00e6r"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679873"
                        ],
                        "name": "Peter Szolovits",
                        "slug": "Peter-Szolovits",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Szolovits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Szolovits"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17602100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "767e4b4cc2fe67f12e7cffccf824f64803f39207",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Global-Conditioning-for-Probabilistic-Inference-in-Shachter-Andersen",
            "title": {
                "fragments": [],
                "text": "Global Conditioning for Probabilistic Inference in Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1904578"
                        ],
                        "name": "I. Meilijson",
                        "slug": "I.-Meilijson",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Meilijson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Meilijson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "Convergence is slow near a local maxima so some implementations switch to con-jugate gradient or other methods (Meilijson, 1989) when near a solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "Convergence is slow near a local maxima so some implementations switch to conjugate gradient or other methods (Meilijson, 1989) when near a solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118850901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f808cd5d49b2fbf848fc6ab55a661e3b71fc982",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm is a numerical technique for the evaluation of maximum likelihood estimates for parameters describing incomplete data models. It is easy to apply in many problems and is stable but slow. The algorithm fails to provide a consistent estimator of the standard errors of the maximum likelihood estimates unless the additional analysis required by the Louis method is performed. Newton-type or other gradient methods are faster and provide error estimates but tend to be unstable and require the analytical evaluation of likelihoods to derive expressions for the score function and (at least) approximations to the Fisher information matrix. The purpose of this paper is to expand on a result by Fisher that permits a unification of EM methodology and Newton methods. The evaluation of the individual observation-by-observation score functions of the incomplete data is a byproduct of the application of the E step of the EM algorithm. Once these become available, the Fisher information matrix may be consistently estimated, and the M step may be replaced by a fast Newton-type step."
            },
            "slug": "A-fast-improvement-to-the-EM-algorithm-on-its-own-Meilijson",
            "title": {
                "fragments": [],
                "text": "A fast improvement to the EM algorithm on its own terms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A result by Fisher is expanded that permits a unification of EM methodology and Newton methods and the evaluation of the individual observation-by-observation score functions of the incomplete data is a byproduct of the application of the E step of the EM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775921"
                        ],
                        "name": "Uffe Kj\u00e6rulff",
                        "slug": "Uffe-Kj\u00e6rulff",
                        "structuredName": {
                            "firstName": "Uffe",
                            "lastName": "Kj\u00e6rulff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uffe Kj\u00e6rulff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 8581122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce412157e629989b0af4abdaf5251255a8c3e29a",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Computational-Scheme-for-Reasoning-in-Dynamic-Kj\u00e6rulff",
            "title": {
                "fragments": [],
                "text": "A Computational Scheme for Reasoning in Dynamic Probabilistic Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840562"
                        ],
                        "name": "R. Kass",
                        "slug": "R.-Kass",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kass",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": "Graphical operationsmanipulate the underlying structure of a problem unhindered by the ne detail of theconnecting functional and distributional equations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 214
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 278
                            }
                        ],
                        "text": "\u2026; 1; 2; 3jM) d( ; 1; 2; 3) : (10)This term is called the evidence for model M , or model likelihood, and is the basis formost Bayesian model selection, model averaging methods, and Bayesian hypothesis testing171\nBuntinemethods using Bayes factors (Smith & Spiegelhalter, 1980; Kass & Raftery, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 132
                            }
                        ],
                        "text": "TheBayes factor is a relative quantity used to compare one model M1 with another M2:Bayes-factor(M2;M1) = p(samplejM2)p(samplejM1) :Kass and Raftery (1993) review the large variety of methods available for computing orestimating the evidence for a model including numerical integration, importance\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 130
                            }
                        ],
                        "text": "If these exact computations are not available, various approximation methods can be usedto compute the evidence or Bayes factors (Kass & Raftery, 1993); some are discussed inSection 7.3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 165
                            }
                        ],
                        "text": "The basiccomputational techniques of probabilistic (Bayesian) inference used in this computationaltheory of learning are widely reviewed (Tanner, 1993; Press, 1989; Kass & Raftery, 1993;Neal, 1993; Bretthorst, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17191147,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42d671ae17a611ac474cb39f59f4cf31f65b51ef",
            "isKey": true,
            "numCitedBy": 266,
            "numCiting": 189,
            "paperAbstract": {
                "fragments": [],
                "text": "In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications. The points we emphasize are: from Jeffreys's Bayesian point of view, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory; Bayes factors offer a way of evaluating evidence in favor of a null hypothesis; Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis; Bayes factors are very general, and do not require alternative models to be nested; several techniques are available for computing Bayes factors, including asymptotic approximations which are easy to compute using the output from standard packages that maximize likelihoods; in \"non-standard\" statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests; the Schwarz criterion (or BIC) gives a crude approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions; when one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty; algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large; Bayes factors are useful for guiding an evolutionary model-building process; and, finally, it is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used."
            },
            "slug": "Bayes-factors-and-model-uncertainty-Kass-Raftery",
            "title": {
                "fragments": [],
                "text": "Bayes factors and model uncertainty"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper reviews and discusses the uses of Bayes factors in the context of five scientific applications and suggests that it is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895150"
                        ],
                        "name": "P. Dagum",
                        "slug": "P.-Dagum",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Dagum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dagum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895150"
                        ],
                        "name": "P. Dagum",
                        "slug": "P.-Dagum",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Dagum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dagum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91275615"
                        ],
                        "name": "A. Galper",
                        "slug": "A.-Galper",
                        "structuredName": {
                            "firstName": "Arkady",
                            "lastName": "Galper",
                            "middleNames": [
                                "Moiseev"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Galper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145479841"
                        ],
                        "name": "E. Horvitz",
                        "slug": "E.-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1869325"
                        ],
                        "name": "A. Seiver",
                        "slug": "A.-Seiver",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Seiver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Seiver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16244317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "482d185000df7c5beb56bfd0b7487634f4742228",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Uncertain-reasoning-and-forecasting-Dagum-Dagum",
            "title": {
                "fragments": [],
                "text": "Uncertain reasoning and forecasting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "The arcreversal operator interchanges the order of two nodes connected by a directed arc (Shachter,1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "This is therole of decision theory and it is modeled in graphical form using in uence diagrams(Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "This makes data analysisproblems explicit in much the same way that utility and decision nodes are used for decisionanalysis problems (Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5770960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4",
            "isKey": false,
            "numCitedBy": 1287,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An influence diagram is a graphical structure for modeling uncertain variables and decisions and explicitly revealing probabilistic dependence and the flow of information. It is an intuitive framework in which to formulate problems as perceived by decision makers and to incorporate the knowledge of experts. At the same time, it is a precise description of information that can be stored and manipulated by a computer. We develop an algorithm that can evaluate any well-formed influence diagram and determine the optimal policy for its decisions. Since the diagram can be analyzed directly, there is no need to construct other representations such as a decision tree. As a result, the analysis can be performed using the decision maker's perspective on the problem. Questions of sensitivity and the value of information are natural and easily posed. Modifications to the model suggested by such analyses can be made directly to the problem formulation, and then evaluated directly."
            },
            "slug": "Evaluating-Influence-Diagrams-Shachter",
            "title": {
                "fragments": [],
                "text": "Evaluating Influence Diagrams"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm is developed that can evaluate any well-formed influence diagram and determine the optimal policy for its decisions and can be performed using the decision maker's perspective on the problem."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895150"
                        ],
                        "name": "P. Dagum",
                        "slug": "P.-Dagum",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Dagum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dagum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145479841"
                        ],
                        "name": "E. Horvitz",
                        "slug": "E.-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Horvitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "The two approaches can be combined insome cases after appropriate reformulation of the problem (Dagum & Horvitz, 1992).4.1 Exact inference without platesThe exact inference approach has been highly re ned for the case where all variables arediscrete."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16727577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0d305018012162a22ebcde6380c9f9c15ccad3d",
            "isKey": true,
            "numCitedBy": 8,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reformulating-Inference-Problems-Through-Selective-Dagum-Horvitz",
            "title": {
                "fragments": [],
                "text": "Reformulating Inference Problems Through Selective Conditioning"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "Markovrandom elds are used in imaging and spatial reasoning (Ripley, 1981; Geman & Geman,1984; Besag et al., 1991) and various stochastic models in neural networks (Hertz, Krogh,& Palmer, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "\u2026main families of these approximate methods.7.1 Gibbs samplingGibbs sampling is the basic tool of simulation and can be applied to most probabilitydistributions (Geman & Geman, 1984; Gilks et al., 1993a; Ripley, 1987) as long as the fulljoint has no zeros (all variable instantiations are possible)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": true,
            "numCitedBy": 18709,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52089885"
                        ],
                        "name": "Wray L. BuntineRIACS",
                        "slug": "Wray-L.-BuntineRIACS",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "BuntineRIACS",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. BuntineRIACS"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13593309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bdb1db3a2ba290cd69c665674c1d205ecd8523a",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Theory reenement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory reenement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incre-mental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally reened from data. Algorithms for reenement of Bayesian networks are presented to illustrate what is meant by \\partial theory\", \\alternative theory repre-sentation\", etc. The algorithms are an incre-mental variant of batch learning algorithms from the literature so can work well in batch and incremental mode."
            },
            "slug": "Theory-Reenement-on-Bayesian-Networks-BuntineRIACS",
            "title": {
                "fragments": [],
                "text": "Theory Reenement on Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Algorithms for reenement of Bayesian networks are presented to illustrate what is meant by partial theory, and are an incre-mental variant of batch learning algorithms from the literature so can work well in batch and incremental mode."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953625"
                        ],
                        "name": "K. Lange",
                        "slug": "K.-Lange",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lange",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3090258"
                        ],
                        "name": "J. Sinsheimer",
                        "slug": "J.-Sinsheimer",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Sinsheimer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sinsheimer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 133
                            }
                        ],
                        "text": "Byintroducing a convolution, these robust regression models can be handled by combining theEM algorithm with standard least squares (Lange & Sinsheimer, 1993).8.2 Feed-forward networks with a linear output layerA similar example is the standard feed-forward network where the nal output layer is\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123101390,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a476a2b0b0debec5632f68406277ada18b21d195",
            "isKey": true,
            "numCitedBy": 320,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Maximum likelihood estimation with nonnormal error distributions provides one method of robust regression. Certain families of normal/independent distributions are particularly attractive for adaptive, robust regression. This article reviews the properties of normal/independent distributions and presents several new results. A major virtue of these distributions is that they lend themselves to EM algorithms for maximum likelihood estimation. EM algorithms are discussed for least Lp regression and for adaptive, robust regression based on the t, slash, and contaminated normal families. Four concrete examples illustrate the performance of the different methods on real data."
            },
            "slug": "Normal/Independent-Distributions-and-Their-in-Lange-Sinsheimer",
            "title": {
                "fragments": [],
                "text": "Normal/Independent Distributions and Their Applications in Robust Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31878411"
                        ],
                        "name": "M. Frydenberg",
                        "slug": "M.-Frydenberg",
                        "structuredName": {
                            "firstName": "Morten",
                            "lastName": "Frydenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Frydenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 163
                            }
                        ],
                        "text": "\u2026of the children and their chain components:Markov-blanket(u) = neighbors(u) [ ndparents(u) [ ndchildren(u) (22)[ ndparents(chain-components(ndchildren(u))) :From Frydenberg (1990) it follows that u is independent of the other non-deterministicvariables in the graph G given the Markov blanket."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "To test independenceusing the method of Frydenberg (1990), each plate must be expanded (that is, duplicate itthe right number of times), moralize the graph, removing the given nodes, and then testfor separability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 162
                            }
                        ],
                        "text": "\u2026combinations of these algorithms exist in the literature, includ-ing the handling of deterministic nodes (Shachter, 1990) and chain graphs and undirectedgraphs (Frydenberg, 1990).4.1.1 Arc reversalTwo basic steps for inference are to marginalize nuisance parameters or to condition on newevidence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 184
                            }
                        ],
                        "text": "This goes as follows:De nition 2.2 Given a subgraph G over some variables X, the chain components are sub-sets of X that are maximal undirected connected subgraphs in a chain graph G (Frydenberg,1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 80
                            }
                        ],
                        "text": "These results are simple applications of known methods for testing independence (Frydenberg, 1990; Lauritzen et al., 1990), with some added complication because of the use of plates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 5
                            }
                        ],
                        "text": "From Frydenberg (1990), we have:p(X jM) = Y 2 (X) Yi2ind( ) YC2cliques( ) gC(Ci) :Furthermore, if u 2 Xi is not known, then the variables in u's Markov blanket will occur inXi, and therefore, if u 2 C for some clique C, then C Xi."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 70
                            }
                        ],
                        "text": "These mixed graphs arecalled chain graphs (Wermuth & Lauritzen, 1989; Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 191
                            }
                        ],
                        "text": "\u2026graphs in terms of independence statements and the implied functional form ofthe joint probability is a combination of the previous two forms given in Equation (2)and Theorem 2.1, based on (Frydenberg, 1990, Theorem 4.1), and on the interpretation ofconditional graphical models in Section 2.3.3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 43
                            }
                        ],
                        "text": "These mixed graphs are called chain graphs (Wermuth & Lauritzen, 1989; Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 106
                            }
                        ],
                        "text": "Then under the distribution p(X), xis independent of X fxg neighbors(x) given neighbors(x) for all x 2 X (Frydenberg(1990) refers to this condition as local G-Markovian) if, and only if, p(X) has the functionalrepresentation p(X) = YC2Cliques(G) fC(C) ; (4)for some functions fC > 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "These results are simple applications of known methods fortesting independence (Frydenberg, 1990; Lauritzen et al., 1990), with some added compli-cation because of the use of plates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 199
                            }
                        ],
                        "text": "Many more sophisticated variations and combinations of these algorithms exist in the literature, including the handling of deterministic nodes (Shachter, 1990) and chain graphs and undirected graphs (Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "2 Given a subgraph G over some variables X, the chain components are subsets of X that are maximal undirected connected subgraphs in a chain graph G (Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116029196,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e3b9ffde49c9b4a57b09a651187a2e194cb35693",
            "isKey": true,
            "numCitedBy": 417,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of graphs, chain graphs, suitable for modelling conditional independencies are introduced and their Markov properties investigated. This class of graphs, which includes the undirected and directed acyclic graphs, enables modelling recursive models with multivariate response variables. Results concerning the equivalence of different definitions of their Markov properties including a factorization of the density are shown. We give a necessary and sufficient condition for two chain graphs to have the same Markov properties"
            },
            "slug": "The-chain-graph-Markov-property-Frydenberg",
            "title": {
                "fragments": [],
                "text": "The chain graph Markov property"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Introductions are given in (Bretthorst, 1994; Press, 1989;Loredo, 1992; Bernardo & Smith, 1994; Cheeseman, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6698730,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b220e238a4ce46df0430838fb0aaa55cc26677c",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "The \"frequentist\" approach to statistics, currently domi\u00ad nating statistical practice in astrophysics, is compared to the historically older Bayesian approach, which is now growing in popularity in other scien\u00ad tific disciplines, and which provides unique, optimal solutions to well-posed problems. The two approaches address the same questions with very dif\u00ad ferent calculations, but in simple cases often give the same final results, confusing the issue of whether one is superior to the other. Here frequentist and Bayesian methods are applied to problems where such a mathemati\u00ad cal coincidence does not occur, allowing assessment of their relative merits based on their performance, rather than philosophical argument. Emphasis is placed on a key distinction between the two approaches: Bayesian meth\u00ad ods, based on comparisons among alternative hypotheses using the single observed data set, consider averages over hypotheses; frequentist methods, in contrast, average over hypothetical alternative data samples and consider hypothesis averaging to be irrelevant. Simple problems are presented that magnify the consequences of this distinction to where common sense can confidently judge between the methods. These demonstrate the irrelevance of sample averaging, and the necessity of hypothesis averaging, revealing frequentist methods to be fundamentally flawed. Bayesian methods are then presented for astrophysically relevant problems using the Poisson distribu\u00ad tion, including the analysis of \"on/off\" measurements of a weak source in a strong background. Weaknesses of the presently used frequentist methods for these problems are straightforwardly overcome using Bayesian meth\u00ad ods. Additional existing applications of Bayesian inference to astrophysical problems are noted."
            },
            "slug": "Promise-of-Bayesian-Inference-for-Astrophysics-Loredo",
            "title": {
                "fragments": [],
                "text": "Promise of Bayesian Inference for Astrophysics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4371968"
                        ],
                        "name": "G. L. Bretthorst",
                        "slug": "G.-L.-Bretthorst",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Bretthorst",
                            "middleNames": [
                                "Larry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. L. Bretthorst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 139
                            }
                        ],
                        "text": "The basic computational techniques of probabilistic (Bayesian) inference used in this computational theory of learning are widely reviewed (Tanner, 1993; Press, 1989; Kass & Raftery, 1993; Neal, 1993; Bretthorst, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16149910,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8363d3d07af73bcf1933876547d3ad44bf1789df",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability theory as logic is founded on three simple desiderata: that degrees of belief should be represented by real numbers, that one should reason consistently, and that the theory should reduce to Aristotelian logic when the truth values of the hypotheses are known. Because this theory represents a probability as a state of knowledge, not a state of nature, hypotheses such as \u201cThe frequency of oscillation of a sinusoidal signal had value \u03c9 when the data were taken,\u201d or \u201cModel x is a better description of the data than model y\u201d make perfect sense. Problems of the first type are generally thought of as parameter estimation problems, while problems of the second type are thought of as model selection problems. However, in probability theory there is no essential distinction between these two types of problems. They are both solved by application of the sum and product rules of probability theory. Model selection problems are conceptually more difficult, because the models may have different functional forms. Consequently, conceptual difficulties enter the problem that are not present in parameter estimation. This paper is a tutorial on model selection. The conceptual problems that arise in model selection will be illustrated in such a way as to automatically avoid any difficulties. A simple example is worked in detail. This example, (radar target identification) illustrates all of the points of principle that must be faced in more complex model selection problems, including how to handle nuisance parameters, uninformative prior probabilities, and incomplete sets of models."
            },
            "slug": "AN-INTRODUCTION-TO-MODEL-SELECTION-USING-THEORY-AS-Bretthorst",
            "title": {
                "fragments": [],
                "text": "AN INTRODUCTION TO MODEL SELECTION USING PROBABILITY THEORY AS LOGIC"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371112"
                        ],
                        "name": "L. Tierney",
                        "slug": "L.-Tierney",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Tierney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tierney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741375972"
                        ],
                        "name": "J. Kadane",
                        "slug": "J.-Kadane",
                        "structuredName": {
                            "firstName": "Joseph (Jay) B.",
                            "lastName": "Kadane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kadane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 124
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16603443,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "aa7138c899fd48d3c8df2ccbb65dce06ca4d12c2",
            "isKey": false,
            "numCitedBy": 1982,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal."
            },
            "slug": "Accurate-Approximations-for-Posterior-Moments-and-Tierney-Kadane",
            "title": {
                "fragments": [],
                "text": "Accurate Approximations for Posterior Moments and Marginal Densities"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "These approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary parameters can also be used to compute approximate predictive densities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706719"
                        ],
                        "name": "Wayne Iba",
                        "slug": "Wayne-Iba",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Iba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Iba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123928894"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21634132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e40ea249dfad6d8d133b7917ca031c0b32410a5",
            "isKey": false,
            "numCitedBy": 1356,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains, and give experimental results on these domains as a check on our reasoning."
            },
            "slug": "An-Analysis-of-Bayesian-Classifiers-Langley-Iba",
            "title": {
                "fragments": [],
                "text": "An Analysis of Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks, and explores the behavioral implications of the analysis by presenting predicted learning curves for artificial domains."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 144
                            }
                        ],
                        "text": "Many moresophisticated variations and combinations of these algorithms exist in the literature, includ-ing the handling of deterministic nodes (Shachter, 1990) and chain graphs and undirectedgraphs (Frydenberg, 1990).4.1.1 Arc reversalTwo basic steps for inference are to marginalize nuisance\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 959868,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "44eae3dc84e1c3655c39ee63ba4c452b2871fe5f",
            "isKey": true,
            "numCitedBy": 94,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Influence diagrams are a directed network representation for decision making under uncertainty. The nodes in the diagram represent uncertain and decision variables, and the arcs indicate probabilistic dependence and observability. This paper examines the graphical orderings underlying the influence diagram and the primitive interchange operations that can reorder the network. These operations are sufficient to determine the maximal independent set and minimal relevant sets for any given inference problem, and a linear time algorithm is developed to obtain those sets. This framework is also used to examine and explain properties of the time structure of general influence diagrams with decisions."
            },
            "slug": "An-ordered-examination-of-influence-diagrams-Shachter",
            "title": {
                "fragments": [],
                "text": "An ordered examination of influence diagrams"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The graphical orderings underlying the influence diagram and the primitive interchange operations that can reorder the network are examined, finding these operations are sufficient to determine the maximal independent set and minimal relevant sets for any given inference problem."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 4
                            }
                        ],
                        "text": "See Charniak (1991), Shachter and Heckerman(1987), and Pearl (1988) for an introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8439549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57d47f58ec3674066ebf6e71170dff1b7bdd9b3a",
            "isKey": false,
            "numCitedBy": 943,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "I give an introduction to Bayesian networks for AI researchers with a limited grounding in probability theory. Over the last few years, this method of reasoning using probabilities has become popular within the AI probability and uncertainty community. Indeed, it is probably fair to say that Bayesian networks are to a large segment of the AI-uncertainty community what resolution theorem proving is to the AIlogic community. Nevertheless, despite what seems to be their obvious importance, the ideas and techniques have not spread much beyond the research community responsible for them. This is probably because the ideas and techniques are not that easy to understand. I hope to rectify this situation by making Bayesian networks more accessible to the probabilistically unsophisticated."
            },
            "slug": "Bayesian-Networks-without-Tears-Charniak",
            "title": {
                "fragments": [],
                "text": "Bayesian Networks without Tears"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An introduction to Bayesian networks for AI researchers with a limited grounding in probability theory is given, to make Bayesian Networks more accessible to the probabilistically unsophisticated."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039092"
                        ],
                        "name": "T. Hrycej",
                        "slug": "T.-Hrycej",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Hrycej",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hrycej"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 167
                            }
                        ],
                        "text": "The second approach to performing inference isapproximate and corresponds to approximate algorithms such as Gibbs sampling, and otherMarkov chain Monte Carlo methods (Hrycej, 1990; Hertz et al., 1991; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34858967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d7e985ae7643fa640f9b76dfad57c77dc99c8ab",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gibbs-Sampling-in-Bayesian-Networks-Hrycej",
            "title": {
                "fragments": [],
                "text": "Gibbs Sampling in Bayesian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796536"
                        ],
                        "name": "Michael P. Wellman",
                        "slug": "Michael-P.-Wellman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wellman",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Wellman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 130
                            }
                        ],
                        "text": "Graphical models are used in domains such as diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59699515,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "f7ad88a9e18237abf09efa8465aaa00ee2cad2a2",
            "isKey": false,
            "numCitedBy": 670,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Planning and Control\" explores planning and control by reformulating the two areas in a common control framework, developing the corresponding techniques side-by-side, and identifying opportunities for integrating their ideas and methods. This book is organized around the central roles of prediction, observation, and computation control. The first three chapters deal with predictive models of physical systems based on temporal logic and the differential calculus. Chapter 4 and 5 present some basic concepts in planning and control, including controllability, observability, stability, feedback control, task reduction, conditional plans, and the relationship between goals and preferences. Chapters 6 and 7 consider issues of uncertainty, covering state estimation and the Kalman filter, stochastic dynamic programming, probabilistic modeling, and graph-based decision models. The remaining chapters investigate selected topics in time-critical decision making, adaptive control, and hybrid control architectures. Throughout, the reader is led to consider critical tradeoffs involving the accuracy of prediction, the availability of information from observation, and the costs and benefits of computation in dynamic environments. This book is useful to researchers in artificial intelligence and control theory, and others concerned with the design of complex applications in robotics, automated manufacturing, and time-critical decision support."
            },
            "slug": "Planning-and-Control-Dean-Wellman",
            "title": {
                "fragments": [],
                "text": "Planning and Control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book is useful to researchers in artificial intelligence and control theory, and others concerned with the design of complex applications in robotics, automated manufacturing, and time-critical decision support."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102392606"
                        ],
                        "name": "Q. Vuong",
                        "slug": "Q.-Vuong",
                        "structuredName": {
                            "firstName": "Quang",
                            "lastName": "Vuong",
                            "middleNames": [
                                "Hieu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Vuong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 54
                            }
                        ],
                        "text": "See Casella and Berger (1990) for an introduction and Vuong (1989) for a recent review."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56312024,
            "fieldsOfStudy": [
                "Economics",
                "Mathematics"
            ],
            "id": "17228a0f580480191c3003289d7c910154370f8e",
            "isKey": false,
            "numCitedBy": 5211,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Using the Kullback-Leibler information criterion to measure the closeness of a model to the truth, the author proposes new likelihood-ratio-based statistics for testing the null hypothesis that the competing models are as close to the true data generating process against the alternative hypothesis that one model is closer. The tests are directional and are derived for the cases where the competing models are non-nested, overlapping, or nested and whether both, one, or neither is misspecified. As a prerequisite, the author fully characterizes the asymptotic distribution of the likelihood ratio statistic under the most general conditions. Copyright 1989 by The Econometric Society."
            },
            "slug": "Likelihood-Ratio-Tests-for-Model-Selection-and-Vuong",
            "title": {
                "fragments": [],
                "text": "Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410937"
                        ],
                        "name": "K. Basford",
                        "slug": "K.-Basford",
                        "structuredName": {
                            "firstName": "Kaye",
                            "lastName": "Basford",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Basford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 113
                            }
                        ],
                        "text": "An unsupervised learning algorithm learns hidden classes (Cheeseman,Self, Kelly, Taylor, Freeman, & Stutz, 1988; McLachlan & Basford, 1988). var1\nvar2 var3 classFigure 9: A simple classi cation problem169\nBuntineThe implied joint for these variables read from the graph is:p(class; var1; var2; var3)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119405289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034a70c9fbe8e0075f5a5b3a7b06bdf7d3cab4a1",
            "isKey": false,
            "numCitedBy": 2074,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "General Introduction Introduction History of Mixture Models Background to the General Classification Problem Mixture Likelihood Approach to Clustering Identifiability Likelihood Estimation for Mixture Models via EM Algorithm Start Values for EMm Algorithm Properties of Likelihood Estimators for Mixture Models Information Matrix for Mixture Models Tests for the Number of Components in a Mixture Partial Classification of the Data Classification Likelihood Approach to Clustering Mixture Models with Normal Components Likelihood Estimation for a Mixture of Normal Distribution Normal Homoscedastic Components Asymptotic Relative Efficiency of the Mixture Likelihood Approach Expected and Observed Information Matrices Assessment of Normality for Component Distributions: Partially Classified Data Assessment of Typicality: Partially Classified Data Assessment of Normality and Typicality: Unclassified Data Robust Estimation for Mixture Models Applications of Mixture Models to Two-Way Data Sets Introduction Clustering of Hemophilia Data Outliers in Darwin's Data Clustering of Rare Events Latent Classes of Teaching Styles Estimation of Mixing Proportions Introduction Likelihood Estimation Discriminant Analysis Estimator Asymptotic Relative Efficiency of Discriminant Analysis Estimator Moment Estimators Minimum Distance Estimators Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture Likelihood Approach to Clustering Introduction Estimators of the Allocation Rates Bias Correction of the Estimated Allocation Rates Estimated Allocation Rates of Hemophilia Data Estimated Allocation Rates for Simulated Data Other Methods of Bias Corrections Bias Correction for Estimated Posterior Probabilities Partitioning of Treatment Means in ANOVA Introduction Clustering of Treatment Means by the Mixture Likelihood Approach Fitting of a Normal Mixture Model to a RCBD with Random Block Effects Some Other Methods of Partitioning Treatment Means Example 1 Example 2 Example 3 Example 4 Mixture Likelihood Approach to the Clustering of Three-Way Data Introduction Fitting a Normal Mixture Model to Three-Way Data Clustering of Soybean Data Multidimensional Scaling Approach to the Analysis of Soybean Data References Appendix"
            },
            "slug": "Mixture-models-:-inference-and-applications-to-McLachlan-Basford",
            "title": {
                "fragments": [],
                "text": "Mixture models : inference and applications to clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Mixture Likelihood Approach to Clustering and the Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture likelihood approach toClustering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 94
                            }
                        ],
                        "text": "These problems are also well understood for feed-forward networks (Werbos,McAvoy, & Su, 1992; Buntine & Weigend, 1994), and graphical models with plates onlyadd some additional complexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 57
                            }
                        ],
                        "text": "Various notations are used for this (Werbos et al.,1992; Buntine & Weigend, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 423305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84df11f8dc44ee0f9be03cd488d41c2fd2f7aa69",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The calculation of second derivatives is required by recent training and analysis techniques of connectionist networks, such as the elimination of superfluous weights, and the estimation of confidence intervals both for weights and network outputs. We review and develop exact and approximate algorithms for calculating second derivatives. For networks with |w| weights, simply writing the full matrix of second derivatives requires O(|w|(2)) operations. For networks of radial basis units or sigmoid units, exact calculation of the necessary intermediate terms requires of the order of 2h+2 backward/forward-propagation passes where h is the number of hidden units in the network. We also review and compare three approximations (ignoring some components of the second derivative, numerical differentiation, and scoring). The algorithms apply to arbitrary activation functions, networks, and error functions."
            },
            "slug": "Computing-second-derivatives-in-feed-forward-a-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Computing second derivatives in feed-forward networks: a review"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The calculation of second derivatives is required by recent training and analysis techniques of connectionist networks, such as the elimination of superfluous weights, and the estimation of confidence intervals both for weights and network outputs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080177657"
                        ],
                        "name": "B. Chan",
                        "slug": "B.-Chan",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Chan",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932960"
                        ],
                        "name": "Ross D. Shachter",
                        "slug": "Ross-D.-Shachter",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Shachter",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross D. Shachter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 143
                            }
                        ],
                        "text": "\u2026models are used in domains such as diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5470790,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a3d73602a27c362b50e7a8b29d478a3bb09d3a0a",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Structural-Controllability-and-Observability-in-Chan-Shachter",
            "title": {
                "fragments": [],
                "text": "Structural Controllability and Observability in Influence Diagrams"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 291
                            }
                        ],
                        "text": "The generalform for this equation for a set of variables X is:p(X jM) = Yx2X p(xjparents(x);M) : (2)This equation is the interpretation of a Bayesian network used in this paper.2.2 Undirected graphical modelsAnother popular form of graphical model is an undirected graph, sometimes called a Markovnetwork (Pearl, 1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 287
                            }
                        ],
                        "text": "\u2026this equation for a set of variables X is:p(X jM) = Yx2X p(xjparents(x);M) : (2)This equation is the interpretation of a Bayesian network used in this paper.2.2 Undirected graphical modelsAnother popular form of graphical model is an undirected graph, sometimes called a Markovnetwork (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "It also requires a notion oflocal dependence, which is called the Markov blanket, following Pearl (1988), since it is ageneralization of the equivalent set for Bayesian networks.De nition 6.2 We have a chain graph G without plates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 55
                            }
                        ],
                        "text": "See Charniak (1991), Shachter and Heckerman(1987), and Pearl (1988) for an introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 283
                            }
                        ],
                        "text": "\u2026models are based on the notion of independence,which is worth repeating here.De nition 2.1 A is independent of B given C if p(A;BjC) = p(AjC)p(BjC) wheneverp(C) 6= 0, for all A;B;C.The theory of independence as a basic tool for knowledge structuring is developed by Dawid(1979) and Pearl (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "Thus, the formula only involves examining the parents,children and children's parents of x, the so-called Markov blanket (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": true,
            "numCitedBy": 18219,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 72
                            }
                        ],
                        "text": "Both these questions have no easy answer but heuristic strategies exist (Ripley, 1987; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "The scheme given below is the Metropolis algorithm(Ripley, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "Both these questions haveno easy answer but heuristic strategies exist (Ripley, 1987; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 117
                            }
                        ],
                        "text": "1 Gibbs sampling Gibbs sampling is the basic tool of simulation and can be applied to most probability distributions (Geman & Geman, 1984; Gilks et al., 1993a; Ripley, 1987) as long as the full joint has no zeros (all variable instantiations are possible)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 95
                            }
                        ],
                        "text": "It is a special case of the general Markov chain Monte Carlo methods for approximate inference (Ripley, 1987; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "The scheme given below is the Metropolis algorithm (Ripley, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "Methods for making subsequent samples independent are known as regenerative simulation (Ripley, 1987) and correspond to sending the temperature back to zero occasionally."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "It is a special case of the generalMarkov chain Monte Carlo methods for approximate inference (Ripley, 1987; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 87
                            }
                        ],
                        "text": "Methodsfor making subsequent samples independent are known as regenerative simulation (Ripley,1987) and correspond to sending the temperature back to zero occasionally."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 204
                            }
                        ],
                        "text": "\u2026main families of these approximate methods.7.1 Gibbs samplingGibbs sampling is the basic tool of simulation and can be applied to most probabilitydistributions (Geman & Geman, 1984; Gilks et al., 1993a; Ripley, 1987) as long as the fulljoint has no zeros (all variable instantiations are possible)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6151205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4180abb06c3e6931474c4a3e0ff079d2d5a0382",
            "isKey": true,
            "numCitedBy": 2209,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "One fifth (4 of 20) of the research articles published in the Journal of Educational Statistics in 1988 include simulation studies that justify or illustrate the authors' conclusions. A similar fraction (6 of 33) of the articles in the 1988 volume of Psychometrika include simulations; comparable proportions could be expected in other journals at the boundary of theoretical statistics and social/psychological applications. Due in part to the complexity of the problems tackled today and in part to the availability of cheap, powerful computing\u2014by no means independent influences\u2014simulation and Monte Carlo methods have become both necessary and practical tools for statisticians and applied workers in quantitative areas of education and psychology. Simulation has become popular\u2014not only in the quantitative social sciences, but in all of the mathematical sciences from physics to operations research to number theory\u2014because it is almost always easy to do. This ease of use makes the simulation experimenter vulnerable to two common pitfalls. Selection of the basic source of \"random numbers\" is often passive: Whatever is available in the computer's standard subroutine library is used. However, the fact that a pseudo-random number generator appears in a popular software package or operating system is hardly reason to trust it, as is shown by the infamous RANDU generator, once popular on IBM mainframes and PDP mini-computers, and by the generators burned into RAM on today's PCs. Simulation design and reporting also deserve special care. Some attempt must be made to assess the accuracy of the simulation estimates: One should accurately estimate and report SE (6) as well as 6. In addition, enough detail should be reported that the interested reader can replicate the study and check the results, just as with other experiments. Yet these considerations are also easy to overlook. Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques. Three themes run throughout the book. First, one shoud not take basic simulation subroutines for granted, especially on minior microcomputers where they tend to be poor implementations, implementations of poor algorithms, or both. Second, design of experiments, or variance reduction as it is known in this field, deserves greater consideration. Third, modern methods make it possible to simulate and analyze processes that are dependent over time, and using such processes opens the door to new simulation techniques, such as simulated annealing in optimization. Ripley intends this book to be a \"comprehensive guide,\" and it is indeed most accurately described as a researcher's handbook with examples and"
            },
            "slug": "Stochastic-simulation-Ripley",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques, and three themes run throughout the book."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley series in probability and mathematical statistics : applied probability and statistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27581584"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Howard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "This is a worthwhile introductory exercise in Bayesian decision theory (Howard, 1970) thatshould be familiar to most students of statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62664493,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6e09e1a42cd00ebc8ed77225e7123c4b292702e9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper illustrates by using a simple coin-tossing example how the new discipline of decision analysis sheds light on the perennial problems of inference, decision, and experimentation. The inference problem is first discussed from the classical viewpoints of maximum likelihood estimation and hypothesis testing, and then from the viewpoint of subjective probability and Bayesian updating. The problem is next placed in a decision setting to demonstrate how an estimate is related to the nature of the loss structure. Experimental possibilities are evaluated for the case where the size of the experiment must be determined a priori and for the case where experimentation can cease at any point. The decision-analysis philosophy allows consideration of all these problems within one philosophical and methodological framework."
            },
            "slug": "Decision-analysis:-Perspectives-on-inference,-and-Howard",
            "title": {
                "fragments": [],
                "text": "Decision analysis: Perspectives on inference, decision, and experimentation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper illustrates by using a simple coin-tossing example how the new discipline of decision analysis sheds light on the perennial problems of inference, decision, and experimentation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145614962"
                        ],
                        "name": "J. York",
                        "slug": "J.-York",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "York",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. York"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12393935"
                        ],
                        "name": "A. Molli\u00e9",
                        "slug": "A.-Molli\u00e9",
                        "structuredName": {
                            "firstName": "Annie",
                            "lastName": "Molli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Molli\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "In the context of Theorem 6.1, suppose there ex-ists a set of chain components j from the graph ignoring plates such that Xj = j [ndparents( j), where unknown(ndparents( j)) = ;."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 28
                            }
                        ],
                        "text": "Because ofthe Decomposition Theorem and its corollary, the Bayes factor for M2 versus M1 can befound by looking at local Bayes factors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 26
                            }
                        ],
                        "text": "Then usingthe notation of Theorem 4.1:ETi = 8<: t(Xi) if U = Vi = ; ;dw( )d 1 d logZi( )d otherwise :8."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 194
                            }
                        ],
                        "text": "The interpretationof these graphs in terms of independence statements and the implied functional form ofthe joint probability is a combination of the previous two forms given in Equation (2)and Theorem 2.1, based on (Frydenberg, 1990, Theorem 4.1), and on the interpretation ofconditional graphical models in Section 2.3.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "In the context in Theorem 4.1, assume the distri-bution for x given y; can be represented by the exponential family."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "The result follows.214\nLearning with Graphical ModelsA.2 Proof of Theorem 6.1It takesO(jX j2) operations to remove the deterministic nodes from a graph using Lemma 6.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "There are several other important consequences of the Pitman-Koopman Theorem orrecursive arc reversal that should not go unnoticed.Comment 4.1 If x; y are discrete and nite valued, then the distribution p(xjy; ) can berepresented as a member of the exponential family."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 58
                            }
                        ],
                        "text": "The common use ofthe exponential family exists because of Theorem 4.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 61
                            }
                        ],
                        "text": "Markov random elds are used in imaging and spatial reasoning (Ripley, 1981; Geman & Geman, 1984; Besag et al., 1991) and various stochastic models in neural networks (Hertz, Krogh, & Palmer, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 114
                            }
                        ],
                        "text": "The general form of this theorem for nite discrete domains is called the Hammersley-Cli ord Theorem (Geman, 1990; Besag et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 75
                            }
                        ],
                        "text": "The following is a simple graphical reinterpretation of the Pitman-Koopman Theoremfrom statistics (Je reys, 1961; DeGroot, 1970)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 99
                            }
                        ],
                        "text": "The general form of this theorem for nite discrete domains is called the HammersleyCli ord Theorem (Geman, 1990; Besag et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "Markovrandom elds are used in imaging and spatial reasoning (Ripley, 1981; Geman & Geman,1984; Besag et al., 1991) and various stochastic models in neural networks (Hertz, Krogh,& Palmer, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "A variant of the theorem is given later in Theorem 2.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 125
                            }
                        ],
                        "text": "In this case, T (x ; y ) is a su cient statistic.181\nBuntine x y \u03b8\nN \u03b8T(x*,y*)Figure 18: The generalized graph for plate removalTheorem 4.1 (Recursive arc-reversal)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 288
                            }
                        ],
                        "text": "Noticethe bottom component cannot be further decomposed because the variable x1 is unknown. var2\nx1\n\u03b81\n\u03b82\nvar1\nN\nx2\n\u03b83\n\u03b84 x3\n\u03b85\n(a) (b)\nvar2\n\u03b81\n\u03b82\nvar1\nN\nvar2\nx1\nvar1\nN\nx2\n\u03b83 \u03b84\nN\nx2\nx3\n\u03b85Figure 25: The incremental decomposition of a modelIn some cases, the functions fi given in the Decomposition Theorem in Equation (23)have a clean interpretation: they are equal to the evidence for the subgraphs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 81
                            }
                        ],
                        "text": "The justi cation for this is the subject of Markovprocess theory (C inlar, 1975, Theorem 2.26)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Theorem 2.1 An undirected graph G is on variables in the set X."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Following Theorem 2.1, thegeneral form is: p(X jY ) = QC2Cliques(G0) fC(C)PX QC2Cliques(G0) fC(C) :The local partial derivative with respect to x becomes:@ log p(X jY )@lx = 0@ XC2Cliques(G0);x2C @ log fC(C)@lx 1A EX jY 0@ XC2Cliques(G0);x2C @ log fC(C)@lx 1A :(20)The di culty here is computing the expected value in the formula, which comes from thenormalizing constant."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 94
                            }
                        ],
                        "text": "The framework is important as an educational tool.213\nBuntineAppendix A. Proofs of Lemmas and TheoremsA.1 Proof of Theorem 2.1A useful property of independence is that A is independent of B given C if and only ifp(A;B;C) = f(A;C)g(B;C) for some functions f and g."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 90
                            }
                        ],
                        "text": "Forthis task there is the following theorem, depicted graphically in Figure 25.195\nBuntineTheorem 6.1 (Decomposition)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "For a graph G in the context of Theorem 6.1,we have two non-deterministic variables U and V such that U is given."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 122
                            }
                        ],
                        "text": "Denote by MS0 the maximal subgraph on knownvariables only (induced by cliques0 as given in the proof of the Decomposition Theorem)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13799826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e4ef9da13090e66d28019b64a886041cb0428e5",
            "isKey": true,
            "numCitedBy": 3463,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been much recent interest in Bayesian image analysis, including such topics as removal of blur and noise, detection of object boundaries, classification of textures, and reconstruction of two- or three-dimensional scenes from noisy lower-dimensional views. Perhaps the most straightforward task is that of image restoration, though it is often suggested that this is an area of relatively minor practical importance. The present paper argues the contrary, since many problems in the analysis of spatial data can be interpreted as problems of image restoration. Furthermore, the amounts of data involved allow routine use of computer intensive methods, such as the Gibbs sampler, that are not yet practicable for conventional images. Two examples are given, one in archeology, the other in epidemiology. These are preceded by a partial review of pixel-based Bayesian image analysis."
            },
            "slug": "Bayesian-image-restoration,-with-two-applications-Besag-York",
            "title": {
                "fragments": [],
                "text": "Bayesian image restoration, with two applications in spatial statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The present paper argues that many problems in the analysis of spatial data can be interpreted as problems of image restoration, since the amounts of data involved allow routine use of computer intensive methods, such as the Gibbs sampler, that are not yet practicable for conventional images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238498"
                        ],
                        "name": "B. N. Larsen",
                        "slug": "B.-N.-Larsen",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Larsen",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. N. Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "74208271"
                        ],
                        "name": "H.-G. Leimer",
                        "slug": "H.-G.-Leimer",
                        "structuredName": {
                            "firstName": "H.-G.",
                            "lastName": "Leimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H.-G. Leimer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": "These results are simple applications of known methods fortesting independence (Frydenberg, 1990; Lauritzen et al., 1990), with some added compli-cation because of the use of plates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 91
                            }
                        ],
                        "text": "Undirected graphs are also important because they simplify the theoryof Bayesian networks (Lauritzen et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20450895,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f78884604e3fb89b1fb24d2a9403191dc9e63bd3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate directed Markov fields over finite graphs without positivity assumptions on the densities involved. A criterion for conditional independence of two groups of variables given a third is given and named as the directed, global Markov property. We give a simple proof of the fact that the directed, local Markov property and directed, global Markov property are equivalent and \u2013 in the case of absolute continuity w. r. t. a product measure \u2013 equivalent to the recursive factorization of densities. It is argued that our criterion is easy to use, it is sharper than that given by Kiiveri, Speed, and Carlin and equivalent to that of Pearl. It follows that our criterion cannot be sharpened."
            },
            "slug": "Independence-properties-of-directed-markov-fields-Lauritzen-Dawid",
            "title": {
                "fragments": [],
                "text": "Independence properties of directed markov fields"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A criterion for conditional independence of two groups of variables given a third is given and named as the directed, global Markov property and it is argued that this criterion is easy to use, it is sharper than that given by Kiiveri, Speed, and Carlin and equivalent to that of Pearl."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "This distinction is sometimes referred to as the diagnostic versus the discriminant approach to classi cation (Dawid, 1976)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 109
                            }
                        ],
                        "text": "This distinction issometimes referred to as the diagnostic versus the discriminant approach to classi cation(Dawid, 1976)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46644233,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "fc34b14dc7c8170433e7f0f502e78b5a670f3bb0",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In applications of statistical methods to medical diagnosis, information on patients' diseases and symptoms is collected and the resulting data-base is used to diagnose new patients. The data-structure is complicated by a number of factors, two of which are examined here: selection bias and unstable population. Under reasonable conditions, no correction for selection bias is required when assessing probabilities for diseases based on symptom information, and it is suggested that these \"diagnostic distributions\" should form the principal object of study. Transformation of these distributions under changing population structure is considered and shown to take on a simple form in many situations. It is argued that the prevailing paradigm of diagnostic statistics, which concentrates on incidence of symptoms for given disease, is largely inappropriate and should be replaced by an emphasis on diagnostic distributions. The generalized logistic model is seen to fit naturally into the new framework."
            },
            "slug": "Properties-of-diagnostic-data-distributions.-Dawid",
            "title": {
                "fragments": [],
                "text": "Properties of diagnostic data distributions."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that the prevailing paradigm of diagnostic statistics, which concentrates on incidence of symptoms for given disease, is largely inappropriate and should be replaced by an emphasis on diagnostic distributions, and the generalized logistic model is seen to fit naturally into the new framework."
            },
            "venue": {
                "fragments": [],
                "text": "Biometrics"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17279285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61039fd2773a00e111d2121a63982a7b7d0b9f92",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statistics. This introduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to Quinlan's information gain, while smoothing and averaging replace pruning. Comparative experiments with reimplementations of a minimum encoding approach,c4 (Quinlanet al., 1987) andcart (Breimanet al., 1984), show that the full Bayesian algorithm can produce more accurate predictions than versions of these other approaches, though pays a computational price."
            },
            "slug": "Learning-classification-trees-Buntine",
            "title": {
                "fragments": [],
                "text": "Learning classification trees"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces Bayesian techniques for splitting, smoothing, and tree averaging, which are similar to Quinlan's information gain, while smoothing and averaging replace pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 73
                            }
                        ],
                        "text": "The setting of priors for feed-forwardnetworks is di cult (MacKay, 1993; Nowlan & Hinton, 1992; Wolpert, 1994), and it willnot be considered here other than assuming a prior is used, p(w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5597033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de75e4e15e22d4376300e5c968e2db44be29ac9e",
            "isKey": false,
            "numCitedBy": 644,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms."
            },
            "slug": "Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton",
            "title": {
                "fragments": [],
                "text": "Simplifying Neural Networks by Soft Weight-Sharing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more complicated penalty term is proposed in which the distribution of weight values is modeled as a mixture of multiple gaussians, which allows the parameters of the mixture model to adapt at the same time as the network learns."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026of a learning toolkit is notnew, and can be seen in the BUGS system by Thomas, Spiegelhalter, and Gilks (1992)(Gilkset al., 1993b), in the work of Cohen (1992) for inductive logic programming, and emergingin software for handling generalized linear models (McCullagh & Nelder, 1989;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57308982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90ed32fa521b9e85f1c9efe356619814a2e79961",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Compiling-prior-knowledge-into-an-explicit-basis-Cohen",
            "title": {
                "fragments": [],
                "text": "Compiling prior knowledge into an explicit basis"
            },
            "venue": {
                "fragments": [],
                "text": "ICML 1992"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "Class probability trees(Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1992), graphs and rules (Rivest,1987; Oliver, 1993; Kohavi, 1994), and feed-forward networks are representations devised toexpress conditional models in di erent ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "The setting of priors for feed-forwardnetworks is di cult (MacKay, 1993; Nowlan & Hinton, 1992; Wolpert, 1994), and it willnot be considered here other than assuming a prior is used, p(w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 59
                            }
                        ],
                        "text": "The setting of priors for feed-forward networks is di cult (MacKay, 1993; Nowlan & Hinton, 1992; Wolpert, 1994), and it will not be considered here other than assuming a prior is used, p(w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3028112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c780a8e49c7529040541a1ac6e449697d36b3136",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The conventional Bayesian justification for backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead, one must add a correction term to backprop. That term biases one towards i-o functions with small description lengths, and in particular favors (some kinds of) feature-selection, pruning, and weight-sharing. This can be viewed as an {\\it a priori} argument in favor of those techniques."
            },
            "slug": "Bayesian-Backpropagation-Over-I-O-Functions-Rather-Wolpert",
            "title": {
                "fragments": [],
                "text": "Bayesian Backpropagation Over I-O Functions Rather Than Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "The conventional Bayesian justification for backprop is that it finds the MAP weight vector, but to find the MAP i-o function instead, one must add a correction term to backprop, which biases one towards i-O functions with small description lengths."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39416713"
                        ],
                        "name": "M. Degroot",
                        "slug": "M.-Degroot",
                        "structuredName": {
                            "firstName": "Morris",
                            "lastName": "Degroot",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Degroot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 111
                            }
                        ],
                        "text": "Further details and more extensive tables can be found in most Bayesian textbooks on probability distributions (DeGroot, 1970; Bernardo & Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 76
                            }
                        ],
                        "text": "Further details can be found in most textbooks on probability distributions (DeGroot, 1970; Bernardo & Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "More extensivesummaries of this are given by DeGroot (1970) and Bernardo and Smith (1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 114
                            }
                        ],
                        "text": "The following is a simple graphical reinterpretation of the Pitman-Koopman Theoremfrom statistics (Je reys, 1961; DeGroot, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "Table 3 gives some standard conjugate prior distributions for those inTable 2, and Table 4 gives their matching posteriors (DeGroot, 1970; Bernardo & Smith,216\nLearning with Graphical Models Distribution Conjugate priorj C-dim multinomial Dirichlet( 1; : : : ; C)yjx Gaussian j d-dim Gaussian( 0; 1\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 282
                            }
                        ],
                        "text": "\u2026fora large class of distributions that allows some averages to be calculated by symbolicmanipulation of the normalizing constant.4.3 The exponential familyThis result generalizes to a much larger class of distributions referred to as the exponentialfamily (Casella & Berger, 1990; DeGroot, 1970)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 78
                            }
                        ],
                        "text": "Fur-ther details can be found in most textbooks on probability distributions (DeGroot, 1970;Bernardo & Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 127
                            }
                        ],
                        "text": "3 The exponential family This result generalizes to a much larger class of distributions referred to as the exponential family (Casella & Berger, 1990; DeGroot, 1970)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 111
                            }
                        ],
                        "text": "Further details and more extensive tables can befound in most Bayesian textbooks on probability distributions (DeGroot, 1970; Bernardo& Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 99
                            }
                        ],
                        "text": "The following is a simple graphical reinterpretation of the Pitman-Koopman Theorem from statistics (Je reys, 1961; DeGroot, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119884967,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81c8a5823de21d98ea395081cbfe647bfb456cd6",
            "isKey": true,
            "numCitedBy": 4235,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword.Preface.PART ONE. SURVEY OF PROBABILITY THEORY.Chapter 1. Introduction.Chapter 2. Experiments, Sample Spaces, and Probability.2.1 Experiments and Sample Spaces.2.2 Set Theory.2.3 Events and Probability.2.4 Conditional Probability.2.5 Binomial Coefficients.Exercises.Chapter 3. Random Variables, Random Vectors, and Distributions Functions.3.1 Random Variables and Their Distributions.3.2 Multivariate Distributions.3.3 Sums and Integrals.3.4 Marginal Distributions and Independence.3.5 Vectors and Matrices.3.6 Expectations, Moments, and Characteristic Functions.3.7 Transformations of Random Variables.3.8 Conditional Distributions.Exercises.Chapter 4. Some Special Univariate Distributions.4.1 Introduction.4.2 The Bernoulli Distributions.4.3 The Binomial Distribution.4.4 The Poisson Distribution.4.5 The Negative Binomial Distribution.4.6 The Hypergeometric Distribution.4.7 The Normal Distribution.4.8 The Gamma Distribution.4.9 The Beta Distribution.4.10 The Uniform Distribution.4.11 The Pareto Distribution.4.12 The t Distribution.4.13 The F Distribution.Exercises.Chapter 5. Some Special Multivariate Distributions.5.1 Introduction.5.2 The Multinomial Distribution.5.3 The Dirichlet Distribution.5.4 The Multivariate Normal Distribution.5.5 The Wishart Distribution.5.6 The Multivariate t Distribution.5.7 The Bilateral Bivariate Pareto Distribution.Exercises.PART TWO. SUBJECTIVE PROBABILITY AND UTILITY.Chapter 6. Subjective Probability.6.1 Introduction.6.2 Relative Likelihood.6.3 The Auxiliary Experiment.6.4 Construction of the Probability Distribution.6.5 Verification of the Properties of a Probability Distribution.6.6 Conditional Likelihoods.Exercises.Chapter 7. Utility.7.1 Preferences Among Rewards.7.2 Preferences Among Probability Distributions.7.3 The Definitions of a Utility Function.7.4 Some Properties of Utility Functions.7.5 The Utility of Monetary Rewards.7.6 Convex and Concave Utility Functions.7.7 The Anxiomatic Development of Utility.7.8 Construction of the Utility Function.7.9 Verification of the Properties of a Utility Function.7.10 Extension of the Properties of a Utility Function to the Class ?E.Exercises.PART THREE. STATISTICAL DECISION PROBLEMS.Chapter 8. Decision Problems.8.1 Elements of a Decision Problem.8.2 Bayes Risk and Bayes Decisions.8.3 Nonnegative Loss Functions.8.4 Concavity of the Bayes Risk.8.5 Randomization and Mixed Decisions.8.6 Convex Sets.8.7 Decision Problems in Which ~2 and D Are Finite.8.8 Decision Problems with Observations.8.9 Construction of Bayes Decision Functions.8.10 The Cost of Observation.8.11 Statistical Decision Problems in Which Both ? and D contains Two Points.8.12 Computation of the Posterior Distribution When the Observations Are Made in More Than One Stage.Exercises.Chapter 9. Conjugate Prior Distributions.9.1 Sufficient Statistics.9.2 Conjugate Families of Distributions.9.3 Construction of the Conjugate Family.9.4 Conjugate Families for Samples from Various Standard Distributions.9.5 Conjugate Families for Samples from a Normal Distribution.9.6 Sampling from a Normal Distribution with Unknown Mean and Unknown Precision.9.7 Sampling from a Uniform Distribution.9.8 A Conjugate Family for Multinomial Observations.9.9 Conjugate Families for Samples from a Multivariate Normal Distribution.9.10 Multivariate Normal Distributions with Unknown Mean Vector and Unknown Precision matrix.9.11 The Marginal Distribution of the Mean Vector.9.12 The Distribution of a Correlation.9.13 Precision Matrices Having an Unknown Factor.Exercises.Chapter 10. Limiting Posterior Distributions.10.1 Improper Prior Distributions.10.2 Improper Prior Distributions for Samples from a Normal Distribution.10.3 Improper Prior Distributions for Samples from a Multivariate Normal Distribution.10.4 Precise Measurement.10.5 Convergence of Posterior Distributions.10.6 Supercontinuity.10.7 Solutions of the Likelihood Equation.10.8 Convergence of Supercontinuous Functions.10.9 Limiting Properties of the Likelihood Function.10.10 Normal Approximation to the Posterior Distribution.10.11 Approximation for Vector Parameters.10.12 Posterior Ratios.Exercises.Chapter 11. Estimation, Testing Hypotheses, and linear Statistical Models.11.1 Estimation.11.2 Quadratic Loss.11.3 Loss Proportional to the Absolute Value of the Error.11.4 Estimation of a Vector.11.5 Problems of Testing Hypotheses.11.6 Testing a Simple Hypothesis About the Mean of a Normal Distribution.11.7 Testing Hypotheses about the Mean of a Normal Distribution.11.8 Deciding Whether a Parameter Is Smaller or larger Than a Specific Value.11.9 Deciding Whether the Mean of a Normal Distribution Is Smaller or larger Than a Specific Value.11.10 Linear Models.11.11 Testing Hypotheses in Linear Models.11.12 Investigating the Hypothesis That Certain Regression Coefficients Vanish.11.13 One-Way Analysis of Variance.Exercises.PART FOUR. SEQUENTIAL DECISIONS.Chapter 12. Sequential Sampling.12.1 Gains from Sequential Sampling.12.2 Sequential Decision Procedures.12.3 The Risk of a Sequential Decision Procedure.12.4 Backward Induction.12.5 Optimal Bounded Sequential Decision procedures.12.6 Illustrative Examples.12.7 Unbounded Sequential Decision Procedures.12.8 Regular Sequential Decision Procedures.12.9 Existence of an Optimal Procedure.12.10 Approximating an Optimal Procedure by Bounded Procedures.12.11 Regions for Continuing or Terminating Sampling.12.12 The Functional Equation.12.13 Approximations and Bounds for the Bayes Risk.12.14 The Sequential Probability-ratio Test.12.15 Characteristics of Sequential Probability-ratio Tests.12.16 Approximating the Expected Number of Observations.Exercises.Chapter 13. Optimal Stopping.13.1 Introduction.13.2 The Statistician's Reward.13.3 Choice of the Utility Function.13.4 Sampling Without Recall.13.5 Further Problems of Sampling with Recall and Sampling without Recall.13.6 Sampling without Recall from a Normal Distribution with Unknown Mean.13.7 Sampling with Recall from a Normal Distribution with Unknown Mean.13.8 Existence of Optimal Stopping Rules.13.9 Existence of Optimal Stopping Rules for Problems of Sampling with Recall and Sampling without Recall.13.10 Martingales.13.11 Stopping Rules for Martingales.13.12 Uniformly Integrable Sequences of Random Variables.13.13 Martingales Formed from Sums and Products of Random Variables.13.14 Regular Supermartingales.13.15 Supermartingales and General Problems of Optimal Stopping.13.16 Markov Processes.13.17 Stationary Stopping Rules for Markov Processes.13.18 Entrance-fee Problems.13.19 The Functional Equation for a Markov Process.Exercises.Chapter 14. Sequential Choice of Experiments.14.1 Introduction.14.2 Markovian Decision Processes with a Finite Number of Stages.14.3 Markovian Decision Processes with an Infinite Number of Stages.14.4 Some Betting Problems.14.5 Two-armed-bandit Problems.14.6 Two-armed-bandit Problems When the Value of One Parameter Is Known.14.7 Two-armed-bandit Problems When the Parameters Are Dependent.14.8 Inventory Problems.14.9 Inventory Problems with an Infinite Number of Stages.14.10 Control Problems.14.11 Optimal Control When the Process Cannot Be Observed without Error.14.12 Multidimensional Control Problems.14.13 Control Problems with Actuation Errors.14.14 Search Problems.14.15 Search Problems with Equal Costs.14.16 Uncertainty Functions and Statistical Decision Problems.14.17 Sufficient Experiments.14.18 Examples of Sufficient Experiments.Exercises.References.Supplementary Bibliography.Name Index.Subject Index."
            },
            "slug": "Optimal-Statistical-Decisions-Degroot",
            "title": {
                "fragments": [],
                "text": "Optimal Statistical Decisions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "Indeed, this computation forms the core of the early Boltzmannmachine algorithm (Hertz et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 279
                            }
                        ],
                        "text": "\u2026because N is large, theposterior variance of the i+ 1-th sample given the i-th sample would be small so that: i+1 E i; 1;i ; 2;i ( i+1) :This approximation is used with Markov chain Monte Carlo methods by the mean eldmethod from statistical physics, popular in neural networks (Hertz et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "The second approach to performing inference isapproximate and corresponds to approximate algorithms such as Gibbs sampling, and otherMarkov chain Monte Carlo methods (Hrycej, 1990; Hertz et al., 1991; Neal, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": false,
            "numCitedBy": 6517,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012264"
                        ],
                        "name": "Jonathan J. Oliver",
                        "slug": "Jonathan-J.-Oliver",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Oliver",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan J. Oliver"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 100
                            }
                        ],
                        "text": "Class probability trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1992), graphs and rules (Rivest, 1987; Oliver, 1993; Kohavi, 1994), and feed-forward networks are representations devised to express conditional models in di erent ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "Class probability trees(Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1992), graphs and rules (Rivest,1987; Oliver, 1993; Kohavi, 1994), and feed-forward networks are representations devised toexpress conditional models in di erent ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16194622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73f1d17df0e1232da9e2331878a802a941f351c6",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain."
            },
            "slug": "Decision-Graphs-An-Extension-of-Decision-Trees-Oliver",
            "title": {
                "fragments": [],
                "text": "Decision Graphs - An Extension of Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "An inference scheme to construct decision graphs using the Minimum Message Length Principle is presented, providing a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308823"
                        ],
                        "name": "Matthew Self",
                        "slug": "Matthew-Self",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Self",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Self"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113380562"
                        ],
                        "name": "James Kelly",
                        "slug": "James-Kelly",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114766082"
                        ],
                        "name": "Will Taylor",
                        "slug": "Will-Taylor",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059655854"
                        ],
                        "name": "Don Freeman",
                        "slug": "Don-Freeman",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Freeman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2951993,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d88ffb6c9a6a6a834da4704224b5663a3a5cc430",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a Bayesian technique for unsupervised classification of data and its computer implementation, AutoClass. Given real valued or discrete data, AutoClass determines the most probable number of classes present in the data, the most probable descriptions of those classes, and each object's probability of membership in each class. The program performs as well as or better than other automatic classification systems when run on the same data and contains no ad hoc similarity measures or stopping criteria. AutoClass has been applied to several databases in which it has discovered classes representing previously unsuspected phenomena."
            },
            "slug": "Bayesian-Classification-Cheeseman-Self",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A Bayesian technique for unsupervised classification of data and its computer implementation, AutoClass, which performs as well as or better than other automatic classification systems when run on the same data and contains no ad hoc similarity measures or stopping criteria."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150444582"
                        ],
                        "name": "David S. Johnson",
                        "slug": "David-S.-Johnson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Johnson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748179"
                        ],
                        "name": "M. Yannakakis",
                        "slug": "M.-Yannakakis",
                        "structuredName": {
                            "firstName": "Mihalis",
                            "lastName": "Yannakakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yannakakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 185
                            }
                        ],
                        "text": "Gradient ascent inreal valued problems corresponds to simple methods from function optimization (Gill et al.,1981) and in discrete problems corresponds to local repair or local search (Johnson et al.,1985; Minton, Johnson, Philips, & Laird, 1990; Selman, Levesque, & Mitchell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 77
                            }
                        ],
                        "text": ", 1981) and in discrete problems corresponds to local repair or local search (Johnson et al., 1985; Minton, Johnson, Philips, & Laird, 1990; Selman, Levesque, & Mitchell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7353225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72d54ba4fed488a31022b6f37333550e041e93bc",
            "isKey": false,
            "numCitedBy": 704,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-easy-is-local-search-Johnson-Papadimitriou",
            "title": {
                "fragments": [],
                "text": "How easy is local search?"
            },
            "venue": {
                "fragments": [],
                "text": "26th Annual Symposium on Foundations of Computer Science (sfcs 1985)"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634377"
                        ],
                        "name": "H. Levesque",
                        "slug": "H.-Levesque",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Levesque",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Levesque"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50066285"
                        ],
                        "name": "D. Mitchell",
                        "slug": "D.-Mitchell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8540521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f8729c0cfc09dae60170b83e6112c19bde7c625",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed. \n \nGSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks."
            },
            "slug": "A-New-Method-for-Solving-Hard-Satisfiability-Selman-Levesque",
            "title": {
                "fragments": [],
                "text": "A New Method for Solving Hard Satisfiability Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A greedy local search procedure called GSAT is introduced for solving propositional satisfiability problems and its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37994193"
                        ],
                        "name": "M. Johnston",
                        "slug": "M.-Johnston",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnston",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Johnston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2648787"
                        ],
                        "name": "Andrew B. Philips",
                        "slug": "Andrew-B.-Philips",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Philips",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew B. Philips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29961301"
                        ],
                        "name": "P. Laird",
                        "slug": "P.-Laird",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Laird",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Laird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34818734,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "ed1483929ee8d0033bc61521de87ac90c818326b",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a simple heuristic method for solving large-scale constraint satisfaction and scheduling problems. Given an initial assignment for the variables in a problem, the method operates by searching though the space of possible repairs. The search is guided by an ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. We demonstrate empirically that the method performs orders of magnitude better than traditional backtracking techniques on certain standard problems. For example, the one million queens problem can be solved rapidly using our approach. We also describe practical scheduling applications where the method has been successfully applied. A theoretical analysis is presented to explain why the method works so well on certain types of problems and to predict when it is likely to be most effective."
            },
            "slug": "Solving-Large-Scale-Constraint-Satisfaction-and-a-Minton-Johnston",
            "title": {
                "fragments": [],
                "text": "Solving Large-Scale Constraint-Satisfaction and Scheduling Problems Using a Heuristic Repair Method"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A theoretical analysis is presented to explain why the heuristic method for solving large-scale constraint satisfaction and scheduling problems works so well on certain types of problems and to predict when it is likely to be most effective."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 185
                            }
                        ],
                        "text": "Incomplete data (or missing values)(Quinlan, 1989), robust methods and modeling of outliers, and various density estimationand non-parametric methods all fall in this family of models (Titterington et al., 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326202"
                        ],
                        "name": "J. Chambers",
                        "slug": "J.-Chambers",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Chambers",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chambers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121361526,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "9eb2ee5d07cfad09ce48e71b0a5b68a8feacf9c7",
            "isKey": false,
            "numCitedBy": 2351,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The interactive data analysis and graphics language S (Becker, Chambers and Wilks, 1988) has become a popular environment for both data analysts and research statisticians. A common complaint, however, has concerned the lack of statistical modeling tools, such as those provided by GLIM\u00a9 or GENSTAT\u00a9."
            },
            "slug": "Statistical-Models-in-S-Chambers-Hastie",
            "title": {
                "fragments": [],
                "text": "Statistical Models in S"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The interactive data analysis and graphics language S has become a popular environment for both data analysts and research statisticians, but a common complaint has concerned the lack of statistical modeling tools, such as those provided by GLIM\u00a9 or GENSTAT\u00a9."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "This graph matches the so-called\\idiot's\" Bayes classi er (Duda & Hart, 1973; Langley, Iba, & Thompson, 1992) used insupervised learning for its speed and simplicity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799422"
                        ],
                        "name": "P. V. Laarhoven",
                        "slug": "P.-V.-Laarhoven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Laarhoven",
                            "middleNames": [
                                "J.",
                                "M.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. V. Laarhoven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796770"
                        ],
                        "name": "E. Aarts",
                        "slug": "E.-Aarts",
                        "structuredName": {
                            "firstName": "Emile",
                            "lastName": "Aarts",
                            "middleNames": [
                                "H.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Aarts"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "Gibbs sampling is also the core algorithm ofsimulated annealing if temperature is held equal to one (van Laarhoven & Aarts, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61815519,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "29b4cdd84e5b657339749c12028ec25310c79764",
            "isKey": false,
            "numCitedBy": 3594,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction.- 2 Simulated annealing.- 3 Asymptotic convergence results.- 4 The relation with statistical physics.- 5 Towards implementing the algorithm.- 6 Performance of the simulated annealing algorithm.- 7 Applications.- 8 Some miscellaneous topics.- 9 Summary and conclusions."
            },
            "slug": "Simulated-Annealing:-Theory-and-Applications-Laarhoven-Aarts",
            "title": {
                "fragments": [],
                "text": "Simulated Annealing: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Performance of the simulated annealing algorithm and the relation with statistical physics and asymptotic convergence results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics and Its Applications"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40220710"
                        ],
                        "name": "S. Srinivas",
                        "slug": "S.-Srinivas",
                        "structuredName": {
                            "firstName": "Sampath",
                            "lastName": "Srinivas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srinivas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778725"
                        ],
                        "name": "J. Breese",
                        "slug": "J.-Breese",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Breese",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Breese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 66
                            }
                        ],
                        "text": "Flexible toolkits and systemsexist for applying these techniques (Srinivas & Breese, 1990; Andersen, Olesen, Jensen, &Jensen, 1989; Cowell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6312194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7565eb729b5c5104cd16233b27fb80c680e885e4",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software environment for creation and evaluation of belief networks and influence diagrams. IDEAL is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework. This paper describes IDEAL and some lessons learned during its development."
            },
            "slug": "IDEAL:-A-Software-Package-for-Analysis-of-Influence-Srinivas-Breese",
            "title": {
                "fragments": [],
                "text": "IDEAL: A Software Package for Analysis of Influence Diagrams"
            },
            "venue": {
                "fragments": [],
                "text": "UAI 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "Gradient ascent inreal valued problems corresponds to simple methods from function optimization (Gill et al.,1981) and in discrete problems corresponds to local repair or local search (Johnson et al.,1985; Minton, Johnson, Philips, & Laird, 1990; Selman, Levesque, & Mitchell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 100
                            }
                        ],
                        "text": "Class probability trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1992), graphs and rules (Rivest, 1987; Oliver, 1993; Kohavi, 1994), and feed-forward networks are representations devised to express conditional models in di erent ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "Class probability trees(Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1992), graphs and rules (Rivest,1987; Oliver, 1993; Kohavi, 1994), and feed-forward networks are representations devised toexpress conditional models in di erent ways."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14601640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e01562239d28f182cfc53465f6cfa1a736101ea",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We report improvements to HOODG, a supervised learning algorithm that induces concepts from labelled instances using oblivious, read-once decision graphs as the underlying hypothesis representation structure. While it is shown that the greedy approach to variable ordering is locally optimal, we also show an inherent limitation of all bottom-up induction algorithms, including HOODG, that construct such decision graphs bottom-up by minimizing the width of levels in the resulting graph. We report our empirical experiments that demonstrate the algorithm's generalization power."
            },
            "slug": "Bottom-Up-Induction-of-Oblivious-Read-Once-Decision-Kohavi",
            "title": {
                "fragments": [],
                "text": "Bottom-Up Induction of Oblivious Read-Once Decision Graphs: Strengths and Limitations"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Improvements to HOODG, a supervised learning algorithm that induces concepts from labelled instances using oblivious, read-once decision graphs as the underlying hypothesis representation structure, are reported."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 58
                            }
                        ],
                        "text": "The general method is summarized in the followingcomment (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "When the mode is used in Step 2(b), ignoring numericalproblems, the EM algorithm converges on a local maxima of the posterior distribution forthe parameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 273
                            }
                        ],
                        "text": "\u2026which will be discussedin Section 7.4.7.4 The expectation maximization (EM) algorithmThe expectation maximization algorithm, widely known as the EM algorithm, correspondsto a deterministic version of Gibbs sampling used to search for the MAP estimate for modelparameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": true,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144196547"
                        ],
                        "name": "D. Clayton",
                        "slug": "D.-Clayton",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Clayton",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Clayton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917749"
                        ],
                        "name": "N. Best",
                        "slug": "N.-Best",
                        "structuredName": {
                            "firstName": "Nicky",
                            "lastName": "Best",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Best"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032189"
                        ],
                        "name": "A. McNeil",
                        "slug": "A.-McNeil",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "McNeil",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McNeil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5139526"
                        ],
                        "name": "L. Sharples",
                        "slug": "L.-Sharples",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Sharples",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sharples"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47643845"
                        ],
                        "name": "A. Kirby",
                        "slug": "A.-Kirby",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Kirby",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kirby"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 98
                            }
                        ],
                        "text": "A version of this toolkit alreadyexists using Gibbs sampling as the general computational scheme (Gilks et al., 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 40
                            }
                        ],
                        "text": "Thomas, Spiegelhalter, and Gilks (1992)(Gilks et al., 1993b) have taken advantage ofthis general applicability of sampling to create a compiler that converts a graphical represen-tation of a data analysis problem, with plates, into a matching Gibbs sampling algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Two standard algorithm schemas forlearning are reviewed in a graphical framework: Gibbs sampling and the expectation max-imization algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 95
                            }
                        ],
                        "text": "It follows that Gibbs sampling is readily applied to learning as ageneral inference algorithm (Gilks et al., 1993a, 1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 183
                            }
                        ],
                        "text": "\u2026main families of these approximate methods.7.1 Gibbs samplingGibbs sampling is the basic tool of simulation and can be applied to most probabilitydistributions (Geman & Geman, 1984; Gilks et al., 1993a; Ripley, 1987) as long as the fulljoint has no zeros (all variable instantiations are possible)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 245
                            }
                        ],
                        "text": "\u2026diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et al., 1993a) and statistics (Whittaker, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59756707,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "id": "556dc057ade0fac6f0ce7f149040dcd4d6940635",
            "isKey": true,
            "numCitedBy": 202,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We review applications of Gibbs sampling in medicine, involving longitudinal, spatial, covariate measurement and survival models. Applications in immunology, pharmacology, transplantation, cancer screening, industrial epidemiology and genetic epidemiology are discussed."
            },
            "slug": "Modelling-Complexity:-Applications-of-Gibbs-in-Gilks-Clayton",
            "title": {
                "fragments": [],
                "text": "Modelling Complexity: Applications of Gibbs Sampling in Medicine"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "Applications of Gibbs sampling in medicine, involving longitudinal, spatial, covariate measurement and survival models, are reviewed, including applications in immunology, pharmacology, transplantation, cancer screening, industrial epidemiology and genetic epidemiology."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 291
                            }
                        ],
                        "text": "The generalform for this equation for a set of variables X is:p(X jM) = Yx2X p(xjparents(x);M) : (2)This equation is the interpretation of a Bayesian network used in this paper.2.2 Undirected graphical modelsAnother popular form of graphical model is an undirected graph, sometimes called a Markovnetwork (Pearl, 1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 287
                            }
                        ],
                        "text": "\u2026this equation for a set of variables X is:p(X jM) = Yx2X p(xjparents(x);M) : (2)This equation is the interpretation of a Bayesian network used in this paper.2.2 Undirected graphical modelsAnother popular form of graphical model is an undirected graph, sometimes called a Markovnetwork (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 128
                            }
                        ],
                        "text": "2 Undirected graphical models Another popular form of graphical model is an undirected graph, sometimes called a Markov network (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "It also requires a notion oflocal dependence, which is called the Markov blanket, following Pearl (1988), since it is ageneralization of the equivalent set for Bayesian networks.De nition 6.2 We have a chain graph G without plates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 55
                            }
                        ],
                        "text": "See Charniak (1991), Shachter and Heckerman(1987), and Pearl (1988) for an introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 122
                            }
                        ],
                        "text": "Thus, the formula only involves examining the parents, children and children's parents of x, the so-called Markov blanket (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 283
                            }
                        ],
                        "text": "\u2026models are based on the notion of independence,which is worth repeating here.De nition 2.1 A is independent of B given C if p(A;BjC) = p(AjC)p(BjC) wheneverp(C) 6= 0, for all A;B;C.The theory of independence as a basic tool for knowledge structuring is developed by Dawid(1979) and Pearl (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "Thus, the formula only involves examining the parents,children and children's parents of x, the so-called Markov blanket (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57437891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bf6f01402e1648b7d1e6c9200ede6cb1af30123",
            "isKey": true,
            "numCitedBy": 4579,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "Incomplete data (or missing values) (Quinlan, 1989), robust methods and modeling of outliers, and various density estimation and non-parametric methods all fall in this family of models (Titterington et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "Incomplete data (or missing values)(Quinlan, 1989), robust methods and modeling of outliers, and various density estimationand non-parametric methods all fall in this family of models (Titterington et al., 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15696344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1d8ff72a970d03c37e776f1222051f3fd6c617c",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unknown-Attribute-Values-in-Induction-Quinlan",
            "title": {
                "fragments": [],
                "text": "Unknown Attribute Values in Induction"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "The general method is summarized in the following comment (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 58
                            }
                        ],
                        "text": "The general method is summarized in the followingcomment (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 273
                            }
                        ],
                        "text": "\u2026which will be discussedin Section 7.4.7.4 The expectation maximization (EM) algorithmThe expectation maximization algorithm, widely known as the EM algorithm, correspondsto a deterministic version of Gibbs sampling used to search for the MAP estimate for modelparameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "When the mode is used in Step 2(b), ignoring numericalproblems, the EM algorithm converges on a local maxima of the posterior distribution forthe parameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 234
                            }
                        ],
                        "text": "4 The expectation maximization (EM) algorithm The expectation maximization algorithm, widely known as the EM algorithm, corresponds to a deterministic version of Gibbs sampling used to search for the MAP estimate for model parameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 159
                            }
                        ],
                        "text": "When the mode is used in Step 2(b), ignoring numerical problems, the EM algorithm converges on a local maxima of the posterior distribution for the parameters (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60618317,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ecb37a4e32d6faef4ac99b45d9ab9b2d92693985",
            "isKey": true,
            "numCitedBy": 1169,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Max-imum-Likelihood-from-Incomplete-Data-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Max-imum Likelihood from Incomplete Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 78
                            }
                        ],
                        "text": "This is the basis of various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan & Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Heckerman, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 151
                            }
                        ],
                        "text": "This kind of computation is done for class probability trees where representative sets of trees are found using a heuristic branch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madigan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 128
                            }
                        ],
                        "text": "Learning with Graphical Models Exact Bayes factors: Model selection and averaging methods are used to deal with multiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 176
                            }
                        ],
                        "text": "It is used explicitly or implicitly in all Bayesian methods for learning decision trees, directed graphical models (with discrete or Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 227
                            }
                        ],
                        "text": "This includes standard undergraduate distributions such as Gaussians, Chi squared, and Gamma, and many more complex distributions constructed from simple components including class probability trees over discrete input domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian network (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning classi cation trees"
            },
            "venue": {
                "fragments": [],
                "text": "Hand, D. (Ed.), Arti cial Intelligence"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 78
                            }
                        ],
                        "text": "This is the basis of various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan & Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Heckerman, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 347,
                                "start": 304
                            }
                        ],
                        "text": "1 holds for MS j for j = 0; 1; : : : ; P , then it follows that the evidence for the model M is equal to the product of the evidence for each subgraph: evidence(M) = P Y i=0 evidence(MS i ) : (24) This holds in general if the original graph G is a Bayesian network, as used in learning Bayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory re nement of Bayesian networks"
            },
            "venue": {
                "fragments": [],
                "text": "D'Ambrosio, B., Smets,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning classiication trees Artiicial Intelligence Frontiers in Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Learning classiication trees Artiicial Intelligence Frontiers in Statistics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Prior probabilities for these models could be generated using a scheme such as in(Buntine, 1991c, p54) or (Heckerman et al., 1994), where a prior probability is assigned by197\nBuntinea domain expert for di erent parts of the model, arcs and parameters, and the prior for afull model found by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 256
                            }
                        ],
                        "text": "\u2026then it follows that theevidence for the model M is equal to the product of the evidence for each subgraph:evidence(M) = PYi=0 evidence(MSi ) : (24)This holds in general if the original graph G is a Bayesian network, as used in learningBayesian networks (Buntine, 1991c; Cooper & Herskovits, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "This kind of computation isdone for class probability trees where representative sets of trees are found using a heuristicbranch and bound algorithm (Buntine, 1991b), and for learning Bayesian networks (Madi-gan & Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "\u2026inall Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "This is the basisof various Bayesian algorithms developed for these problems (Buntine, 1991b; Madigan &Raftery, 1994; Buntine, 1991c; Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Hecker-man, Geiger, & Chickering, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "\u2026a manner requiring fewer parameters, as is Heckerman's similarity networks (1991).188\nLearning with Graphical ModelsExact Bayes factors: Model selection and averaging methods are used to deal withmultiple models (Kass & Raftery, 1993; Buntine, 1991b; Stewart, 1987; Madigan& Raftery, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 180
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "\u2026Chi squared, and Gamma, and many more complex distri-butions constructed from simple components including class probability trees over discreteinput domains (Buntine, 1991b), simple discrete and Gaussian versions of a Bayesian net-work (Whittaker, 1990), and linear regression with a Gaussian error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "A similar situation exists with trees (Buntine, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "\u2026algorithms|such as k-meansclustering versus approximate methods for learning hidden Markovmodels, learning decisiontrees versus learning Bayesian networks (Buntine, 1991a), and Gibbs sampling versus theexpectation maximization algorithm in Section 7.4|to be understood as variations of oneanother."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiiers: A theoretical and empirical study"
            },
            "venue": {
                "fragments": [],
                "text": "International Joint Conference on Artiicial Intelligence"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 136
                            }
                        ],
                        "text": "This makes data analysis problems explicit in much the same way that utility and decision nodes are used for decision analysis problems (Shachter, 1986)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 270
                            }
                        ],
                        "text": "For learning to be goal directed, additional information needs to be included in the graph: how is learned knowledge evaluated or how can subsequent performance be measured? This is the role of decision theory and it is modeled in graphical form using in uence diagrams (Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "This makes data analysisproblems explicit in much the same way that utility and decision nodes are used for decisionanalysis problems (Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "This is therole of decision theory and it is modeled in graphical form using in uence diagrams(Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 90
                            }
                        ],
                        "text": "The arc reversal operator interchanges the order of two nodes connected by a directed arc (Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "The arcreversal operator interchanges the order of two nodes connected by a directed arc (Shachter,1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating in uence diagrams"
            },
            "venue": {
                "fragments": [],
                "text": "Operations Research, 34 (6), 871{882."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 236
                            }
                        ],
                        "text": "3 Bayesian networks with missing variablesClass probability trees and discrete Bayesian networks can be learned e ciently by notic-ing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper &Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Bayesian methods for learning decision trees, directed graphical models (with discreteor Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993).198\nLearning with Graphical ModelsFor instance, if the normalizing constant Z ( ) in Lemma 4.1 was known in closed\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 176
                            }
                        ],
                        "text": "It is used explicitly or implicitly in all Bayesian methods for learning decision trees, directed graphical models (with discrete or Gaussian variables), and linear regression (Buntine, 1991b; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 179
                            }
                        ],
                        "text": "3 Bayesian networks with missing variables Class probability trees and discrete Bayesian networks can be learned e ciently by noticing that their basic form is exponential family (Buntine, 1991b, 1991a, 1991c; Cooper & Herskovits, 1992; Spiegelhalter et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian analysis in expert"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3266660"
                        ],
                        "name": "A. Rukhin",
                        "slug": "A.-Rukhin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rukhin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rukhin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "The basiccomputational techniques of probabilistic (Bayesian) inference used in this computationaltheory of learning are widely reviewed (Tanner, 1993; Press, 1989; Kass & Raftery, 1993;Neal, 1993; Bretthorst, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "Graphical operationsmanipulate the underlying structure of a problem unhindered by the ne detail of theconnecting functional and distributional equations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "Several of these are introduced in Section 7 andmore detail is given, for instance, by Tanner (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121255267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fa72680ca044a7432b9f553c226300898103b2b",
            "isKey": true,
            "numCitedBy": 838,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tools-for-statistical-inference-Rukhin",
            "title": {
                "fragments": [],
                "text": "Tools for statistical inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 267
                            }
                        ],
                        "text": "\u2026models are based on the notion of independence,which is worth repeating here.De nition 2.1 A is independent of B given C if p(A;BjC) = p(AjC)p(BjC) wheneverp(C) 6= 0, for all A;B;C.The theory of independence as a basic tool for knowledge structuring is developed by Dawid(1979) and Pearl (1988)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115903686,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7c0ab0348620413211135f9cdf1f855809597131",
            "isKey": true,
            "numCitedBy": 1388,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conditional-Independence-in-Statistical-Theory-Dawid",
            "title": {
                "fragments": [],
                "text": "Conditional Independence in Statistical Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 70
                            }
                        ],
                        "text": "Another related form that appliesto undirected graphs is developed by Dawid and Lauritzen (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "Similar results are covered by Dawid and Lauritzen (1993)for a family of models they call hyper-Markov."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 53
                            }
                        ],
                        "text": "A similar property forundirected graphs is given in (Dawid & Lauritzen, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121850609,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a79313b40c75124d20ba645516823e332303e3c2",
            "isKey": true,
            "numCitedBy": 530,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hyper-Markov-Laws-in-the-Statistical-Analysis-of-Dawid-Lauritzen",
            "title": {
                "fragments": [],
                "text": "Hyper Markov Laws in the Statistical Analysis of Decomposable Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "The arcreversal operator interchanges the order of two nodes connected by a directed arc (Shachter,1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "This is therole of decision theory and it is modeled in graphical form using in uence diagrams(Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "Graphical models provide a representation for the decomposition of complex problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "This makes data analysisproblems explicit in much the same way that utility and decision nodes are used for decisionanalysis problems (Shachter, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating innuence diagrams An ordered examination of innuence diagrams. Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Operations Research"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 143
                            }
                        ],
                        "text": "Many more sophisticated variations and combinations of these algorithms exist in the literature, including the handling of deterministic nodes (Shachter, 1990) and chain graphs and undirected graphs (Frydenberg, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 144
                            }
                        ],
                        "text": "Many moresophisticated variations and combinations of these algorithms exist in the literature, includ-ing the handling of deterministic nodes (Shachter, 1990) and chain graphs and undirectedgraphs (Frydenberg, 1990).4.1.1 Arc reversalTwo basic steps for inference are to marginalize nuisance\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An ordered examination of in uence diagrams"
            },
            "venue": {
                "fragments": [],
                "text": "Networks, 20, 535{563."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": "The pa-rameters for these priors can be set using standard reference priors (Box & Tiao, 1973;Bernardo & Smith, 1994) or elicited from a domain expert."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 64
                            }
                        ],
                        "text": "More extensivesummaries of this are given by DeGroot (1970) and Bernardo and Smith (1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "Introductions are given in (Bretthorst, 1994; Press, 1989;Loredo, 1992; Bernardo & Smith, 1994; Cheeseman, 1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 92
                            }
                        ],
                        "text": "Fur-ther details can be found in most textbooks on probability distributions (DeGroot, 1970;Bernardo & Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 79
                            }
                        ],
                        "text": "The choice of priors for these distributionsis discussed in (Box & Tiao, 1973; Bernardo & Smith, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Theory"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 82
                            }
                        ],
                        "text": "For this situation, the function w 1, whenit exists, is called the link function (McCullagh & Nelder, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "In statistics, the conditional distributions arealso represented as regression models and generalized linear models (McCullagh & Nelder,1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 236
                            }
                        ],
                        "text": "\u2026is notnew, and can be seen in the BUGS system by Thomas, Spiegelhalter, and Gilks (1992)(Gilkset al., 1993b), in the work of Cohen (1992) for inductive logic programming, and emergingin software for handling generalized linear models (McCullagh & Nelder, 1989; Becker,Chambers, & Wilks, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 272
                            }
                        ],
                        "text": "\u2026k, same as w, andthe Jacobian of w with respect to is invertible, det dw( )d 6= 0), then various momentsof the distribution can be easily found:Exjy; (t(x; y)) = dw( )d 1 dZ( )d : (28)The vector function w( ) now has an inverse and it is referred to as the link function(McCullagh & Nelder, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models (second edition) Learning with Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized Linear Models (second edition) Learning with Graphical Models"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "The two approaches can be combined insome cases after appropriate reformulation of the problem (Dagum & Horvitz, 1992).4.1 Exact inference without platesThe exact inference approach has been highly re ned for the case where all variables arediscrete."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reformulating inference problems through selective"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "Several of these are introduced in Section 7 andmore detail is given, for instance, by Tanner (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "This would bethe method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992)covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 111
                            }
                        ],
                        "text": "This would be the method used for the Laplace approximation (Buntine & Weigend, 1991; MacKay, 1992) covered in (Tanner, 1993; Tierney & Kadane, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 139
                            }
                        ],
                        "text": "The basic computational techniques of probabilistic (Bayesian) inference used in this computational theory of learning are widely reviewed (Tanner, 1993; Press, 1989; Kass & Raftery, 1993; Neal, 1993; Bretthorst, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "The basiccomputational techniques of probabilistic (Bayesian) inference used in this computationaltheory of learning are widely reviewed (Tanner, 1993; Press, 1989; Kass & Raftery, 1993;Neal, 1993; Bretthorst, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tools for Statistical Inference (Second edition)"
            },
            "venue": {
                "fragments": [],
                "text": "Springer-Verlag."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 82
                            }
                        ],
                        "text": "For this situation, the function w 1, whenit exists, is called the link function (McCullagh & Nelder, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "In statistics, the conditional distributions arealso represented as regression models and generalized linear models (McCullagh & Nelder,1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 236
                            }
                        ],
                        "text": "\u2026is notnew, and can be seen in the BUGS system by Thomas, Spiegelhalter, and Gilks (1992)(Gilkset al., 1993b), in the work of Cohen (1992) for inductive logic programming, and emergingin software for handling generalized linear models (McCullagh & Nelder, 1989; Becker,Chambers, & Wilks, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 272
                            }
                        ],
                        "text": "\u2026k, same as w, andthe Jacobian of w with respect to is invertible, det dw( )d 6= 0), then various momentsof the distribution can be easily found:Exjy; (t(x; y)) = dw( )d 1 dZ( )d : (28)The vector function w( ) now has an inverse and it is referred to as the link function(McCullagh & Nelder, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models (second edition)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3192025"
                        ],
                        "name": "E. \u00c7inlar",
                        "slug": "E.-\u00c7inlar",
                        "structuredName": {
                            "firstName": "Erhan",
                            "lastName": "\u00c7inlar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. \u00c7inlar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222363719,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cb369b4890ddc9736cffff3bb89b04c39deae665",
            "isKey": false,
            "numCitedBy": 1384,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-stochastic-processes-\u00c7inlar",
            "title": {
                "fragments": [],
                "text": "Introduction to stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2454215"
                        ],
                        "name": "G. Corliss",
                        "slug": "G.-Corliss",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Corliss",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corliss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122363856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b93bfbc6e4bbb17b4ac75d691d02bab13e5b48de",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-differentiation-of-algorithms-:-theory,-Griewank-Corliss",
            "title": {
                "fragments": [],
                "text": "Automatic differentiation of algorithms : theory, implementation, and application"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 99
                            }
                        ],
                        "text": "The general form of this theorem for nite discrete domains is called the HammersleyCli ord Theorem (Geman, 1990; Besag et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "The general form of this theorem for nite discrete domains is called the Hammersley-Cli ord Theorem (Geman, 1990; Besag et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118130758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0d2f7fa01ba450312c6f85e048eebed5c7ca00f2",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Random-fields-and-inverse-problems-in-imaging-Geman",
            "title": {
                "fragments": [],
                "text": "Random fields and inverse problems in imaging"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113632699"
                        ],
                        "name": "David W. Scott",
                        "slug": "David-W.-Scott",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Scott",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David W. Scott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122810465,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7850aa4517584d6ee5852b26a1fd15b99a4f22b5",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-New-S-Language-Scott",
            "title": {
                "fragments": [],
                "text": "The New S Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An analysis of Bayesian classi ers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Mixture models (Titterington, Smith, & Makov, 1985;Poland, 1994) are used to model unsupervised learning, incomplete data in the classi ca-tion problems, robust regression, and general density estimation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decision Analysis with Continuous and Discrete Variables: A Mixture Distribution Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Decision Analysis with Continuous and Discrete Variables: A Mixture Distribution Approach"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classiication"
            },
            "venue": {
                "fragments": [],
                "text": "Seventh National Conference on Artiicial Intelligence"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Introductions are given in (Bretthorst, 1994; Press, 1989;Loredo, 1992; Bernardo & Smith, 1994; Cheeseman, 1990)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The promise of Bayesian inference for astrophysics Statistical Challenges in Modern Astronomy"
            },
            "venue": {
                "fragments": [],
                "text": "The promise of Bayesian inference for astrophysics Statistical Challenges in Modern Astronomy"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 61
                            }
                        ],
                        "text": "This meta-level use of graphical models was rst suggested by Spiegelhalter and Lauritzen (1990) inthe context of learning probabilities for Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional probabilities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 143
                            }
                        ],
                        "text": "\u2026models are used in domains such as diagnosis, probabilistic expert systems,and, more recently, in planning and control (Dean & Wellman, 1991; Chan & Shachter,1992), dynamic systems and time-series (Kj ru , 1992; Dagum, Galper, Horvitz, & Seiver,1994), and general data analysis (Gilks et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structural controllability and observability in in uence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 59
                            }
                        ],
                        "text": "The setting of priors for feed-forwardnetworks is di cult (MacKay, 1993; Nowlan & Hinton, 1992; Wolpert, 1994), and it willnot be considered here other than assuming a prior is used, p(w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 59
                            }
                        ],
                        "text": "The setting of priors for feed-forward networks is di cult (MacKay, 1993; Nowlan & Hinton, 1992; Wolpert, 1994), and it will not be considered here other than assuming a prior is used, p(w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian non-linear modeling for the energy prediction competition"
            },
            "venue": {
                "fragments": [],
                "text": "Report Draft"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random elds and inverse problems in imaging Ecole d' Et e de Probabilit es de Saint-Flour XVIII -1988"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Mathematics"
            },
            "year": 1427
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "Flexible toolkits and systemsexist for applying these techniques (Srinivas & Breese, 1990; Andersen, Olesen, Jensen, &Jensen, 1989; Cowell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BAIES|a probabilistic expert system shell with qualitative and quantitative learning Bayesian Statistics 4"
            },
            "venue": {
                "fragments": [],
                "text": "BAIES|a probabilistic expert system shell with qualitative and quantitative learning Bayesian Statistics 4"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "Introductions are given in (Bretthorst, 1994; Press, 1989;Loredo, 1992; Bernardo & Smith, 1994; Cheeseman, 1990)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On nding the most probable model"
            },
            "venue": {
                "fragments": [],
                "text": "Shrager, J., & Langley,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 297
                            }
                        ],
                        "text": "Other special classes of inference algorithms include the cases where the model is a multivariate Gaussian (Shachter & Kenley, 1989; Whittaker, 1990), or corresponds to some speci c diagnostic structure, such as two-level believe networks with a level of symptoms connected to a level of diseases (Henrion, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 274
                            }
                        ],
                        "text": "\u2026of inference algorithms include the cases where the model is a multivariate Gaussian(Shachter & Kenley, 1989; Whittaker, 1990), or corresponds to some speci c diagnosticstructure, such as two-level believe networks with a level of symptoms connected to a levelof diseases (Henrion, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards e cient inference in multiply connected belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new method for solving hard satis "
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 21
                            }
                        ],
                        "text": "See Charniak (1991), Shachter and Heckerman(1987), and Pearl (1988) for an introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 181
                            }
                        ],
                        "text": "\u2026variable names have been162\nLearning with Graphical Modelsabbreviated).p(Age;Occ; Clim;Dis; SympjM) = (1)p(AgejM) p(OccjM) p(ClimjM) p(DisjAge;Occ; Clim;M) p(SympjDis;M) ;where M is the conditioning context, for instance the expert's prior knowledge and thechoice of the graphical model in Figure 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 24
                            }
                        ],
                        "text": "Figure 2, adapted from (Shachter & Heckerman, 1987) shows a simple Bayesian network for"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thinking backwards for knowledge acquisition"
            },
            "venue": {
                "fragments": [],
                "text": "Thinking backwards for knowledge acquisition"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 0
                            }
                        ],
                        "text": "Thomas, Spiegelhalter, and Gilks (1992)(Gilks et al., 1993b) have taken advantage ofthis general applicability of sampling to create a compiler that converts a graphical represen-tation of a data analysis problem, with plates, into a matching Gibbs sampling algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 82
                            }
                        ],
                        "text": "The notion of a learning toolkit is notnew, and can be seen in the BUGS system by Thomas, Spiegelhalter, and Gilks (1992)(Gilkset al., 1993b), in the work of Cohen (1992) for inductive logic programming, and emergingin software for handling generalized linear models (McCullagh & Nelder, 1989;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BUGS: A program to perform Bayesian inference using Gibbs sampling Bayesian Statistics 4"
            },
            "venue": {
                "fragments": [],
                "text": "BUGS: A program to perform Bayesian inference using Gibbs sampling Bayesian Statistics 4"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "This is a worthwhile introductory exercise in Bayesian decision theory (Howard, 1970) that should be familiar to most students of statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "This is a worthwhile introductory exercise in Bayesian decision theory (Howard, 1970) thatshould be familiar to most students of statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decision analysis: perspectives on inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 37
                            }
                        ],
                        "text": "Various notations are used for this (Werbos et al.,1992; Buntine & Weigend, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks, system identiication, and control in the chemical process industry"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Intelligent Control"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 274
                            }
                        ],
                        "text": "\u2026of inference algorithms include the cases where the model is a multivariate Gaussian(Shachter & Kenley, 1989; Whittaker, 1990), or corresponds to some speci c diagnosticstructure, such as two-level believe networks with a level of symptoms connected to a levelof diseases (Henrion, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards eecient inference in multiply connected belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Innuence Diagrams, Belief Nets and Decision Analysis"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new method for solving hard satissability problems"
            },
            "venue": {
                "fragments": [],
                "text": "Tenth National Conference on Artiicial Intelligence"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 113
                            }
                        ],
                        "text": "An unsupervised learning algorithm learns hidden classes (Cheeseman,Self, Kelly, Taylor, Freeman, & Stutz, 1988; McLachlan & Basford, 1988). var1\nvar2 var3 classFigure 9: A simple classi cation problem169\nBuntineThe implied joint for these variables read from the graph is:p(class; var1; var2; var3)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixture Models: Inference and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "Mixture models (Titterington, Smith, & Makov, 1985;Poland, 1994) are used to model unsupervised learning, incomplete data in the classi ca-tion problems, robust regression, and general density estimation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 15
                            }
                        ],
                        "text": "Mixture models (Titterington, Smith, & Makov, 1985; Poland, 1994) are used to model unsupervised learning, incomplete data in the classi cation problems, robust regression, and general density estimation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decision Analysis with Continuous and Discrete Variables: A Mixture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "Other specialclasses of inference algorithms include the cases where the model is a multivariate Gaussian(Shachter & Kenley, 1989; Whittaker, 1990), or corresponds to some speci c diagnosticstructure, such as two-level believe networks with a level of symptoms connected to a levelof diseases\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gaussian innuence diagrams"
            },
            "venue": {
                "fragments": [],
                "text": "Management Science"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HUGIN|a shell for building"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model selection and accounting for model uncertainty"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Buntine"
            },
            "venue": {
                "fragments": [],
                "text": "Buntine"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 66
                            }
                        ],
                        "text": "Flexible toolkits and systems exist for applying these techniques (Srinivas & Breese, 1990; Andersen, Olesen, Jensen, & Jensen, 1989; Cowell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "Flexible toolkits and systemsexist for applying these techniques (Srinivas & Breese, 1990; Andersen, Olesen, Jensen, &Jensen, 1989; Cowell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BAIES|a probabilistic expert system shell with qualitative and quanti"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving large-scale constraint"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Probability (third edition)"
            },
            "venue": {
                "fragments": [],
                "text": "Theory of Probability (third edition)"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uncertain reasoning and forecast"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Planning and Control Optimal Statistical Decisions"
            },
            "venue": {
                "fragments": [],
                "text": "Learning with Graphical Models DeGroot, M"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "Introductions are given in (Bretthorst, 1994; Press, 1989;Loredo, 1992; Bernardo & Smith, 1994; Cheeseman, 1990)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On nding the most probable model Computational Models of Discovery and Theory Formation"
            },
            "venue": {
                "fragments": [],
                "text": "On nding the most probable model Computational Models of Discovery and Theory Formation"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 186
                            }
                        ],
                        "text": "Incomplete data (or missing values) (Quinlan, 1989), robust methods and modeling of outliers, and various density estimation and non-parametric methods all fall in this family of models (Titterington et al., 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 185
                            }
                        ],
                        "text": "Incomplete data (or missing values)(Quinlan, 1989), robust methods and modeling of outliers, and various density estimationand non-parametric methods all fall in this family of models (Titterington et al., 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis of Finite Mixture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Global conditioning for probabilis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 102
                            }
                        ],
                        "text": "The following is a simple graphical reinterpretation of the Pitman-Koopman Theoremfrom statistics (Je reys, 1961; DeGroot, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 169
                            }
                        ],
                        "text": "In this case, T (x ; y ) is an invertiblefunction of the k averages: 1N NXj=1 ti(xj) : i = 1; : : : ; k :In some cases, this extends to domains X and Y dependent on (Je reys, 1961)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Probability (third edition)"
            },
            "venue": {
                "fragments": [],
                "text": "Oxford: Clarendon Press."
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 0
                            }
                        ],
                        "text": "Thomas, Spiegelhalter, and Gilks (1992)(Gilks et al., 1993b) have taken advantage ofthis general applicability of sampling to create a compiler that converts a graphical represen-tation of a data analysis problem, with plates, into a matching Gibbs sampling algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 82
                            }
                        ],
                        "text": "The notion of a learning toolkit is notnew, and can be seen in the BUGS system by Thomas, Spiegelhalter, and Gilks (1992)(Gilkset al., 1993b), in the work of Cohen (1992) for inductive logic programming, and emergingin software for handling generalized linear models (McCullagh & Nelder, 1989;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BUGS: A program to perform Bayesian"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Diierentiation of Algorithms: Theory, Implementation, and Application"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic Diierentiation of Algorithms: Theory, Implementation, and Application"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 57,
            "methodology": 70,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 132,
        "totalPages": 14
    },
    "page_url": "https://www.semanticscholar.org/paper/Operations-for-Learning-with-Graphical-Models-Buntine/fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc?sort=total-citations"
}