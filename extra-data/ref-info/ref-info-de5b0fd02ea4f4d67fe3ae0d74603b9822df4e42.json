{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003762"
                        ],
                        "name": "J. Ziegler",
                        "slug": "J.-Ziegler",
                        "structuredName": {
                            "firstName": "Julius",
                            "lastName": "Ziegler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziegler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760556"
                        ],
                        "name": "C. Stiller",
                        "slug": "C.-Stiller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Stiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stiller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "All approaches use stereo with the exception of VISO2-M [21] which employs only monocular images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "In our evaluation, VISO2-S [21] comes closest to the ground truth trajectories with an average translation error of 2.2% and an average rotation error of 0.016 deg/m. Akin to our optical flow experiments, large motion impacts performance, especially in terms of translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "We evaluate five different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and without Local Bundle Adjustment (LBA) [32] as well as the flow separation approach of [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "In our evaluation, VISO2-S [21] comes closest to the ground truth trajectories with an average translation error of 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16284071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e7b64024ed4c5cf233c7d678234eb8328b0b9d5",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions."
            },
            "slug": "StereoScan:-Dense-3d-reconstruction-in-real-time-Geiger-Ziegler",
            "title": {
                "fragments": [],
                "text": "StereoScan: Dense 3d reconstruction in real-time"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time from a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm, and shows that the proposed odometry method achieves state-of-the-art accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Intelligent Vehicles Symposium (IV)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746718"
                        ],
                        "name": "J. Blanco",
                        "slug": "J.-Blanco",
                        "structuredName": {
                            "firstName": "Jos\u00e9-Luis",
                            "lastName": "Blanco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Blanco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32272511"
                        ],
                        "name": "F. Moreno",
                        "slug": "F.-Moreno",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Moreno",
                            "middleNames": [
                                "Angel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moreno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152989638"
                        ],
                        "name": "Javier Gonz\u00e1lez",
                        "slug": "Javier-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Javier Gonz\u00e1lez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "To date, datasets falling into this category are either monocular and short [43] or consist of low quality imagery [42, 4, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12266387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efeba75fcfb119fa4d0e7e0d1b48e76ce5335317",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth becomes specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a testbed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download http://babel.isa.uma.es/mrpt/papers/dataset2009/."
            },
            "slug": "A-collection-of-outdoor-robotic-datasets-with-truth-Blanco-Moreno",
            "title": {
                "fragments": [],
                "text": "A collection of outdoor robotic datasets with centimeter-accuracy ground truth"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work addressed both the practical and theoretical issues found while building a collection of six outdoor datasets, discussing how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of SLAM methods."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31778202"
                        ],
                        "name": "C. G. Keller",
                        "slug": "C.-G.-Keller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Keller",
                            "middleNames": [
                                "Gustav"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X Daimler [8] 1 56,000 X Caltech Pedestrian [13] 1 350,000 X COIL-100 [33] 100 72 X 72 bins EPFL Multi-View Car [34] 20 90 X 90 bins Caltech 3D Objects [31] 100 144 X 144 bins Proposed Dataset 2 80,000 X X continuous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ce846cd30c942529af9d12268590ddace22d9a",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a rapidly evolving area in the intelligent vehicles domain. Stereo vision is an attractive sensor for this purpose. But unlike for monocular vision, there are no realistic, large scale benchmarks available for stereo-based pedestrian detection, to provide a common point of reference for evaluation. This paper introduces the Daimler Stereo-Vision Pedestrian Detection benchmark, which consists of several thousands of pedestrians in the training set, and a 27-min test drive through urban environment and associated vehicle data. The data, including ground truth, is made publicly available for non-commercial purposes. The paper furthermore quantifies the benefit of stereo vision for ROI generation and localization; at equal detection rates, false positives are reduced by a factor of 4\u20135 with stereo over mono, using the same HOG/linSVM classification component."
            },
            "slug": "A-new-benchmark-for-stereo-based-pedestrian-Keller-Enzweiler",
            "title": {
                "fragments": [],
                "text": "A new benchmark for stereo-based pedestrian detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper furthermore quantifies the benefit of stereo vision for ROI generation and localization; at equal detection rates, false positives are reduced by a factor of 4\u20135 with stereo over mono, using the same HOG/linSVM classification component."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Intelligent Vehicles Symposium (IV)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728631"
                        ],
                        "name": "P. Alcantarilla",
                        "slug": "P.-Alcantarilla",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Alcantarilla",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Alcantarilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683950"
                        ],
                        "name": "L. M. Bergasa",
                        "slug": "L.-M.-Bergasa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Bergasa",
                            "middleNames": [
                                "Miguel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. Bergasa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038264"
                        ],
                        "name": "F. Dellaert",
                        "slug": "F.-Dellaert",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dellaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dellaert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "We evaluate five different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and without Local Bundle Adjustment (LBA) [32] as well as the flow separation approach of [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16905489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73ab676bb2fa548e6be2026f8ca9dba6ea3c4dda",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main drawbacks of standard visual EKF-SLAM techniques is the assumption of a general camera motion model. Usually this motion model has been implemented in the literature as a constant linear and angular velocity model. Because of this, most approaches cannot deal with sudden camera movements, causing them to lose accurate camera pose and leading to a corrupted 3D scene map. In this work we propose increasing the robustness of EKF-SLAM techniques by replacing this general motion model with a visual odometry prior, which provides a real-time relative pose prior by tracking many hundreds of features from frame to frame. We perform fast pose estimation using the two-stage RANSAC-based approach from [1]: a two-point algorithm for rotation followed by a one-point algorithm for translation. Then we integrate the estimated relative pose into the prediction step of the EKF. In the measurement update step, we only incorporate a much smaller number of landmarks into the 3D map to maintain real-time operation. Incorporating the visual odometry prior in the EKF process yields better and more robust localization and mapping results when compared to the constant linear and angular velocity model case. Our experimental results, using a handheld stereo camera as the only sensor, clearly show the benets of our method against the standard constant velocity model."
            },
            "slug": "Visual-odometry-priors-for-robust-EKF-SLAM-Alcantarilla-Bergasa",
            "title": {
                "fragments": [],
                "text": "Visual odometry priors for robust EKF-SLAM"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work performs fast pose estimation using the two-stage RANSAC-based approach from [1], andorporating the visual odometry prior in the EKF process yields better and more robust localization and mapping results when compared to the constant linear and angular velocity model case."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Robotics and Automation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086328"
                        ],
                        "name": "Christoph Rhemann",
                        "slug": "Christoph-Rhemann",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Rhemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph Rhemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115485"
                        ],
                        "name": "A. Hosni",
                        "slug": "A.-Hosni",
                        "structuredName": {
                            "firstName": "Asmaa",
                            "lastName": "Hosni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hosni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873656"
                        ],
                        "name": "M. Bleyer",
                        "slug": "M.-Bleyer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bleyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bleyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990797"
                        ],
                        "name": "M. Gelautz",
                        "slug": "M.-Gelautz",
                        "structuredName": {
                            "firstName": "Margrit",
                            "lastName": "Gelautz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gelautz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "Purely local methods [5, 38] fail if fronto-parallel surfaces are assumed, as this assumption is often strongly violated in real-world scenes (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": ", guided cost-volume filtering [38], pixel-wise graph cuts [26]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1680724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f908d2fb9cfcaff17030a912e4811fb02aaeec03",
            "isKey": false,
            "numCitedBy": 879,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision tasks can be formulated as labeling problems. The desired solution is often a spatially smooth labeling where label transitions are aligned with color edges of the input image. We show that such solutions can be efficiently achieved by smoothing the label costs with a very fast edge preserving filter. In this paper we propose a generic and simple framework comprising three steps: (i) constructing a cost volume (ii) fast cost volume filtering and (iii) winner-take-all label selection. Our main contribution is to show that with such a simple framework state-of-the-art results can be achieved for several computer vision applications. In particular, we achieve (i) disparity maps in real-time, whose quality exceeds those of all other fast (local) approaches on the Middlebury stereo benchmark, and (ii) optical flow fields with very fine structures as well as large displacements. To demonstrate robustness, the few parameters of our framework are set to nearly identical values for both applications. Also, competitive results for interactive image segmentation are presented. With this work, we hope to inspire other researchers to leverage this framework to other application areas."
            },
            "slug": "Fast-cost-volume-filtering-for-visual-and-beyond-Rhemann-Hosni",
            "title": {
                "fragments": [],
                "text": "Fast cost-volume filtering for visual correspondence and beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a generic and simple framework comprising three steps: constructing a cost volume, fast cost volume filtering and winner-take-all label selection, and achieves state-of-the-art results that achieve disparity maps in real-time, and optical flow fields with very fine structures as well as large displacements."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48527910"
                        ],
                        "name": "P. Doll\u00e1r",
                        "slug": "P.-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X Daimler [8] 1 56,000 X Caltech Pedestrian [13] 1 350,000 X COIL-100 [33] 100 72 X 72 bins EPFL Multi-View Car [34] 20 90 X 90 bins Caltech 3D Objects [31] 100 144 X 144 bins Proposed Dataset 2 80,000 X X continuous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206764948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34e0ba2daabfa4d3d22913ade8265aff50b5f917",
            "isKey": false,
            "numCitedBy": 2761,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate the performance of sixteen pretrained state-of-the-art detectors across six data sets. Our study allows us to assess the state of the art and provides a framework for gauging future efforts. Our experiments show that despite significant progress, performance still has much room for improvement. In particular, detection is disappointing at low resolutions and for partially occluded pedestrians."
            },
            "slug": "Pedestrian-Detection:-An-Evaluation-of-the-State-of-Doll\u00e1r-Wojek",
            "title": {
                "fragments": [],
                "text": "Pedestrian Detection: An Evaluation of the State of the Art"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An extensive evaluation of the state of the art in a unified framework of monocular pedestrian detection using sixteen pretrained state-of-the-art detectors across six data sets and proposes a refined per-frame evaluation methodology."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128253"
                        ],
                        "name": "F. Moosmann",
                        "slug": "F.-Moosmann",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Moosmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moosmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2738114"
                        ],
                        "name": "Omer Car",
                        "slug": "Omer-Car",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Car",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Car"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057784291"
                        ],
                        "name": "Bernhard Schuster",
                        "slug": "Bernhard-Schuster",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schuster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12339854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "140a6bdfb0564eb18a1f51a39dff36f20272a461",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera-to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experiments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions."
            },
            "slug": "Automatic-camera-and-range-sensor-calibration-using-Geiger-Moosmann",
            "title": {
                "fragments": [],
                "text": "Automatic camera and range sensor calibration using a single shot"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that the proposed checkerboard corner detector significantly outperforms current state-of-the-art and the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Robotics and Automation"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2659339"
                        ],
                        "name": "M. Kaess",
                        "slug": "M.-Kaess",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kaess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kaess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50769898"
                        ],
                        "name": "K. Ni",
                        "slug": "K.-Ni",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Ni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038264"
                        ],
                        "name": "F. Dellaert",
                        "slug": "F.-Dellaert",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dellaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dellaert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 300
                            }
                        ],
                        "text": "We evaluate five different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and without Local Bundle Adjustment (LBA) [32] as well as the flow separation approach of [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1502228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae48abb20b23c0edaa434a1da635d518897c871a",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Separating sparse flow provides fast and robust stereo visual odometry that deals with nearly degenerate situations that often arise in practical applications.We make use of the fact that in outdoor situations different constraints are provided by close and far structure, where the notion of close depends on the vehicle speed. The motion of distant features determines the rotational component that we recover with a robust two-point algorithm. Once the rotation is known, we recover the translational component from close features using a robust one-point algorithm. The overall algorithm is faster than estimating the motion in one step by a standard RANSAC-based three-point algorithm. And in contrast to other visual odometry work, we avoid the problem of nearly degenerate data, under which RANSAC is known to return inconsistent results. We confirm our claims on data from an outdoor robot equipped with a stereo rig."
            },
            "slug": "Flow-separation-for-fast-and-robust-stereo-odometry-Kaess-Ni",
            "title": {
                "fragments": [],
                "text": "Flow separation for fast and robust stereo odometry"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Separating sparse flow provides fast and robust stereo visual odometry that deals with nearly degenerate situations that often arise in practical applications and avoids the problem of nearly degenerated data under which RANSAC is known to return inconsistent results."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Robotics and Automation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779676"
                        ],
                        "name": "J\u00fcrgen Sturm",
                        "slug": "J\u00fcrgen-Sturm",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Sturm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00fcrgen Sturm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705753"
                        ],
                        "name": "S. Magnenat",
                        "slug": "S.-Magnenat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Magnenat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Magnenat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337344"
                        ],
                        "name": "Nikolas Engelhard",
                        "slug": "Nikolas-Engelhard",
                        "structuredName": {
                            "firstName": "Nikolas",
                            "lastName": "Engelhard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolas Engelhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84130061"
                        ],
                        "name": "F. Pomerleau",
                        "slug": "F.-Pomerleau",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Pomerleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pomerleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144089495"
                        ],
                        "name": "F. Colas",
                        "slug": "F.-Colas",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Colas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Colas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725973"
                        ],
                        "name": "W. Burgard",
                        "slug": "W.-Burgard",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Burgard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Burgard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720483"
                        ],
                        "name": "R. Siegwart",
                        "slug": "R.-Siegwart",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Siegwart",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Siegwart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Visual Odometry / SLAM setting #sequences length #frames resolution ground truth metric TUM RGB-D [43] indoor 27 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "To date, datasets falling into this category are either monocular and short [43] or consist of low quality imagery [42, 4, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6639131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "318cb91c41307135781a0a01bc9e0b6a6e123b0f",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a large dataset containing RGB-D image sequences and the ground-truth camera trajectories with the goal to establish a benchmark for the evaluation of visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor and the ground-truth trajectory of camera poses. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems."
            },
            "slug": "Towards-a-benchmark-for-RGB-D-SLAM-evaluation-Sturm-Magnenat",
            "title": {
                "fragments": [],
                "text": "Towards a benchmark for RGB-D SLAM evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A large dataset containing RGB-D image sequences and the ground-truth camera trajectories is provided and an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "RSS 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681995"
                        ],
                        "name": "Ashutosh Saxena",
                        "slug": "Ashutosh-Saxena",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashutosh Saxena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49084213"
                        ],
                        "name": "J. Schulte",
                        "slug": "J.-Schulte",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Schulte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schulte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Our calibration pipeline proceeds as follows: First, we calibrate the four video cameras intrinsically and extrinsically and rectify the input images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1216740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49531103099c8d17ea34eb09433688e84de4f35f",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Depth estimation in computer vision and robotics is most commonly done via stereo vision (stereopsis), in which images from two cameras are used to triangulate and estimate distances. However, there are also numerous monocular visual cues--such as texture variations and gradients, defocus, color/haze, etc. --that have heretofore been little exploited in such systems. Some of these cues apply even in regions without texture, where stereo would work poorly. In this paper, we apply a Markov Random Field (MRF) learning algorithm to capture some of these monocular cues, and incorporate them into a stereo system. We show that by adding monocular cues to stereo (triangulation) ones, we obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone. This holds true for a large variety of environments, including both indoor environments and unstructured outdoor environments containing trees/forests, buildings, etc. Our approach is general, and applies to incorporating monocular cues together with any off-the-shelf stereo system."
            },
            "slug": "Depth-Estimation-Using-Monocular-and-Stereo-Cues-Saxena-Schulte",
            "title": {
                "fragments": [],
                "text": "Depth Estimation Using Monocular and Stereo Cues"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that by adding monocular cues to stereo (triangulation) ones, it is shown that significantly more accurate depth estimates are obtained than is possible using either monocular or stereo cues alone."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X"
                    },
                    "intents": []
                }
            ],
            "corpusId": 40604341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c7dfc71122c99959e561b62a7fc2764f13a401a",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 123,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis describes an effort to construct a scene understanding system that is able to analyze the content of real images. While constructing the system we had to provide solutions to many of the fundamental questions that every student of object recognition deals with daily. These include the choice of data set, the choice of success measurement, the representation of the image content, the selection of inference engine, and the representation of the relations between objects. \nThe main test-bed for our system is the CBCL StreetScenes data base. It is a carefully labeled set of images, much larger than any similar data set available at the time it was collected. Each image in this data set was labeled for 9 common classes such as cars, pedestrians, roads and trees. Our system represents each image using a set of features that are based on a model of the human visual system constructed in our lab. We demonstrate that this biologically motivated image representation, along with its extensions, constitutes an effective representation for object detection, facilitating unprecedented levels of detection accuracy. Similarly to biological vision systems, our system uses hierarchical representations. We therefore explore the possible ways of combining information across the hierarchy into the final perception. \nOur system is trained using standard machine learning machinery, which was first applied to computer vision in earlier work of Prof. Poggio and others. We demonstrate how the same standard methods can be used to model relations between objects in images as well, capturing context information. The resulting system detects and localizes, using a unified set of tools and image representations, compact objects such as cars, amorphous objects such as trees and roads, and the relations between objects within the scene. The same representation also excels in identifying objects in clutter without scanning the image. \nMuch of the work presented in the thesis was devoted to a rigorous comparison of our system to alternative object recognition systems. The results of these experiments support the effectiveness of simple feed-forward systems for the basic tasks involved in scene understanding. We make our results fully available to the public by publishing our code and data sets in hope that others may improve and extend our results. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Street-Scenes:-towards-scene-understanding-in-still-Bileschi",
            "title": {
                "fragments": [],
                "text": "Street Scenes: towards scene understanding in still images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This thesis describes an effort to construct a scene understanding system that is able to analyze the content of real images using a model of the human visual system constructed in the lab, and demonstrates that this biologically motivated image representation constitutes an effective representation for object detection, facilitating unprecedented levels of detection accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153024876"
                        ],
                        "name": "J. P. Lewis",
                        "slug": "J.-P.-Lewis",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 76
                            }
                        ],
                        "text": "As Table 2 shows, errors on our benchmark are higher than those reported on Middlebury [41], indicating\nthe increased level of difficulty of our real-world dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 165
                            }
                        ],
                        "text": "In the past few years an increasing number of benchmarks have been developed to push forward the performance of visual recognitions systems, e.g., Caltech-101\n[17], Middlebury for stereo [41] and optical flow [2] evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 66
                            }
                        ],
                        "text": "This is mainly due to the differences in the data sets: Since the Middlebury benchmark is largely well textured and provides a smaller label set, methods concentrating on accurate object boundary segmentation peform well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 98
                            }
                        ],
                        "text": "Perhaps not surprisingly, many algorithms that do well on established datasets such as Middlebury [41, 2] struggle on our benchmark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In contrast to [41, 2], our images are not downsampled."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "Interestingly, methods ranking high on Middlebury, perform particularly bad on our dataset, e.g., guided cost-volume filtering [38], pixel-wise graph cuts [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "3 Mpx dense Middlebury [2] laboratory 24 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Compared to previous datasets [41, 2, 30, 29], this is the first one with realistic non-synthetic imagery and accurate ground truth."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "[17], Middlebury for stereo [41] and optical flow [2] evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 316800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804836b8ad86ef8042e3dcbd45442a52f031ee03",
            "isKey": true,
            "numCitedBy": 2377,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1)\u00a0sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2)\u00a0realistic synthetic sequences, (3)\u00a0high frame-rate video used to study interpolation error, and (4)\u00a0modified stereo sequences of static scenes. In addition to the average angular error used by Barron et\u00a0al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them."
            },
            "slug": "A-Database-and-Evaluation-Methodology-for-Optical-Baker-Scharstein",
            "title": {
                "fragments": [],
                "text": "A Database and Evaluation Methodology for Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms and analyzes the results obtained to date to draw a large number of conclusions."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38160276"
                        ],
                        "name": "S. Morales",
                        "slug": "S.-Morales",
                        "structuredName": {
                            "firstName": "Sandino",
                            "lastName": "Morales",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Morales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729664"
                        ],
                        "name": "R. Klette",
                        "slug": "R.-Klette",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Klette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Klette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Stereo Matching type #images resolution ground truth uncorrelated metric EISATS [30] synthetic 498 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Optical Flow type #images resolution ground truth uncorrelated metric EISATS [30] synthetic 498 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Compared to previous datasets [41, 2, 30, 29], this is the first one with realistic non-synthetic imagery and accurate ground truth."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9762458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4050a4661c1496ec22a43dc55b9c9d26eb41fe63",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Current stereo algorithms are capable to calculate accurate (as defined, e.g., by needs in vision-based driver assistance) dense disparity maps in real time. They have become the source of three-dimensional data for several indoor and outdoor applications. However, ground truthbased evaluation of such algorithms has been typically limited to data sets generated indoors in laboratories. In this paper we present a new approach to evaluate stereo algorithms using ground-truth over real world data sets. Ground truth is generated using range measurements acquired with a high-end laser range-finder. For evaluating as many points as possible in a given disparity map, we use two evaluation approaches: A direct comparison for those pixels with available range data, and a confidence measure for the remaining pixels."
            },
            "slug": "Ground-Truth-Evaluation-of-Stereo-Algorithms-for-Morales-Klette",
            "title": {
                "fragments": [],
                "text": "Ground Truth Evaluation of Stereo Algorithms for Real World Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a new approach to evaluate stereo algorithms using ground-truth over real world data sets, generated using range measurements acquired with a high-end laser range-finder."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV Workshops"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645730"
                        ],
                        "name": "Mike Smith",
                        "slug": "Mike-Smith",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144738357"
                        ],
                        "name": "Ian A. Baldwin",
                        "slug": "Ian-A.-Baldwin",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Baldwin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian A. Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145779748"
                        ],
                        "name": "W. Churchill",
                        "slug": "W.-Churchill",
                        "structuredName": {
                            "firstName": "Winston",
                            "lastName": "Churchill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Churchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3619154"
                        ],
                        "name": "Rohan Paul",
                        "slug": "Rohan-Paul",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Paul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohan Paul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214578"
                        ],
                        "name": "P. Newman",
                        "slug": "P.-Newman",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Newman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Newman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "To date, datasets falling into this category are either monocular and short [43] or consist of low quality imagery [42, 4, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206500040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80ccc2feda0a65344bf282af782527b68d086857",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a large dataset intended for use in mobile robotics research. Gathered from a robot driving several kilometers through a park and campus, it contains a five-degree-of-freedom dead-reckoned trajectory, laser range/reflectance data and 20 Hz stereoscopic and omnidirectional imagery. All data is carefully timestamped and all data logs are in human readable form with the images in standard formats. We provide a set of tools to access the data and detailed tagging and segmentations to facilitate its use."
            },
            "slug": "The-New-College-Vision-and-Laser-Data-Set-Smith-Baldwin",
            "title": {
                "fragments": [],
                "text": "The New College Vision and Laser Data Set"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A large dataset intended for use in mobile robotics research, gathered from a robot driving several kilometers through a park and campus, contains a five-degree-of-freedom dead-reckoned trajectory, laser range/reflectance data and 20 Hz stereoscopic and omnidirectional imagery."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2774325"
                        ],
                        "name": "Ren\u00e9 Ranftl",
                        "slug": "Ren\u00e9-Ranftl",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Ranftl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9 Ranftl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762392"
                        ],
                        "name": "S. Gehrig",
                        "slug": "S.-Gehrig",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Gehrig",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gehrig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730097"
                        ],
                        "name": "T. Pock",
                        "slug": "T.-Pock",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Pock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16350102,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b89b2ca2f3d9cf8d456c993e4cc837405c4e9b0",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine high accuracy stereo estimation for binocular sequences that where obtained from a mobile platform. The ultimate goal is to improve the range of stereo systems without altering the setup. Based on a well-known variational optical flow model, we introduce a novel stereo model that features a second-order regularization, which both allows sub-pixel accurate solutions and piecewise planar disparity maps. The model incorporates a robust fidelity term to account for adverse illumination conditions that frequently arise in real-world scenes. Using several sequences that were taken from a mobile platform we show the robustness and accuracy of the proposed model."
            },
            "slug": "Pushing-the-limits-of-stereo-using-variational-Ranftl-Gehrig",
            "title": {
                "fragments": [],
                "text": "Pushing the limits of stereo using variational stereo estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel stereo model is introduced that features a second-order regularization, which both allows sub-pixel accurate solutions and piecewise planar disparity maps and incorporates a robust fidelity term to account for adverse illumination conditions that frequently arise in real-world scenes."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2908338"
                        ],
                        "name": "K. Yamaguchi",
                        "slug": "K.-Yamaguchi",
                        "structuredName": {
                            "firstName": "Koichiro",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918412"
                        ],
                        "name": "Tamir Hazan",
                        "slug": "Tamir-Hazan",
                        "structuredName": {
                            "firstName": "Tamir",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamir Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Stereo Non-Occluded All Density PCBP [46] 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "3 shows the best and worst test results for the (currently) top ranked stereo method PCBP [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 276625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bf9dbc09c2386ae30e267163b1936a9a28d1f1e",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth. We formulate the problem as one of inference in a hybrid MRF composed of both continuous (i.e., slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables. This allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments, as well as potentials encoding which junctions are physically possible. Our approach outperforms the state-of-the-art on Middlebury high resolution imagery [1] as well as in the more challenging KITTI dataset [2], while being more efficient than existing slanted plane MRF methods, taking on average 2 minutes to perform inference on high resolution imagery."
            },
            "slug": "Continuous-Markov-Random-Fields-for-Robust-Stereo-Yamaguchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Continuous Markov Random Fields for Robust Stereo Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth is presented, which outperforms the state-of-the-art on Middlebury high resolution imagery and in the more challenging KITTI dataset, while being more efficient than existing slanted plane MRF methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2819639"
                        ],
                        "name": "Mustafa \u00d6zuysal",
                        "slug": "Mustafa-\u00d6zuysal",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "\u00d6zuysal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa \u00d6zuysal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "COIL-100 [33] 100 72 X 72 bins EPFL Multi-View Car [34] 20 90 X 90 bins Caltech 3D Objects [31] 100 144 X 144 bins Proposed Dataset 2 80,000 X X continuous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 600022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe6544a0965e1c6182737b0f86d549036d5c5bcc",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to overcome the two main challenges of 3D multiview object detection and localization: The variation of object features due to changes in the viewpoint and the variation in the size and aspect ratio of the object. Our approach proceeds in three steps. Given an initial bounding box of fixed size, we first refine its aspect ratio and size. We can then predict the viewing angle, under the hypothesis that the bounding box actually contains an object instance. Finally, a classifier tuned to this particular viewpoint checks the existence of an instance. As a result, we can find the object instances and estimate their poses, without having to search over all window sizes and potential orientations. We train and evaluate our method on a new object database specifically tailored for this task, containing real-world objects imaged over a wide range of smoothly varying viewpoints and significant lighting changes. We show that the successive estimations of the bounding box and the viewpoint lead to better localization results."
            },
            "slug": "Pose-estimation-for-category-specific-multiview-\u00d6zuysal-Lepetit",
            "title": {
                "fragments": [],
                "text": "Pose estimation for category specific multiview object localization"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An approach to overcome the two main challenges of 3D multiview object detection and localization: the variation of object features due to changes in the viewpoint and the variation in the size and aspect ratio of the object."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 98
                            }
                        ],
                        "text": "Perhaps not surprisingly, many algorithms that do well on established datasets such as Middlebury [41, 2] struggle on our benchmark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In contrast to [41, 2], our images are not downsampled."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "As Table 2 shows, errors on our benchmark are higher than those reported on Middlebury [41], indicating"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "3 Mpx dense Middlebury [41] laboratory 38 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Compared to previous datasets [41, 2, 30, 29], this is the first one with realistic non-synthetic imagery and accurate ground truth."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "[17], Middlebury for stereo [41] and optical flow [2] evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195859047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "isKey": true,
            "numCitedBy": 3003,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms."
            },
            "slug": "A-Taxonomy-and-Evaluation-of-Dense-Two-Frame-Stereo-Scharstein-Szeliski",
            "title": {
                "fragments": [],
                "text": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper has designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393616664"
                        ],
                        "name": "Gaurav Pandey",
                        "slug": "Gaurav-Pandey",
                        "structuredName": {
                            "firstName": "Gaurav",
                            "lastName": "Pandey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gaurav Pandey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153453301"
                        ],
                        "name": "J. McBride",
                        "slug": "J.-McBride",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McBride",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McBride"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721484"
                        ],
                        "name": "R. Eustice",
                        "slug": "R.-Eustice",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Eustice",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eustice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2611891,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81d5294e093ca79c27b15aa08275d729118b83ce",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a data set collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit, a Velodyne three-dimensional lidar scanner, two push-broom forward-looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. Here we present the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research Campus and downtown Dearborn, MI, during November\u2013December 2009. The vehicle path trajectory in these data sets contains several large- and small-scale loop closures, which should be useful for testing various state-of-the-art computer vision and simultaneous localization and mapping algorithms."
            },
            "slug": "Ford-Campus-vision-and-lidar-data-set-Pandey-McBride",
            "title": {
                "fragments": [],
                "text": "Ford Campus vision and lidar data set"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The vehicle path trajectory in these data sets contains several large- and small-scale loop closures, which should be useful for testing various state-of-the-art computer vision and simultaneous localization and mapping algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288124"
                        ],
                        "name": "R. K\u00fcmmerle",
                        "slug": "R.-K\u00fcmmerle",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "K\u00fcmmerle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. K\u00fcmmerle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413377"
                        ],
                        "name": "Bastian Steder",
                        "slug": "Bastian-Steder",
                        "structuredName": {
                            "firstName": "Bastian",
                            "lastName": "Steder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bastian Steder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573148"
                        ],
                        "name": "C. Dornhege",
                        "slug": "C.-Dornhege",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Dornhege",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dornhege"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39851219"
                        ],
                        "name": "M. Ruhnke",
                        "slug": "M.-Ruhnke",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ruhnke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ruhnke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737531"
                        ],
                        "name": "G. Grisetti",
                        "slug": "G.-Grisetti",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Grisetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grisetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722062"
                        ],
                        "name": "C. Stachniss",
                        "slug": "C.-Stachniss",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stachniss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stachniss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46547377"
                        ],
                        "name": "A. Kleiner",
                        "slug": "A.-Kleiner",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kleiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kleiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Furthermore, we also plan to include visual SLAM with loop-closure capabilities, object tracking, segmentation, structure-from-motion and 3D scene understanding into our evaluation framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "where F is a set of frames (i, j), p\u0302 \u2208 SE(3) and p \u2208 SE(3) are estimated and true camera poses respectively, \u2296 denotes the inverse compositional operator [28] and \u2220[\u00b7] is the rotation angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for stereo, optical flow, visual odometry / SLAM and 3D object detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Thus often only qualitative results are presented, with the notable exception of laser-based SLAM [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry / SLAM and 3D object detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "For our visual odometry / SLAM evaluation we select long sequences of varying speed with high-quality localization, yielding a set of 41.000 frames captured at 10 fps and a total driving distance of 39.2 km with frequent loop closures which are of interest in SLAM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Evaluating visual odometry/SLAM approaches based on the error of the trajectory end-point can be misleading, as this measure depends strongly on the point in time where the error has been made, e.g., rotational errors earlier in the sequence lead to larger end-point errors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "The ground truth for visual odometry/SLAM is directly given by the output of the GPS/IMU localization unit pro-\njected into the coordinate system of the left camera after rectification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] proposed to compute the average of all relative relations at a fixed distance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "We evaluate five different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and without Local Bundle Adjustment (LBA) [32] as well as the flow separation approach of [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "They typically do not provide an evaluation metric, and as a consequence there is no consensus on which benchmark should be used to evaluate visual odometry / SLAM approaches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Our 3D visual odometry / SLAM dataset consists of 22 stereo sequences, with a total length of 39.2 km."
                    },
                    "intents": []
                }
            ],
            "corpusId": 281452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78447ada47d4eb4a96d57ce83a08c5879588bbe3",
            "isKey": true,
            "numCitedBy": 282,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of creating an objective benchmark for evaluating SLAM approaches. We propose a framework for analyzing the results of a SLAM approach based on a metric for measuring the error of the corrected trajectory. This metric uses only relative relations between poses and does not rely on a global reference frame. This overcomes serious shortcomings of approaches using a global reference frame to compute the error. Our method furthermore allows us to compare SLAM approaches that use different estimation techniques or different sensor modalities since all computations are made based on the corrected trajectory of the robot.We provide sets of relative relations needed to compute our metric for an extensive set of datasets frequently used in the robotics community. The relations have been obtained by manually matching laser-range observations to avoid the errors caused by matching algorithms. Our benchmark framework allows the user to easily analyze and objectively compare different SLAM approaches."
            },
            "slug": "On-measuring-the-accuracy-of-SLAM-algorithms-K\u00fcmmerle-Steder",
            "title": {
                "fragments": [],
                "text": "On measuring the accuracy of SLAM algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A framework for analyzing the results of a SLAM approach based on a metric for measuring the error of the corrected trajectory is proposed, which overcomes serious shortcomings of approaches using a global reference frame to compute the error."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 81
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16824129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "590941c26d057f917c7f5275824d350e9aac7ba3",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the challenging problem of simultaneous pedestrian detection and ground-plane estimation from video while walking through a busy pedestrian zone. Our proposed system integrates robust stereo depth cues, ground-plane estimation, and appearance-based object detection in a principled fashion using a graphical model. Object-object occlusions lead to complex interactions in this model that make an exact solution computationally intractable. We therefore propose a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure. This approach leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa. We quantitatively evaluate the performance of our proposed approach on several challenging test sequences showing strolls through busy shopping streets. Comparisons to various baseline systems show that it outperforms both a system using no scene geometry and one just relying on structure-from-motion without dense stereo."
            },
            "slug": "Depth-and-Appearance-for-Mobile-Scene-Analysis-Ess-Leibe",
            "title": {
                "fragments": [],
                "text": "Depth and Appearance for Mobile Scene Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure, which leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50976789"
                        ],
                        "name": "Martin Roser",
                        "slug": "Martin-Roser",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Roser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Roser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5535646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c97127f828705328bceb6c5a50e2b1aefbb28ff",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel approach to binocular stereo for fast matching of high-resolution images. Our approach builds a prior on the disparities by forming a triangulation on a set of support points which can be robustly matched, reducing the matching ambiguities of the remaining points. This allows for efficient exploitation of the disparity search space, yielding accurate dense reconstruction without the need for global optimization. Moreover, our method automatically determines the disparity range and can be easily parallelized. We demonstrate the effectiveness of our approach on the large-scale Middlebury benchmark, and show that state-of-the-art performance can be achieved with significant speedups. Computing the left and right disparity maps for a one Megapixel image pair takes about one second on a single CPU core."
            },
            "slug": "Efficient-Large-Scale-Stereo-Matching-Geiger-Roser",
            "title": {
                "fragments": [],
                "text": "Efficient Large-Scale Stereo Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A novel approach to binocular stereo for fast matching of high-resolution images by building a prior on the disparities by forming a triangulation on a set of support points which can be robustly matched, reducing the matching ambiguities of the remaining points."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717435"
                        ],
                        "name": "B. G. Schunck",
                        "slug": "B.-G.-Schunck",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Schunck",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. G. Schunck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "We observed that classical variational approaches [24, 44, 45] work best on our images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18199642,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7cd33e39fb1578f70f663c714e4d52a33b9087ac",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\"Determining-optical-flow\":-A-Retrospective-Horn-Schunck",
            "title": {
                "fragments": [],
                "text": "\"Determining optical flow\": A Retrospective"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217189"
                        ],
                        "name": "E. Mouragnon",
                        "slug": "E.-Mouragnon",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Mouragnon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mouragnon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2745663"
                        ],
                        "name": "M. Lhuillier",
                        "slug": "M.-Lhuillier",
                        "structuredName": {
                            "firstName": "Maxime",
                            "lastName": "Lhuillier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lhuillier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756828"
                        ],
                        "name": "M. Dhome",
                        "slug": "M.-Dhome",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Dhome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dhome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34999782"
                        ],
                        "name": "F. Dekeyser",
                        "slug": "F.-Dekeyser",
                        "structuredName": {
                            "firstName": "Fabien",
                            "lastName": "Dekeyser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dekeyser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910025"
                        ],
                        "name": "P. Sayd",
                        "slug": "P.-Sayd",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Sayd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sayd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 252
                            }
                        ],
                        "text": "We evaluate five different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and without Local Bundle Adjustment (LBA) [32] as well as the flow separation approach of [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5963070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e860c03ac0cd1628a6b12c4a438fcca5d814e6b",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generic-and-real-time-structure-from-motion-using-Mouragnon-Lhuillier",
            "title": {
                "fragments": [],
                "text": "Generic and real-time structure from motion using local bundle adjustment"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064794036"
                        ],
                        "name": "Jan Cech",
                        "slug": "Jan-Cech",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Cech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144953681"
                        ],
                        "name": "R. S\u00e1ra",
                        "slug": "R.-S\u00e1ra",
                        "structuredName": {
                            "firstName": "Radim",
                            "lastName": "S\u00e1ra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S\u00e1ra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16206680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f0cbbbe579ede5203e6f819ba8b79cc4f211e1e",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple stereo matching algorithm is proposed that visits only a small fraction of disparity space in order to find a semi-dense disparity map. It works by growing from a small set of correspondence seeds. Unlike in known seed-growing algorithms, it guarantees matching accuracy and correctness, even in the presence of repetitive patterns. This success is based on the fact it solves a global optimization task. The algorithm can recover from wrong initial seeds to the extent they can even be random. The quality of correspondence seeds influences computing time, not the quality of the final disparity map. We show that the proposed algorithm achieves similar results as an exhaustive disparity space search but it is two orders of magnitude faster. This is very unlike the existing growing algorithms which are fast but erroneous. Accurate matching on 2-megapixel images of complex scenes is routinely obtained in a few seconds on a common PC from a small number of seeds, without limiting the disparity search range."
            },
            "slug": "Efficient-Sampling-of-Disparity-Space-for-Fast-And-Cech-S\u00e1ra",
            "title": {
                "fragments": [],
                "text": "Efficient Sampling of Disparity Space for Fast And Accurate Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A simple stereo matching algorithm is proposed that visits only a small fraction of disparity space in order to find a semi-dense disparity map by growing from a small set of correspondence seeds, which is very unlike the existing growing algorithms which are fast but erroneous."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575716"
                        ],
                        "name": "Jana Kostkov\u00e1",
                        "slug": "Jana-Kostkov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Kostkov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jana Kostkov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144953681"
                        ],
                        "name": "R. S\u00e1ra",
                        "slug": "R.-S\u00e1ra",
                        "structuredName": {
                            "firstName": "Radim",
                            "lastName": "S\u00e1ra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S\u00e1ra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15456612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9e88562ec905fd098d49266aefc5635e996b802",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Local joint image modeling in stereo matching brings more discriminable and stable matching features. Such features reduce the need for strong prior models (continuity) and thus algorithms that are less prone to false positive artefacts in general complex scenes can be applied. One of the principal quality factors in area-based dense stereo is the matching window shape. As it cannot be selected without having any initial matching hypothesis we propose a stratified matching approach. The window adapts to high-correlation structures in disparity space found in pre-matching which is then followed by final matching. In a rigorous ground-truth experiment we show that Stratified Dense Matching is able to increase matching density 3\u00d7, matching accuracy 1.8\u00d7, and occlusion boundary detection 2\u00d7 as compared to a fixed-size rectangular windows algorithm. Performance on real outdoor complex scenes is also evaluated."
            },
            "slug": "Stratified-Dense-Matching-for-Stereopsis-in-Complex-Kostkov\u00e1-S\u00e1ra",
            "title": {
                "fragments": [],
                "text": "Stratified Dense Matching for Stereopsis in Complex Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Stratified Dense Matching is able to increase matching density 3\u00d7, matching accuracy 1.8\u00d7, and occlusion boundary detection 2\u00d7 as compared to a fixed-size rectangular windows algorithm, and performance on real outdoor complex scenes is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232265"
                        ],
                        "name": "Deqing Sun",
                        "slug": "Deqing-Sun",
                        "structuredName": {
                            "firstName": "Deqing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deqing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "We observed that classical variational approaches [24, 44, 45] work best on our images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206591220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a87428c6b2205240485ee6bb9cfb00fd9ed359c",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that \u201cclassical\u201d flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a nonlocal term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark."
            },
            "slug": "Secrets-of-optical-flow-estimation-and-their-Sun-Roth",
            "title": {
                "fragments": [],
                "text": "Secrets of optical flow estimation and their principles"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is discovered that \u201cclassical\u201d flow formulations perform surprisingly well when combined with modern optimization and implementation techniques, and while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064794036"
                        ],
                        "name": "Jan Cech",
                        "slug": "Jan-Cech",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Cech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401515040"
                        ],
                        "name": "Jordi Sanchez-Riera",
                        "slug": "Jordi-Sanchez-Riera",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Sanchez-Riera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordi Sanchez-Riera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3830469,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "101b3894e639ecaaec910d039e543ab98cf672f8",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple seed growing algorithm for estimating scene flow in a stereo setup is presented. Two calibrated and synchronized cameras observe a scene and output a sequence of image pairs. The algorithm simultaneously computes a disparity map between the image pairs and optical flow maps between consecutive images. This, together with calibration data, is an equivalent representation of the 3D scene flow, i.e. a 3D velocity vector is associated with each reconstructed point. The proposed method starts from correspondence seeds and propagates these correspondences to their neighborhood. It is accurate for complex scenes with large motions and produces temporally-coherent stereo disparity and optical flow results. The algorithm is fast due to inherent search space reduction. An explicit comparison with recent methods of spatiotemporal stereo and variational optical and scene flow is provided."
            },
            "slug": "Scene-flow-estimation-by-growing-correspondence-Cech-Sanchez-Riera",
            "title": {
                "fragments": [],
                "text": "Scene flow estimation by growing correspondence seeds"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A simple seed growing algorithm for estimating scene flow in a stereo setup that is accurate for complex scenes with large motions and produces temporally-coherent stereo disparity and optical flow results is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713941"
                        ],
                        "name": "C. Zach",
                        "slug": "C.-Zach",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Zach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730097"
                        ],
                        "name": "T. Pock",
                        "slug": "T.-Pock",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Pock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15250191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c",
            "isKey": false,
            "numCitedBy": 1505,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational methods are among the most successful approaches to calculate the optical flow between two image frames. A particularly appealing formulation is based on total variation (TV) regularization and the robust L1 norm in the data fidelity term. This formulation can preserve discontinuities in the flow field and offers an increased robustness against illumination changes, occlusions and noise. In this work we present a novel approach to solve the TV-L1 formulation. Our method results in a very efficient numerical scheme, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step. Additionally, our approach can be accelerated by modern graphics processing units. We demonstrate the real-time performance (30 fps) of our approach for video inputs at a resolution of 320 \u00d7 240 pixels."
            },
            "slug": "A-Duality-Based-Approach-for-Realtime-TV-L1-Optical-Zach-Pock",
            "title": {
                "fragments": [],
                "text": "A Duality Based Approach for Realtime TV-L1 Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a novel approach to solve the TV-L1 formulation, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35137706"
                        ],
                        "name": "J. Bouguet",
                        "slug": "J.-Bouguet",
                        "structuredName": {
                            "firstName": "J.-Y.",
                            "lastName": "Bouguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bouguet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 94
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9350588,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aa972b40c0f8e20b07e02d1fd320bc7ebadfdfc7",
            "isKey": false,
            "numCitedBy": 2527,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Problem Statement Let I and J be two 2D grayscaled images. The two quantities I(x) = I(x, y) and J(x) = J(x, y) are then the grayscale value of the two images are the location x = [x y] , where x and y are the two pixel coordinates of a generic image point x. The image I will sometimes be referenced as the first image, and the image J as the second image. For practical issues, the images I and J are discret function (or arrays), and the upper left corner pixel coordinate vector is [0 0] . Let nx and ny be the width and height of the two images. Then the lower right pixel coordinate vector is [nx \u2212 1 ny \u2212 1] . Consider an image point u = [ux uy] on the first image I. The goal of feature tracking is to find the location v = u + d = [ux+dx uy +dy] on the second image J such as I(u) and J(v) are \u201csimilar\u201d. The vector d = [dx dy] is the image velocity at x, also known as the optical flow at x. Because of the aperture problem, it is essential to define the notion of similarity in a 2D neighborhood sense. Let \u03c9x and \u03c9y two integers. We define the image velocity d as being the vector that minimizes the residual function defined as follows:"
            },
            "slug": "Pyramidal-implementation-of-the-lucas-kanade-Bouguet",
            "title": {
                "fragments": [],
                "text": "Pyramidal implementation of the lucas kanade feature tracker"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is essential to define the notion of similarity in a 2D neighborhood sense and the image velocity d is defined as being the vector that minimizes the residual function defined as follows."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462051"
                        ],
                        "name": "Pierre Moreels",
                        "slug": "Pierre-Moreels",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Moreels",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Moreels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 370561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0481840f39b8c1560b7137ad3ad0eae0346fe11e",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the performance of a number of popular feature detectors and descriptors in matching 3D object features across viewpoints and lighting conditions. To this end we design a method, based on intersecting epipolar constraints, for providing ground truth correspondence automatically. These correspondences are based purely on geometric information, and do not rely on the choice of a specific feature appearance descriptor. We test detector-descriptor combinations on a database of 100 objects viewed from 144 calibrated viewpoints under three different lighting conditions. We find that the combination of Hessian-affine feature finder and SIFT features is most robust to viewpoint change. Harris-affine combined with SIFT and Hessian-affine combined with shape context descriptors were best respectively for lighting change and change in camera focal length. We also find that no detector-descriptor combination performs well with viewpoint changes of more than 25\u201330\u2218."
            },
            "slug": "Evaluation-of-Features-Detectors-and-Descriptors-on-Moreels-Perona",
            "title": {
                "fragments": [],
                "text": "Evaluation of Features Detectors and Descriptors based on 3D Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A method is designed, based on intersecting epipolar constraints, for providing ground truth correspondence automatically, which is based purely on geometric information, and does not rely on the choice of a specific feature appearance descriptor."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 81
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4129821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffd97ed7408fa75b86d4c8e4280f8d730ffa2cf0",
            "isKey": false,
            "numCitedBy": 1304,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied."
            },
            "slug": "Large-Displacement-Optical-Flow:-Descriptor-in-Brox-Malik",
            "title": {
                "fragments": [],
                "text": "Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A way to approach the problem of dense optical flow estimation by integrating rich descriptors into the variational optical flow setting, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17], Middlebury for stereo [41] and optical flow [2] evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 81
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aedb8df8f953429ec5a6df99fda5c5d71dbee4ff",
            "isKey": false,
            "numCitedBy": 2326,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "We compute HOG features [12] on all cropped and resized bounding boxes with 19 \u00d7 13 blocks, 8\u00d78 pixel cells and 12 orientation bins."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29263,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144031005"
                        ],
                        "name": "Andr\u00e9s Bruhn",
                        "slug": "Andr\u00e9s-Bruhn",
                        "structuredName": {
                            "firstName": "Andr\u00e9s",
                            "lastName": "Bruhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9s Bruhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188270"
                        ],
                        "name": "N. Papenberg",
                        "slug": "N.-Papenberg",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Papenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7789445"
                        ],
                        "name": "J. Weickert",
                        "slug": "J.-Weickert",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Weickert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weickert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 76390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91228e00fe33ed6072cfe849ab9e98160461549d",
            "isKey": false,
            "numCitedBy": 2733,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise."
            },
            "slug": "High-Accuracy-Optical-Flow-Estimation-Based-on-a-Brox-Bruhn",
            "title": {
                "fragments": [],
                "text": "High Accuracy Optical Flow Estimation Based on a Theory for Warping"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By proving that this scheme implements a coarse-to-fine warping strategy, this work gives a theoretical foundation for warping which has been used on a mainly experimental basis so far and demonstrates its excellent robustness under noise."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "We first subdivide the training set into 16 orientation classes and use 100 nonoccluded examples per class for training the part-based object detector of [18] using three different settings: We train the model in an unsupervised fashion (variable), by initializing the components to the 16 classes but letting the components vary during optimization (fixed init) and by initializing the components and additionally fixing the latent variables to the 16 classes (fixed)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803584"
                        ],
                        "name": "F. Dornaika",
                        "slug": "F.-Dornaika",
                        "structuredName": {
                            "firstName": "Fadi",
                            "lastName": "Dornaika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dornaika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "Next, we randomly sample 1000 pairs of poses from this sequence and obtain the desired result using [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "We cannot rely on visual correspondences, however, if motion estimates from both sensors are provided, the problem becomes identical to the well-known hand-eye calibration problem, which has been extensively explored in the robotics community [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 657388,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a1037e9087e66b89a8a9ca2cf1492d3c50977312",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Zhuang et al. (1994) proposed a method that allows simultaneous computation of the rigid transformations from world frame to robot base frame and from hand frame to camera frame. Their method attempts to solve a homogeneous matrix equation of the for, AX=ZB. They use quaternions to derive explicit linear solution for X and Z. In this paper, we present two new solutions that attempt to solve the homogeneous matrix equation mentioned above: 1) a closed-form method which uses quaternion algebra and a positive quadratic error function associated with this representation; 2) a method based on nonlinear constrained minimization and which simultaneously solves for rotations and translations. These results may be useful to other problems that can be formulated in the same mathematical form. We perform a sensitivity analysis for both our two methods and the linear method developed by Zhuang et al. This analysis allows the comparison of the three methods. In the light of this comparison, the nonlinear optimization method, which solves for rotations and translations simultaneously, seems to be the most stable one with respect to noise and to measurement errors."
            },
            "slug": "Simultaneous-robot-world-and-hand-eye-calibration-Dornaika-Horaud",
            "title": {
                "fragments": [],
                "text": "Simultaneous robot-world and hand-eye calibration"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two new solutions that attempt to solve the homogeneous matrix equation of the for, AX=ZB are presented: a closed-form method which uses quaternion algebra and a positive quadratic error function associated with this representation; and a method based on nonlinear constrained minimization and which simultaneously solves for rotations and translations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": ", guided cost-volume filtering [38], pixel-wise graph cuts [26]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2457778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22d606ae7351aa4e9598ed6194797d03557abe12",
            "isKey": false,
            "numCitedBy": 1271,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Several new algorithms for visual correspondence based on graph cuts have recently been developed. While these methods give very strong results in practice, they do not handle occlusions properly. Specifically, they treat the two input images asymmetrically, and they do not ensure that a pixel corresponds to at most one pixel in the other image. In this paper, we present a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms. We give experimental results for stereo as well as motion, which demonstrate that our method performs well both at detecting occlusions and computing disparities."
            },
            "slug": "Computing-visual-correspondence-with-occlusions-Kolmogorov-Zabih",
            "title": {
                "fragments": [],
                "text": "Computing visual correspondence with occlusions using graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms, and gives experimental results for stereo as well as motion, which demonstrate that the method performs well both at detecting occlusion and computing disparities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7749011"
                        ],
                        "name": "M. Goebl",
                        "slug": "M.-Goebl",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Goebl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goebl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98762326"
                        ],
                        "name": "G. Farber",
                        "slug": "G.-Farber",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Farber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Farber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 378
                            }
                        ],
                        "text": "We equipped a standard station wagon with two color and two grayscale PointGrey Flea2 video cameras (10 Hz, resolution: 1392\u00d7 512 pixels, opening: 90\u00d7 35), a Velodyne HDL-64E 3D laser scanner (10 Hz, 64 laser beams, range: 100 m), a GPS/IMU localization unit with RTK correction signals (open sky localization errors < 5 cm) and a powerful computer running a real-time database [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17875422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef8559be95cbd55503ef48074118b624917df340",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Cognitive automobiles consist of a set of algorithms that cover a wide range of processing levels: from low-level image acquisition and feature extraction up to situation assessment and decision making. The modules implementing them are naturally characterized by decreasing data rates at higher levels, because raw data is discarded after evaluation, and increasing processing intervals, as knowledge based levels require longer computation times. The architecture presented in this papers offers a method to interchange information with different temporal resolutions liberally among modules with distinct cycle times and realtime demands. It allows effortless buffering of raw data for subsequent data fusion and verification, facilitating innovative processing structures. The paper is completed by measurements demonstrating the achieved real-time capabilities on our selected hardware architecture."
            },
            "slug": "A-Real-Time-capable-Hard-and-Software-Architecture-Goebl-Farber",
            "title": {
                "fragments": [],
                "text": "A Real-Time-capable Hard-and Software Architecture for Joint Image and Knowledge Processing in Cognitive Automobiles"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The architecture presented in this papers offers a method to interchange information with different temporal resolutions liberally among modules with distinct cycle times and realtime demands, allowing effortless buffering of raw data for subsequent data fusion and verification, facilitating innovative processing structures."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116158963"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Nayar",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "COIL-100 [33] 100 72 X 72 bins EPFL Multi-View Car [34] 20 90 X 90 bins Caltech 3D Objects [31] 100 144 X 144 bins Proposed Dataset 2 80,000 X X continuous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58758670,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "77afac8f4d7f47c8b34371d8f8355cefbea1d4f6",
            "isKey": false,
            "numCitedBy": 2004,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Columbia Object Image Library COIL is a database of color images of objects The objects were placed on a motorized turntable against a black background The turntable was rotated through degrees to vary object pose with respect to a xed color camera Images of the objects were taken at pose intervals of degrees This corresponds to poses per object The images were size normalized COIL is available online via ftp"
            },
            "slug": "Columbia-Object-Image-Library-(COIL100)-Nayar",
            "title": {
                "fragments": [],
                "text": "Columbia Object Image Library (COIL100)"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Columbia Object Image Library COIL is a database of color images of objects that were placed on a motorized turntable against a black background and rotated through degrees to vary object pose with respect to a xed color camera."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "For the regression task, Gaussian Process regression [36] performs best."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1430472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82266f6103bade9005ec555ed06ba20b5210ff22",
            "isKey": false,
            "numCitedBy": 17879,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes."
            },
            "slug": "Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics, and deals with the supervised learning problem for both regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "We found that for the classification task SVMs [11] clearly outperform nearest neighbor classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "All Classification Similarity SVM[11] 0."
                    },
                    "intents": []
                }
            ],
            "corpusId": 961425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "isKey": false,
            "numCitedBy": 40077,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "slug": "LIBSVM:-A-library-for-support-vector-machines-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "LIBSVM: A library for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 81
                            }
                        ],
                        "text": "While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [16], we asked the annotators to additionally mark each bounding box as either visible, semioccluded, fully occluded or truncated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "A notable exception is the PASCAL VOC challenge [16] for detection and segmentation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "Our 3D object detection and orientation estimation benchmark is split into three parts: First, we evaluate classical 2D object detection by measuring performance using the well established average precision (AP) metric as described in [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "#labels/category occlusion labels 3D labels orientations Caltech 101 [17] 101 40-800 MIT StreetScenes [3] 9 3,000 LabelMe [39] 3997 60 ETHZ Pedestrian [15] 1 12,000 PASCAL 2011 [16] 20 1,150 X"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes"
            },
            "venue": {
                "fragments": [],
                "text": "Challenge 2011 (VOC2011) Results"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Optical Flow Results for TGV2CENSUS [45]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "However, the top performing approach TGV2CENSUS [45] still produces about 11% of errors on average."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "We observed that classical variational approaches [24, 44, 45] work best on our images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "57 % Optical Flow Non-Occluded All Density TGV2CENSUS [45] 11."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convex Approaches for High Performance Video Processing"
            },
            "venue": {
                "fragments": [],
                "text": "phdthesis, Graz University of Technology,"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141732326"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Our benchmarks are available online at: www.cvlibs.net/datasets/kitti"
                    },
                    "intents": []
                }
            ],
            "corpusId": 63925014,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0ec48ac86456cea3d6d6172ca81ef68e98b21a61",
            "isKey": false,
            "numCitedBy": 3322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-Zhang",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728641"
                        ],
                        "name": "Lubor Ladicky",
                        "slug": "Lubor-Ladicky",
                        "structuredName": {
                            "firstName": "Lubor",
                            "lastName": "Ladicky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubor Ladicky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3274976"
                        ],
                        "name": "Paul Sturgess",
                        "slug": "Paul-Sturgess",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Sturgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Sturgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145485799"
                        ],
                        "name": "Chris Russell",
                        "slug": "Chris-Russell",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35127175"
                        ],
                        "name": "S. Sengupta",
                        "slug": "S.-Sengupta",
                        "structuredName": {
                            "firstName": "Sunando",
                            "lastName": "Sengupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sengupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788636"
                        ],
                        "name": "Y. Bastanlar",
                        "slug": "Y.-Bastanlar",
                        "structuredName": {
                            "firstName": "Yalin",
                            "lastName": "Bastanlar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bastanlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923840"
                        ],
                        "name": "W. Clocksin",
                        "slug": "W.-Clocksin",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Clocksin",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Clocksin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Compared to previous datasets [41, 2, 30, 29], this is the first one with realistic non-synthetic imagery and accurate ground truth."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 52849487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73c707d0fb4da97f6d46750e852b3ead18a79050",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Joint-Optimisation-for-Object-Class-Segmentation-Ladicky-Sturgess",
            "title": {
                "fragments": [],
                "text": "Joint Optimisation for Object Class Segmentation and Dense Stereo Reconstruction"
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335378"
                        ],
                        "name": "H. Hirschm\u00fcller",
                        "slug": "H.-Hirschm\u00fcller",
                        "structuredName": {
                            "firstName": "Heiko",
                            "lastName": "Hirschm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hirschm\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Missing disparities are filled-in for each algorithm using background interpolation [23] to produce dense disparity maps which can then be compared."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18327083,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "04433840795f9add85ac60e98cf4a4b5a14caca1",
            "isKey": false,
            "numCitedBy": 3121,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Semi-Global Matching (SGM) stereo method. It uses a pixelwise, Mutual Information based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. SGM performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement and multi-baseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed.A comparison on standard stereo images shows that SGM is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2s on typical test images. An in depth evaluation of the Mutual Information based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems."
            },
            "slug": "Stereo-Processing-by-Semiglobal-Matching-and-Mutual-Hirschm\u00fcller",
            "title": {
                "fragments": [],
                "text": "Stereo Processing by Semiglobal Matching and Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper describes the Semi-Global Matching (SGM) stereo method, which uses a pixelwise, Mutual Information based matching cost for compensating radiometric differences of input images and demonstrates a tolerance against a wide range of radiometric transformations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convex Approaches for High Performance Video Processing. phdthesis"
            },
            "venue": {
                "fragments": [],
                "text": "Convex Approaches for High Performance Video Processing. phdthesis"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "Purely local methods [5, 38] fail if fronto-parallel surfaces are assumed, as this assumption is often strongly violated in real-world scenes (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 94
                            }
                        ],
                        "text": "For optical flow we evaluate state-of-the-art variational [24, 6, 48, 44, 7, 9, 45] and local [5, 47] methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The opencv library"
            },
            "venue": {
                "fragments": [],
                "text": "Dr. Dobb\u2019s Journal of Software Tools,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The opencv library. Dr. Dobb's Journal of Software Tools"
            },
            "venue": {
                "fragments": [],
                "text": "The opencv library. Dr. Dobb's Journal of Software Tools"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Therefore we rely on a semi-automatic technique: First, we register both sensors using the fully automatic method of [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "the cameras and optimize all parameters by minimizing the average reprojection error [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A toolbox for automatic calibration of range and camera sensors using a single shot"
            },
            "venue": {
                "fragments": [],
                "text": "In ICRA,"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "cars are hard to see? Bad illumination conditions"
            },
            "venue": {
                "fragments": [],
                "text": "cars are hard to see? Bad illumination conditions"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 30,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Are-we-ready-for-autonomous-driving-The-KITTI-suite-Geiger-Lenz/de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42?sort=total-citations"
}