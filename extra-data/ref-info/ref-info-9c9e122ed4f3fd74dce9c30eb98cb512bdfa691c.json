{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177109"
                        ],
                        "name": "Y. Kusachi",
                        "slug": "Y.-Kusachi",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Kusachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kusachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146982808"
                        ],
                        "name": "Akira Suzuki",
                        "slug": "Akira-Suzuki",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akira Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054172965"
                        ],
                        "name": "N. Ito",
                        "slug": "N.-Ito",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1934433"
                        ],
                        "name": "K. Arakawa",
                        "slug": "K.-Arakawa",
                        "structuredName": {
                            "firstName": "Kenichi",
                            "lastName": "Arakawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Arakawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [115])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33756328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5186838748a10808188dfa232c33fb056a8d628",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "With the goal of indexing scene images, we propose a novel recognition method for Kanji characters captured in scene images. Our method scans multi-resolution images and classifies clipped regions with recognition dictionaries generated by learning a large amount of partial patterns of characters with large geometric transformation. The problem of scanning time, which tends to be unpractically long, is solved by using multi-compression coarse-to-fine scanning, and by detecting peak points after coarse searching. Despite the wrong results generated in the background, our method well supports image retrieval since it uses the regular spacing of characters. Experimental results show that this recognition method recognized characters at the rate of 82%. Precision was 84% and recall was 64% for image retrieval."
            },
            "slug": "Kanji-recognition-in-scene-images-without-detection-Kusachi-Suzuki",
            "title": {
                "fragments": [],
                "text": "Kanji recognition in scene images without detection of text fields - robust against variation of viewpoint, contrast, and background texture"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This method scans multi-resolution images and classifies clipped regions with recognition dictionaries generated by learning a large amount of partial patterns of characters with large geometric transformation by using multi-compression coarse-to-fine scanning and detecting peak points after coarse searching."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46422831"
                        ],
                        "name": "Xiaodong Huang",
                        "slug": "Xiaodong-Huang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144258295"
                        ],
                        "name": "Huadong Ma",
                        "slug": "Huadong-Ma",
                        "structuredName": {
                            "firstName": "Huadong",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huadong Ma"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[75] (Harris corner), Huang and Ma [76] (Dense corner point area), Zhao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22956216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9563600a288e58759d85760845d82ef38ce2a665",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Video scene text contains semantic information and thus can contribute significantly to video indexing and summarization. However, most of the previous approaches to detecting scene text from videos experience difficulties in handling texts with various character size and text alignments. In this paper, we propose a novel algorithm of scene text detection and localization in video. Based on our observation that text character strokes show intensive edge details in the fixed orientation no matter what text alignment and size are, a stroke map is first generated. In the scene text detection, we extract the texture feature of stroke map to locate text lines. The detected scene text lines are accurately located by using Harris\u2019 corners in the stroke map. Experimental results show that this approach is robust and can be effectively applied to scene text detection and localization in video."
            },
            "slug": "Automatic-Detection-and-Localization-of-Natural-in-Huang-Ma",
            "title": {
                "fragments": [],
                "text": "Automatic Detection and Localization of Natural Scene Text in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel algorithm of scene text detection and localization in video based on the observation that text character strokes show intensive edge details in the fixed orientation no matter what text alignment and size are is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2010 20th International Conference on Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 11 2] (A da B oo st ),"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, as introduced in survey papers [2, 3] and this chapter, there are many papers which tackle various problems around camera-based OCR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A couple of survey papers are found as [2, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [2], Jung et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": true,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11796155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a8ff86566538103c6116f9047a4c3128e1542c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection,-recognition-in-images-and-video-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Text detection, recognition in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23341564"
                        ],
                        "name": "F. Chassaing",
                        "slug": "F.-Chassaing",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Chassaing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Chassaing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15872163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4f762d9a5acd964411d8c737073c24ce16a3c8",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The systems currently available for content based image and video retrieval work without semantic knowledge, i.e. they use image processing methods to extract low level features of the data. The similarity obtained by these approaches does not always correspond to the similarity a human user would expect. A way to include more semantic knowledge into the indexing process is to use the text included in the images and video sequences. It is rich in information but easy to use, e.g. by key word based queries. In this paper we present an algorithm to localize artificial text in images and videos using a measure of accumulated gradients and morphological post processing to detect the text. The quality of the localized text is improved by robust multiple frame integration. Anew technique for the binarization of the text boxes is proposed. Finally, detection and OCR results for a commercial OCR are presented."
            },
            "slug": "Text-localization,-enhancement-and-binarization-in-Wolf-Jolion",
            "title": {
                "fragments": [],
                "text": "Text localization, enhancement and binarization in multimedia documents"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm to localize artificial text in images and videos using a measure of accumulated gradients and morphological post processing to detect the text is presented and the quality of the localized text is improved by robust multiple frame integration."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37742741"
                        ],
                        "name": "Blake Carpenter",
                        "slug": "Blake-Carpenter",
                        "structuredName": {
                            "firstName": "Blake",
                            "lastName": "Carpenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Blake Carpenter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065131508"
                        ],
                        "name": "Carl Case",
                        "slug": "Carl-Case",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Case",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Case"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39086009"
                        ],
                        "name": "B. Suresh",
                        "slug": "B.-Suresh",
                        "structuredName": {
                            "firstName": "Bipin",
                            "lastName": "Suresh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suresh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41154933"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16657844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12244deb997152492d96c6246ec21b2b9804800d",
            "isKey": false,
            "numCitedBy": 400,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading text from photographs is a challenging problem that has received a significant amount of attention. Two key components of most systems are (i) text detection from images and (ii) character recognition, and many recent methods have been proposed to design better feature representations and models for both. In this paper, we apply methods recently developed in machine learning -- specifically, large-scale algorithms for learning the features automatically from unlabeled data -- and show that they allow us to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end-to-end system."
            },
            "slug": "Text-Detection-and-Character-Recognition-in-Scene-Coates-Carpenter",
            "title": {
                "fragments": [],
                "text": "Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper applies large-scale algorithms for learning the features automatically from unlabeled data to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end-to-end system."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3168007"
                        ],
                        "name": "Egyul Kim",
                        "slug": "Egyul-Kim",
                        "structuredName": {
                            "firstName": "Egyul",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Egyul Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108641611"
                        ],
                        "name": "Seonghun Lee",
                        "slug": "Seonghun-Lee",
                        "structuredName": {
                            "firstName": "Seonghun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seonghun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152672892"
                        ],
                        "name": "J. H. Kim",
                        "slug": "J.-H.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18084451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "736cb6c7c7ba99cb61c4bf6754ede65d7a6f3f5c",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Robust extraction of text from scene images is essential for successful scene text recognition. Scene images usually have non-uniform illumination, complex background, and existence of text-like objects. The common assumption of a homogeneous text region on a nearly uniform background cannot be maintained in real applications. We proposed a text extraction method that utilizes user's hint on the location of the text within the image. A resizable square rim in the viewfinder of the mobile camera, referred to here as a 'focus', is the interface used to help the user indicate the target text. With the hint from the focus, the color of the target text is easily estimated by clustering colors only within the focused section. Image binarization with the estimated color is performed to extract connected components. After obtaining the text region within the focused section, the text region is expanded iteratively by searching neighboring regions with the updated text color. Such an iterative method would prevent the problem of one text region being separated into more than one component due to non-uniform illumination and reflection. A text verification process is conducted on the extracted components to determine the true text region. It is demonstrated that the proposed method achieved high accuracy of text extraction for moderately difficult examples from the ICDAR 2003 database."
            },
            "slug": "Scene-Text-Extraction-Using-Focus-of-Mobile-Camera-Kim-Lee",
            "title": {
                "fragments": [],
                "text": "Scene Text Extraction Using Focus of Mobile Camera"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A text extraction method that utilizes user's hint on the location of the text within the image to achieve high accuracy of text extraction for moderately difficult examples from the ICDAR 2003 database."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 450338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da8154af82fd62944399fc7fad65e44d82ee9ee2",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method for text localization and recognition in real-world images is presented. The proposed method is novel, as it (i) departs from a strict feed-forward pipeline and replaces it by a hypothesesverification framework simultaneously processing multiple text line hypotheses, (ii) uses synthetic fonts to train the algorithm eliminating the need for time-consuming acquisition and labeling of real-world training data and (iii) exploits Maximally Stable Extremal Regions (MSERs) which provides robustness to geometric and illumination conditions. \n \nThe performance of the method is evaluated on two standard datasets. On the Char74k dataset, a recognition rate of 72% is achieved, 18% higher than the state-of-the-art. The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset. The text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "slug": "A-Method-for-Text-Localization-and-Recognition-in-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "A Method for Text Localization and Recognition in Real-World Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset, and the text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206591895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b595c9e969e5605f62da51b6c16dad8aad3e0e",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by \u201cfalse positives\u201d caused by detected watermark text in the dataset."
            },
            "slug": "Real-time-scene-text-localization-and-recognition-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "Real-time scene text localization and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The proposed end-to-end real-time scene text localization and recognition method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end- to-end text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109365947"
                        ],
                        "name": "Xiaoqing Liu",
                        "slug": "Xiaoqing-Liu",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804589"
                        ],
                        "name": "J. Samarabandu",
                        "slug": "J.-Samarabandu",
                        "structuredName": {
                            "firstName": "Jagath",
                            "lastName": "Samarabandu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Samarabandu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[73] (Spectrum of edge image), Liu and Samarabandu [74] (Multiscale edge)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17603163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49b3bfc48a6f9fa03889b219233f5fcc248e747",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in images contains important and useful information. Detection and extraction of text in images have been used in many applications. In this paper, we propose a multiscale edge-based text extraction algorithm, which can automatically detect and extract text in complex images. The proposed method is a general-purpose text detection and extraction algorithm, which can deal not only with printed document images but also with scene text. It is robust with respect to the font size, style, color, orientation, and alignment of text and can be used in a large variety of application fields, such as mobile robot navigation, vehicle license detection and recognition, object identification, document retrieving, page segmentation, etc"
            },
            "slug": "Multiscale-Edge-Based-Text-Extraction-from-Complex-Liu-Samarabandu",
            "title": {
                "fragments": [],
                "text": "Multiscale Edge-Based Text Extraction from Complex Images"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A multiscale edge-based text extraction algorithm, which can automatically detect and extract text in complex images, and is robust with respect to the font size, style, color, orientation, and alignment of text."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144660954"
                        ],
                        "name": "Xuewen Wang",
                        "slug": "Xuewen-Wang",
                        "structuredName": {
                            "firstName": "Xuewen",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuewen Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507765"
                        ],
                        "name": "X. Ding",
                        "slug": "X.-Ding",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107903806"
                        ],
                        "name": "Changsong Liu",
                        "slug": "Changsong-Liu",
                        "structuredName": {
                            "firstName": "Changsong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6748782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e66e806792ee71b8d293c831a0f09ca7c008d79",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "With the proposal of the concept of a \"smart camera\", character recognition in natural scene images has become an interesting but difficult task nowadays. In this paper, we propose an algorithm for extracting characters from text regions of natural scene images with complex backgrounds. Our method first clusters the color feature vectors of the text regions into a number of color classes by applying a modified coarse-fine fuzzy c-means algorithm. Then, different slices are constructed according to these color classes. Characters are eventually extracted from the images using the information of segmentation and recognition. Some experiments have shown that this method is a promising starting point for such applications."
            },
            "slug": "Character-extraction-and-recognition-in-natural-Wang-Ding",
            "title": {
                "fragments": [],
                "text": "Character extraction and recognition in natural scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method first clusters the color feature vectors of the text regions into a number of color classes by applying a modified coarse-fine fuzzy c-means algorithm, and different slices are constructed according to these color classes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5272396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aefca18101764f904edbca6ace7991045f1392e3",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite advances in the archiving of digital video, we are still unable to efficiently search and retrieve the portions that interest us. Video indexing by shot segmentation has been a proposed solution and several research efforts are seen in the literature. Shot segmentation alone cannot solve the problem of content based access to video. Recognition of text in video has been proposed as an additional feature. Several research efforts are found in the literature for text extraction from complex images and video with applications for video indexing. We present an update of our system for detection and extraction of an unconstrained variety of text from general purpose video. The text detection results from a variety of methods are fused and each single text instance is segmented to enable it for OCR. Problems in segmenting text from video are similar to those faced in detection and localization phases. Video has low resolution and the text often has poor contrast with a changing background. The proposed system applies a variety of methods and takes advantage of the temporal redundancy in video resulting in good text segmentation."
            },
            "slug": "Robust-extraction-of-text-in-video-Antani-Crandall",
            "title": {
                "fragments": [],
                "text": "Robust extraction of text in video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An update of the system for detection and extraction of an unconstrained variety of text from general purpose video and takes advantage of the temporal redundancy in video resulting in good text segmentation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38486814"
                        ],
                        "name": "Bongkee Sin",
                        "slug": "Bongkee-Sin",
                        "structuredName": {
                            "firstName": "Bongkee",
                            "lastName": "Sin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bongkee Sin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109631931"
                        ],
                        "name": "Seon-Kyu Kim",
                        "slug": "Seon-Kyu-Kim",
                        "structuredName": {
                            "firstName": "Seon-Kyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seon-Kyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2668078"
                        ],
                        "name": "Beom-Joon Cho",
                        "slug": "Beom-Joon-Cho",
                        "structuredName": {
                            "firstName": "Beom-Joon",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beom-Joon Cho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39283552,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "a5baf0480b140a17339547e24fc397aaf33937d3",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images. The method consists of two steps that can be applied in succession or independently: the frequency of edge pixels across vertical and horizontal scan lines, and the fundamental frequency in the Fourier domain. The frequency feature of text images is highly intuitive, and this is the focus of the research. The detection of rectangles using a Hough transform is also addressed. Texts that are meaningful to many viewers usually appear in rectangles of colours of high contrast to the background. Hence it is natural to assume that the detection of rectangles may be helpful for locating desired texts correctly in natural outdoor scene images."
            },
            "slug": "Locating-characters-in-scene-images-using-frequency-Sin-Kim",
            "title": {
                "fragments": [],
                "text": "Locating characters in scene images using frequency features"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images using the frequency of edge pixels across vertical and horizontal scan lines and the fundamental frequency in the Fourier domain."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061097"
                        ],
                        "name": "N. Ezaki",
                        "slug": "N.-Ezaki",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Ezaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ezaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806816"
                        ],
                        "name": "M. Bulacu",
                        "slug": "M.-Bulacu",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Bulacu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bulacu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799278"
                        ],
                        "name": "Lambert Schomaker",
                        "slug": "Lambert-Schomaker",
                        "structuredName": {
                            "firstName": "Lambert",
                            "lastName": "Schomaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lambert Schomaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 16 ]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", [16,17])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2561294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95e6599c7ac506446c4feefbf5a22841d24b08b0",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons. This paper describes the system design and evaluates several character extraction methods. Automatic text recognition from natural images receives a growing attention because of potential applications in image retrieval, robotics and intelligent transport system. Camera-based document analysis becomes a real possibility with the increasing resolution and availability of digital cameras. However, in the case of a blind person, finding the text region is the first important problem that must be addressed, because it cannot be assumed that the acquired image contains only characters. At first, our system tries to find in the image areas with small characters. Then it zooms into the found areas to retake higher resolution images necessary for character recognition. In the present paper, we propose four character-extraction methods based on connected components. We tested the effectiveness of our methods on the ICDAR 2003 Robust Reading Competition data. The performance of the different methods depends on character size. In the data, bigger characters are more prevalent and the most effective extraction method proves to be the sequence: Sobel edge detection, Otsu binarization, connected component extraction and rule-based connected component filtering."
            },
            "slug": "Text-detection-from-natural-scene-images:-towards-a-Ezaki-Bulacu",
            "title": {
                "fragments": [],
                "text": "Text detection from natural scene images: towards a system for visually impaired persons"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons and evaluates several character extraction methods based on connected components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31446769"
                        ],
                        "name": "M. Sawaki",
                        "slug": "M.-Sawaki",
                        "structuredName": {
                            "firstName": "Minako",
                            "lastName": "Sawaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sawaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82910116"
                        ],
                        "name": "H. Murase",
                        "slug": "H.-Murase",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Murase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781078"
                        ],
                        "name": "N. Hagita",
                        "slug": "N.-Hagita",
                        "structuredName": {
                            "firstName": "Norihiro",
                            "lastName": "Hagita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hagita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38279239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c572f4c48f2d26f23957a013c8780261444f422",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes a method for adaptively acquiring templates for degraded characters in scene images. Characters in scene images are often degraded because of poor printing and viewing conditions. To cope with the degradation problem, we proposed the idea of \"context-based image templates\" which include neighboring characters of parts thereof and so represent more contextual information than single-letter templates. However, our previous method manually selects the learning samples to make the context-based image templates and is time-consuming. Therefore, we attempt to make the context-based image templates automatically from single-letter templates and learning text-line images. The context-based image templates are iteratively created using the k-nearest neighbor rule. Experiments with 3,467 alpha-numeric characters in nine bookshelf images show that the high recognition rates for test samples possible with this method asymptotically approach those achieved with manual selection."
            },
            "slug": "Automatic-acquisition-of-context-based-images-for-Sawaki-Murase",
            "title": {
                "fragments": [],
                "text": "Automatic acquisition of context-based images templates for degraded character recognition in scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work attempts to make the context-based image templates automatically from single-letter templates and learning text-line images and is iteratively created using the k-nearest neighbor rule."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Clark and Mirmehdi [54] have proposed a method on this approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1028027,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54c72b7999fe51f7054c73683034941c78183093",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for extracting text from images where the text plane is not necessarily fronto-parallel to the camera. Initially, we locate local image features such as borders and page edges. We then use perceptual grouping on these features to find rectangular regions in the scene. These regions are hypothesized to be pages or planes that may contain text. Edge distributions are then used for the assessment of these potential regions, providing a measure of confidence. It will be shown that the text may then be transformed to a fronto- parallel view suitable, for example, for an OCR system or other higher level recognition. The proposed method is scale independent (of the size of the text). We illustrate the algorithm using various examples."
            },
            "slug": "Location-and-recovery-of-text-on-oriented-surfaces-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Location and recovery of text on oriented surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method for extracting text from images where the text plane is not necessarily fronto-parallel to the camera, and it will be shown that the text may then be transformed to a backo- parallel view suitable for an OCR system or other higher level recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061097"
                        ],
                        "name": "N. Ezaki",
                        "slug": "N.-Ezaki",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Ezaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ezaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488101"
                        ],
                        "name": "K. Kiyota",
                        "slug": "K.-Kiyota",
                        "structuredName": {
                            "firstName": "Kimiyasu",
                            "lastName": "Kiyota",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kiyota"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236058"
                        ],
                        "name": "B. T. Minh",
                        "slug": "B.-T.-Minh",
                        "structuredName": {
                            "firstName": "Bui",
                            "lastName": "Minh",
                            "middleNames": [
                                "Truong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. T. Minh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806816"
                        ],
                        "name": "M. Bulacu",
                        "slug": "M.-Bulacu",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Bulacu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bulacu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799278"
                        ],
                        "name": "Lambert Schomaker",
                        "slug": "Lambert-Schomaker",
                        "structuredName": {
                            "firstName": "Lambert",
                            "lastName": "Schomaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lambert Schomaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[71] (Bimodality by Fisher\u2019s Discriminant Rate)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5910360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44650dd32ba20eb1073d4bfb290d07c4ea8b466e",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text recognition from natural images receives a growing attention because of potential applications in image retrieval, robotics and intelligent transport system. Camera-based document analysis becomes a real possibility with the increasing resolution and availability of digital cameras. Our research objective is a system that reads the text encountered in natural scenes with the aim to provide assistance to visually impaired persons. In the case of a blind person, finding the text region is the first important problem that must be addressed, because it cannot be assumed that the acquired image contains only characters. In a previous paper (N. Ezaki et al., 2004), we propose four text-detection methods based on connected components. Finding small characters needed significant improvement. This paper describes a new text-detection method geared for small text characters. This method uses Fisher's discriminant rate (FDR) to decide whether an image area should be binarized using local or global thresholds. Fusing the new method with a previous morphology-based one yields improved results. Using a controllable Webcam and a laptop PC, we developed a prototype that works in real time. At first, our system tries to find in the image areas with small characters. Then it zooms into the found areas to retake higher resolution images necessary for character recognition. Going from this proof-of-concept to a complete system requires further research effort."
            },
            "slug": "Improved-text-detection-methods-for-a-camera-based-Ezaki-Kiyota",
            "title": {
                "fragments": [],
                "text": "Improved text-detection methods for a camera-based text reading system for blind persons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new text-detection method geared for small text characters that uses Fisher's discriminant rate (FDR) to decide whether an image area should be binarized using local or global thresholds and fusion with a previous morphology-based method yields improved results."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144740362"
                        ],
                        "name": "Hiromitsu Miyazaki",
                        "slug": "Hiromitsu-Miyazaki",
                        "structuredName": {
                            "firstName": "Hiromitsu",
                            "lastName": "Miyazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiromitsu Miyazaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2779846"
                        ],
                        "name": "H. Sakoe",
                        "slug": "H.-Sakoe",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Sakoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sakoe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[139]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recognition of a text captured by a handheld video camera Combination of video mosaicing and text recognition A simultaneous optimization of video mosaicing and text recognition is found in [139] Uchida et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9503084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7d091332f068c4b08d8da5dddd6f375d0b3f66a",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mosaicing-by-recognition-for-video-based-text-Uchida-Miyazaki",
            "title": {
                "fragments": [],
                "text": "Mosaicing-by-recognition for video-based text recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624076"
                        ],
                        "name": "Q. Luong",
                        "slug": "Q.-Luong",
                        "structuredName": {
                            "firstName": "Quang-Tuan",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48804780"
                        ],
                        "name": "James A. Herson",
                        "slug": "James-A.-Herson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Herson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Herson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Different from [57], this method tries to utilize directions of multiple text lines for estimating perspective distortion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This two-step approach is similar to [57]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[57] have proposed"
                    },
                    "intents": []
                }
            ],
            "corpusId": 29394851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4599b80a96821ed9276476edd17c6d70380f150c",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Real-world text on street signs, nameplates, etc. often lies in an oblique plane and hence cannot be recognized by traditional OCR systems due to perspective distortion. Furthermore, such text often comprises only one or two lines, preventing the use of existing perspective rectification methods that were primarily designed for images of document pages. We propose an approach that reliably rectifies and subsequently recognizes individual lines of text. Our system, which includes novel algorithms for extraction of text from real-world scenery, perspective rectification, and binarization, has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "slug": "Rectification-and-recognition-of-text-in-3-D-scenes-Myers-Bolles",
            "title": {
                "fragments": [],
                "text": "Rectification and recognition of text in 3-D scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an approach that reliably rectifies and subsequently recognizes individual lines of text in real-world text that has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38340927"
                        ],
                        "name": "Yi-Feng Pan",
                        "slug": "Yi-Feng-Pan",
                        "structuredName": {
                            "firstName": "Yi-Feng",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Feng Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761961"
                        ],
                        "name": "Xinwen Hou",
                        "slug": "Xinwen-Hou",
                        "structuredName": {
                            "firstName": "Xinwen",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinwen Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10018912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "751266eaeeacfe73d9cf879e905b17387bae6037",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a robust system to accurately detect and localize texts in natural scene images. For text detection, a region-based method utilizing multiple features and cascade AdaBoost classifier is adopted. For text localization, a window grouping method integrating text line competition analysis is used to generate text lines. Then within each text line, local binarization is used to extract candidate connected components (CCs) and non-text CCs are filtered out by Markov Random Fields (MRF) model, through which text line can be localized accurately. Experiments on the public benchmark ICDAR 2003 Robust Reading and Text Locating Dataset show that our system is comparable to the best existing methods both in accuracy and speed."
            },
            "slug": "A-Robust-System-to-Detect-and-Localize-Texts-in-Pan-Hou",
            "title": {
                "fragments": [],
                "text": "A Robust System to Detect and Localize Texts in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A region-based method utilizing multiple features and cascade AdaBoost classifier is adopted for text detection and a window grouping method integrating text line competition analysis is used to generate text lines."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [114])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331461"
                        ],
                        "name": "S. Hanif",
                        "slug": "S.-Hanif",
                        "structuredName": {
                            "firstName": "Shehzad",
                            "lastName": "Hanif",
                            "middleNames": [
                                "Muhammad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554802"
                        ],
                        "name": "L. Prevost",
                        "slug": "L.-Prevost",
                        "structuredName": {
                            "firstName": "Lionel",
                            "lastName": "Prevost",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Prevost"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17474464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5233651e7c6436ce63d24b8d74a03a34925d09b6",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have proposed a complete system for text detection and localization in gray scale scene images. A boosting framework integrating feature and weak classifier selection based on computational complexity is proposed to construct efficient text detectors. The proposed scheme uses a small set of heterogeneous features which are spatially combined to build a large set of features. A neural network based localizer learns necessary rules for localization. The evaluation is done on the challenging ICDAR 2003 robust reading and text locating database. The results are encouraging and our system can localize text of various font sizes and styles in complex background."
            },
            "slug": "Text-Detection-and-Localization-in-Complex-Scene-Hanif-Prevost",
            "title": {
                "fragments": [],
                "text": "Text Detection and Localization in Complex Scene Images using Constrained AdaBoost Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A boosting framework integrating feature and weak classifier selection based on computational complexity is proposed to construct efficient text detectors and a neural network based localizer learns necessary rules for localization in gray scale scene images."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3186240"
                        ],
                        "name": "Weihua Huang",
                        "slug": "Weihua-Huang",
                        "structuredName": {
                            "firstName": "Weihua",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weihua Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13084689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "578bfdbc4a5c1a228dedbe8b6b1e95cae56259ec",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Accurate-video-text-detection-through-of-low-and-Shivakumara-Huang",
            "title": {
                "fragments": [],
                "text": "Accurate video text detection through classification of low and high contrast images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757655"
                        ],
                        "name": "S. Bhattacharjee",
                        "slug": "S.-Bhattacharjee",
                        "structuredName": {
                            "firstName": "Sushil",
                            "lastName": "Bhattacharjee",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhattacharjee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Wavelet/ Gabor Pros: Multiresolution is suitable to scale-invariant character detection Jain and Bhattacharjee [89] (Gabor) Haritaoglu [13] (Edgeness by DWT), Saoi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13639250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "808f205f834cfdfa823b97fbf1f20bf82ddaa8d7",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a considerable interest in designing automatic systems that will scan a given paper document and store it on electronic media for easier storage, manipulation, and access. Most documents contain graphics and images in addition to text. Thus, the document image has to be segmented to identify the text regions, so that OCR techniques may be applied only to those regions. In this paper, we present a simple method for document image segmentation in which text regions in a given document image are automatically identified. The proposed segmentation method for document images is based on a multichannel filtering approach to texture segmentation. The text in the document is considered as a textured region. Nontext contents in the document, such as blank spaces, graphics, and pictures, are considered as regions with different textures. Thus, the problem of segmenting document images into text and nontext regions can be posed as a texture segmentation problem. Two-dimensional Gabor filters are used to extract texture features for each of these regions. These filters have been extensively used earlier for a variety of texture segmentation tasks. Here we apply the same filters to the document image segmentation problem. Our segmentation method does not assume any a priori knowledge about the content or font styles of the document, and is shown to work even for skewed images and handwritten text. Results of the proposed segmentation method are presented for several test images which demonstrate the robustness of this technique."
            },
            "slug": "Text-segmentation-using-gabor-filters-for-automatic-Jain-Bhattacharjee",
            "title": {
                "fragments": [],
                "text": "Text segmentation using gabor filters for automatic document processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a simple method for document image segmentation in which text regions in a given document image are automatically identified and is shown to work even for skewed images and handwritten text."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572873"
                        ],
                        "name": "K. C. Kim",
                        "slug": "K.-C.-Kim",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kim",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. C. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144036125"
                        ],
                        "name": "H. Byun",
                        "slug": "H.-Byun",
                        "structuredName": {
                            "firstName": "Hyeran",
                            "lastName": "Byun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Byun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111340535"
                        ],
                        "name": "Y. Song",
                        "slug": "Y.-Song",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Song",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2167591743"
                        ],
                        "name": "Young-Woo Choi",
                        "slug": "Young-Woo-Choi",
                        "structuredName": {
                            "firstName": "Young-Woo",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young-Woo Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066686052"
                        ],
                        "name": "S. Chi",
                        "slug": "S.-Chi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chi",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144368895"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kye",
                            "lastName": "Kim",
                            "middleNames": [
                                "Kyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2133127067"
                        ],
                        "name": "Y. Chung",
                        "slug": "Y.-Chung",
                        "structuredName": {
                            "firstName": "YunKoo",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[93], Chen et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45144473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b9cc09a70e4ea58efc232ed5f04ff401dfd880d",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method that extracts text regions in natural scene images using low-level image features and that verifies the extracted regions through a high-level text stroke feature. Then the two level features are combined hierarchically. The low-level features are color continuity, gray-level variation and color variance. The color continuity is used since most of the characters in a text region have the same color, and the gray-level variation is used since the text strokes are distinctive to the background in their gray-level values. Also, the color variance is used since the text strokes are distinctive in their colors to the background, and this value is more sensitive than the gray-level variations. As a high level feature, text stroke is examined using multi-resolution wavelet transforms on local image areas and the feature vector is input to a SVM (support vector machine) for verification. We tested the proposed method with various kinds of the natural scene images and confirmed that extraction rates are high even in complex images."
            },
            "slug": "Scene-text-extraction-in-natural-scene-images-using-Kim-Byun",
            "title": {
                "fragments": [],
                "text": "Scene text extraction in natural scene images using hierarchical feature combining and verification"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The proposed method that extracts text regions in natural scene images using low-level image features and that verifies the extracted regions through a high-level text stroke feature confirmed that extraction rates are high even in complex images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705095"
                        ],
                        "name": "H. Aso",
                        "slug": "H.-Aso",
                        "structuredName": {
                            "firstName": "Hirotomo",
                            "lastName": "Aso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Heuristics Connect neighboring candidate regions if they satisfy handmade rules Pros: Simple Cons: Manual parameter setting is necessary Goto and Aso [131], Wang and Kangas [122], Epshtein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25163389,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "547405c82d54406af9d0f72d323b6a8a9579a39c",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Recent remarkable progress in computer systems and printing devices has made it easier to produce printed documents with various designs. Text characters are often printed on colored backgrounds, and sometimes on complex backgrounds such as photographs, computer graphics, etc. Some methods have been developed for character pattern extraction from document images and scene images with complex backgrounds. However, the previous methods are suitable only for extracting rather large characters, and the processes often fail to extract small characters with thin strokes. This paper proposes a new method by which character patterns can be extracted from document images with complex backgrounds. The method is based on local multilevel thresholding and pixel labeling, and region growing. This framework is very useful for extracting character patterns from badly illuminated document images. The performance of extracting small character patterns has been improved by suppressing the influence of mixed-color pixels around character edges. Experimental results show that the method is capable of extracting very small character patterns from main text blocks in various documents, separating characters and complex backgrounds, as long as the thickness of the character strokes is more than about 1.5 pixels."
            },
            "slug": "Character-pattern-extraction-from-documents-with-Goto-Aso",
            "title": {
                "fragments": [],
                "text": "Character pattern extraction from documents with complex backgrounds"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experimental results show that the proposed new method is capable of extracting very small character patterns from main text blocks in various documents, separating characters and complex backgrounds, as long as the thickness of the character strokes is more than about 1.5 pixels."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098811521"
                        ],
                        "name": "Min Su Cho",
                        "slug": "Min-Su-Cho",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cho",
                            "middleNames": [
                                "Su"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Su Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875555"
                        ],
                        "name": "Jae-Hyun Seok",
                        "slug": "Jae-Hyun-Seok",
                        "structuredName": {
                            "firstName": "Jae-Hyun",
                            "lastName": "Seok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jae-Hyun Seok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108641611"
                        ],
                        "name": "Seonghun Lee",
                        "slug": "Seonghun-Lee",
                        "structuredName": {
                            "firstName": "Seonghun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seonghun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152672892"
                        ],
                        "name": "J. H. Kim",
                        "slug": "J.-H.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 489940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d0d14a13a07863c90cf3a77494d725a81f0e857",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Features and relationships based on character color, edge, stroke and context plays a role for text extraction in natural scene images, but any single feature or relationship is not enough to do the job. This paper presents a novel approach for combining features and relationships within the Conditional Random Field (CRF) framework. By a simple homogeneity measure, an input image is over segmented into perceptually meaningful super pixels and then the text extraction task is formulated as a problem of super pixel labeling. Such a formulation allows us to achieve parameter learning from training images and probabilistic inferences by combining all the features and relationships of the input image. The proposed method shows high performance, in terms of quality, on both the KAIST scene text DB and the ICDAR 2003 DB."
            },
            "slug": "Scene-Text-Extraction-by-Superpixel-CRFs-Combining-Cho-Seok",
            "title": {
                "fragments": [],
                "text": "Scene Text Extraction by Superpixel CRFs Combining Multiple Character Features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a novel approach for combining features and relationships within the Conditional Random Field (CRF) framework, and shows high performance, in terms of quality, on both the KAIST scene text DB and the ICDAR 2003 DB."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2641821"
                        ],
                        "name": "Kongqiao Wang",
                        "slug": "Kongqiao-Wang",
                        "structuredName": {
                            "firstName": "Kongqiao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kongqiao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145800809"
                        ],
                        "name": "J. Kangas",
                        "slug": "J.-Kangas",
                        "structuredName": {
                            "firstName": "Jari",
                            "lastName": "Kangas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kangas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Heuristics Connect neighboring candidate regions if they satisfy handmade rules Pros: Simple Cons: Manual parameter setting is necessary Goto and Aso [131], Wang and Kangas [122], Epshtein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25684247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c374e691d990e68a655457df43255d7ce4ea0a0c",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Character-location-in-scene-images-from-digital-Wang-Kangas",
            "title": {
                "fragments": [],
                "text": "Character location in scene images from digital camera"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3146185"
                        ],
                        "name": "Yuki Shigeyoshi",
                        "slug": "Yuki-Shigeyoshi",
                        "structuredName": {
                            "firstName": "Yuki",
                            "lastName": "Shigeyoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuki Shigeyoshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347079"
                        ],
                        "name": "Yasuhiro Kunishige",
                        "slug": "Yasuhiro-Kunishige",
                        "structuredName": {
                            "firstName": "Yasuhiro",
                            "lastName": "Kunishige",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiro Kunishige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806377"
                        ],
                        "name": "Yaokai Feng",
                        "slug": "Yaokai-Feng",
                        "structuredName": {
                            "firstName": "Yaokai",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaokai Feng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6173711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f8caf4f53fac193dfc799bafaa8d34f586a70d",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new approach toward scenery character detection. This is a key point-based approach where local features and a saliency map are fully utilized. Local features, such as SIFT and SURF, have been commonly used for computer vision and object pattern recognition problems, however, they have been rarely employed in character recognition and detection problems. Local feature, however, is similar to directional features, which have been employed in character recognition applications. In addition, local feature can detect corners and thus it is suitable for detecting characters, which are generally comprised of many corners. For evaluating the performance of the local feature, an experimental result was done and its results showed that SURF, i.e., a simple gradient feature, can detect about 70% of characters in scenery images. Then the saliency map was employed as an additional feature to the local feature. This trial is based on the expectation that scenery characters are generally printed to be salient and thus higher salient area will have a higher probability to be a character area. An experimental result showed that this expectation was reasonable and we can have better discrimination accuracy with the saliency map."
            },
            "slug": "A-Keypoint-Based-Approach-toward-Scenery-Character-Uchida-Shigeyoshi",
            "title": {
                "fragments": [],
                "text": "A Keypoint-Based Approach toward Scenery Character Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new approach toward scenery character detection is proposed where local features and a saliency map are fully utilized and the expectation that scenery characters are generally printed to be salient and thus higher salient area will have a higher probability to be a character area is shown."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681758"
                        ],
                        "name": "Wumo Pan",
                        "slug": "Wumo-Pan",
                        "structuredName": {
                            "firstName": "Wumo",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wumo Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144957197"
                        ],
                        "name": "T. Bui",
                        "slug": "T.-Bui",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Bui",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 230672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "443bf555d3ac1b78d90715ef09228833622e3530",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A sparse representation based method is proposed for text detection from scene images. We start with edge information extracted using Canny operator and then group these edge points into connected components. Each connected component is labeled as text or non-text by a two-level labeling process: pixel level labeling and connected component labeling. The core of the labeling process is a sparsity test using an over-complete dictionary, which is learned from edge segments of isolated character images. Layout analysis is further applied to verify these text candidates. Experimental results show that improvements in both recall rate and detection accuracy in text detection have been achieved."
            },
            "slug": "Text-detection-from-scene-images-using-sparse-Pan-Bui",
            "title": {
                "fragments": [],
                "text": "Text detection from scene images using sparse representation"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A sparse representation based method is proposed for text detection from scene images that starts with edge information extracted using Canny operator and then group these edge points into connected components that are labeled as text or non-text by a two-level labeling process."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13597946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f6886a7a70c800aa3c899d16e784a611dc9ba2",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-identification-and-skew-estimation-of-in-Messelodi-Modena",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144228782"
                        ],
                        "name": "T. Campos",
                        "slug": "T.-Campos",
                        "structuredName": {
                            "firstName": "Te\u00f3filo",
                            "lastName": "Campos",
                            "middleNames": [
                                "Em\u00eddio",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Campos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6516879"
                        ],
                        "name": "Bodla Rakesh Babu",
                        "slug": "Bodla-Rakesh-Babu",
                        "structuredName": {
                            "firstName": "Bodla",
                            "lastName": "Babu",
                            "middleNames": [
                                "Rakesh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodla Rakesh Babu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4826173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbbd5fdc09349bbfdee7aa7365a9d37716852b32",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper tackles the problem of recognizing characters in images of natural scenes. In particular, we focus on recognizing characters in situations that would traditionally not be handled well by OCR techniques. We present an annotated database of images containing English and Kannada characters. The database comprises of images of street scenes taken in Bangalore, India using a standard camera. The problem is addressed in an object cateogorization framework based on a bag-of-visual-words representation. We assess the performance of various features based on nearest neighbour and SVM classification. It is demonstrated that the performance of the proposed method, using as few as 15 training images, can be far superior to that of commercial OCR systems. Furthermore, the method can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "slug": "Character-Recognition-in-Natural-Images-Campos-Babu",
            "title": {
                "fragments": [],
                "text": "Character Recognition in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the performance of the proposed method can be far superior to that of commercial OCR systems, and can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43395565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c4ed0ef1c978defe1c44868029790aaad21752",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-digital-news-archive-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for digital news archive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation filter, multi-frame integration and a combination of four filters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704629"
                        ],
                        "name": "M. Anthimopoulos",
                        "slug": "M.-Anthimopoulos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Anthimopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthimopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748249"
                        ],
                        "name": "I. Pratikakis",
                        "slug": "I.-Pratikakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pratikakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratikakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[85] (LBP)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 18523324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "980087708326d4c4411ca2b76a9f2afb3f68238b",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-stage-scheme-for-text-detection-in-video-Anthimopoulos-Gatos",
            "title": {
                "fragments": [],
                "text": "A two-stage scheme for text detection in video images"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10764222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7fdc23beb7fb1344a80be00a951e9c0f155774e",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "As camera resolution increases, high-speed non-contact text capture through a digital camera is opening up a new channel for document capture and understanding. Unfortunately, perspective and geometric distortions in camera image of documents make it hard to recognize the document content properly. In this paper, we propose a character recognition technique, which is capable of recognizing camera text lying over a planar or smoothly curved surface in perspective views. In our proposed method, a few perspective invariants including character ascender and descender, centroid intersection numbers, and water reservior are first detected. Camera texts are then recognized using a classification and regression tree (CART) structure. Experimental results show our method is fast and improves recognition performance greatly"
            },
            "slug": "Camera-Text-Recognition-based-on-Perspective-Lu-Tan",
            "title": {
                "fragments": [],
                "text": "Camera Text Recognition based on Perspective Invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A character recognition technique, which is capable of recognizing camera text lying over a planar or smoothly curved surface in perspective views, and which is fast and improves recognition performance greatly."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[94] (Edge + color), Pan et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6109448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5295b6770ebbbc27a4651ed44b4b7e184d884f8e",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection. We have applied the approach in developing a Chinese sign translation system, which can automatically detect and recognize Chinese signs as input from a camera, and translate the recognized text into English."
            },
            "slug": "Automatic-detection-and-recognition-of-signs-from-Chen-Yang",
            "title": {
                "fragments": [],
                "text": "Automatic detection and recognition of signs from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900372"
                        ],
                        "name": "Wonjun Kim",
                        "slug": "Wonjun-Kim",
                        "structuredName": {
                            "firstName": "Wonjun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonjun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145568138"
                        ],
                        "name": "Changick Kim",
                        "slug": "Changick-Kim",
                        "structuredName": {
                            "firstName": "Changick",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changick Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[78], Wong and Chen [79], Kim and Kim [80] (Precise gradient around character edge), Phan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10712944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61c41f1cea644ea2d65455f9c3277ffe3e35aff2",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Overlay text brings important semantic clues in video content analysis such as video information retrieval and summarization, since the content of the scene or the editor's intention can be well represented by using inserted text. Most of the previous approaches to extracting overlay text from videos are based on low-level features, such as edge, color, and texture information. However, existing methods experience difficulties in handling texts with various contrasts or inserted in a complex background. In this paper, we propose a novel framework to detect and extract the overlay text from the video scene. Based on our observation that there exist transient colors between inserted text and its adjacent background, a transition map is first generated. Then candidate regions are extracted by a reshaping method and the overlay text regions are determined based on the occurrence of overlay text in each candidate. The detected overlay text regions are localized accurately using the projection of overlay text pixels in the transition map and the text extraction is finally conducted. The proposed method is robust to different character size, position, contrast, and color. It is also language independent. Overlay text region update between frames is also employed to reduce the processing time. Experiments are performed on diverse videos to confirm the efficiency of the proposed method."
            },
            "slug": "A-New-Approach-for-Overlay-Text-Detection-and-From-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A New Approach for Overlay Text Detection and Extraction From Complex Video Scene"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a novel framework to detect and extract the overlay text from the video scene that is robust to different character size, position, contrast, and color, and is also language independent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390869207"
                        ],
                        "name": "Xu Zhao",
                        "slug": "Xu-Zhao",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148774182"
                        ],
                        "name": "Kai-Hsiang Lin",
                        "slug": "Kai-Hsiang-Lin",
                        "structuredName": {
                            "firstName": "Kai-Hsiang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Hsiang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145692782"
                        ],
                        "name": "Yun Fu",
                        "slug": "Yun-Fu",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yun Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689532"
                        ],
                        "name": "Yuxiao Hu",
                        "slug": "Yuxiao-Hu",
                        "structuredName": {
                            "firstName": "Yuxiao",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxiao Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117416965"
                        ],
                        "name": "Yuncai Liu",
                        "slug": "Yuncai-Liu",
                        "structuredName": {
                            "firstName": "Yuncai",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuncai Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[77] (Harris corner)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11816081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcbdd4ea38c2617f7a52e05b647f547d8a7dc60e",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text and caption from videos is important and in great demand for video retrieval, annotation, indexing, and content analysis. In this paper, we present a corner based approach to detect text and caption from videos. This approach is inspired by the observation that there exist dense and orderly presences of corner points in characters, especially in text and caption. We use several discriminative features to describe the text regions formed by the corner points. The usage of these features is in a flexible manner, thus, can be adapted to different applications. Language independence is an important advantage of the proposed method. Moreover, based upon the text features, we further develop a novel algorithm to detect moving captions in videos. In the algorithm, the motion features, extracted by optical flow, are combined with text features to detect the moving caption patterns. The decision tree is adopted to learn the classification criteria. Experiments conducted on a large volume of real video shots demonstrate the efficiency and robustness of our proposed approaches and the real-world system. Our text and caption detection system was recently highlighted in a worldwide multimedia retrieval competition, Star Challenge, by achieving the superior performance with the top ranking."
            },
            "slug": "Text-From-Corners:-A-Novel-Approach-to-Detect-Text-Zhao-Lin",
            "title": {
                "fragments": [],
                "text": "Text From Corners: A Novel Approach to Detect Text and Caption in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A corner based approach to detect text and caption from videos inspired by the observation that there exist dense and orderly presences of corner points in characters, especially in text and title, which was recently highlighted in a worldwide multimedia retrieval competition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[92] (Gabor)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5416971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b2a523d48cee04c09c327e14fb8928c5feff03c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition (STR) is the recognition of text anywhere in the environment, such as signs and storefronts. Relative to document recognition, it is challenging because of font variability, minimal language context, and uncontrolled conditions. Much information available to solve this problem is frequently ignored or used sequentially. Similarity between character images is often overlooked as useful information. Because of language priors, a recognizer may assign different labels to identical characters. Directly comparing characters to each other, rather than only a model, helps ensure that similar instances receive the same label. Lexicons improve recognition accuracy but are used post hoc. We introduce a probabilistic model for STR that integrates similarity, language properties, and lexical decision. Inference is accelerated with sparse belief propagation, a bottom-up method for shortening messages by reducing the dependency between weakly supported hypotheses. By fusing information sources in one model, we eliminate unrecoverable errors that result from sequential processing, improving accuracy. In experimental results recognizing text from images of signs in outdoor scenes, incorporating similarity reduces character recognition error by 19 percent, the lexicon reduces word recognition error by 35 percent, and sparse belief propagation reduces the lexicon words considered by 99.9 percent with a 12X speedup and no loss in accuracy."
            },
            "slug": "Scene-Text-Recognition-Using-Similarity-and-a-with-Weinman-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition Using Similarity and a Lexicon with Sparse Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model for scene text recognition is introduced that integrates similarity, language properties, and lexical decision and is fusing information sources in one model to eliminate unrecoverable errors that result from sequential processing, improving accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171087367"
                        ],
                        "name": "Lianli Xu",
                        "slug": "Lianli-Xu",
                        "structuredName": {
                            "firstName": "Lianli",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianli Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144183522"
                        ],
                        "name": "H. Nagayoshi",
                        "slug": "H.-Nagayoshi",
                        "structuredName": {
                            "firstName": "Hiroto",
                            "lastName": "Nagayoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagayoshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1957622"
                        ],
                        "name": "H. Sako",
                        "slug": "H.-Sako",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Sako",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sako"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[112] (MRF), Wang and Belongie [82], Cho et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29383070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2111ba073b31df1feaa09e47a7a7bc3052013f51",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Character recognition in complex real scene images is a very challenging undertaking. The most popular approach is to segment the text area using some extra pre-knowledge, such as \"characters are in a signboard'', etc. This approach makes it possible to construct a very time-consuming method, but generality is still a problem. In this paper, we propose a more general method by utilizing only character features. Our algorithm consists of five steps: pre-processing to extract connected components, initial classification using primitive rules, strong classification using AdaBoost, Markov random field (MRF) clustering to combine connected components with similar properties, and post-processing using optical character recognition (OCR) results. The results of experiments using 11 images containing 1691 characters (including characters in bad condition) indicated the effectiveness of the proposed system, namely, that 52.9% of characters were extracted correctly with 625 noise components extracted as characters."
            },
            "slug": "Kanji-Character-Detection-from-Complex-Real-Scene-Xu-Nagayoshi",
            "title": {
                "fragments": [],
                "text": "Kanji Character Detection from Complex Real Scene Images based on Character Properties"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a more general character recognition method by utilizing only character features, which consists of five steps: pre-processing to extract connected components, initial classification using primitive rules, strong classification using AdaBoost, Markov random field (MRF) clustering to combine connected components with similar properties, and post-processing using optical character recognition (OCR) results."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117646"
                        ],
                        "name": "Jiang Gao",
                        "slug": "Jiang-Gao",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16214442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "758db8b752a0ec38d8df9dc6d8e81bfdfb7289a1",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new adaptive algorithm for automatic detection of text from a natural scene. The initial cues of text regions are first detected from the captured image/video. An adaptive color modeling and searching algorithm is then utilized near the initial text cues, to discriminate text/non-text regions. EM optimization algorithm is used for color modeling, under the constraint of text layout relations for a specific language. The proposed algorithm combines the advantages of several previous approaches for text detection, and utilizes a focus-of-attention approach for text finding. The whole algorithm is applied in a prototype system that can automatically detect and recognize sign input from a video camera, and translate the signs into English text or voice streams. We present evaluation results of our algorithm on this system."
            },
            "slug": "An-adaptive-algorithm-for-text-detection-from-Gao-Yang",
            "title": {
                "fragments": [],
                "text": "An adaptive algorithm for text detection from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new adaptive algorithm for automatic detection of text from a natural scene that combines the advantages of several previous approaches for text detection, and utilizes a focus-of-attention approach for text finding is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1968869"
                        ],
                        "name": "Tomoyuki Saoi",
                        "slug": "Tomoyuki-Saoi",
                        "structuredName": {
                            "firstName": "Tomoyuki",
                            "lastName": "Saoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomoyuki Saoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115790073"
                        ],
                        "name": "Hiroaki Kobayashi",
                        "slug": "Hiroaki-Kobayashi",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Kobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Kobayashi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10745173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d23db78964f8f70021fb3706767c0578c0d6b265",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Texts in natural scenes provide us with much useful information. In order to use such information automatically, it is necessary to make computers detect text regions in the images. Gllavata et. al. proposed a method based on unsupervised classification of high frequency wavelet coefficients for text detection in video frames [Gllavata et. al. (2004)]. Although the method is very accurate, it does not work so well with some color images, since it lacks the ability of discriminating color difference. This paper proposes an enhanced version of the method. We develop a new unsupervised clustering technique for the classification of multi-channel wavelet features to deal with color images. Experimental results show that the new method yields better results for color scene images."
            },
            "slug": "Text-detection-in-color-scene-images-based-on-of-Saoi-Goto",
            "title": {
                "fragments": [],
                "text": "Text detection in color scene images based on unsupervised clustering of multi-channel wavelet features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new unsupervised clustering technique for the classification of multi-channel wavelet features to deal with color images is developed and Experimental results show that the new method yields better results for color scene images."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5728901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66b71064b99331f908b60cb6d138f2ebea5bdcca",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition has gained significant attention from the computer vision community in recent years. Recognizing such text is a challenging problem, even more so than the recognition of scanned documents. In this work, we focus on the problem of recognizing text extracted from street images. We present a framework that exploits both bottom-up and top-down cues. The bottom-up cues are derived from individual character detections from the image. We build a Conditional Random Field model on these detections to jointly model the strength of the detections and the interactions between them. We impose top-down cues obtained from a lexicon-based prior, i.e. language statistics, on the model. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We show significant improvements in accuracies on two challenging public datasets, namely Street View Text (over 15%) and ICDAR 2003 (nearly 10%)."
            },
            "slug": "Top-down-and-bottom-up-cues-for-scene-text-Mishra-Karteek",
            "title": {
                "fragments": [],
                "text": "Top-down and bottom-up cues for scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a framework that exploits both bottom-up and top-down cues in the problem of recognizing text extracted from street images, and shows significant improvements in accuracies on two challenging public datasets, namely Street View Text and ICDAR 2003."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[78], Wong and Chen [79], Kim and Kim [80] (Precise gradient around character edge), Phan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20048852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5b1f30c7f3f172c6e230a49315c8c0040a6c4b4",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-robust-algorithm-for-video-text-extraction-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A new robust algorithm for video text extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42539643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aaca7527d703a6945ba73ce15e7e7353258fc8a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-text-extraction-with-selective-metric-based-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction with selective metric-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2435807"
                        ],
                        "name": "U. Bhattacharya",
                        "slug": "U.-Bhattacharya",
                        "structuredName": {
                            "firstName": "Ujjwal",
                            "lastName": "Bhattacharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Bhattacharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702114"
                        ],
                        "name": "S. K. Parui",
                        "slug": "S.-K.-Parui",
                        "structuredName": {
                            "firstName": "Swapan",
                            "lastName": "Parui",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Parui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31968222"
                        ],
                        "name": "Srikanta Mondal",
                        "slug": "Srikanta-Mondal",
                        "structuredName": {
                            "firstName": "Srikanta",
                            "lastName": "Mondal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srikanta Mondal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14041586,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a916e623ef3136e96c4fd918629d3f57b665808a",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of digital cameras attached with various handheld devices, many new computational challenges have gained significance. One such problem is extraction of texts from natural scene images captured by such devices. The extracted text can be sent to OCR or to a text-to-speech engine for recognition. In this article, we propose a novel and effective scheme based on analysis of connected components for extraction of Devanagari and Bangla texts from camera captured scene images. A common unique feature of these two scripts is the presence of headline and the proposed scheme uses mathematical morphology operations for their extraction. Additionally, we consider a few criteria for robust filtering of text components from such scene images. Moreover, we studied the problem of binarization of such scene images and observed that there are situations when repeated binarization by a well-known global thresholding approach is effective. We tested our algorithm on a repository of 100 scene images containing texts of Devanagari and / or Bangla."
            },
            "slug": "Devanagari-and-Bangla-Text-Extraction-from-Natural-Bhattacharya-Parui",
            "title": {
                "fragments": [],
                "text": "Devanagari and Bangla Text Extraction from Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel and effective scheme based on analysis of connected components for extraction of Devanagari and Bangla texts from camera captured scene images and it is observed that there are situations when repeated binarization by a well-known global thresholding approach is effective."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76548769a142f858acf9d32e9bc4a2c5445fc9de",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33852772"
                        ],
                        "name": "H. Kuwano",
                        "slug": "H.-Kuwano",
                        "structuredName": {
                            "firstName": "Hidetaka",
                            "lastName": "Kuwano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuwano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113938"
                        ],
                        "name": "Y. Taniguchi",
                        "slug": "Y.-Taniguchi",
                        "structuredName": {
                            "firstName": "Yukinobu",
                            "lastName": "Taniguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Taniguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153027997"
                        ],
                        "name": "Hiroyuki Arai",
                        "slug": "Hiroyuki-Arai",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroyuki Arai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113759578"
                        ],
                        "name": "M. Mori",
                        "slug": "M.-Mori",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267020"
                        ],
                        "name": "S. Kurakake",
                        "slug": "S.-Kurakake",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Kurakake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurakake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054536982"
                        ],
                        "name": "H. Kojima",
                        "slug": "H.-Kojima",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kojima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[72] (Edge pair on scanline), Sin et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26591876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddcc1bae71d70c52be48301c2fe45bc570d2e69b",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval. Superimposed texts are important as they provide semantic information about scene contents. Their attributes such as fonts, size, and position in a frame are important as they are carefully designed by the video editor and so reflect the intent of captioning. In news programs, for instance, the headline text is displayed in larger fonts than the subtitles. Our system takes into account not only the texts themselves but also their attributes for structuring videos. We describe: (i) novel methods for detecting and extracting texts that are robust against the presence of complex backgrounds and intensity degradation of the character patterns, and (ii) a method for structuring a video based on the text attributes."
            },
            "slug": "Telop-on-demand:-video-structuring-and-retrieval-on-Kuwano-Taniguchi",
            "title": {
                "fragments": [],
                "text": "Telop-on-demand: video structuring and retrieval based on text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval and a method for structuring a video based on the text attributes."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10251113"
                        ],
                        "name": "C. Jacobs",
                        "slug": "C.-Jacobs",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40163092"
                        ],
                        "name": "James Rinker",
                        "slug": "James-Rinker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Rinker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11552850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a70bebec3fd698e63e036981a5b7c7c37523bc15",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Cheap and versatile cameras make it possible to easily and quickly capture a wide variety of documents. However, low resolution cameras present a challenge to OCR because it is virtually impossible to do character segmentation independently from recognition. In this paper we solve these problems simultaneously by applying methods borrowed from cursive handwriting recognition. To achieve maximum robustness, we use a machine learning approach based on a convolutional neural network. When our system is combined with a language model using dynamic programming, the overall performance is in the vicinity of 80-95% word accuracy on pages captured with a 1024/spl times/768 webcam and 10-point text."
            },
            "slug": "Text-recognition-of-low-resolution-document-images-Jacobs-Simard",
            "title": {
                "fragments": [],
                "text": "Text recognition of low-resolution document images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper uses a machine learning approach based on a convolutional neural network to achieve maximum robustness in OCR, and when combined with a language model using dynamic programming, the overall performance is in the vicinity of 80-95% word accuracy on pages captured with a 1024/spl times/768 webcam and 10-point text."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108381415"
                        ],
                        "name": "Ben M. Chen",
                        "slug": "Ben-M.-Chen",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688202"
                        ],
                        "name": "C. Ko",
                        "slug": "C.-Ko",
                        "structuredName": {
                            "firstName": "Chi",
                            "lastName": "Ko",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The difference from [59] is that their method estimates horizontal"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[59], stroke boundary and top and bottom tip points are first detected at each character."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14897061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b7891c1c270fbf696a06052be437919f5b078a",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perspective-rectification-of-document-images-using-Lu-Chen",
            "title": {
                "fragments": [],
                "text": "Perspective rectification of document images using fuzzy set and morphological operations"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "H an if an d Pr ev os t[ 11 3] (A da B oo st ),"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[3 1, 32 ] is of te n em pl oy ed B er tin ie ta l."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, as introduced in survey papers [2, 3] and this chapter, there are many papers which tackle various problems around camera-based OCR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5053740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82b6f95e805a92887f8efccf5a0dc8d5783676f5",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development."
            },
            "slug": "Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras, and some sample applications under development and feasible ideas for future development is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A simplest version of super-resolution is simple averaging of multiple frames [34, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5452218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d4e472e87cb932c52b5f59abc6007ba904e648a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a superresolution-based text enhancement scheme to improve optical character recognition (OCR) and readability of text in digital video. The quality of video text is degraded by factors such as low resolution, antialiasing, clutter and blurring. We use the fact that the same text string often exists in consecutive frames to explore the temporal information and enhance the text image. For graphic text, we register text blocks to subpixel accuracy and fuse them to a new block with high resolution and a cleaner background. We use a projection onto convex sets based method to deblur scene text to improve readability. Experimental results show our scheme can improve OCR for graphic text and readability for scene text significantly."
            },
            "slug": "Superresolution-based-enhancement-of-text-in-video-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Superresolution-based enhancement of text in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Experimental results show the scheme can improve OCR for graphic text and readability for scene text significantly and register text blocks to subpixel accuracy and fuse them to a new block with high resolution and a cleaner background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109862994"
                        ],
                        "name": "Sunil Kumar",
                        "slug": "Sunil-Kumar",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunil Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110344379"
                        ],
                        "name": "Rajat Gupta",
                        "slug": "Rajat-Gupta",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajat Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48676526"
                        ],
                        "name": "N. Khanna",
                        "slug": "N.-Khanna",
                        "structuredName": {
                            "firstName": "Nitin",
                            "lastName": "Khanna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Khanna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144725842"
                        ],
                        "name": "S. Chaudhury",
                        "slug": "S.-Chaudhury",
                        "structuredName": {
                            "firstName": "Santanu",
                            "lastName": "Chaudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705669"
                        ],
                        "name": "S. Joshi",
                        "slug": "S.-Joshi",
                        "structuredName": {
                            "firstName": "Shiv",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Dutt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[91] (Matched DWT), Weinman et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1223283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909f2c6dec43e702d425b6e5166043d878e42996",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we have proposed a novel scheme for the extraction of textual areas of an image using globally matched wavelet filters. A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images. We have extended our text extraction scheme for the segmentation of document images into text, background, and picture components (which include graphics and continuous tone images). Multiple, two-class Fisher classifiers have been used for this purpose. We also exploit contextual information by using a Markov random field formulation-based pixel labeling scheme for refinement of the segmentation results. Experimental results have established effectiveness of our approach."
            },
            "slug": "Text-Extraction-and-Document-Image-Segmentation-and-Kumar-Gupta",
            "title": {
                "fragments": [],
                "text": "Text Extraction and Document Image Segmentation Using Matched Wavelets and MRF Model"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images and a text extraction scheme for the segmentation of document images into text, background, and picture components is extended."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8249814"
                        ],
                        "name": "Xujun Peng",
                        "slug": "Xujun-Peng",
                        "structuredName": {
                            "firstName": "Xujun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xujun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39784761"
                        ],
                        "name": "Huaigu Cao",
                        "slug": "Huaigu-Cao",
                        "structuredName": {
                            "firstName": "Huaigu",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huaigu Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36073757"
                        ],
                        "name": "R. Prasad",
                        "slug": "R.-Prasad",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145603129"
                        ],
                        "name": "P. Natarajan",
                        "slug": "P.-Natarajan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Natarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Natarajan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40964563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "573e882c39be7ea52a60af0f01211e598dfd130c",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an approach to extract text from broadcast videos. Candidate blocks are detected based on edge extraction results. Corners and geometrical features are used for the purpose of initial classification which is carried out by using a support vector machine (SVM). Considering the spatial inter-dependencies of different regions in the image, we propose a novel conditional random field (CRF) based framework which integrates the outputs of SVM into the system to improve the accuracy of labeling for blocks. The experimental results show that the proposed system achieves reliable performance for text detection/extraction from videos."
            },
            "slug": "Text-Extraction-from-Video-Using-Conditional-Random-Peng-Cao",
            "title": {
                "fragments": [],
                "text": "Text Extraction from Video Using Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel conditional random field (CRF) based framework which integrates the outputs of SVM into the system to improve the accuracy of labeling for blocks to achieve reliable performance for text detection/extraction from videos."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8742498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5c4a0067f01260c9d91fca79473b740552e19a3",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a scene text extraction system for handheld devices to provide enhanced information perception services to the user. It uses a color camera attached to a personal digital assistant as an input device to capture scene images from the real world and it employs image enhancement and segmentation methods to extract written information from the scene, convert them to text information and show them to the user so that he/she can see both the real world and information together. We implemented a prototype application: an automatic sign/text language translation for foreign travelers, where people can use the system whenever they want to see text or signs in their own language where they are originally written in a foreign language in the scene."
            },
            "slug": "Scene-text-extraction-and-translation-for-handheld-Haritaoglu",
            "title": {
                "fragments": [],
                "text": "Scene text extraction and translation for handheld devices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An automatic sign/text language translation for foreign travelers, where people can use the system whenever they want to see text or signs in their own language where they are originally written in a foreign language in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0c4294d517de3243fd4f8a09359e265fbf2f1ea",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an efficient text detection method based on the Laplacian operator. The maximum gradient difference value is computed for each pixel in the Laplacian-filtered image. K-means is then used to classify all the pixels into two clusters: text and non-text. For each candidate text region, the corresponding region in the Sobel edge map of the input image undergoes projection profile analysis to determine the boundary of the text blocks. Finally, we employ empirical rules to eliminate false positives based on geometrical properties. Experimental results show that the proposed method is able to detect text of different fonts, contrast and backgrounds. Moreover, it outperforms three existing methods in terms of detection and false positive rates."
            },
            "slug": "A-Laplacian-Method-for-Video-Text-Detection-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "A Laplacian Method for Video Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed text detection method outperforms three existing methods in terms of detection and false positive rates and employs empirical rules to eliminate false positives based on geometrical properties."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[102] have tried to detect texts by finding planar areas using a shape-frommotion technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5447028,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "db1a2b260892554c47d0205e4535da21e5d3de63",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores an approach for extracting scene text from a sequence of images with relative motion between the camera and the scene. It is assumed that the scene text lies on planar surfaces, whereas the other features are likely to be at random depths or undergoing independent motion. The motion model parameters of these planar surfaces are estimated using gradient based methods, and multiple motion segmentation. The equations of the planar surfaces, as well as the camera motion parameters are extracted by combining the motion models of multiple planar surfaces. This approach is expected to improve the reliability and robustness of the estimates, which are used to perform perspective correction on the individual surfaces. Perspective correction can lead to improvement in OCR performance. This work can be useful for detecting road signs and bill-boards from a moving vehicle."
            },
            "slug": "Application-of-planar-motion-segmentation-for-scene-Gandhi-Kasturi",
            "title": {
                "fragments": [],
                "text": "Application of planar motion segmentation for scene text extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An approach for extracting scene text from a sequence of images with relative motion between the camera and the scene by combining the motion models of multiple planar surfaces to improve the reliability and robustness of the estimates, which are used to perform perspective correction on the individual surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143780910"
                        ],
                        "name": "R. Huang",
                        "slug": "R.-Huang",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18192274"
                        ],
                        "name": "Shinpei Oba",
                        "slug": "Shinpei-Oba",
                        "structuredName": {
                            "firstName": "Shinpei",
                            "lastName": "Oba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinpei Oba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6846176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9930f865f02b6626762f6a3dae29aa0574414f3e",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "To handle the diversity of scene characters, we propose a multiple hypotheses framework which consists of an image operator set module, an optical character recognition (OCR) module, and an integration module. Image operators detect multiple suspicious character areas. The OCR engine is then applied to each detected area and returns multiple candidates with weight values for future integration. Without the aid of heuristic constraints on area, aspect ratio or color etc., the integration module prunes the redundant detection and pads the missing detection based on the outputs of OCR. The experimental results demonstrate that the whole multiple hypotheses outperforms each operator's hypotheses and be comparable with existing methods in terms of recall, precision, F-measure and recognition rate."
            },
            "slug": "Scene-character-detection-and-recognition-based-on-Huang-Oba",
            "title": {
                "fragments": [],
                "text": "Scene character detection and recognition based on multiple hypotheses framework"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The experimental results demonstrate that the whole multiple hypotheses outperforms each operator's hypotheses and be comparable with existing methods in terms of recall, precision, F-measure and recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11631790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b552f05038ad3f93dca68d044b0f093d95e42c9",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel algorithm, based on stroke components and descriptive Gabor filters, to detect text regions in natural scene images. Text characters and strings are constructed by stroke components as basic units. Gabor filters are used to describe and analyze the stroke components in text characters or strings. We define a suitability measurement to analyze the confidence of Gabor filters in describing stroke component and the suitability of Gabor filters on an image window. From the training set, we compute a set of Gabor filters that can describe principle stroke components of text by their parameters. Then a K-means algorithm is applied to cluster the descriptive Gabor filters. The clustering centers are defined as Stroke Gabor Words (SGWs) to provide a universal description of stroke components. By suitability evaluation on positive and negative training samples respectively, each SGW generates a pair of characteristic distributions of suitability measurements. On a testing natural scene image, heuristic layout analysis is applied first to extract candidate image windows. Then we compute the principle SGWs for each image window to describe its principle stroke components. Characteristic distributions generated by principle SGWs are used to classify text or non-text windows. Experimental results on benchmark datasets demonstrate that our algorithm can handle complex backgrounds and variant text patterns (font, color, scale, etc.)."
            },
            "slug": "Text-Detection-in-Natural-Scene-Images-by-Stroke-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text Detection in Natural Scene Images by Stroke Gabor Words"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A novel algorithm, based on stroke components and descriptive Gabor filters, to detect text regions in natural scene images and can handle complex backgrounds and variant text patterns (font, color, scale, etc.)."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12945125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac435a179a3308a045f43908f57884b3ed10b56b",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paper, we propose a camera-based assistive system for visually impaired or blind persons to read text from signage and objects that are held in the hand. The system is able to read text from complex backgrounds and then communicate this information aurally. To localize text regions in images with complex backgrounds, we design a novel text localization algorithm by learning gradient features of stroke orientations and distributions of edge pixels in an Adaboost model. Text characters in the localized regions are recognized by off-the-shelf optical character recognition (OCR) software and transformed into speech outputs. The performance of the proposed system is evaluated on ICDAR 2003 Robust Reading Dataset. Experimental results demonstrate that our algorithm outperforms previous algorithms on some measures. Our prototype system was further evaluated on a dataset collected by 10 blind persons, with the system effectively reading text from complex backgrounds."
            },
            "slug": "Assistive-Text-Reading-from-Complex-Background-for-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Assistive Text Reading from Complex Background for Blind Persons"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A camera-based assistive system for visually impaired or blind persons to read text from signage and objects that are held in the hand and then communicate this information aurally, which outperforms previous algorithms on some measures."
            },
            "venue": {
                "fragments": [],
                "text": "CBDAR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8908523"
                        ],
                        "name": "R. Beaufort",
                        "slug": "R.-Beaufort",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Beaufort",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beaufort"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1736591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7675d08a1a90b51e50d16dd5b9c857e0a8e0e533",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing market of cheap cameras, natural scene text has to be handled in an efficient way. Some works deal with text detection in the image while more recent ones point out the challenge of text extraction and recognition. We propose here an OCR correction system to handle traditional issues of recognizer errors but also the ones due to natural scene images, i.e. cut characters, artistic display, incomplete sentences (present in advertisements) and out- of-vocabulary (OOV) words such as acronyms and so on. The main algorithm bases on finite-state machines (FSMs) to deal with learned OCR confusions, capital/accented letters and lexicon look-up. Moreover, as OCR is not considered as a black box, several outputs are taken into account to intermingle recognition and correction steps. Based on a public database of natural scene words, detailed results are also presented along with future works."
            },
            "slug": "A-Weighted-Finite-State-Framework-for-Correcting-in-Beaufort-Mancas-Thillou",
            "title": {
                "fragments": [],
                "text": "A Weighted Finite-State Framework for Correcting Errors in Natural Scene OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An OCR correction system to handle traditional issues of recognizer errors but also the ones due to natural scene images, i.e. cut characters, artistic display, incomplete sentences and out- of-vocabulary (OOV) words such as acronyms and so on."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146275501"
                        ],
                        "name": "Yi Ma",
                        "slug": "Yi-Ma",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14015069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "955028d46ab7237a30cfaab3a351c34f38ee0be5",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes."
            },
            "slug": "Detecting-texts-of-arbitrary-orientations-in-images-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "Detecting texts of arbitrary orientations in natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system which detects texts of arbitrary orientations in natural images using a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts to better evaluate its algorithm and compare it with other competing algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34180232"
                        ],
                        "name": "Yuval Netzer",
                        "slug": "Yuval-Netzer",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Netzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Netzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726358"
                        ],
                        "name": "A. Bissacco",
                        "slug": "A.-Bissacco",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bissacco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bissacco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144397975"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16852518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks."
            },
            "slug": "Reading-Digits-in-Natural-Images-with-Unsupervised-Netzer-Wang",
            "title": {
                "fragments": [],
                "text": "Reading Digits in Natural Images with Unsupervised Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new benchmark dataset for research use is introduced containing over 600,000 labeled digits cropped from Street View images, and variants of two recently proposed unsupervised feature learning methods are employed, finding that they are convincingly superior on benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693680"
                        ],
                        "name": "N. Chaddha",
                        "slug": "N.-Chaddha",
                        "structuredName": {
                            "firstName": "Navin",
                            "lastName": "Chaddha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chaddha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153216710"
                        ],
                        "name": "R. Sharma",
                        "slug": "R.-Sharma",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075308385"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Avneesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041740460"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58358347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9754dba62f5a329c9dd64b7d091c7900f489f5f0",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Block based algorithms have found widespread use in image and video compression. However, popular algorithms such as JPEG, which are very effective in compressing continuous tone images, do not perform well with mixed-mode images which have a substantial text component. With a growing number of applications where such images occur, e.g., color facsimile, digital libraries and educational videos, there are advantages in being able to classify each block as being text or continuous tone. With such a classification, different compression parameters or even algorithms may be employed for the two kinds of data to obtain high compression with minimal loss in visual quality. In this paper we analyze and compare four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods. Our evaluation of each scheme is based on the accuracy of segmentation, robustness across different types of images and sensitivity to the threshold used for segmentation. Our results show that DCT based segmentation offers the best accuracy and robustness. Another advantage of DCT is that it is compatible with standards like JPEG, MPEG and H.261.<<ETX>>"
            },
            "slug": "Text-segmentation-in-mixed-mode-images-Chaddha-Sharma",
            "title": {
                "fragments": [],
                "text": "Text segmentation in mixed-mode images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper analyzes and compares four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods, and shows that DCTbased segmentation offers the best accuracy and robustness."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118671548"
                        ],
                        "name": "Chuang Li",
                        "slug": "Chuang-Li",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507765"
                        ],
                        "name": "X. Ding",
                        "slug": "X.-Ding",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115859002"
                        ],
                        "name": "Youshou Wu",
                        "slug": "Youshou-Wu",
                        "structuredName": {
                            "firstName": "Youshou",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youshou Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41953629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162a9556143bdbde27ca76440bbe0d61cf6ee13f",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A new connected component based segmentation algorithm which automatically extracts text regions from natural scene images is proposed in this paper. This approach utilizes a multichannel decomposition method to locate text blocks in complex backgrounds. Block alignment analysis and recognition confidence values are used in the combination and identification of the connected components. The algorithm is applied to a test image database and shows promising results."
            },
            "slug": "Automatic-text-location-in-natural-scene-images-Li-Ding",
            "title": {
                "fragments": [],
                "text": "Automatic text location in natural scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new connected component based segmentation algorithm which automatically extracts text regions from natural scene images is proposed in this paper, utilizing a multichannel decomposition method to locate text blocks in complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463454"
                        ],
                        "name": "H. Koo",
                        "slug": "H.-Koo",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Koo",
                            "middleNames": [
                                "Il"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116515452"
                        ],
                        "name": "Jinho Kim",
                        "slug": "Jinho-Kim",
                        "structuredName": {
                            "firstName": "Jinho",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinho Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707645"
                        ],
                        "name": "N. Cho",
                        "slug": "N.-Cho",
                        "structuredName": {
                            "firstName": "Nam",
                            "lastName": "Cho",
                            "middleNames": [
                                "Ik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[65] have proposed a similar image composition method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11687774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb5bf2036e4f5bc6088b50de46fd848c0a7a966",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an algorithm to compose a geometrically dewarped and visually enhanced image from two document images taken by a digital camera at different angles. Unlike the conventional works that require special equipments or assumptions on the contents of books or complicated image acquisition steps, we estimate the unfolded book or document surface from the corresponding points between two images. For this purpose, the surface and camera matrices are estimated using structure reconstruction, 3-D projection analysis, and random sample consensus-based curve fitting with the cylindrical surface model. Because we do not need any assumption on the contents of books, the proposed method can be applied not only to optical character recognition (OCR), but also to the high-quality digitization of pictures in documents. In addition to the dewarping for a structurally better image, image mosaic is also performed for further improving the visual quality. By finding better parts of images (with less out of focus blur and/or without specular reflections) from either of views, we compose a better image by stitching and blending them. These processes are formulated as energy minimization problems that can be solved using a graph cut method. Experiments on many kinds of book or document images show that the proposed algorithm robustly works and yields visually pleasing results. Also, the OCR rate of the resulting image is comparable to that of document images from a flatbed scanner."
            },
            "slug": "Composition-of-a-Dewarped-and-Enhanced-Document-Two-Koo-Kim",
            "title": {
                "fragments": [],
                "text": "Composition of a Dewarped and Enhanced Document Image From Two View Images"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An algorithm to compose a geometrically dewarped and visually enhanced image from two document images taken by a digital camera at different angles is proposed and robustly works and yields visually pleasing results."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[87], Goto [88]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10576633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc0d458369772f8328128bb4edc93325ffe5c424",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze some spatial frequency-based features used for text region detection in natural scene images, and redefine the DCT-based feature. We employ Fisher\u2019s discriminant analysis to improve the DCT-based feature and to achieve higher accuracy. An unsupervised thresholding method for discriminating text and non-text regions is introduced and tested as well. Experimental results show that a wide high frequency band, covering some lower-middle frequency components, is generally more suitable for scene text detection despite the original definition of the DCT-based feature."
            },
            "slug": "Redefining-the-DCT-based-feature-for-scene-text-Goto",
            "title": {
                "fragments": [],
                "text": "Redefining the DCT-based feature for scene text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results show that a wide high frequency band, covering some lower-middle frequency components, is generally more suitable for scene text detection despite the original definition of the DCT-based feature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49794847"
                        ],
                        "name": "Xiaogang Chen",
                        "slug": "Xiaogang-Chen",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706670"
                        ],
                        "name": "Xiangjian He",
                        "slug": "Xiangjian-He",
                        "structuredName": {
                            "firstName": "Xiangjian",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangjian He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688428"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144066903"
                        ],
                        "name": "Qiang Wu",
                        "slug": "Qiang-Wu",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 623667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1262307c2b17313c0522383d669705e9954eac49",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Deblurring camera-based document image is an important task in digital document processing, since it can improve both the accuracy of optical character recognition systems and the visual quality of document images. Traditional deblurring algorithms have been proposed to work for natural-scene images. However the natural-scene images are not consistent with document images. In this paper, the distinct characteristics of document images are investigated. We propose a content-aware prior for document image deblurring. It is based on document image foreground segmentation. Besides, an upper-bound constraint combined with total variation based method is proposed to suppress the rings in the deblurred image. Comparing with the traditional general purpose deblurring methods, the proposed deblurring algorithm can produce more pleasing results on document images. Encouraging experimental results demonstrate the efficacy of the proposed method."
            },
            "slug": "An-effective-document-image-deblurring-algorithm-Chen-He",
            "title": {
                "fragments": [],
                "text": "An effective document image deblurring algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A content-aware prior is proposed based on document image foreground segmentation based on an upper-bound constraint combined with total variation based method to suppress the rings in the deblurred image."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38606738"
                        ],
                        "name": "Tomohiko Tsuji",
                        "slug": "Tomohiko-Tsuji",
                        "structuredName": {
                            "firstName": "Tomohiko",
                            "lastName": "Tsuji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomohiko Tsuji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17702037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd50d71bb3486d17ccea8c487c29d417eacada35",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses how to quickly recognize a character pattern using a lot of case examples without learning. Here without learning means just finding the most similar example from the case examples, and pretend as if the OCR understands the definition of the character. This strategy is expected to work well in most cases with a large dataset, however, also expected to take a lot of time for finding the most similar example. In this paper, we show that a lot of case examples can be processed in a short time. As a testbed, we handle recognition problem of camera-captured printed characters. Using a database storing 100 fonts, the proposed method achieved 97.0% of recognition rate for images captured from the right angle and 95.8% for those from 45 deg. with 4.56ms of processing time, that is about 220 characters per second including every process."
            },
            "slug": "Memory-based-recognition-of-camera-captured-Iwamura-Tsuji",
            "title": {
                "fragments": [],
                "text": "Memory-based recognition of camera-captured characters"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that a lot of case examples can be processed in a short time, and the proposed method achieved 97.0% of recognition rate for images captured from the right angle and 95.8% for those from 45 deg."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062011"
                        ],
                        "name": "Tomokazu Sato",
                        "slug": "Tomokazu-Sato",
                        "structuredName": {
                            "firstName": "Tomokazu",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomokazu Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2167126"
                        ],
                        "name": "Sei Ikeda",
                        "slug": "Sei-Ikeda",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Ikeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sei Ikeda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796418"
                        ],
                        "name": "M. Kanbara",
                        "slug": "M.-Kanbara",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Kanbara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kanbara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327451"
                        ],
                        "name": "A. Iketani",
                        "slug": "A.-Iketani",
                        "structuredName": {
                            "firstName": "Akihiko",
                            "lastName": "Iketani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Iketani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3010963"
                        ],
                        "name": "N. Nakajima",
                        "slug": "N.-Nakajima",
                        "structuredName": {
                            "firstName": "Noboru",
                            "lastName": "Nakajima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nakajima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771769"
                        ],
                        "name": "N. Yokoya",
                        "slug": "N.-Yokoya",
                        "structuredName": {
                            "firstName": "Naokazu",
                            "lastName": "Yokoya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Yokoya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831345"
                        ],
                        "name": "Keiji Yamada",
                        "slug": "Keiji-Yamada",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keiji Yamada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5662150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "633f446d95344e508192f591d92abf3f0fad8af6",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, document and photograph digitization from a paper is very important for digital archiving and personal data transmission through the internet. Though many people wish to digitize documents on a paper easily, now heavy and large image scanners are required to obtain high quality digitization. To realize easy and high quality digitization of documents and photographs, we propose a novel digitization method that uses a movie captured by a hand-held camera. In our method, first, 6-DOF(Degree Of Freedom) position and posture parameters of the mobile camera are estimated in each frame by tracking image features automatically. Next, re-appearing feature points in the image sequence are detected and stitched for minimizing accumulated estimation errors. Finally, all the images are merged as a high-resolution mosaic image using the optimized parameters. Experiments have successfully demonstrated the feasibility of the proposed method. Our prototype system can acquire initial estimates of extrinsic camera parameters in real-time with capturing images."
            },
            "slug": "High-resolution-video-mosaicing-for-documents-and-Sato-Ikeda",
            "title": {
                "fragments": [],
                "text": "High-resolution video mosaicing for documents and photos by estimating camera motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a novel digitization method that uses a movie captured by a hand-held camera to realize easy and high quality digitization of documents and photographs."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A simplest version of super-resolution is simple averaging of multiple frames [34, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18856719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1484e9fc337e99fbd70ece46005c7cf6a8541bb7",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a multiple frame based technique to enhance text in digital video is presented. After extracting a reference text block, we use an image matching technique to find the corresponding text blocks in consecutive frames. We register these text blocks to subpixel levels by using image interpolation techniques to improve both correspondence and text resolution. The registered text blocks are averaged to obtain a new text block with a clean background and a higher resolution. Experiments conducted on several video sequences show that our enhancement scheme can improve the accuracy of commercial off-the-shelf OCR considerably."
            },
            "slug": "Text-enhancement-in-digital-video-using-multiple-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Text enhancement in digital video using multiple frame integration"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments conducted on several video sequences show that the multiple frame based enhancement scheme can improve the accuracy of commercial off-the-shelf OCR considerably."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740235"
                        ],
                        "name": "S. Omachi",
                        "slug": "S.-Omachi",
                        "structuredName": {
                            "firstName": "Shinichiro",
                            "lastName": "Omachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113537246"
                        ],
                        "name": "Masaki Inoue",
                        "slug": "Masaki-Inoue",
                        "structuredName": {
                            "firstName": "Masaki",
                            "lastName": "Inoue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masaki Inoue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705095"
                        ],
                        "name": "H. Aso",
                        "slug": "H.-Aso",
                        "structuredName": {
                            "firstName": "Hirotomo",
                            "lastName": "Aso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another idea is to understand the global character structure [137]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[137], Yokobayashi and Wakahara [147]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite of the importance of this strategy, there have been only a few research trials, such as [137]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12247389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d30c850734d3facadd8bd45ed1db4dfdb1e69290",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Decorated characters are widely used in various documents. Practical optical character reader is required to deal with not only common fonts but also complex designed fonts. However, since the appearances of decorated characters are complicated, most general character recognition systems cannot give good performances on decorated characters. In this paper, an algorithm that can extract character's essential structure from a decorated character is proposed. This algorithm is applied in preprocessing of character recognition. The proposed algorithm consists of three procedures: global structure extraction, interpolation of structure and smoothing. By using multiscale images, topographical features, such as ridges and ravines are detected for structure extraction. Ridges are used for extracting global structure and ravines are used for interpolation. Experimental results show character structures can be clearly extracted from very complex decorated characters."
            },
            "slug": "Structure-Extraction-from-Decorated-Characters-Omachi-Inoue",
            "title": {
                "fragments": [],
                "text": "Structure Extraction from Decorated Characters Using Multiscale Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show character structures can be clearly extracted from very complex decorated characters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068391558"
                        ],
                        "name": "Katherine Donaldson",
                        "slug": "Katherine-Donaldson",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Donaldson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Donaldson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38494850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3fcb864548d6fdbc6a70bde8d31d91d829bfa1a",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "To increase the range of sizes of video scene text recognizable by optical character recognition (OCR), we developed a Bayesian super-resolution algorithm that uses a text-specific bimodal prior. We evaluated the effectiveness of the bimodal prior, compared with and in conjunction with a piecewise smoothness prior, visually and by measuring the accuracy of the OCR results on the variously super-resolved images. The bimodal prior improved the readability of 4- to 7-pixel-high scene text significantly better than bicubic interpolation, and increased the accuracy of OCR results better than the piecewise smoothness prior."
            },
            "slug": "Bayesian-Super-Resolution-of-Text-in-Video-with-a-Donaldson-Myers",
            "title": {
                "fragments": [],
                "text": "Bayesian Super-Resolution of Text in Video with a Text-Specific Bimodal Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Bayesian super-resolution algorithm that uses a text-specific bimodal prior improved the readability of 4- to 7-pixel-high scene text significantly better than bicubic interpolation, and increased the accuracy of OCR results better than the piecewise smoothness prior."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298866"
                        ],
                        "name": "Jyotirmoy Banerjee",
                        "slug": "Jyotirmoy-Banerjee",
                        "structuredName": {
                            "firstName": "Jyotirmoy",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyotirmoy Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 824406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c531e0a742941a27d75c61965e67357d172b947",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an edge-directed super-resolution algorithm for document images without using any training set. This technique creates an image with smooth regions in both the foreground and the background, while allowing sharp discontinuities across and smoothness along the edges. Our method preserves sharp corners in text images by using the local edge direction, which is computed first by evaluating the gradient field and then taking its tangent. Super-resolution of document images is characterized by bimodality, smoothness along the edges as well as subsampling consistency. These characteristics are enforced in a Markov random field (MRF) framework by defining an appropriate energy function. In our method, subsampling of super-resolution image will return the original low-resolution one, proving the correctness of the method. The super-resolution image, is generated by iteratively reducing this energy function. Experimental results on a variety of input images, demonstrate the effectiveness of our method for document image super-resolution."
            },
            "slug": "Super-Resolution-of-Text-Images-Using-Edge-Directed-Banerjee-Jawahar",
            "title": {
                "fragments": [],
                "text": "Super-Resolution of Text Images Using Edge-Directed Tangent Field"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents an edge-directed super-resolution algorithm for document images without using any training set, which creates an image with smooth regions in both the foreground and the background, while allowing sharp discontinuities across and smoothness along the edges."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18471475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66b45e2cf76a66ccac120e6d69bfc1cd8dde6269",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Time-stamps and URLs overlaid artificially on images add useful meta information which can be used for automatic indexing of images and videos. In this paper, we propose a method based on an attention-based model of visual saliency to extract overlaid text and time-stamps that are rendered on images. Our model of visual saliency is based on a Bayesian framework and works very well for the task of time-stamp detection and segmentation as is evident by overall object recall of 80% and precision of 70%. Our method produces a clean text segmented binarized image, which can be used for recognition directly by an OCR system. Furthermore, our technique is robust against variation of font styles and color of time-stamp and overlaid text."
            },
            "slug": "Bayesian-Approach-to-Photo-Time-Stamp-Recognition-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "Bayesian Approach to Photo Time-Stamp Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a method based on an attention-based model of visual saliency to extract overlaid text and time-stamps that are rendered on images that can be used for recognition directly by an OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Dance [56] has proposed a similar method of estimating two vanishing points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206409349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d1292478a176ac9cfa39390a7db5402d37f8f53",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been increasing interest in document capture with digital cameras, since they are often more convenient to use than conventional devices such as flatbed scanners. Unlike flatbed scanners, cameras can acquire document images with arbitrary perspectives. Without correction, perspective distortions are unappealing to human readers. They also make subsequent image analysis slower, more complicated and less reliable. The novel contribution of this paper is to view perspective estimation as a generalization of the well-studied skew estimation problem. Rather than estimating one angle of rotation we must determine four angles describing the perspective. In our method, separate estimates are made for angles describing lines that are parallel and perpendicular to text lines. Each of these estimates is based on a twice-iterated projection profile computation. We give a probabilistic argument for the method and describe an efficient implementation. Our results illustrate its primary benefits: it is robust and accurate. The method is efficient compared with the time required to warp the image to correct for perspective."
            },
            "slug": "Perspective-estimation-for-document-images-Dance",
            "title": {
                "fragments": [],
                "text": "Perspective estimation for document images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The novel contribution of this paper is to view perspective estimation as a generalization of the well-studied skew estimation problem, where rather than estimating one angle of rotation the authors must determine four angles describing the perspective."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2591506"
                        ],
                        "name": "Minoru Yokobayashi",
                        "slug": "Minoru-Yokobayashi",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Yokobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minoru Yokobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[137], Yokobayashi and Wakahara [147]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A review of deformable templates is found in [136] Yokobayashi and Wakahara [147]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10678544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b8e87e9d7f1edebc8cb6992ed4eac17b4a8682c",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds. The key ideas are twofold. One is to automatically select one axis in the RGB color space that maximizes the between-class separability by a suitably chosen threshold for segmentation of character and background or binarization. The other is affine-invariant or distortion-tolerant grayscale character recognition using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images. In experiments, we use a total of 698 test images extracted from the public ICDAR 2003 robust OCR dataset containing a variety of single-character images in natural scenes. In advance, we classify those images into seven groups according to the degree of image degradations and/or background complexity. On the other hand, we only prepare a single-font set of 62 alphanumerics for templates. Experimental results show an average recognition rate of 81.4%, ranging from 94.5% for clear images to 39.3% for seriously distorted images"
            },
            "slug": "Binarization-and-Recognition-of-Degraded-Characters-Yokobayashi-Wakahara",
            "title": {
                "fragments": [],
                "text": "Binarization and Recognition of Degraded Characters Using a Maximum Separability Axis in Color Space and GAT Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2198635"
                        ],
                        "name": "Youngsu Moon",
                        "slug": "Youngsu-Moon",
                        "structuredName": {
                            "firstName": "Youngsu",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngsu Moon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18522972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e9a4a9ebdff7fdbf8119241bd62d144a426f31",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Most existing methods of text segmentation in video images are not robust because they do not consider the intrinsic characteristics of text. In this paper, we propose a novel method of text segmentation based on stroke filter (SF). First, we give the definition of text, which is realized in the form of stroke filter based on local region analysis. Based on stroke filter response, text polarity determination and local region growing modules are performed successively. The effectiveness of our method is validated by experiments on a challenging database."
            },
            "slug": "Text-segmentation-based-on-stroke-filter-Liu-Jung",
            "title": {
                "fragments": [],
                "text": "Text segmentation based on stroke filter"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper gives the definition of text, which is realized in the form of stroke filter based on local region analysis, and proposes a novel method of text segmentation based on stroke filter (SF)."
            },
            "venue": {
                "fragments": [],
                "text": "MM '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153322035"
                        ],
                        "name": "Takuma Yamaguchi",
                        "slug": "Takuma-Yamaguchi",
                        "structuredName": {
                            "firstName": "Takuma",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takuma Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740697"
                        ],
                        "name": "M. Maruyama",
                        "slug": "M.-Maruyama",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Maruyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maruyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593830"
                        ],
                        "name": "H. Miyao",
                        "slug": "H.-Miyao",
                        "structuredName": {
                            "firstName": "Hidetoshi",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737939"
                        ],
                        "name": "Y. Nakano",
                        "slug": "Y.-Nakano",
                        "structuredName": {
                            "firstName": "Yasuaki",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[58], the perspective distortion of each individual character is estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21081388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a906f8d8cda8dd34b8c510f6bb7622bf9d3b4da7",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper proposes a method to recognize digits in a natural scene, such as telephone numbers on a signboard. Candidate regions of digits are extracted from an image through contrast enhancement, edge extraction, and labeling. Since the target text patterns are in a 3D space, unlike traditional character recognition problems, we have to deal with the image transformation effect due to the orientation in the 3D space and projection. We have to cancel the effect as much as possible before digit recognition. In our method, the image transformation effect is modeled as skew and slant. In the proposed method, simplified Hough transform is used for the skew normalization. After the skew normalization, the remaining effect of image transformation is corrected by circumscribing digit patterns with tilted rectangles and affine transformation. In experiments, we tested a total of 1,332 images of signboards with 11,939 digits. We obtained a digit extraction rate of 99.2% and a correct digit recognition rate of 98.8%."
            },
            "slug": "Digit-recognition-in-a-natural-scene-with-skew-and-Yamaguchi-Maruyama",
            "title": {
                "fragments": [],
                "text": "Digit recognition in a natural scene with skew and slant normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper proposes a method to recognize digits in a natural scene, such as telephone numbers on a signboard, through contrast enhancement, edge extraction, and labeling, and the image transformation effect is modeled as skew and slant."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2530942"
                        ],
                        "name": "H. Hase",
                        "slug": "H.-Hase",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Hase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188516"
                        ],
                        "name": "T. Shinokawa",
                        "slug": "T.-Shinokawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Shinokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71114145"
                        ],
                        "name": "M. Yoneda",
                        "slug": "M.-Yoneda",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoneda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[133]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 40333068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b24ea59f58374750894ad050dbafe88c81ed54f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Character-string-extraction-from-color-documents-Hase-Shinokawa",
            "title": {
                "fragments": [],
                "text": "Character string extraction from color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068391558"
                        ],
                        "name": "Katherine Donaldson",
                        "slug": "Katherine-Donaldson",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Donaldson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Donaldson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Like the bimodal prior of [39], it is possible to introduce other priors suitable for MAP-based document image super-resolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Donaldson and Myers [39] have proposed an improved version of [37] by introducing a new prior to the MAP framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 826973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5506a2bdf6093847bea49175a13377d5c665755",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.To increase the range of sizes of video scene text recognizable by optical character recognition (OCR), we developed a Bayesian super-resolution algorithm that uses a text-specific bimodal prior. We evaluated the effectiveness of the bimodal prior, compared and in conjunction with a piecewise smoothness prior, visually and by measuring the accuracy of the OCR results on the variously super-resolved images. The bimodal prior improved the readability of 4- to 7-pixel-high scene text significantly better than bicubic interpolation and increased the accuracy of OCR results better than the piecewise smoothness prior."
            },
            "slug": "Bayesian-super-resolution-of-text-in-videowith-a-Donaldson-Myers",
            "title": {
                "fragments": [],
                "text": "Bayesian super-resolution of text in videowith a text-specific bimodal prior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Bayesian super-resolution algorithm that uses a text-specific bimodal prior improved the readability of 4- to 7-pixel-high scene text significantly better than bicubic interpolation and increased the accuracy of OCR results better than the piecewise smoothness prior."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4054887"
                        ],
                        "name": "Soma Shiraishi",
                        "slug": "Soma-Shiraishi",
                        "structuredName": {
                            "firstName": "Soma",
                            "lastName": "Shiraishi",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soma Shiraishi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806377"
                        ],
                        "name": "Yaokai Feng",
                        "slug": "Yaokai-Feng",
                        "structuredName": {
                            "firstName": "Yaokai",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaokai Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14499664,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "34202d811ecf4d42834f5e797c849050bfdef9ad",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a part-based skew estimation method which is more robust to larger varieties of text images, such as camera-captured scene images. Specifically, the skew angle at each local part of the input image is estimated independently by referring the local part of upright character images stored as a database. Then the global skew angle is estimated by aggregating the estimated local skews. The proposed method does not assume that characters are laid-out in straight lines and thus have more robustness to the varieties of text images than conventional methods. The experimental results show the advantage of the proposed method over the conventional methods under several conditions."
            },
            "slug": "A-Part-Based-Skew-Estimation-Method-Shiraishi-Feng",
            "title": {
                "fragments": [],
                "text": "A Part-Based Skew Estimation Method"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper proposes a part-based skew estimation method which is more robust to larger varieties of text images, such as camera-captured scene images, and shows the advantage of the proposed method over the conventional methods under several conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2378082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdc6ff7b7fc4f1769e8e76897065282e436c634b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for the fronto-parallel recovery of paragraphs of text under full perspective transformation is presented. The horizontal vanishing point of the text plane is found using an extension of 2D projection profiles. This allows the accurate segmentation of the lines of text. Analysis of the lines will then reveal the style of justification of the paragraph, and provide an estimate of the vertical vanishing point of the plane. The text is finally recovered to a fronto-parallel view suitable for OCR or other higher-level recognition."
            },
            "slug": "Estimating-the-Orientation-and-Recovery-of-Text-in-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Estimating the Orientation and Recovery of Text Planes in a Single Image"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A method for the fronto-parallel recovery of paragraphs of text under full perspective transformation is presented, and the horizontal vanishing point of the text plane is found using an extension of 2D projection profiles for accurate segmentation of the lines of text."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 926374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06388e33a797b32b5bfddf39c5db7ecf478a813f",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational models of visual attention use image features to identify salient locations in an image that are likely to attract human attention. Attention models have been quite effectively used for various object detection tasks. However, their use for scene text detection is under-investigated. As a general observation, scene text often conveys important information and is usually prominent or salient in the scene itself. In this paper, we evaluate four state-of-the-art attention models for their response to scene text. Initial results indicate that saliency maps produced by these attention models can be used for aiding scene text detection algorithms by suppressing non-text regions."
            },
            "slug": "How-Salient-is-Scene-Text-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "How Salient is Scene Text?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Initial results indicate that saliency maps produced by these attention models can be used for aiding scene text detection algorithms by suppressing non-text regions."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772880"
                        ],
                        "name": "David P. Capel",
                        "slug": "David-P.-Capel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Capel",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David P. Capel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14170862,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fde2a3281701e5a85e8f8c1256e48950a4840578",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is the super-resolution enhancement of image sequences. We consider in particular images of scenes for which the point-to-point image transformation is a plane projective transformation. We first describe the imaging model, and a maximum likelihood (ML) estimator of the super-resolution image. We demonstrate the extreme noise sensitivity of the unconstrained ML estimator. We show that the Irani and Peleg (1991, 1993) super-resolution algorithm does not suffer from this sensitivity, and explain that this stability is due to the error back-projection method which effectively constrains the solution. We then propose two estimators suitable for the enhancement of text images: a maximum a posteriori (MAP) estimator based on a Huber prior and an estimator regularized using the total variation norm. We demonstrate the improved noise robustness of these approaches over the Irani and Peleg estimator. We also show the effects of a poorly estimated point spread function (PSF) on the super-resolution result and explain conditions necessary for this parameter to be included in the optimization. Results are evaluated on both real and synthetic sequences of text images. In the case of the real images, the projective transformations relating the images are estimated automatically from the image data, so that the entire algorithm is automatic."
            },
            "slug": "Super-resolution-enhancement-of-text-image-Capel-Zisserman",
            "title": {
                "fragments": [],
                "text": "Super-resolution enhancement of text image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two estimators suitable for the enhancement of text images are proposed: a maximum a posteriori (MAP) estimator based on a Huber prior and an estimator regularized using the total variation norm, which demonstrates the improved noise robustness of these approaches over the Irani and Peleg estimator."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084106453"
                        ],
                        "name": "G. Dalley",
                        "slug": "G.-Dalley",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Dalley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dalley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822613"
                        ],
                        "name": "J. Marks",
                        "slug": "J.-Marks",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Marks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[45] also have developed a similar method independently based on their past trial [46] on"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11941852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed5f46ad177517dbdd8d69106c852cf850e5158",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of text super-resolution: given a single image of text scanned in at low resolution from a piece of paper, return the image that is mostly likely to be generated from a noiseless high-resolution scan of the same piece of paper. In doing so, we wish to: (1) avoid introducing artifacts in the high-resolution image such as blurry edges and rounded corners, (2) recover from quantization noise and grid-alignment effects that introduce errors in the low-resolution image, and (3) handle documents with very large glyph sets such as Japanese's Kanji. Applications for this technology include improving the display of: fax documents, low-resolution scans of archival documents, and low-resolution bitmapped fonts on high-resolution output devices."
            },
            "slug": "Single-frame-text-super-resolution:-a-Bayesian-Dalley-Freeman",
            "title": {
                "fragments": [],
                "text": "Single-frame text super-resolution: a Bayesian approach"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work addresses the problem of text super-resolution: given a single image of text scanned in at low resolution from a piece of paper, return the image that is mostly likely to be generated from a noiseless high-resolution scan of the same piece ofpaper."
            },
            "venue": {
                "fragments": [],
                "text": "2004 International Conference on Image Processing, 2004. ICIP '04."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802201"
                        ],
                        "name": "S. Impedovo",
                        "slug": "S.-Impedovo",
                        "structuredName": {
                            "firstName": "Sebastiano",
                            "lastName": "Impedovo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Impedovo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805615"
                        ],
                        "name": "R. Modugno",
                        "slug": "R.-Modugno",
                        "structuredName": {
                            "firstName": "Raffaele",
                            "lastName": "Modugno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Modugno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40164420"
                        ],
                        "name": "A. Ferrante",
                        "slug": "A.-Ferrante",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Ferrante",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ferrante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2935605"
                        ],
                        "name": "E. Stasolla",
                        "slug": "E.-Stasolla",
                        "structuredName": {
                            "firstName": "Erasmo",
                            "lastName": "Stasolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stasolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17771789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "042cf528cbb469621946c490365cae331cfccfd7",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwritten document analysis and recognition deals with several different application fields. In document processing, one of the first problems that must be solved is data acquisition. The selection of the appropriate acquisition device plays a fundamental role; it depends on the different types of source documents and on the different application domains. Digital scanners allow both massive document acquisition and conversion of paper based documents into electronic documents. Depending on the environment requirements, the original quality of the analog signal must be appropriately captured and preserved in the digital conversion and the right scanner need to be selected. This paper presents an overview of the main characteristics of the scanners today available on the market and highlights their main properties. By considering the operation difficulties of actual scanners and the properties of the new plastic materials, this paper presents some ideas for future possible developments in digital scanning."
            },
            "slug": "New-Trends-in-Digital-Scanning-Processes-Impedovo-Modugno",
            "title": {
                "fragments": [],
                "text": "New Trends in Digital Scanning Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An overview of the main characteristics of the scanners today available on the market and highlights their main properties is presented and some ideas for future possible developments in digital scanning are presented."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31423777"
                        ],
                        "name": "D. DeMenthon",
                        "slug": "D.-DeMenthon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "DeMenthon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeMenthon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[60] also have developed a similar method where vertical strokes are used for the vertical component."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1599704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c5ac89658511a0df08a108c2c85880a0f454f02",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Compared to typical scanners, handheld cameras offer convenient, flexible, portable, and noncontact image capture, which enables many new applications and breathes new life into existing ones. However, camera-captured documents may suffer from distortions caused by a nonplanar document shape and perspective projection, which lead to the failure of current optical character recognition (OCR) technologies. We present a geometric rectification framework for restoring the frontal-flat view of a document from a single camera-captured image. Our approach estimates the 3D document shape from texture flow information obtained directly from the image without requiring additional 3D/metric data or prior camera calibration. Our framework provides a unified solution for both planar and curved documents and can be applied in many, especially mobile, camera-based document analysis applications. Experiments show that our method produces results that are significantly more OCR compatible than the original images."
            },
            "slug": "Geometric-Rectification-of-Camera-Captured-Document-Liang-DeMenthon",
            "title": {
                "fragments": [],
                "text": "Geometric Rectification of Camera-Captured Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a geometric rectification framework for restoring the frontal-flat view of a document from a single camera-captured image and estimates the 3D document shape from texture flow information obtained directly from the image without requiring additional 3D/metric data or prior camera calibration."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109552014"
                        ],
                        "name": "Dong-Qing Zhang",
                        "slug": "Dong-Qing-Zhang",
                        "structuredName": {
                            "firstName": "Dong-Qing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong-Qing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14341205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df6e78f220d598fb961aab165a68697f67be7d30",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text in natural 3D scenes is a challenging problem due to background clutter and photometric/gemetric variations of scene text. Most prior systems adopt approaches based on deterministic rules, lacking a systematic and scalable framework. In this paper, we present a parts-based approach for 3D scene text detection using a higher-order MRF model. The higher-order structure is used to capture the spatial-feature relations among multiple parts in scene text. The use of higher-order structure and the feature-dependent potential function represents significant departure from the conventional pairwise MRF, which has been successfully applied in several low-level applications. We further develop a variational approximation method, in the form of belief propagation, for inference in the higher-order model. Our experiments using the ICDAR'03 benchmark showed promising results in detecting scene text with significant geometric variations, background clutter on planar surfaces or non-planar surfaces with limited angles."
            },
            "slug": "Learning-to-Detect-Scene-Text-Using-a-Higher-Order-Zhang-Chang",
            "title": {
                "fragments": [],
                "text": "Learning to Detect Scene Text Using a Higher-Order MRF with Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A parts-based approach for 3D scene text detection using a higher-order MRF model that captures the spatial-feature relations among multiple parts in scene text and develops a variational approximation method, in the form of belief propagation, for inference in the higher- order model."
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32648591"
                        ],
                        "name": "Xing Yu Qi",
                        "slug": "Xing-Yu-Qi",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Qi",
                            "middleNames": [
                                "Yu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Yu Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152827827"
                        ],
                        "name": "Li Zhang",
                        "slug": "Li-Zhang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15710341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3eb9ad1a51a16db42b2c4df6c66e30c80d4848b",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the problem of blurring caused by motion during image capture of text documents. Motion blurring prevents proper optical character recognition of the document text contents. One area of such applications is to deblur name card images obtained from handheld cameras. In this paper, a complete motion deblurring procedure for document images has been proposed. The method handles both uniform linear motion blur and uniform acceleration motion blur. Experiments on synthetic and real-life blurred images prove the feasibility and reliability of this algorithm provided that the motion is not too irregular. The restoration procedure consumes only small amount of computation time."
            },
            "slug": "Motion-deblurring-for-optical-character-recognition-Qi-Zhang",
            "title": {
                "fragments": [],
                "text": "Motion deblurring for optical character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments on synthetic and real-life blurred images prove the feasibility and reliability of this algorithm provided that the motion is not too irregular and the restoration procedure consumes only small amount of computation time."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2034418"
                        ],
                        "name": "Jangkyun Park",
                        "slug": "Jangkyun-Park",
                        "structuredName": {
                            "firstName": "Jangkyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jangkyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779402"
                        ],
                        "name": "Younghee Kwon",
                        "slug": "Younghee-Kwon",
                        "structuredName": {
                            "firstName": "Younghee",
                            "lastName": "Kwon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younghee Kwon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18222049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a2f25fde70bb292efb23b47ee9344bed5290c59f",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a prior model for text image super-resolution in the Bayesian framework. In contrast to generic image super-resolution task, super-resolution of text images can be benefited from strong prior knowledge of the image class: firstly, low-resolution images are assumed to be generated from a high-resolution image by a sort of degradation which can be grasped through example pairs of the original and the corresponding degradation; secondly, text images are composed of two homogeneous regions, text and background regions. These properties were represented in a Markov random field (MRF) framework. Experiments showed that our model is more appropriate to text image super-resolution than the other prior models."
            },
            "slug": "An-example-based-prior-model-for-text-image-Park-Kwon",
            "title": {
                "fragments": [],
                "text": "An example-based prior model for text image super-resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents a prior model for text image super-resolution in the Bayesian framework and shows that this model is more appropriate to text imagesuper-resolution than the other prior models."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141036803"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291081"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753831"
                        ],
                        "name": "S. Naoi",
                        "slug": "S.-Naoi",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Naoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Naoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646558"
                        ],
                        "name": "K. Fujimoto",
                        "slug": "K.-Fujimoto",
                        "structuredName": {
                            "firstName": "Katsuhito",
                            "lastName": "Fujimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fujimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578348"
                        ],
                        "name": "Y. Fujii",
                        "slug": "Y.-Fujii",
                        "structuredName": {
                            "firstName": "Yusaku",
                            "lastName": "Fujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Fujii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069745654"
                        ],
                        "name": "Koji Kurokawa",
                        "slug": "Koji-Kurokawa",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Kurokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Koji Kurokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067752"
                        ],
                        "name": "Hiroaki Takebe",
                        "slug": "Hiroaki-Takebe",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Takebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Takebe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20174193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96c0bb89d71778ff06612b64da3ff28e12317f05",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Document images captured by a mobile phone camera often have perspective distortions. Efficiency and accuracy are two important issues in designing a rectification system for such perspective documents. In this paper, we propose a new perspective rectification system based on vanishing point detection. This system achieves both the desired efficiency and accuracy using a multi-stage strategy: at the first stage, document boundaries and straight lines are used to compute vanishing points; at the second stage, text baselines and block aligns are utilized; and at the last stage, character tilt orientations are voted for the vertical vanishing point. A profit function is introduced to evaluate the reliability of detected vanishing points at each stage. If vanishing points at one stage are reliable, then rectification is ended at that stage. Otherwise, our method continues to seek more reliable vanishing points in the next stage. We have tested this method with more than 400 images including paper documents, signboards and posters. The image acceptance rate is more than 98.5% with an average speed of only about 60 ms."
            },
            "slug": "A-Multi-Stage-Strategy-to-Perspective-Rectification-Yin-Sun",
            "title": {
                "fragments": [],
                "text": "A Multi-Stage Strategy to Perspective Rectification for Mobile Phone Camera-Based Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a new perspective rectification system based on vanishing point detection that achieves both the desired efficiency and accuracy using a multi-stage strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840318"
                        ],
                        "name": "K. Iwata",
                        "slug": "K.-Iwata",
                        "structuredName": {
                            "firstName": "Kazumasa",
                            "lastName": "Iwata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Iwata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740235"
                        ],
                        "name": "S. Omachi",
                        "slug": "S.-Omachi",
                        "structuredName": {
                            "firstName": "Shinichiro",
                            "lastName": "Omachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omachi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20258648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49ba99dfc278bba257a8353d5f53d428adb0b7fb",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method of recovering digital ink for an intelligent camera pen, which is characterized by the functions that (1) it works on ordinary paper and (2) if an electronic document is printed on the paper the recovered digital ink is associated with the document. Two technologies called paper fingerprint and document image retrieval are integrated for realizing the above functions. The key of the integration is the introduction of image mosaicing and fast retrieval of previously seen fingerprints based on hashing of SURF local features. From the experimental results of 50 handwritings, we have confirmed that the proposed method is effective to recover and locate the digital ink from the handwriting on a physical paper."
            },
            "slug": "Tracking-and-Retrieval-of-Pen-Tip-Positions-for-an-Iwata-Kise",
            "title": {
                "fragments": [],
                "text": "Tracking and Retrieval of Pen Tip Positions for an Intelligent Camera Pen"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "From the experimental results of 50 handwritings, it is confirmed that the proposed method is effective to recover and locate the digital ink from the handwriting on a physical paper."
            },
            "venue": {
                "fragments": [],
                "text": "2010 12th International Conference on Frontiers in Handwriting Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347079"
                        ],
                        "name": "Yasuhiro Kunishige",
                        "slug": "Yasuhiro-Kunishige",
                        "structuredName": {
                            "firstName": "Yasuhiro",
                            "lastName": "Kunishige",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiro Kunishige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806377"
                        ],
                        "name": "Yaokai Feng",
                        "slug": "Yaokai-Feng",
                        "structuredName": {
                            "firstName": "Yaokai",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaokai Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12251046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f613479a2dcc5393ab4a6537c5d9adb6b298df6",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "For scenery character detection, we introduce environmental context, which is modeled by scene components, such as sky and building. Environmental context is expected to regulate the probability of character existence at a specific region in a scenery image. For example, if a region looks like a part of a building, the region has a higher probability than another region like a part of the sky. In this paper, environmental context is represented by state-of-the-art texture and color features and utilized in two different ways. Through experimental results, it was clearly shown that the environmental context has an effect of improving detection accuracy."
            },
            "slug": "Scenery-Character-Detection-with-Environmental-Kunishige-Feng",
            "title": {
                "fragments": [],
                "text": "Scenery Character Detection with Environmental Context"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "In this paper, environmental context is represented by state-of-the-art texture and color features and utilized in two different ways and it was clearly shown that the environmental context has an effect of improving detection accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403823956"
                        ],
                        "name": "Carlos Merino-Gracia",
                        "slug": "Carlos-Merino-Gracia",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Merino-Gracia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Merino-Gracia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3257286"
                        ],
                        "name": "Karel Lenc",
                        "slug": "Karel-Lenc",
                        "structuredName": {
                            "firstName": "Karel",
                            "lastName": "Lenc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karel Lenc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16811390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6037f073b1284e6830e0d0b5a9e33f2354591a2",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a mobile head-mounted device for detecting and tracking text that is encased in an ordinary flat-cap hat. The main parts of the device are an integrated camera and audio webcam together with a simple remote control system, all connected via a USB hub to a laptop. A near to real-time text detection algorithm (around 14 fps for 640\u00d7480 images) which uses Maximal Stable Extremal Regions (MSERs) for image segmentation is proposed. Comparative text detection results against the ICDAR 2003 text locating competition database along with performance figures are presented."
            },
            "slug": "A-Head-Mounted-Device-for-Recognizing-Text-in-Merino-Gracia-Lenc",
            "title": {
                "fragments": [],
                "text": "A Head-Mounted Device for Recognizing Text in Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A near to real-time text detection algorithm which uses Maximal Stable Extremal Regions (MSERs) for image segmentation is proposed and Comparative text detection results against the ICDAR 2003 text locating competition database along with performance figures are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CBDAR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "HOG (histogram of oriented gradients) also evaluates 2D gradient Wang and Belongie [82] (HOG), Uchida et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[112] (MRF), Wang and Belongie [82], Cho et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14911813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs - text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines - one open source and one proprietary - with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16% on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem."
            },
            "slug": "Word-Spotting-in-the-Wild-Wang-Belongie",
            "title": {
                "fragments": [],
                "text": "Word Spotting in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the appearance of words in the wild spans this range of difficulties and a new word recognition approach based on state-of-the-art methods from generic object recognition is proposed, in which object categories are considered to be the words themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155699044"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This research topic can be further divided into two types [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A review is found in [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "M an ca sT hi llo u an d G os se lin [1 23 ]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A couple of survey papers are found as [2, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28161769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0567609da19ae90f1742800f1ff873b9f1bd411",
            "isKey": true,
            "numCitedBy": 195,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research."
            },
            "slug": "Extraction-of-Text-Objects-in-Video-Documents:-Zhang-Kasturi",
            "title": {
                "fragments": [],
                "text": "Extraction of Text Objects in Video Documents: Recent Progress"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The progress made in text extraction in video documents is introduced and promising directions for future research are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793476"
                        ],
                        "name": "M. Donoser",
                        "slug": "M.-Donoser",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Donoser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Donoser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516462"
                        ],
                        "name": "Clemens Arth",
                        "slug": "Clemens-Arth",
                        "structuredName": {
                            "firstName": "Clemens",
                            "lastName": "Arth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clemens Arth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16875557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "176984f9096c03912862d1f3ff37210ac7d8a5d6",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel real-time framework which enables detection, tracking and recognition of license plates from video sequences. An efficient algorithm based on analysis of Maximally Stable Extremal Region (MSER) detection results allows localization of international license plates in single images without the need of any learning scheme. After a one-time detection of a plate it is robustly tracked through the sequence by applying a modified version of the MSER tracking framework which provides accurate localization results and additionally segmentations of the individual characters. Therefore, tracking and character segmentation is handled simultaneously. Finally, support vector machines are used to recognize the characters on the plate. An experimental evaluation shows the high accuracy and efficiency of the detection and tracking algorithm. Furthermore, promising results on a challenging data set are presented and the significant improvement of the recognition rate due to the robust tracking scheme is proved."
            },
            "slug": "Detecting,-Tracking-and-Recognizing-License-Plates-Donoser-Arth",
            "title": {
                "fragments": [],
                "text": "Detecting, Tracking and Recognizing License Plates"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel real-time framework which enables detection, tracking and recognition of license plates from video sequences by applying a modified version of the MSER tracking framework which provides accurate localization results and additionally segmentations of the individual characters."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696199"
                        ],
                        "name": "C. Anagnostopoulos",
                        "slug": "C.-Anagnostopoulos",
                        "structuredName": {
                            "firstName": "Christos-Nikolaos",
                            "lastName": "Anagnostopoulos",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anagnostopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145176819"
                        ],
                        "name": "I. Anagnostopoulos",
                        "slug": "I.-Anagnostopoulos",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Anagnostopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Anagnostopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2692971"
                        ],
                        "name": "V. Loumos",
                        "slug": "V.-Loumos",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Loumos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Loumos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231139"
                        ],
                        "name": "E. Kayafas",
                        "slug": "E.-Kayafas",
                        "structuredName": {
                            "firstName": "Eleftherios",
                            "lastName": "Kayafas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kayafas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17107667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "467024c6cc8fe73c64e501f8a12cdbafbf9561b0",
            "isKey": false,
            "numCitedBy": 732,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a new algorithm for vehicle license plate identification is proposed, on the basis of a novel adaptive image segmentation technique (sliding concentric windows) and connected component analysis in conjunction with a character recognition neural network. The algorithm was tested with 1334 natural-scene gray-level vehicle images of different backgrounds and ambient illumination. The camera focused in the plate, while the angle of view and the distance from the vehicle varied according to the experimental setup. The license plates properly segmented were 1287 over 1334 input images (96.5%). The optical character recognition system is a two-layer probabilistic neural network (PNN) with topology 108-180-36, whose performance for entire plate recognition reached 89.1%. The PNN is trained to identify alphanumeric characters from car license plates based on data obtained from algorithmic image processing. Combining the above two rates, the overall rate of success for the license-plate-recognition algorithm is 86.0%. A review in the related literature presented in this paper reveals that better performance (90% up to 95%) has been reported, when limitations in distance, angle of view, illumination conditions are set, and background complexity is low"
            },
            "slug": "A-License-Plate-Recognition-Algorithm-for-System-Anagnostopoulos-Anagnostopoulos",
            "title": {
                "fragments": [],
                "text": "A License Plate-Recognition Algorithm for Intelligent Transportation System Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A review in the related literature presented in this paper reveals that better performance has been reported, when limitations in distance, angle of view, illumination conditions are set, and background complexity is low."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12075023"
                        ],
                        "name": "M. Wienecke",
                        "slug": "M.-Wienecke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Wienecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wienecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749475"
                        ],
                        "name": "G. Fink",
                        "slug": "G.-Fink",
                        "structuredName": {
                            "firstName": "Gernot",
                            "lastName": "Fink",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689610"
                        ],
                        "name": "G. Sagerer",
                        "slug": "G.-Sagerer",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Sagerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sagerer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in [19], handwritten characters on whiteboard are to be recognized by a video camera system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ex tr em al re gi on (M SE R ) M SE R [1 19 ] is a re gi on w ith su ffi ci en t"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10368264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92891b862b1f87a2477daccaaaa201d6234d9d70",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing popularity of whiteboards as a popular tool in meeting rooms has been accompanied by a growing interest in making use of the whiteboard as a user interface for human-computer interaction. Therefore, systems based on electronic whiteboards have been developed in order to serve as meeting assistants for, e.g., collaborative working. However, as special pens and erasers are required, natural interaction is restricted. In order to render this communication method more natural, it was proposed to retain ordinary whiteboard and pens and to visually observe the writing process using a video camera [22, 25]. In this paper a prototype system for automatic video-based whiteboard reading is presented. The system is designed for recognizing unconstrained handwritten text and is further characterized by an incremental processing strategy in order to facilitate recognizing portions of text as soon as they have been written on the board. We will present the methods employed for extracting text regions, preprocessing, feature extraction, and statistical modeling and recognition. Evaluation results on a writer-independent unconstrained handwriting recognition task demonstrate the feasibility of the proposed approach."
            },
            "slug": "Toward-automatic-video-based-whiteboard-reading-Wienecke-Fink",
            "title": {
                "fragments": [],
                "text": "Toward automatic video-based whiteboard reading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A prototype system for automatic video-based whiteboard reading is presented and is designed for recognizing unconstrained handwritten text and is further characterized by an incremental processing strategy in order to facilitate recognizing portions of text as soon as they have been written on the board."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116741074"
                        ],
                        "name": "Michael J. Taylor",
                        "slug": "Michael-J.-Taylor",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [49], a traditional smoothness prior has also introduced for deblurring with special consideration to suppress its side effect around character edges."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in [49], a regularization-based method is proposed for determining the original image with a given PSF."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9198704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53f279573f7fd37c932431cfeb941bbc4fc265b8",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "As digital cameras become cheaper and more powerful, driven by the consumer digital photography market, we anticipate significant value in extending their utility as a general office peripheral by adding a paper scanning capability. The main technical challenges in realizing this new scanning interface are insufficient resolution, blur and lighting variations. We have developed an efficient technique for the recovery of text from digital camera images, which simultaneously treats these three problems, unlike other local thresholding algorithms which do not cope with blur and resolution enhancement. The technique first performs deblurring by deconvolution, and then resolution enhancement by linear interpolation. We compare the performance of a threshold derived from the local mean and variance of all pixel values within a neighborhood with a threshold derived from the local mean of just those pixels with high gradient. We assess performance using OCR error scores."
            },
            "slug": "Enhancement-of-document-images-from-cameras-Taylor-Dance",
            "title": {
                "fragments": [],
                "text": "Enhancement of document images from cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work has developed an efficient technique for the recovery of text from digital camera images, which simultaneously treats these three problems, unlike other local thresholding algorithms which do not cope with blur and resolution enhancement."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145099732"
                        ],
                        "name": "S. Pollard",
                        "slug": "S.-Pollard",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pollard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar text line extraction process is utilized in [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 78 ], W on g an d C he n [7 9] ,"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[7 5] (T em po ra ls ta bi lit y"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [7], a dualflash system is introduced, where two pictures are taken under different flashlights"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1528859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fce503ff0b4fba11e7a573b91f0fd69a89d00058",
            "isKey": true,
            "numCitedBy": 22,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper explores those aspects of document capture that are specific to cameras. Each of them must be addressed in order to close the gap between taking a photograph of a document and capturing the document itself. We present results in five areas: (1) framing documents using structured light, (2) robustly dealing with ambient illumination when capturing glossy documents, (3) improving text quality when using mosaiced color sensors, (4) robustly and passively recovering perspective and image plane skew using text flow, and (5) measuring and undoing page curl using structured light and an applicable surface model. The ultimate success of subsequent document recognition will be heavily dependent on the successful completion of these tasks."
            },
            "slug": "Building-cameras-for-capturing-documents-Pollard-Pilu",
            "title": {
                "fragments": [],
                "text": "Building cameras for capturing documents"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Those aspects of document capture that are specific to cameras that must be addressed in order to close the gap between taking a photograph of a document and capturing the document itself are explored."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107536791"
                        ],
                        "name": "S. Lu",
                        "slug": "S.-Lu",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Lu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2721366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4707e3d85169b245eb0782d0c4f5c8b65fb0ca85",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an identification technique that automatically detects the underlying script and orientation of scanned document images. In the proposed technique, document script and orientation are identified by using the stroke density and distribution, which convert each document image into a document vector. For each script at each orientation, a number of reference document vectors are first constructed. Script and orientation of the query document are then determined according to the similarity between the query document vector and multiple pre- constructed reference document vectors by using the K-nearest neighbor algorithm. Experiments show that the proposed technique is tolerant to the document skew and able to detect orientations of documents of different scripts."
            },
            "slug": "Automatic-Detection-of-Document-Script-and-Lu-Tan",
            "title": {
                "fragments": [],
                "text": "Automatic Detection of Document Script and Orientation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An identification technique that automatically detects the underlying script and orientation of scanned document images by using the stroke density and distribution and is tolerant to the document skew and able to detect orientations of documents of different scripts."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40913460"
                        ],
                        "name": "X. Liu",
                        "slug": "X.-Liu",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [9, 10])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ve ri fic at io n) ,C he n an d Y ui lle [1 10 ]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 10 9] ,E za ki"
                    },
                    "intents": []
                }
            ],
            "corpusId": 4920306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb6d8df1cc061f56453bd3e399421d7e1c2e2952",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an image based document retrieval system which runs on camera enabled mobile devices. \u201cMobile Retriever\u201d aims to seamlessly link physical and digital documents by allowing users to snap a picture of the text of a document and retrieve its electronic version from a database. Experiments show that for a database of 100,093 pages, the correct document can be retrieved in less than 4 s at a success rate over 95%. Our system extracts token pairs from the text, to efficiently index and retrieve candidate pages using only a small portion of the image. We use token triplets that define the orientation of three corresponding tokens to effectively prune the false positives and identify the correct page to retrieve. We stress the importance of geometrical relationship between feature points and show its effectiveness in our camera based image retrieval system."
            },
            "slug": "Mobile-Retriever:-access-to-digital-documents-from-Liu-Doermann",
            "title": {
                "fragments": [],
                "text": "Mobile Retriever: access to digital documents from their physical source"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "An image based document retrieval system which runs on camera enabled mobile devices that uses token triplets that define the orientation of three corresponding tokens to effectively prune the false positives and identify the correct page to retrieve."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065499715"
                        ],
                        "name": "Kazuki Ashida",
                        "slug": "Kazuki-Ashida",
                        "structuredName": {
                            "firstName": "Kazuki",
                            "lastName": "Ashida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuki Ashida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055905787"
                        ],
                        "name": "Hiroki Nagai",
                        "slug": "Hiroki-Nagai",
                        "structuredName": {
                            "firstName": "Hiroki",
                            "lastName": "Nagai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroki Nagai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47471571"
                        ],
                        "name": "Masayuki Okamoto",
                        "slug": "Masayuki-Okamoto",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Okamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masayuki Okamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152933693"
                        ],
                        "name": "Hiroaki Yamamoto",
                        "slug": "Hiroaki-Yamamoto",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593830"
                        ],
                        "name": "H. Miyao",
                        "slug": "H.-Miyao",
                        "structuredName": {
                            "firstName": "Hidetoshi",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146280843"
                        ],
                        "name": "JunMin Zhu",
                        "slug": "JunMin-Zhu",
                        "structuredName": {
                            "firstName": "JunMin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JunMin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873679"
                        ],
                        "name": "WuWen Ou",
                        "slug": "WuWen-Ou",
                        "structuredName": {
                            "firstName": "WuWen",
                            "lastName": "Ou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WuWen Ou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844464"
                        ],
                        "name": "L. Todoran",
                        "slug": "L.-Todoran",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Todoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Todoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46520089"
                        ],
                        "name": "Xiaofan Lin",
                        "slug": "Xiaofan-Lin",
                        "structuredName": {
                            "firstName": "Xiaofan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofan Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is noteworthy that competitions on text localization, called \u201cICDAR Robust Reading Competitions,\u201d have been organized [66\u201368] and played an important role to make this research topic very active."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The most widely used datasets are provided at ICDAR Robust Reading Competitions [66\u201368]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2250003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a01deac56a81646e8d84cb7bf2d905714ff00808",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper describes the robust reading competitions for ICDAR 2003. With the rapid growth in research over the last few years on recognizing text in natural scenes, there is an urgent need to establish some common benchmark datasets and gain a clear understanding of the current state of the art. We use the term \u2018robust reading\u2019 to refer to text images that are beyond the capabilities of current commercial OCR packages. We chose to break down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system. The subproblems we chose were text locating, character recognition and word recognition. By breaking down the problem in this way, we hoped to gain a better understanding of the state of the art in each of the subproblems. Furthermore, our methodology involved storing detailed results of applying each algorithm to each image in the datasets, allowing researchers to study in depth the strengths and weaknesses of each algorithm. The text-locating contest was the only one to have any entries. We give a brief description of each entry and present the results of this contest, showing cases where the leading entries succeed and fail. We also describe an algorithm for combining the outputs of the individual text locators and show how the combination scheme improves on any of the individual systems."
            },
            "slug": "ICDAR-2003-robust-reading-competitions:-entries,-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions: entries, results, and future directions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper broke down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system, and described an algorithm for combining the outputs of the individual text locators and showed how the combination scheme improves on any of theindividual systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801509"
                        ],
                        "name": "M. Bertini",
                        "slug": "M.-Bertini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Bertini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144300261"
                        ],
                        "name": "C. Colombo",
                        "slug": "C.-Colombo",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Colombo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Colombo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8196487"
                        ],
                        "name": "A. Bimbo",
                        "slug": "A.-Bimbo",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bimbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bimbo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[75] (Harris corner), Huang and Ma [76] (Dense corner point area), Zhao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Morphology Neighboring text-candidate regions are connected by a morphological operation Pros: Simple Cons: No function to exclude non-character regions Huang and Ma [75] (Filling space among corner points)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13504703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15b30cedcdc0c64b748145d2f95851be890e202a",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Broadcasters are demonstrating interest in building digital archives of their assets for reuse of archive materials for TV programs, on-line availability, and archiving. This requires tools for video indexing and retrieval by content exploiting high-level video information such as that contained in super-imposed text captions. In this paper we present a method to automatically detect and localize captions in digital video using temporal and spatial local properties of salient points in video frames. Results of experiments on both high-resolutionDV sequences and standard VHS videos are presented and discussed."
            },
            "slug": "Automatic-caption-localization-in-videos-using-Bertini-Colombo",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in videos using salient points"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method to automatically detect and localize captions in digital video using temporal and spatial local properties of salient points in video frames is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Multimedia and Expo, 2001. ICME 2001."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2779846"
                        ],
                        "name": "H. Sakoe",
                        "slug": "H.-Sakoe",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Sakoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sakoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, deformable template (also called image warping, nonlinear registration, and elastic matching) [136] is a possible choice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A review of deformable templates is found in [136] Yokobayashi and Wakahara [147]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17639662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31153a880bf42227099b3dced3dd8bfc4b8ca9ee",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a survey of elastic matching (EM) techniques employed in handwritten character recognition. EM is often called deformable template, flexible matching, or nonlinear template matching, and defined as the optimization problem of two-dimensional warping (2DW) which specifies the pixel-to-pixel correspondence between two subjected character image patterns. The pattern distance evaluated under optimized 2DW is invariant to a certain range of geometric deformations. Thus, by using the EM distance as a discriminant function, recognition systems robust to the deformations of handwritten characters can be realized. In this paper, EM techniques are classified according to the type of 2DW and the properties of each class are outlined. Several topics around EM, such as the category-dependent deformation tendency of handwritten characters, are also discussed."
            },
            "slug": "A-Survey-of-Elastic-Matching-Techniques-for-Uchida-Sakoe",
            "title": {
                "fragments": [],
                "text": "A Survey of Elastic Matching Techniques for Handwritten Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Elastic matching techniques are classified according to the type of 2DW and the properties of each class are outlined, and several topics around EM, such as the category-dependent deformation tendency of handwritten characters, are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646100"
                        ],
                        "name": "T. Jones",
                        "slug": "T.-Jones",
                        "structuredName": {
                            "firstName": "Thouis",
                            "lastName": "Jones",
                            "middleNames": [
                                "Raymond"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891751"
                        ],
                        "name": "E. Pasztor",
                        "slug": "E.-Pasztor",
                        "structuredName": {
                            "firstName": "Egon",
                            "lastName": "Pasztor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Pasztor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[45] also have developed a similar method independently based on their past trial [46] on"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6775458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e484ef6b9a1b38a86a8c79b5559a3070f327a033",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We call methods for achieving high-resolution enlargements of pixel-based images super-resolution algorithms. Many applications in graphics or image processing could benefit from such resolution independence, including image-based rendering (IBR), texture mapping, enlarging consumer photographs, and converting NTSC video content to high-definition television. We built on another training-based super-resolution algorithm and developed a faster and simpler algorithm for one-pass super-resolution. Our algorithm requires only a nearest-neighbor search in the training set for a vector derived from each patch of local image data. This one-pass super-resolution algorithm is a step toward achieving resolution independence in image-based representations. We don't expect perfect resolution independence-even the polygon representation doesn't have that-but increasing the resolution independence of pixel-based representations is an important task for IBR."
            },
            "slug": "Example-Based-Super-Resolution-Freeman-Jones",
            "title": {
                "fragments": [],
                "text": "Example-Based Super-Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work built on another training-based super- resolution algorithm and developed a faster and simpler algorithm for one-pass super-resolution that requires only a nearest-neighbor search in the training set for a vector derived from each patch of local image data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer Graphics and Applications"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32907550"
                        ],
                        "name": "N. Ozay",
                        "slug": "N.-Ozay",
                        "structuredName": {
                            "firstName": "Nedret",
                            "lastName": "Ozay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ozay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145940271"
                        ],
                        "name": "B. Sankur",
                        "slug": "B.-Sankur",
                        "structuredName": {
                            "firstName": "B\u00fclent",
                            "lastName": "Sankur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sankur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12774435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "949cc299a7a56a2a9e9379c35efa7dda0f102051",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this study1, we present a fully automatic TV logo identification system. TV logos are detected in static regions given by time-averaged edges subjected to post-processing operations. Once the region of interest of a logo candidate is established, TV logos are recognized via their subspace features. Comparative analysis of features has indicated that ICA-II architecture yields the most discriminative with an accuracy of 99.2% in a database of 3040 logo images (152 varieties). Online tests for both detection and recognition on running videos have achieved 96.0% average accuracy. A more reliable logo identifier will be feasible by improving the accuracy of the extracted logo mask."
            },
            "slug": "Automatic-TV-logo-detection-and-classification-in-Ozay-Sankur",
            "title": {
                "fragments": [],
                "text": "Automatic TV logo detection and classification in broadcast videos"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A fully automatic TV logo identification system that detects TV logos in static regions given by time-averaged edges subjected to post-processing operations and recognizes them via their subspace features."
            },
            "venue": {
                "fragments": [],
                "text": "2009 17th European Signal Processing Conference"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146300157"
                        ],
                        "name": "Yeun-Bae Kim",
                        "slug": "Yeun-Bae-Kim",
                        "structuredName": {
                            "firstName": "Yeun-Bae",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeun-Bae Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105963409"
                        ],
                        "name": "Tetsuya Takeda",
                        "slug": "Tetsuya-Takeda",
                        "structuredName": {
                            "firstName": "Tetsuya",
                            "lastName": "Takeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tetsuya Takeda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10167499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ae735749334af00d99a78a71df2595913ea5b4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a camera system which translates Japanese texts in a scene. The system is portable and consists of four components: digital camera, character image extraction process, character recognition process, and translation process. The system extracts character strings from a region which a user specifies, and translates them into English."
            },
            "slug": "Translation-camera-Watanabe-Okada",
            "title": {
                "fragments": [],
                "text": "Translation camera"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A camera system which translates Japanese texts in a scene using a digital camera, which extracts character strings from a region which a user specifies, and translates them into English."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801509"
                        ],
                        "name": "M. Bertini",
                        "slug": "M.-Bertini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Bertini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8196487"
                        ],
                        "name": "A. Bimbo",
                        "slug": "A.-Bimbo",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bimbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bimbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2308851"
                        ],
                        "name": "W. Nunziati",
                        "slug": "W.-Nunziati",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Nunziati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Nunziati"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5574277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c167008408c301935bade9536084a527527ec74",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In soccer videos, most significant actions are usually followed by close--up shots of players that take part in the action itself. Automatically annotating the identity of the players present in these shots would be considerably valuable for indexing and retrieval applications. Due to high variations in pose and illumination across shots however, current face recognition methods are not suitable for this task. We show how the inherent multiple media structure of soccer videos can be exploited to understand the players' identity without relying on direct face recognition. The proposed method is based on a combination of interest point detector to \"read\" textual cues that allow to label a player with its name, such as the number depicted on its jersey, or the superimposed text caption showing its name. Players not identified by this process are then assigned to one of the labeled faces by means of a face similarity measure, again based on the appearance of local salient patches. We present results obtained from soccer videos taken from various recent games between national teams."
            },
            "slug": "Automatic-detection-of-player's-identity-in-soccer-Bertini-Bimbo",
            "title": {
                "fragments": [],
                "text": "Automatic detection of player's identity in soccer videos using faces and text cues"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how the inherent multiple media structure of soccer videos can be exploited to understand the players' identity without relying on direct face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "MM '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052190427"
                        ],
                        "name": "Megumi Sakai",
                        "slug": "Megumi-Sakai",
                        "structuredName": {
                            "firstName": "Megumi",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Megumi Sakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740235"
                        ],
                        "name": "S. Omachi",
                        "slug": "S.-Omachi",
                        "structuredName": {
                            "firstName": "Shinichiro",
                            "lastName": "Omachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52805409,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b3829b3e93c4b66273020b3ba1610ae6e35e2be8",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel skew estimation method by instances. The instances to be learned (i.e., stored) are rotation invariants and a\u00a0\u00a0rotation variant for each character category. Using the instances, it is possible to estimate a skew angle of each individual character on a document. This fact implies that the proposed method can estimate the skew angle of a document where characters do not form long straight text lines. Thus, the proposed method will be applicable to various\u00a0\u00a0documents such as signboard images captured by a camera. Experimental\u00a0\u00a0evaluation using synthetic and real images revealed the expected\u00a0\u00a0robustness against various character layouts."
            },
            "slug": "Skew-Estimation-by-Instances-Uchida-Sakai",
            "title": {
                "fragments": [],
                "text": "Skew Estimation by Instances"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The proposed skew estimation method by instances will be applicable to various\u00a0\u00a0documents such as signboard images captured by a camera and revealed the expected\u00a0\u00a0robustness against various character layouts."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012891"
                        ],
                        "name": "Seung-Bo Park",
                        "slug": "Seung-Bo-Park",
                        "structuredName": {
                            "firstName": "Seung-Bo",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung-Bo Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065219250"
                        ],
                        "name": "Kyung-Jin Oh",
                        "slug": "Kyung-Jin-Oh",
                        "structuredName": {
                            "firstName": "Kyung-Jin",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyung-Jin Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580650"
                        ],
                        "name": "Heung-Nam Kim",
                        "slug": "Heung-Nam-Kim",
                        "structuredName": {
                            "firstName": "Heung-Nam",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heung-Nam Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144089347"
                        ],
                        "name": "Geun-Sik Jo",
                        "slug": "Geun-Sik-Jo",
                        "structuredName": {
                            "firstName": "Geun-Sik",
                            "lastName": "Jo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geun-Sik Jo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[103] have tried to estimate the position of caption texts by the existence of a speaker."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18786168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d7f792d907cba9ffd4f7db6e131bb092bd38d73",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of online video, efficient captioning and displaying the captioned text (subtitles) have also been issued with the accessibility. However, in most cases, subtitles are shown on a separate display below a screen. As a result, some viewers lose condensed information about the contents of the video. To elevate readability and visibility of viewers, in this paper, we present a framework for displaying synchronized text around a speaker in video. The proposed approach first identifies speakers using face detection technologies and subsequently detects a subtitles region. In addition, we adapt DFXP, which is interoperable timed text format of W3C, to support interchanging with existing legacy system. In order to achieve smooth playback of multimedia presentation, such as SMIL and DFXP, a prototype system, namely MoNaPlayer, has been implemented. Our case studies show that the proposed system is feasible to several multimedia applications."
            },
            "slug": "Automatic-Subtitles-Localization-through-Speaker-in-Park-Oh",
            "title": {
                "fragments": [],
                "text": "Automatic Subtitles Localization through Speaker Identification in Multimedia System"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a framework for displaying synchronized text around a speaker in video, which identifies speakers using face detection technologies and subsequently detects a subtitles region and adapts DFXP, which is interoperable timed text format of W3C, to support interchanging with existing legacy system."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Workshop on Semantic Computing and Applications"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705470"
                        ],
                        "name": "W. Newman",
                        "slug": "W.-Newman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Newman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Newman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920225402"
                        ],
                        "name": "Alex S. Taylor",
                        "slug": "Alex-S.-Taylor",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Taylor",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex S. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111123186"
                        ],
                        "name": "S. Taylor",
                        "slug": "S.-Taylor",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Taylor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116741074"
                        ],
                        "name": "Michael J. Taylor",
                        "slug": "Michael-J.-Taylor",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2687725"
                        ],
                        "name": "T. Aldhous",
                        "slug": "T.-Aldhous",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Aldhous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Aldhous"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecff4edac0a85829fc4ff1a98567279be40accb0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design and evaluation of CamWorks, a system that employs a video camera as a means of supporting capture from paper sources during reading and writing. The user can view a live video image of the source document alongside the electronic document in preparation. We describe a novel user interface developed to support selection of text in the video window, and several new techniques for segmentation, restoration and resolution enhancement of camera images. An evaluation shows substantially faster text capture than with flatbed scanning."
            },
            "slug": "CamWorks:-a-video-based-tool-for-efficient-capture-Newman-Dance",
            "title": {
                "fragments": [],
                "text": "CamWorks: a video-based tool for efficient capture from paper source documents"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The design and evaluation of CamWorks, a system that employs a video camera as a means of supporting capture from paper sources during reading and writing, shows substantially faster text capture than with flatbed scanning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075418943"
                        ],
                        "name": "Pan Pan",
                        "slug": "Pan-Pan",
                        "structuredName": {
                            "firstName": "Pan",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pan Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854285"
                        ],
                        "name": "Yuanping Zhu",
                        "slug": "Yuanping-Zhu",
                        "structuredName": {
                            "firstName": "Yuanping",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanping Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291081"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753831"
                        ],
                        "name": "S. Naoi",
                        "slug": "S.-Naoi",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Naoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Naoi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5070520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06e5055fb6d4f989533292e548851639de6205ea",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel method to recognize characters with severe perspective distortion using hash tables and perspective invariants. The proposed algorithm consists of storage and voting stages. With the help of perspective invariants, the combinations of 4-tuple bases for the perspective invariant coordinate system are searched out in an efficiently way. The bases are further selected so that the resulting transformation is effective. The characters' features under the perspective invariant coordinate system determine an entry in a one dimensional hash table, which is applied for storage and retrieval. Experimental results show the superior performance of the proposed method in comparison to other existing methods."
            },
            "slug": "Recognizing-Characters-with-Severe-Perspective-Hash-Pan-Zhu",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters with Severe Perspective Distortion Using Hash Tables and Perspective Invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel method to recognize characters with severe perspective distortion using hash tables and perspective invariants and results show the superior performance of the proposed method in comparison to other existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052190427"
                        ],
                        "name": "Megumi Sakai",
                        "slug": "Megumi-Sakai",
                        "structuredName": {
                            "firstName": "Megumi",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Megumi Sakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740235"
                        ],
                        "name": "S. Omachi",
                        "slug": "S.-Omachi",
                        "structuredName": {
                            "firstName": "Shinichiro",
                            "lastName": "Omachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6402753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c59737970ec1becb4fe901f8fc64cb9179c599ff",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with a universal pattern, which is defined as a character pattern designed to have high machine-readability. This universal pattern is a character pattern printed with stripes. The cross ratio calculated from the widths of the stripes represents the character class. Thus, if the boundaries of the stripes can be detected for measuring the widths, the class can be determined without ordinary recognition process. Furthermore, since the cross ratio is invariant to projective distortions, the correct class will be still determined under those distortions. This paper describes a practical scheme to recognize this universal pattern. The proposed scheme includes a novel algorithm to detect the stripe boundaries stably even from the universal pattern image contaminated by non-uniform lighting and noise. The algorithm is realized by a combination of a dynamic programming-based optimal boundary detection and a finite state automaton which represents the property of the universal pattern. Experimental results showed the proposed scheme could recognize 99.6% of the universal pattern images which underwent heavy projective distortions and non-uniform lighting."
            },
            "slug": "Extraction-of-Embedded-Class-Information-from-Uchida-Sakai",
            "title": {
                "fragments": [],
                "text": "Extraction of Embedded Class Information from Universal Character Pattern"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed scheme includes a novel algorithm to detect the stripe boundaries stably even from the universal pattern image contaminated by non-uniform lighting and noise, realized by a combination of a dynamic programming-based optimal boundary detection and a finite state automaton which represents the property of theuniversal pattern."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49606029"
                        ],
                        "name": "Jinqiao Wang",
                        "slug": "Jinqiao-Wang",
                        "structuredName": {
                            "firstName": "Jinqiao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinqiao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7667912"
                        ],
                        "name": "Ling-yu Duan",
                        "slug": "Ling-yu-Duan",
                        "structuredName": {
                            "firstName": "Ling-yu",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling-yu Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145367435"
                        ],
                        "name": "Zhenglong Li",
                        "slug": "Zhenglong-Li",
                        "structuredName": {
                            "firstName": "Zhenglong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenglong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46701354"
                        ],
                        "name": "J. Liu",
                        "slug": "J.-Liu",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694235"
                        ],
                        "name": "Hanqing Lu",
                        "slug": "Hanqing-Lu",
                        "structuredName": {
                            "firstName": "Hanqing",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanqing Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50736578"
                        ],
                        "name": "Jesse S. Jin",
                        "slug": "Jesse-S.-Jin",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Jin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse S. Jin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12481681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2af59ec662b75eb50d9b6c3aec5002a006c4770e",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Most broadcast stations rely on TV logos to claim video content ownership or visually distinguish the broadcast from the interrupting commercial block. Detecting and tracking a TV logo is of interest to TV commercial skipping applications and logo-based broadcasting surveillance (abnormal signal is accompanied by logo absence). Pixel-wise difference computing within predetermined logo regions cannot address semi-transparent TV logos well for the blending effects of a logo itself and inconstant background images. Edge-based template matching is weak for semi-transparent ones when incomplete edges appear. In this paper we present a more robust approach to detect and track TV logos in video streams on the basis of multispectral images gradient. Instead of single frame based detection, our approach makes use of the temporal correlation of multiple consecutive frames. Since it is difficult to manually delineate logos of irregular shape, an adaptive threshold is applied to the gradient image in subpixel space to extract the logo mask. TV logo tracking is finally carried out by matching the masked region with a known template. An extensive comparison experiment has shown our proposed algorithm outperforms traditional methods such as frame difference, single frame-based edge matching. Our experimental dataset comes from part of TRECVID2005 news corpus and several Chinese TV channels with challenging TV logos"
            },
            "slug": "A-Robust-Method-for-TV-Logo-Tracking-in-Video-Wang-Duan",
            "title": {
                "fragments": [],
                "text": "A Robust Method for TV Logo Tracking in Video Streams"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a more robust approach to detect and track TV logos in video streams on the basis of multispectral images gradient that makes use of the temporal correlation of multiple consecutive frames."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645506"
                        ],
                        "name": "P. Over",
                        "slug": "P.-Over",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Over",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Over"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740640"
                        ],
                        "name": "Wessel Kraaij",
                        "slug": "Wessel-Kraaij",
                        "structuredName": {
                            "firstName": "Wessel",
                            "lastName": "Kraaij",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wessel Kraaij"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "N uu m an n an d M at as [1 28 ]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "TRECVID [28] is a famous competition for improving content-based video retrieval and annotation and other video understanding technologies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7677566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb1f624b56eca597d65d27b8da7c4f2cd4b8531",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The TREC Video Retrieval Evaluation (TRECVid)is an international benchmarking activity to encourage research in video information retrieval by providing a large test collection, uniform scoring procedures, and a forum for organizations 1 interested in comparing their results. TRECVid completed its fifth annual cycle at the end of 2005 and in 2006 TRECVid will involve almost 70 research organizations, universities and other consortia. Throughout its existence, TRECVid has benchmarked both interactive and automatic/manual searching for shots from within a video corpus,automatic detection of a variety of semantic and low-level video features, shot boundary detection and the detection of story boundaries in broadcast TV news. This paper will give an introduction to information retrieval (IR) evaluation from both a user and a system perspective, high-lighting that system evaluation is by far the most prevalent type of evaluation carried out. We also include a summary of TRECVid as an example of a system evaluation bench-marking campaign and this allows us to discuss whether such campaigns are a good thing or a bad thing. There are arguments for and against these campaigns and we present some of them in the paper concluding that on balance they have had a very positive impact on research progress."
            },
            "slug": "Evaluation-campaigns-and-TRECVid-Smeaton-Over",
            "title": {
                "fragments": [],
                "text": "Evaluation campaigns and TRECVid"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An introduction to information retrieval (IR) evaluation from both a user and a system perspective is given, high-lighting that system evaluation is by far the most prevalent type of evaluation carried out."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rough text position by user\u2019s interaction, it can extract the corresponding text region precisely by using, for example, grabcut [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6202829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a953aaf29ef67ee094943d4be50d753b3744573",
            "isKey": false,
            "numCitedBy": 5201,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools."
            },
            "slug": "\"GrabCut\":-interactive-foreground-extraction-using-Rother-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "\"GrabCut\": interactive foreground extraction using iterated graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more powerful, iterative version of the optimisation of the graph-cut approach is developed and the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109552037"
                        ],
                        "name": "Dongqing Zhang",
                        "slug": "Dongqing-Zhang",
                        "structuredName": {
                            "firstName": "Dongqing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongqing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1 29 ], R on g et al ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Zhang and Chang [29] have developed an event analysis method for baseball game videos based on caption text recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 583652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ec6dee3f7cd0c1571d179d756afc823cda636b",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a novel system for baseball video event detection and summarization using superimposed caption text detection and recognition. The system detects different types of semantic level events in baseball video including scoring and last pitch of each batter. The system has two components: event detection and event boundary detection. Event detection is realized by change detection and recognition of game stat texts (such as text information showing in score box). Event boundary detection is achieved using our previously developed algorithm, which detects the pitch view as the event beginning and nonactive view as potential endings of the event. One unique contribution of the system is its capability to accurately detect the semantic level events by combining video text recognition with camera view recognition. Another unique feature is the real-time processing speed by taking advantage of compressed-domain approaches in part of the algorithms such as caption detection. To the best of our knowledge, this is the first system achieving accurate detection of multiple types of high-level semantic events in baseball videos."
            },
            "slug": "Event-detection-in-baseball-video-using-caption-Zhang-Chang",
            "title": {
                "fragments": [],
                "text": "Event detection in baseball video using superimposed caption recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "To the best of the knowledge, this is the first system achieving accurate detection of multiple types of high-level semantic events in baseball videos by combining video text recognition with camera view recognition."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39691427"
                        ],
                        "name": "Yongmei Liu",
                        "slug": "Yongmei-Liu",
                        "structuredName": {
                            "firstName": "Yongmei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongmei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692911"
                        ],
                        "name": "Toshimitsu Tanaka",
                        "slug": "Toshimitsu-Tanaka",
                        "structuredName": {
                            "firstName": "Toshimitsu",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshimitsu Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453680"
                        ],
                        "name": "T. Yamamura",
                        "slug": "T.-Yamamura",
                        "structuredName": {
                            "firstName": "Tsuyoshi",
                            "lastName": "Yamamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yamamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707492"
                        ],
                        "name": "N. Ohnishi",
                        "slug": "N.-Ohnishi",
                        "structuredName": {
                            "firstName": "Noboru",
                            "lastName": "Ohnishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ohnishi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36671363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cbdeda810ed2b65ea5eb2137ecc2c9143e0dcb0",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for navigating a mobile robot in the environment using signboards in scenes as landmarks. This method will enable us to realize human-like navigation and map-generation, and also can benefit human-robot communication. We choose signboards on walls and doors as landmarks. An environment map that contains the approximate position of each signboard in the environment is generated by the robot when the robot explores the environment the first time, and it will be used for re-localizing and navigating the robot afterward. First, characters in scene images are detected by using several heuristics of characters and character lines. Then, we calculate the relative orientation of the camera and a detected signboard from a single view using two methods based on the geometry of perspective projection. Finally, we reconstruct a distorted signboard image using two corresponding methods, and this makes the recognition of characters under a wider range of viewing condition much easier."
            },
            "slug": "Character-based-mobile-robot-navigation-Liu-Tanaka",
            "title": {
                "fragments": [],
                "text": "Character-based mobile robot navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A method for navigating a mobile robot in the environment using signboards in scenes as landmarks will enable human-like navigation and map-generation, and also can benefit human-robot communication."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129618"
                        ],
                        "name": "Qigong Zheng",
                        "slug": "Qigong-Zheng",
                        "structuredName": {
                            "firstName": "Qigong",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qigong Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kanungo and Zheng [51] have proved that the knowledge of the font type of the target document is helpful to estimate the parameters of document image degradations, such as blurring."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16461896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81668aba53d3c640ae691bfd1194d8472d1007f9",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Noise models are crucial for designing image restoration algorithms, generating synthetic training data, and predicting algorithm performance. There are two related but distinct estimation scenarios. The first is model calibration, where it is assumed that the input ideal bitmap and the output of the degradation process are both known. The second is the general estimation problem, where only the image from the output of the degradation process is given. While researchers have addressed the problem of calibration of models, issues with the general estimation problems have not been addressed in the literature. In this paper, we describe a parameter estimation algorithm for a morphological, binary, page-level image degradation model. The inputs to the estimation algorithm are 1) the degraded image and 2) information regarding the font type (italic, bold, serif, sans serif). We simulate degraded images using our model and search for the optimal parameter by looking for a parameter value for which the local neighborhood pattern distributions in the simulated image and the given degraded image are most similar. The parameter space is searched using a direct search optimization algorithm. We use the p-value of the Kolmogorov-Smirnov test as the measure of similarity between the two neighborhood pattern distributions. We show results of our algorithm on degraded document images."
            },
            "slug": "Estimating-degradation-model-parameters-using-an-Kanungo-Zheng",
            "title": {
                "fragments": [],
                "text": "Estimating degradation model parameters using neighborhood pattern distributions: an optimization approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a parameter estimation algorithm for a morphological, binary, page-level image degradation model, and uses the p-value of the Kolmogorov-Smirnov test as the measure of similarity between the two neighborhood pattern distributions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003044"
                        ],
                        "name": "Mario E. Munich",
                        "slug": "Mario-E.-Munich",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Munich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mario E. Munich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [20], an early attempt of a video camera-based system has been proposed, where handwriting patterns are captured by pen-tip tracking."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30755492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bb7046907e84619d78e281fd8d5c55321123b3c",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwriting may be captured using a video camera, rather than the customary pressure-sensitive tablet. This paper presents a simple system based on correlation and recursive prediction methods that can track the tip of the pen in real time with sufficient spatio-temporal resolution and accuracy to enable handwritten character recognition. The system is tested on a large and heterogeneous set of examples and its performance is compared to that of three human operators and a commercial high-resolution pressure-sensitive tablet."
            },
            "slug": "Visual-input-for-pen-based-computers-Munich-Perona",
            "title": {
                "fragments": [],
                "text": "Visual input for pen-based computers"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A simple system based on correlation and recursive prediction methods that can track the tip of the pen in real time with sufficient spatio-temporal resolution and accuracy to enable handwritten character recognition is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144406261"
                        ],
                        "name": "Shmuel Peleg",
                        "slug": "Shmuel-Peleg",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Peleg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shmuel Peleg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4834546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42d60f7faaa2f6fdd2b928c352d65eb57b4791aa",
            "isKey": false,
            "numCitedBy": 1933,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-resolution-by-image-registration-Irani-Peleg",
            "title": {
                "fragments": [],
                "text": "Improving resolution by image registration"
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067522034"
                        ],
                        "name": "Martin Urban",
                        "slug": "Martin-Urban",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Urban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Urban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, MSER [119] has often been chosen as a more stable method for connected component analysis; however, even MSER may not be perfect on, for example, the case of this \u201cM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41908779,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "996767727f783c1297d1fb0a4fd5e37e9bf0cd1c",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-wide-baseline-stereo-from-maximally-stable-Matas-Chum",
            "title": {
                "fragments": [],
                "text": "Robust wide-baseline stereo from maximally stable extremal regions"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111818764"
                        ],
                        "name": "Linlin Li",
                        "slug": "Linlin-Li",
                        "structuredName": {
                            "firstName": "Linlin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linlin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", perspective) invariant feature In [134, 135], the matching process is accelerated by a hash technique Lu and Tan [145], Lin and Tan [146], Iwamura et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8257967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95e50d653974b4107a23c7de167fda6ae7a4a0a0",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A common problem encountered in recognizing real-scene symbols is the perspective deformation. In this paper, a recognition method resistant to perspective deformation is proposed, based on Cross-Ratio Spectrum descriptor. This method shows good resistance to severe perspective deformation and good discriminating power to similar symbols."
            },
            "slug": "Recognizing-Planar-Symbols-with-Severe-Perspective-Li-Tan",
            "title": {
                "fragments": [],
                "text": "Recognizing Planar Symbols with Severe Perspective Deformation"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A recognition method resistant to perspective deformation is proposed, based on Cross-Ratio Spectrum descriptor, which shows good resistance to severe perspectiveDeformation and good discriminating power to similar symbols."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2450108"
                        ],
                        "name": "G. Cheung",
                        "slug": "G.-Cheung",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Cheung",
                            "middleNames": [
                                "K.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cheung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859983"
                        ],
                        "name": "Ahmad Abdulkader",
                        "slug": "Ahmad-Abdulkader",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Abdulkader",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Abdulkader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935554"
                        ],
                        "name": "M. Zennaro",
                        "slug": "M.-Zennaro",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Zennaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zennaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115595151"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726358"
                        ],
                        "name": "A. Bissacco",
                        "slug": "A.-Bissacco",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bissacco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bissacco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665814"
                        ],
                        "name": "H. Neven",
                        "slug": "H.-Neven",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Neven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073317450"
                        ],
                        "name": "Luc Vincent",
                        "slug": "Luc-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luc Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 11 5]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In some scene browsing service, such as Google Street View, car license plates should be detected and hidden for privacy preservation [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1964985,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01cb576adb5657959e0d925f0d510ed602648813",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The last two years have witnessed the introduction and rapid expansion of products based upon large, systematically-gathered, street-level image collections, such as Google Street View, EveryScape, and Mapjack. In the process of gathering images of public spaces, these projects also capture license plates, faces, and other information considered sensitive from a privacy standpoint. In this work, we present a system that addresses the challenge of automatically detecting and blurring faces and license plates for the purpose of privacy protection in Google Street View. Though some in the field would claim face detection is \u201csolved\u201d, we show that state-of-the-art face detectors alone are not sufficient to achieve the recall desired for large-scale privacy protection. In this paper we present a system that combines a standard sliding-window detector tuned for a high recall, low-precision operating point with a fast post-processing stage that is able to remove additional false positives by incorporating domain-specific information not available to the sliding-window detector. Using a completely automatic system, we are able to sufficiently blur more than 89% of faces and 94 \u2013 96% of license plates in evaluation sets sampled from Google Street View imagery."
            },
            "slug": "Large-scale-privacy-protection-in-Google-Street-Frome-Cheung",
            "title": {
                "fragments": [],
                "text": "Large-scale privacy protection in Google Street View"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a system that combines a standard sliding-window detector tuned for a high recall, low-precision operating point with a fast post-processing stage that is able to remove additional false positives by incorporating domain-specific information not available to the sliding- window detector."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8021225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "078da887d298a56257a8c94136fd2a0ab351da25",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a super-resolution technique specifically aimed at enhancing low-resolution text images from handheld devices. The Teager filter, a quadratic unsharp masking filter, is used to highlight high frequencies which are then combined with the warped and interpolated image sequence following motion estimation using Taylor series decomposition. Comparative performance evaluation is presented in the form of OCR results of the super-resolution"
            },
            "slug": "Super-Resolution-Text-using-the-Teager-Filter-Mancas-Thillou-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Super-Resolution Text using the Teager Filter"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The Teager filter, a quadratic unsharp masking filter, is used to highlight high frequencies which are then combined with the warped and interpolated image sequence following motion estimation using Taylor series decomposition to create a super-resolution image."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32295201"
                        ],
                        "name": "Kazuya Sono",
                        "slug": "Kazuya-Sono",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Sono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Sono"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961490"
                        ],
                        "name": "Kazuya Yokomizo",
                        "slug": "Kazuya-Yokomizo",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Yokomizo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Yokomizo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [11\u201314])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11008332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44f2146d9176db4ec619287f4372252c6650e644",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we report a mobile application, which translates Japanese texts in a scene into English. The application is designed to run on a mobile phone with a camera. It recognizes Japanese characters sensed by an onboard camera of a mobile phone and translates them into English."
            },
            "slug": "Translation-camera-on-mobile-phone-Watanabe-Sono",
            "title": {
                "fragments": [],
                "text": "Translation camera on mobile phone"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A mobile application, which translates Japanese texts in a scene into English by recognizing Japanese characters sensed by an onboard camera of a mobile phone and translates them into English."
            },
            "venue": {
                "fragments": [],
                "text": "2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907152"
                        ],
                        "name": "M. Cutter",
                        "slug": "M.-Cutter",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cutter",
                            "middleNames": [
                                "Patrick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342880"
                        ],
                        "name": "J. V. Beusekom",
                        "slug": "J.-V.-Beusekom",
                        "structuredName": {
                            "firstName": "Joost",
                            "lastName": "Beusekom",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Beusekom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145461897"
                        ],
                        "name": "S. S. Bukhari",
                        "slug": "S.-S.-Bukhari",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Bukhari",
                            "middleNames": [
                                "Saqib"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S. Bukhari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5544009,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "be1427cbf7febeb9f48397321fa6ee93b83c199b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Scholarly content needs to be online, and for much mass produced content, that migration has already happened. Unfortunately, the online presence of scholarly content is much more sporadic for long tail material such as small journals, original source materials in the humanities and social sciences, non-journal periodicals, and more. A large barrier to this content being available is the cost and complexity of setting up a digitization project for small and scattered collections coupled with a lack of revenue opportunities to recoup those costs. Collections with limited audiences and hence limited revenue opportunities are nonetheless often of considerable scholarly importance within their domains. The expense and difficulty of digitization presents a significant obstacle to making such paper archives available online. To address this problem, the Decapod project aims at providing a solution that is primarily suitable for small to medium paper archives with material that is rare or unique and is of sufficient interest that it warrants being made more widely available. This paper gives an overview of the project and presents its current status."
            },
            "slug": "Decapod:-A-Flexible,-Low-Cost-Digitization-Solution-Shafait-Cutter",
            "title": {
                "fragments": [],
                "text": "Decapod: A Flexible, Low Cost Digitization Solution for Small and Medium Archives"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The Decapod project aims at providing a solution that is primarily suitable for small to medium paper archives with material that is rare or unique and is of sufficient interest that it warrants being made more widely available."
            },
            "venue": {
                "fragments": [],
                "text": "CBDAR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34550850"
                        ],
                        "name": "Yibin Tian",
                        "slug": "Yibin-Tian",
                        "structuredName": {
                            "firstName": "Yibin",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yibin Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067998326"
                        ],
                        "name": "Wei Ming",
                        "slug": "Wei-Ming",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Ming",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Ming"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29894385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61232b2480cb31b2a85f6d15fb892c3e10d72cef",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Deblurring-for-Camera-Based-Document-Image-Tian-Ming",
            "title": {
                "fragments": [],
                "text": "Adaptive Deblurring for Camera-Based Document Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": "ISVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080329"
                        ],
                        "name": "B. Bayarsaikhan",
                        "slug": "B.-Bayarsaikhan",
                        "structuredName": {
                            "firstName": "Battulga",
                            "lastName": "Bayarsaikhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bayarsaikhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779402"
                        ],
                        "name": "Younghee Kwon",
                        "slug": "Younghee-Kwon",
                        "structuredName": {
                            "firstName": "Younghee",
                            "lastName": "Kwon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younghee Kwon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[41] have proposed a prior evaluating the total variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52801116,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0f909c729e320d778e89052d5d5ec8ccff9e0349",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a text image super resolution algorithm based on total variation (TV). Text images typically consist of slim strokes on background. Thus, there are three different local characteristics as homogeneous, directed and complex on text image. Homogeneous region corresponds to background and directed means the region with dominant stroke direction and remaining is complex region. We proposed higher order smoothing on homogeneous region and anisotropic regularization on directed region which encodes the preference of edge direction by smoothing along preferred direction only. Required regularization terms are combined in proposed anisotropic TV functional and controlled by relating parameters. We calculated relating parameters byutilizing structure tensor field. Also to reduce the computational cost, we previously estimated nonchanging pixels and exclude them from calculation for speed up. Experiments shown that, proposed method performs better with low computational cost than general purpose TV on text image."
            },
            "slug": "Anisotropic-Total-Variation-Method-for-Text-Image-Bayarsaikhan-Kwon",
            "title": {
                "fragments": [],
                "text": "Anisotropic Total Variation Method for Text Image Super-Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A text image super resolution algorithm based on total variation (TV) which encodes the preference of edge direction by smoothing along preferred direction only and performs better with low computational cost than general purpose TV on text image."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 17
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 136,
        "totalPages": 14
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-Localization-and-Recognition-in-Images-and-Uchida/9c9e122ed4f3fd74dce9c30eb98cb512bdfa691c?sort=total-citations"
}