{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403031665"
                        ],
                        "name": "B. Romera-Paredes",
                        "slug": "B.-Romera-Paredes",
                        "structuredName": {
                            "firstName": "Bernardino",
                            "lastName": "Romera-Paredes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Romera-Paredes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [33, 31], an RNN is used to produce one object mask per time step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Instead of labeling pixels, [4] predicts a polygon outlining the object instance using a Recur-\nrent Neural Network (RNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "However, since RNNs typically do not perform well when the number of time steps is large, these methods have difficulties in scaling up to the challenging multi-object street scenes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Finally, RNN [33, 31] was used to predict an object label at each time step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7001625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6011f8993aa49da62572e23923c588a65700c4c5",
            "isKey": true,
            "numCitedBy": 293,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting."
            },
            "slug": "Recurrent-Instance-Segmentation-Romera-Paredes-Torr",
            "title": {
                "fragments": [],
                "text": "Recurrent Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work proposes a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially, based on a recurrent neural network that sequentially finds objects and their segmentations one at a time."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540599"
                        ],
                        "name": "Mengye Ren",
                        "slug": "Mengye-Ren",
                        "structuredName": {
                            "firstName": "Mengye",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengye Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "However, since RNNs typically do not perform well when the number of time steps is large, these methods have difficulties in scaling up to the challenging multi-object street scenes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Instead of labeling pixels, [4] predicts a polygon outlining the object instance using a Recur-\nrent Neural Network (RNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "Thus we also report the Mean Weighted Coverage (MWCov) [31, 37], which is the average IoU of prediction matched with ground-truth instances weighted by the size of the ground-truth instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Finally, RNN [33, 31] was used to predict an object label at each time step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [33, 31], an RNN is used to produce one object mask per time step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16364905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3f4f163e87b28901389e189bb7f0f655995793f",
            "isKey": true,
            "numCitedBy": 60,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset and KITTI vehicle segmentation dataset."
            },
            "slug": "End-to-End-Instance-Segmentation-and-Counting-with-Ren-Zemel",
            "title": {
                "fragments": [],
                "text": "End-to-End Instance Segmentation and Counting with Recurrent Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31638576"
                        ],
                        "name": "Anurag Arnab",
                        "slug": "Anurag-Arnab",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Arnab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Arnab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 2], a CRF is used to assign each pixel to an object detection box by exploiting semantic segmentation maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14707395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a1b4be7ee1445bb71fd1091b8312c5c35bd72a1",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional Scene Understanding problems such as Object Detection and Semantic Segmentation have made breakthroughs in recent years due to the adoption of deep learning. However, the former task is not able to localise objects at a pixel level, and the latter task has no notion of different instances of objects of the same class. We focus on the task of Instance Segmentation which recognises and localises objects down to a pixel level. Our model is based on a deep neural network trained for semantic segmentation. This network incorporates a Conditional Random Field with end-to-end trainable higher order potentials based on object detector outputs. This allows us to reason about instances from an initial, category-level semantic segmentation. Our simple method effectively leverages the great progress recently made in semantic segmentation and object detection. The accurate instance-level segmentations that our network produces is reflected by the considerable improvements obtained over previous work."
            },
            "slug": "Bottom-up-Instance-Segmentation-using-Deep-CRFs-Arnab-Torr",
            "title": {
                "fragments": [],
                "text": "Bottom-up Instance Segmentation using Deep Higher-Order CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work focuses on the task of Instance Segmentation which recognises and localises objects down to a pixel level and uses a deep neural network trained for semantic segmentation to reason about instances."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31638576"
                        ],
                        "name": "Anurag Arnab",
                        "slug": "Anurag-Arnab",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Arnab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Arnab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "Note that the LRR model we use is pre-trained on MS COCO [25], which is also used by DIN [1] for pre-training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "As shown in Table 5 our method outperforms DIN [1] by 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "Most of the instance segmentation approaches employ a two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection boxes [15, 16, 1, 29, 30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "As pointed in [3, 1], AP favors detection-based methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 2], a CRF is used to assign each pixel to an object detection box by exploiting semantic segmentation maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "As shown in Table 5 our method outperforms DIN [1] by 1.1 points in terms of APravg."
                    },
                    "intents": []
                }
            ],
            "corpusId": 689238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b34c8448426cc08165c17f4fd72459ca157fdd",
            "isKey": true,
            "numCitedBy": 185,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our substantial improvements at high APr thresholds."
            },
            "slug": "Pixelwise-Instance-Segmentation-with-a-Dynamically-Arnab-Torr",
            "title": {
                "fragments": [],
                "text": "Pixelwise Instance Segmentation with a Dynamically Instantiated Network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label, and far more precise segmentations are achieved, as shown by substantial improvements at high APr thresholds."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065478626"
                        ],
                        "name": "Min Bai",
                        "slug": "Min-Bai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Note that most instance segmentation algorithms cannot tackle the fragmentation problem [3, 26, 18, 41, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "More recently, [3] learned a convolutional net to predict the energy of the watershed transform."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Following [3, 18], we utilize semantic segmentation to identify foreground pixels, and restrict our reasoning to them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Following [3], we remove small instances and use semantic scores from semantic segmentation to rank predictions for {train, bus, car, truck}."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "As pointed in [3, 1], AP favors detection-based methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Recently, Bai and Urtasun [3] utilized a CNN to learn an energy of the watershed transform."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18195291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6066637ab60a75731118659778d9616f6d3be4c",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In this paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as energy basins. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model achieves more than double the performance over the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task."
            },
            "slug": "Deep-Watershed-Transform-for-Instance-Segmentation-Bai-Urtasun",
            "title": {
                "fragments": [],
                "text": "Deep Watershed Transform for Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a simple yet powerful end-to-end convolutional neural network that achieves more than double the performance over the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028761"
                        ],
                        "name": "Zeeshan Hayder",
                        "slug": "Zeeshan-Hayder",
                        "structuredName": {
                            "firstName": "Zeeshan",
                            "lastName": "Hayder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeeshan Hayder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33913193"
                        ],
                        "name": "Xuming He",
                        "slug": "Xuming-He",
                        "structuredName": {
                            "firstName": "Xuming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862871"
                        ],
                        "name": "M. Salzmann",
                        "slug": "M.-Salzmann",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Salzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Salzmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "Most of the instance segmentation approaches employ a two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection boxes [15, 16, 1, 29, 30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "SAIS [16] improved MNC by learning to predict distances to instance boundaries, which are then decoded into high-quality masks by a set of convolution operations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10367240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "888ddf8f543b2e0794f4a021d80a23d4eb05c8af",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of instance-level semantic segmentation, which aims at jointly detecting, segmenting and classifying every individual object in an image. In this context, existing methods typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each such proposal. As a consequence, they cannot recover from errors in the object candidate generation process, such as too small or shifted boxes. In this paper, we introduce a novel object segment representation based on the distance transform of the object masks. We then design an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask. This allows us to predict masks that go beyond the scope of the bounding boxes and are thus robust to inaccurate object candidates. We integrate our OMN into a Multitask Network Cascade framework, and learn the resulting shape-aware instance segmentation (SAIS) network in an end-to-end manner. Our experiments on the PASCAL VOC 2012 and the CityScapes datasets demonstrate the benefits of our approach, which outperforms the state-of-the-art in both object proposal generation and instance segmentation."
            },
            "slug": "Shape-aware-Instance-Segmentation-Hayder-He",
            "title": {
                "fragments": [],
                "text": "Shape-aware Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces a novel object segment representation based on the distance transform of the object masks, and designs an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Similarly, MNC [9] designed an end-to-end trainable multi-task network cascade to unify bounding box proposal generation, pixel-wise mask proposal generation and classifica-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "In [29, 30], highquality mask proposals were generated using CNNs. Similarly, MNC [9] designed an end-to-end trainable multi-task network cascade to unify bounding box proposal generation, pixel-wise mask proposal generation and classifica-\n1\ntion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "SAIS [16] improved MNC by learning to predict distances to instance boundaries, which are then decoded into high-quality masks by a set of convolution operations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8510667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e9b1f6061ef779e3ad0819c2832a29168eaeb9d",
            "isKey": false,
            "numCitedBy": 984,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place."
            },
            "slug": "Instance-Aware-Semantic-Segmentation-via-Multi-task-Dai-He",
            "title": {
                "fragments": [],
                "text": "Instance-Aware Semantic Segmentation via Multi-task Network Cascades"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents Multitask Network Cascades for instance-aware semantic segmentation, which consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects, and develops an algorithm for the nontrivial end-to-end training of this causal, cascaded structure."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779129"
                        ],
                        "name": "Shu Liu",
                        "slug": "Shu-Liu",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50844674"
                        ],
                        "name": "Xiaojuan Qi",
                        "slug": "Xiaojuan-Qi",
                        "structuredName": {
                            "firstName": "Xiaojuan",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojuan Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788070"
                        ],
                        "name": "Jianping Shi",
                        "slug": "Jianping-Shi",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46701933"
                        ],
                        "name": "Hong Zhang",
                        "slug": "Hong-Zhang",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Note that most instance segmentation algorithms cannot tackle the fragmentation problem [3, 26, 18, 41, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "MPA [26] modeled objects as composed of generic parts, which are predicted in a sliding window fashion and then aggregated to produce instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15238184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e5a1da43c62ea9b0b5df832221a7b1cb641427e",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Aiming at simultaneous detection and segmentation (SD-S), we propose a proposal-free framework, which detect and segment object instances via mid-level patches. We design a unified trainable network on patches, which is followed by a fast and effective patch aggregation algorithm to infer object instances. Our method benefits from end-to-end training. Without object proposal generation, computation time can also be reduced. In experiments, our method yields results 62.1% and 61.8% in terms of mAPr on VOC2012 segmentation val and VOC2012 SDS val, which are state-of-the-art at the time of submission. We also report results on Microsoft COCO test-std/test-dev dataset in this paper."
            },
            "slug": "Multi-scale-Patch-Aggregation-(MPA)-for-Detection-Liu-Qi",
            "title": {
                "fragments": [],
                "text": "Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A unified trainable network on patches is designed, which is followed by a fast and effective patch aggregation algorithm to infer object instances via mid-level patches, which benefits from end-to-end training."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "We believe that APr at higher IoU thresholds are more informative."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "Most of the instance segmentation approaches employ a two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection boxes [15, 16, 1, 29, 30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "IoU as a function of ground-truth sizes.\nwe average APr at IoU threshold ranging from 0.5 to 0.9 with step-size 0.1 (instead of 0.1 to 0.9)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 45
                            }
                        ],
                        "text": "The pioneering work of instance segmentation [15, 8] aimed at both classifying object proposals as well as labeling an object within each box."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "We use APr [15] as the evaluation metric, representing the region AP at a specific IoU threshold."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "As shown in Table 5 our method outperforms DIN [1] by 1.1 points in terms of APravg."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9272368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "342786659379879f58bf5c4ff43c84c83a6a7389",
            "isKey": true,
            "numCitedBy": 1062,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work."
            },
            "slug": "Simultaneous-Detection-and-Segmentation-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Simultaneous Detection and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work builds on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN), introducing a novel architecture tailored for SDS, and uses category-specific, top-down figure-ground predictions to refine the bottom-up proposals."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095572"
                        ],
                        "name": "Ziyu Zhang",
                        "slug": "Ziyu-Zhang",
                        "structuredName": {
                            "firstName": "Ziyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068227"
                        ],
                        "name": "A. Schwing",
                        "slug": "A.-Schwing",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schwing",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schwing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "Methods of [41, 40] extracted patches from the image and used a CNN to directly infer instance IDs inside each patch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Note that most instance segmentation algorithms cannot tackle the fragmentation problem [3, 26, 18, 41, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [41, 40], large patches are exhaustively extracted from an image and a Convolutional Neural Network (CNN) is trained to predict instance labels inside each patch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1894392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2bafe41bd237ec67fdd21410eecffdac9bb16aa",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance ID encodes the depth ordering within image patches. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm. It aims to predict accurate instance-level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show good performance on both tasks."
            },
            "slug": "Monocular-Object-Instance-Segmentation-and-Depth-Zhang-Schwing",
            "title": {
                "fragments": [],
                "text": "Monocular Object Instance Segmentation and Depth Ordering with CNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A Markov random field is developed which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm and aims to predict accurate instance-level segmentation and depth ordering."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144843400"
                        ],
                        "name": "Alexander Kirillov",
                        "slug": "Alexander-Kirillov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kirillov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kirillov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154212"
                        ],
                        "name": "Evgeny Levinkov",
                        "slug": "Evgeny-Levinkov",
                        "structuredName": {
                            "firstName": "Evgeny",
                            "lastName": "Levinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evgeny Levinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16576043"
                        ],
                        "name": "Bjoern Andres",
                        "slug": "Bjoern-Andres",
                        "structuredName": {
                            "firstName": "Bjoern",
                            "lastName": "Andres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bjoern Andres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792728"
                        ],
                        "name": "Bogdan Savchynskyy",
                        "slug": "Bogdan-Savchynskyy",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Savchynskyy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bogdan Savchynskyy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Note that most instance segmentation algorithms cannot tackle the fragmentation problem [3, 26, 18, 41, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [18], semantic segmentation and object boundary prediction were exploited to separate instances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Following [3, 18], we utilize semantic segmentation to identify foreground pixels, and restrict our reasoning to them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8334461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1226a8943e813bf60035bd59442bea7de83cbc3d",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This work addresses the task of instance-aware semantic segmentation. Our key motivation is to design a simple method with a new modelling-paradigm, which therefore has a different trade-off between advantages and disadvantages compared to known approaches. Our approach, we term InstanceCut, represents the problem by two output modalities: (i) an instance-agnostic semantic segmentation and (ii) all instance-boundaries. The former is computed from a standard convolutional neural network for semantic segmentation, and the latter is derived from a new instance-aware edge detection model. To reason globally about the optimal partitioning of an image into instances, we combine these two modalities into a novel MultiCut formulation. We evaluate our approach on the challenging CityScapes dataset. Despite the conceptual simplicity of our approach, we achieve the best result among all published methods, and perform particularly well for rare object classes."
            },
            "slug": "InstanceCut:-From-Edges-to-Instances-with-MultiCut-Kirillov-Levinkov",
            "title": {
                "fragments": [],
                "text": "InstanceCut: From Edges to Instances with MultiCut"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "To reason globally about the optimal partitioning of an image into instances, the authors combine these two modalities into a novel MultiCut formulation, which achieves the best result among all published methods, and performs particularly well for rare object classes."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40250403"
                        ],
                        "name": "Xiaodan Liang",
                        "slug": "Xiaodan-Liang",
                        "structuredName": {
                            "firstName": "Xiaodan",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodan Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737218"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49020088"
                        ],
                        "name": "Yunchao Wei",
                        "slug": "Yunchao-Wei",
                        "structuredName": {
                            "firstName": "Yunchao",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchao Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720987"
                        ],
                        "name": "Xiaohui Shen",
                        "slug": "Xiaohui-Shen",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Similarly, PFN [23] utilized semantic segmentation to predict the bounding box each pixel belongs to."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206768798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca3995875ad949b86fb4d07953d89505292e3623",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Instance-level object segmentation is an important yet under-explored task. Most of state-of-the-art methods rely on region proposal methods to extract candidate segments and then utilize object classification to produce final results. Nonetheless, generating reliable region proposals itself is a quite challenging and unsolved task. In this work, we propose a Proposal-Free Network (PFN) to address the instance-level object segmentation problem, which outputs the numbers of instances of different categories and the pixel-level information on i) the coordinates of the instance bounding box each pixel belongs to, and ii) the confidences of different categories for each pixel, based on pixel-to-pixel deep convolutional neural network. All the outputs together, by using any off-the-shelf clustering method for simple post-processing, can naturally generate the ultimate instance-level object segmentation results. The whole PFN can be easily trained without the requirement of a proposal generation stage. Extensive evaluations on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate the effectiveness of the proposed PFN solution without relying on any proposal generation methods."
            },
            "slug": "Proposal-Free-Network-for-Instance-Level-Object-Liang-Lin",
            "title": {
                "fragments": [],
                "text": "Proposal-Free Network for Instance-Level Object Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Proposal-Free Network (PFN) is proposed to address the instance-level object segmentation problem, which outputs the numbers of instances of different categories and the pixel-level information on i) the coordinates of the instance bounding box each pixel belongs to, and ii) the confidences ofDifferent categories for each pixel, based on pixel-to-pixel deep convolutional neural network."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31688710"
                        ],
                        "name": "Alejandro Newell",
                        "slug": "Alejandro-Newell",
                        "structuredName": {
                            "firstName": "Alejandro",
                            "lastName": "Newell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alejandro Newell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18036051"
                        ],
                        "name": "Zhiao Huang",
                        "slug": "Zhiao-Huang",
                        "structuredName": {
                            "firstName": "Zhiao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Pixel association was learned in [28] to differentiate between instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 340420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "239f500b6396cc97f51e5c53d165fae9ce13f661",
            "isKey": false,
            "numCitedBy": 544,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets."
            },
            "slug": "Associative-Embedding:-End-to-End-Learning-for-and-Newell-Huang",
            "title": {
                "fragments": [],
                "text": "Associative Embedding: End-to-End Learning for Joint Detection and Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Associative embedding is introduced, a novel method for supervising convolutional neural networks for the task of detection and grouping for multi-person pose estimation and state-of-the-art performance on the MPII and MS-COCO datasets is reported."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115288815"
                        ],
                        "name": "Long Jin",
                        "slug": "Long-Jin",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145422041"
                        ],
                        "name": "Zeyu Chen",
                        "slug": "Zeyu-Chen",
                        "structuredName": {
                            "firstName": "Zeyu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeyu Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Different types of label transformation were investigated in [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14800060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74d8cdd5f21fb499e0cc83c3e40f357a1acf8bed",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Instance segmentation has attracted recent attention in computer vision and existing methods in this domain mostly have an object detection stage. In this paper, we study the intrinsic challenge of the instance segmentation problem, the presence of a quotient space (swapping the labels of different instances leads to the same result), and propose new methods that are object proposal- and object detection- free. We propose three alternative methods, namely pixel-based affinity mapping, superpixel-based affinity learning, and boundary-based component segmentation, all focusing on performing labeling transformations to cope with the quotient space problem. By adopting fully convolutional neural networks (FCN) like models, our framework attains competitive results on both the PASCAL dataset (object-centric) and the Gland dataset (texture-centric), which the existing methods are not able to do. Our work also has the advantages in its transparency, simplicity, and being all segmentation based."
            },
            "slug": "Object-Detection-Free-Instance-Segmentation-With-Jin-Chen",
            "title": {
                "fragments": [],
                "text": "Object Detection Free Instance Segmentation With Labeling Transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper study the intrinsic challenge of the instance segmentation problem, the presence of a quotient space, and propose new methods that are object proposal- and object detection- free, which attains competitive results on both the PASCAL dataset (object-centric) and the Gland dataset (texture-centric)."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "We use a modified VGG16 [35], and make it fully convolutional as in FCN [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "The community has achieved remarkable progress for tasks such as object detection [32, 10] and semantic segmentation [27, 6] in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1629541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317aee7fc081f2b137a85c4f20129007fd8e717e",
            "isKey": false,
            "numCitedBy": 15654,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image."
            },
            "slug": "Fully-Convolutional-Networks-for-Semantic-Shelhamer-Long",
            "title": {
                "fragments": [],
                "text": "Fully Convolutional Networks for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that convolutional networks by themselves, trained end- to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40250403"
                        ],
                        "name": "Xiaodan Liang",
                        "slug": "Xiaodan-Liang",
                        "structuredName": {
                            "firstName": "Xiaodan",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodan Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49020088"
                        ],
                        "name": "Yunchao Wei",
                        "slug": "Yunchao-Wei",
                        "structuredName": {
                            "firstName": "Yunchao",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchao Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720987"
                        ],
                        "name": "Xiaohui Shen",
                        "slug": "Xiaohui-Shen",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750647"
                        ],
                        "name": "Zequn Jie",
                        "slug": "Zequn-Jie",
                        "structuredName": {
                            "firstName": "Zequn",
                            "lastName": "Jie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zequn Jie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33221685"
                        ],
                        "name": "Jiashi Feng",
                        "slug": "Jiashi-Feng",
                        "structuredName": {
                            "firstName": "Jiashi",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiashi Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737218"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "A recursive process was proposed in [22] to iteratively refine the predicted pixel-wise masks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1212971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e930e9b2012fc3596e1e8493b76fc027cc327bd1",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two subnetworks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the APr over 20 classes at 0:5 IoU achieves 66:7%, which significantly outperforms the results of 58:7% by PFN [17] and 46:3% by [22]."
            },
            "slug": "Reversible-Recursive-Instance-Level-Object-Liang-Wei",
            "title": {
                "fragments": [],
                "text": "Reversible Recursive Instance-Level Object Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This work proposes a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework that consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub- network that generates the foreground mask of the dominant object instance in each proposal."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682629"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7217794"
                        ],
                        "name": "Haozhi Qi",
                        "slug": "Haozhi-Qi",
                        "structuredName": {
                            "firstName": "Haozhi",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haozhi Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7807689"
                        ],
                        "name": "Xiangyang Ji",
                        "slug": "Xiangyang-Ji",
                        "structuredName": {
                            "firstName": "Xiangyang",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangyang Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732264"
                        ],
                        "name": "Yichen Wei",
                        "slug": "Yichen-Wei",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [21], the authors utilized a fully convolutional network by learning to combine different relative parts into an instance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14350475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0366b36006a6b37c673a42aad03ae77e8ef6ecda",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation [29] and instance mask proposal [5]. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The network architecture is highly integrated and efficient. It achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at https://github.com/daijifeng001/TA-FCN."
            },
            "slug": "Fully-Convolutional-Instance-Aware-Semantic-Li-Qi",
            "title": {
                "fragments": [],
                "text": "Fully Convolutional Instance-Aware Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The first fully convolutional end-to-end solution for instance-aware semantic segmentation task, which achieves state-of-the-art performance in both accuracy and efficiency, wins the COCO 2016 segmentation competition by a large margin."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393535"
                        ],
                        "name": "J. Uhrig",
                        "slug": "J.-Uhrig",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "Uhrig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uhrig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841796"
                        ],
                        "name": "Marius Cordts",
                        "slug": "Marius-Cordts",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Cordts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Cordts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582788"
                        ],
                        "name": "Uwe Franke",
                        "slug": "Uwe-Franke",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [36], the authors predict the direction to the instance\u2019s center for each pixel, and exploits templates to infer the location of instances on the predicted angle map."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5571650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff3850ff71c9998589c6fd0b778d89883a51fa72",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent approaches for instance-aware semantic labeling have augmented convolutional neural networks (CNNs) with complex multi-task architectures or computationally expensive graphical models. We present a method that leverages a fully convolutional network (FCN) to predict semantic labels, depth and an instance-based encoding using each pixel\u2019s direction towards its corresponding instance center. Subsequently, we apply low-level computer vision techniques to generate state-of-the-art instance segmentation on the street scene datasets KITTI and Cityscapes. Our approach outperforms existing works by a large margin and can additionally predict absolute distances of individual instances from a monocular image as well as a pixel-level semantic labeling."
            },
            "slug": "Pixel-Level-Encoding-and-Depth-Layering-for-Uhrig-Cordts",
            "title": {
                "fragments": [],
                "text": "Pixel-Level Encoding and Depth Layering for Instance-Level Semantic Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a method that leverages a fully convolutional network (FCN) to predict semantic labels, depth and an instance-based encoding using each pixel\u2019s direction towards its corresponding instance center."
            },
            "venue": {
                "fragments": [],
                "text": "GCPR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898210"
                        ],
                        "name": "Golnaz Ghiasi",
                        "slug": "Golnaz-Ghiasi",
                        "structuredName": {
                            "firstName": "Golnaz",
                            "lastName": "Ghiasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Golnaz Ghiasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Note that the LRR model we use is pre-trained on MS COCO [25], which is also used by DIN [1] for pre-training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "As shown in Table 3, we achieve reasonable results using LRR, however, much better results are obtained when exploiting PSP. Results improve significantly in both cases when using the MergerNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "We compare the performance of our approach when using LRR [13], which is based on VGG-16, with PSP [42], which is based on Resnet-101."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "LRR [13] PSP [42] MergerNet AP AP 50% MWCov X 20."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "By default, we use semantic segmentation prediction from PSP [42] on Cityscapes and LRR [13] on PASCAL VOC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Note that LineNet and MergerNet are not finetuned for LRR prediction."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6989586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "752fc36f9813ebff3837cb12f790820d2f851c14",
            "isKey": true,
            "numCitedBy": 44,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps. This approach yields state-of-the-art semantic segmentation results on PASCAL without resorting to more complex CRF or detection driven architectures."
            },
            "slug": "Laplacian-Reconstruction-and-Refinement-for-Ghiasi-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Laplacian Reconstruction and Refinement for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps is described."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 45
                            }
                        ],
                        "text": "The pioneering work of instance segmentation [15, 8] aimed at both classifying object proposals as well as labeling an object within each box."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206593096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ad998a9b2c071c4a1971048f8a2d754530f08e8",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) [13]. The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and \u201cstuff\u201d (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed."
            },
            "slug": "Convolutional-feature-masking-for-joint-object-and-Dai-He",
            "title": {
                "fragments": [],
                "text": "Convolutional feature masking for joint object and stuff segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a joint method to handle objects and \u201cstuff\u201d (e.g., grass, sky, water) in the same framework and presents state-of-the-art results on benchmarks of PASCAL VOC and new PASCal-CONTEXT."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145231047"
                        ],
                        "name": "V. Koltun",
                        "slug": "V.-Koltun",
                        "structuredName": {
                            "firstName": "Vladlen",
                            "lastName": "Koltun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 68
                            }
                        ],
                        "text": "To enlarge the receptive field, we make use of dilated convolutions [39, 6] in the conv5 and conv6 layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17127188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
            "isKey": false,
            "numCitedBy": 5426,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
            },
            "slug": "Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Context Aggregation by Dilated Convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work develops a new convolutional network module that is specifically designed for dense prediction, and shows that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708655"
                        ],
                        "name": "Pedro H. O. Pinheiro",
                        "slug": "Pedro-H.-O.-Pinheiro",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Pinheiro",
                            "middleNames": [
                                "H.",
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro H. O. Pinheiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "Most of the instance segmentation approaches employ a two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection boxes [15, 16, 1, 29, 30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [29, 30], highquality mask proposals were generated using CNNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 140529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b8d0df903496699e52b4daee5d1815b7b784cf7",
            "isKey": false,
            "numCitedBy": 671,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation."
            },
            "slug": "Learning-to-Segment-Object-Candidates-Pinheiro-Collobert",
            "title": {
                "fragments": [],
                "text": "Learning to Segment Object Candidates"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new way to generate object proposals is proposed, introducing an approach based on a discriminative convolutional network that obtains substantially higher object recall using fewer proposals and is able to generalize to unseen categories it has not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708655"
                        ],
                        "name": "Pedro H. O. Pinheiro",
                        "slug": "Pedro-H.-O.-Pinheiro",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Pinheiro",
                            "middleNames": [
                                "H.",
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro H. O. Pinheiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "Most of the instance segmentation approaches employ a two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection boxes [15, 16, 1, 29, 30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [29, 30], highquality mask proposals were generated using CNNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "Similar to the methods in [30, 24, 12], we augment the network by connecting lower layers to higher ones in order to capture fine details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15278025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "361b19d2c00d086fa8ef860374f5e1d862fd2f30",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse \u2018mask encoding\u2019 in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10\u201320% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50 % faster than the original DeepMask network (under .8 s per image)."
            },
            "slug": "Learning-to-Refine-Object-Segments-Pinheiro-Lin",
            "title": {
                "fragments": [],
                "text": "Learning to Refine Object Segments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to augment feedforward nets for object segmentation with a novel top-down refinement approach that is capable of efficiently generating high-fidelity object masks and is 50 % faster than the original DeepMask network."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "The community has achieved remarkable progress for tasks such as object detection [32, 10] and semantic segmentation [27, 6] in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 68
                            }
                        ],
                        "text": "To enlarge the receptive field, we make use of dilated convolutions [39, 6] in the conv5 and conv6 layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1996665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39ad6c911f3351a3b390130a6e4265355b4d593b",
            "isKey": false,
            "numCitedBy": 3346,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
            },
            "slug": "Semantic-Image-Segmentation-with-Deep-Convolutional-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3429309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "isKey": false,
            "numCitedBy": 9409,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online."
            },
            "slug": "DeepLab:-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work addresses the task of semantic image segmentation with Deep Learning and proposes atrous\u00a0spatial pyramid pooling (ASPP), which is proposed to robustly segment objects at multiple scales, and improves the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "Note that the LRR model we use is pre-trained on MS COCO [25], which is also used by DIN [1] for pre-training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19785,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817030"
                        ],
                        "name": "Saining Xie",
                        "slug": "Saining-Xie",
                        "structuredName": {
                            "firstName": "Saining",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saining Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "To mitigate this effect, similar to HED [38], we re-weight the cross-entropy loss based on inverse of the frequency of each class in the mini-batch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6423078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4100414ad0763bdc91bd15bb6e0424a44d7a35fe",
            "isKey": false,
            "numCitedBy": 1958,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a new edge detection algorithm that addresses two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSDS500 dataset (ODS F-score of 0.790) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 s per image) that is orders of magnitude faster than some CNN-based edge detection algorithms developed before HED. We also observe encouraging results on other boundary detection benchmark datasets such as Multicue and PASCAL-Context."
            },
            "slug": "Holistically-Nested-Edge-Detection-Xie-Tu",
            "title": {
                "fragments": [],
                "text": "Holistically-Nested Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HED performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets, and automatically learns rich hierarchical representations that are important in order to resolve the challenging ambiguity in edge and object boundary detection."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149140383"
                        ],
                        "name": "Ke Li",
                        "slug": "Ke-Li",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "IIS [20] is an iterative approach to refine the instance masks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 869181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "408d107f4cd06ed1ad5672a3166ffdbf2f8d1491",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing methods for pixel-wise labelling tasks generally disregard the underlying structure of labellings, often leading to predictions that are visually implausible. While incorporating structure into the model should improve prediction quality, doing so is challenging - manually specifying the form of structural constraints may be impractical and inference often becomes intractable even if structural constraints are given. We sidestep this problem by reducing structured prediction to a sequence of unconstrained prediction problems and demonstrate that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification. On the instance segmentation task, this method outperforms the state-of-the-art, achieving a mean APr of 63:6% at 50% overlap and 43:3% at 70% overlap."
            },
            "slug": "Iterative-Instance-Segmentation-Li-Hariharan",
            "title": {
                "fragments": [],
                "text": "Iterative Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work reduces structured prediction to a sequence of unconstrained prediction problems and demonstrates that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "Similar to the methods in [30, 24, 12], we augment the network by connecting lower layers to higher ones in order to capture fine details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10716717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "isKey": false,
            "numCitedBy": 9354,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
            },
            "slug": "Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Feature Pyramid Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost and achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3436589"
                        ],
                        "name": "Llu\u00eds Castrej\u00f3n",
                        "slug": "Llu\u00eds-Castrej\u00f3n",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Castrej\u00f3n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds Castrej\u00f3n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204339"
                        ],
                        "name": "Kaustav Kundu",
                        "slug": "Kaustav-Kundu",
                        "structuredName": {
                            "firstName": "Kaustav",
                            "lastName": "Kundu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaustav Kundu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [4] proposed to predict polygons outlining each object instance which has the advantage of efficiently capturing object shape."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Instead of labeling pixels, [4] predicts a polygon outlining the object instance using a Recurrent Neural Network (RNN)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2913088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "539b5ceef48624882ba7d0b3c65e42480d8c24f9",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78:4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82:2%. We further show generalization capabilities of our approach to unseen datasets."
            },
            "slug": "Annotating-Object-Instances-with-a-Polygon-RNN-Castrej\u00f3n-Kundu",
            "title": {
                "fragments": [],
                "text": "Annotating Object Instances with a Polygon-RNN"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work proposes an approach for semi-automatic annotation of object instances that takes as input an image crop and sequentially produces vertices of the polygon outlining the object, producing as accurate segmentation as desired by the annotator."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3459894"
                        ],
                        "name": "Hengshuang Zhao",
                        "slug": "Hengshuang-Zhao",
                        "structuredName": {
                            "firstName": "Hengshuang",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hengshuang Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788070"
                        ],
                        "name": "Jianping Shi",
                        "slug": "Jianping-Shi",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50844674"
                        ],
                        "name": "Xiaojuan Qi",
                        "slug": "Xiaojuan-Qi",
                        "structuredName": {
                            "firstName": "Xiaojuan",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojuan Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "(h) input image semantic segmentation [42] our results ground-truth Figure 9."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "By default, we use semantic segmentation prediction from PSP [42] on Cityscapes and LRR [13] on PASCAL VOC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "LRR [13] PSP [42] MergerNet AP AP 50% MWCov X 20."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "We compare the performance of our approach when using LRR [13], which is based on VGG-16, with PSP [42], which is based on Resnet-101."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5299559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1031a69923b80ad01cf3fbb703d10757a80e699b",
            "isKey": false,
            "numCitedBy": 5543,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
            },
            "slug": "Pyramid-Scene-Parsing-Network-Zhao-Shi",
            "title": {
                "fragments": [],
                "text": "Pyramid Scene Parsing Network"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper exploits the capability of global context information by different-region-based context aggregation through the pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet) to produce good quality results on the scene parsing task."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32566,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095572"
                        ],
                        "name": "Ziyu Zhang",
                        "slug": "Ziyu-Zhang",
                        "structuredName": {
                            "firstName": "Ziyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "Methods of [41, 40] extracted patches from the image and used a CNN to directly infer instance IDs inside each patch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Note that most instance segmentation algorithms cannot tackle the fragmentation problem [3, 26, 18, 41, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [41, 40], large patches are exhaustively extracted from an image and a Convolutional Neural Network (CNN) is trained to predict instance labels inside each patch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15291118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acd7911493436e3ae63e20383dc5abc88ddfa6d9",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Our aim is to provide a pixel-wise instance-level labeling of a monocular image in the context of autonomous driving. We build on recent work [32] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image. A simple Markov random field model using several heuristics was then proposed in [32] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field and show how to encode various intuitive potentials in a way that is amenable to efficient mean field inference [15]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [8] demonstrate that our method achieves a significant performance boost over the baseline [32]."
            },
            "slug": "Instance-Level-Segmentation-for-Autonomous-Driving-Zhang-Fidler",
            "title": {
                "fragments": [],
                "text": "Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper forms the global labeling problem with a novel densely connected Markov random field and shows how to encode various intuitive potentials in a way that is amenable to efficient mean field inference."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682629"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 82
                            }
                        ],
                        "text": "The community has achieved remarkable progress for tasks such as object detection [32, 10] and semantic segmentation [27, 6] in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7428689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "isKey": false,
            "numCitedBy": 3872,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn."
            },
            "slug": "R-FCN:-Object-Detection-via-Region-based-Fully-Dai-Li",
            "title": {
                "fragments": [],
                "text": "R-FCN: Object Detection via Region-based Fully Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work presents region-based, fully convolutional networks for accurate and efficient object detection, and proposes position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62224,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1892247"
                        ],
                        "name": "Shenlong Wang",
                        "slug": "Shenlong-Wang",
                        "structuredName": {
                            "firstName": "Shenlong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenlong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065478626"
                        ],
                        "name": "Min Bai",
                        "slug": "Min-Bai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8768514"
                        ],
                        "name": "G. M\u00e1ttyus",
                        "slug": "G.-M\u00e1ttyus",
                        "structuredName": {
                            "firstName": "Gell\u00e9rt",
                            "lastName": "M\u00e1ttyus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. M\u00e1ttyus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372537"
                        ],
                        "name": "Hang Chu",
                        "slug": "Hang-Chu",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49756115"
                        ],
                        "name": "Wenjie Luo",
                        "slug": "Wenjie-Luo",
                        "structuredName": {
                            "firstName": "Wenjie",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenjie Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49188662"
                        ],
                        "name": "Binh Yang",
                        "slug": "Binh-Yang",
                        "structuredName": {
                            "firstName": "Binh",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binh Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50685693"
                        ],
                        "name": "Justin Liang",
                        "slug": "Justin-Liang",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7584882"
                        ],
                        "name": "Joel Cheverie",
                        "slug": "Joel-Cheverie",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Cheverie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Cheverie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "Thus we also report the Mean Weighted Coverage (MWCov) [31, 37], which is the average IoU of prediction matched with ground-truth instances weighted by the size of the ground-truth instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1888683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c020809d0d7fa65358a053d102382c70902a7bb7",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce the TorontoCity benchmark, which covers the full greater Toronto area (GTA) with 712.5km2 of land, 8439km of road and around 400, 000 buildings. Our benchmark provides different perspectives of the world captured from airplanes, drones and cars driving around the city. Manually labeling such a large scale dataset is infeasible. Instead, we propose to utilize different sources of high-precision maps to create our ground truth. Towards this goal, we develop algorithms that allow us to align all data sources with the maps while requiring minimal human supervision. We have designed a wide variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction (reorganization), semantic labeling and scene type classification (recognition). Our pilot study shows that most of these tasks are still difficult for modern convolutional neural networks."
            },
            "slug": "TorontoCity:-Seeing-the-World-with-a-Million-Eyes-Wang-Bai",
            "title": {
                "fragments": [],
                "text": "TorontoCity: Seeing the World with a Million Eyes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The TorontoCity benchmark is introduced, which covers the full greater Toronto area with 712.5km2 of land, 8439km of road and around 400, 000 buildings, and a wide variety of tasks including building height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling and scene type classification are designed."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "We evaluate our method on the challenging dataset Cityscapes [7] as well as PASCAL VOC 2012 [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Results on PASCAL VOC We also conduct experiments on PASCAL VOC 2012 [11], which contains 20 classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11692,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "As is common practice, for training images, we additionally use annotations from the SBD dataset [14], resulting in 10, 582 images with instance annotation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6683607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82fae97673a353271b1d4c001afda1af6ef6dc23",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7]."
            },
            "slug": "Semantic-contours-from-inverse-detectors-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours is presented and a principled way of combining information from different part detectors and across categories is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "Implementation Details For our breakpoint prediction network, we initialize conv1 to conv5 layers from VGG16 [35] pre-trained on Imagenet [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": false,
            "numCitedBy": 25492,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22748016"
                        ],
                        "name": "A. Ranga",
                        "slug": "A.-Ranga",
                        "structuredName": {
                            "firstName": "Ananth",
                            "lastName": "Ranga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ranga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36183106"
                        ],
                        "name": "A. Tyagi",
                        "slug": "A.-Tyagi",
                        "structuredName": {
                            "firstName": "Ambrish",
                            "lastName": "Tyagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tyagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "Similar to the methods in [30, 24, 12], we augment the network by connecting lower layers to higher ones in order to capture fine details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7691159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94183191183a368bf07eb544654bae4b3cbf407",
            "isKey": false,
            "numCitedBy": 1204,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \\times 513$ input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset."
            },
            "slug": "DSSD-:-Deconvolutional-Single-Shot-Detector-Fu-Liu",
            "title": {
                "fragments": [],
                "text": "DSSD : Deconvolutional Single Shot Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper combines a state-of-the-art classifier with a fast detection framework and augments SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154212"
                        ],
                        "name": "Evgeny Levinkov",
                        "slug": "Evgeny-Levinkov",
                        "structuredName": {
                            "firstName": "Evgeny",
                            "lastName": "Levinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evgeny Levinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831081930"
                        ],
                        "name": "Siyu Tang",
                        "slug": "Siyu-Tang",
                        "structuredName": {
                            "firstName": "Siyu",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siyu Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205238"
                        ],
                        "name": "Eldar Insafutdinov",
                        "slug": "Eldar-Insafutdinov",
                        "structuredName": {
                            "firstName": "Eldar",
                            "lastName": "Insafutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eldar Insafutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16576043"
                        ],
                        "name": "Bjoern Andres",
                        "slug": "Bjoern-Andres",
                        "structuredName": {
                            "firstName": "Bjoern",
                            "lastName": "Andres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bjoern Andres"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[19] 6."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14959459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c9df1b498ce27bfcb17d3702d6e025913df51c6",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph. This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we propose generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define a local search algorithm that converges monotonously to a local optimum, offering a feasible solution at any time. To demonstrate the effectiveness of this algorithm in solving computer vision tasks, we report running times and competitive solutions for two above-mentioned applications."
            },
            "slug": "Joint-Graph-Decomposition-and-Node-Labeling-by-Levinkov-Tang",
            "title": {
                "fragments": [],
                "text": "Joint Graph Decomposition and Node Labeling by Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A local search algorithm is defined that converges monotonously to a local optimum, offering a feasible solution at any time, in a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841796"
                        ],
                        "name": "Marius Cordts",
                        "slug": "Marius-Cordts",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Cordts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Cordts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187309"
                        ],
                        "name": "Mohamed Omran",
                        "slug": "Mohamed-Omran",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Omran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Omran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39940699"
                        ],
                        "name": "Sebastian Ramos",
                        "slug": "Sebastian-Ramos",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ramos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ramos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393153"
                        ],
                        "name": "Timo Rehfeld",
                        "slug": "Timo-Rehfeld",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Rehfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo Rehfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798000"
                        ],
                        "name": "Rodrigo Benenson",
                        "slug": "Rodrigo-Benenson",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Benenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Benenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582788"
                        ],
                        "name": "Uwe Franke",
                        "slug": "Uwe-Franke",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "Our experiments showed that our approach significantly outperforms existing approaches on the challenging Cityscapes dataset and works well on PASCAL VOC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Dataset The Cityscapes dataset [7] contains imagery of complex urban scenes with high pixel resolution of 1, 024\u21e5 2, 048."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "In street scenes, such as those in the Cityscapes dataset [7], current methods merely reach 20% accuracy in terms of average precision, which is still far from satisfactory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 53
                            }
                        ],
                        "text": "On average 3.5 instances per image are fragmented in Cityscapes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 27
                            }
                        ],
                        "text": "Metrics The metric used by Cityscapes is Average Precision (AP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 69
                            }
                        ],
                        "text": "By default, we use semantic segmentation prediction from PSP [42] on Cityscapes and LRR [13] on PASCAL VOC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "We evaluate our method on the challenging dataset Cityscapes [7] as well as PASCAL VOC 2012 [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "On average, the number of operations that need to be made is 4802 per image on Cityscapes and 1014 on PASCAL VOC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 88
                            }
                        ],
                        "text": "Visual Results We show qualitative results of all intermediate steps in our approach on Cityscapes val in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 118
                            }
                        ],
                        "text": "We achieve an improvement of 5% absolute and 25% in relative performance compared to state-of-the-art reported on the Cityscapes website, captured at the moment of our submission."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 42
                            }
                        ],
                        "text": "We evaluate our method on the challenging Cityscapes dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "We focus our experiments on Cityscapes as it is much more challenging."
                    },
                    "intents": []
                }
            ],
            "corpusId": 502946,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
            "isKey": true,
            "numCitedBy": 6031,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."
            },
            "slug": "The-Cityscapes-Dataset-for-Semantic-Urban-Scene-Cordts-Omran",
            "title": {
                "fragments": [],
                "text": "The Cityscapes Dataset for Semantic Urban Scene Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling, and exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 29
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/SGN:-Sequential-Grouping-Networks-for-Instance-Liu-Jia/ede3af38e30ca332af0c1ce3bd5144070f7fb7f3?sort=total-citations"
}