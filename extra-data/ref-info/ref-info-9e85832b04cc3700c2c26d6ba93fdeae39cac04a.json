{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "gs from words and POS tags. Both have tested di\ufb00erent feature combinations before \ufb01nding an optimal one and their \ufb01nal results are close to each other. Three systems use system combination. Tjong Kim Sang (2000) trained and tested \ufb01ve memory-based learning systems to produce different representations of the chunk tags. A combination of the \ufb01ve by majority voting performed better than the individual parts. Va"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 10
                            }
                        ],
                        "text": "Tjong Kim Sang (2000) trained and tested five memory-based learning systems to produce different representations of the chunk tags."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Combined systems: Tjong Kim Sang; Van Halteren; Kudoh and Matsumoto."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Tjong Kim Sang is funded by the European TMR network Learning Computational Grammars."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12841881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e8de77316a43fd8675d546880b4607433793c31",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We will apply a system-internal combination of memory-based learning classifiers to the CoNLL-2000 shared task: finding base chunks. Apart from testing different combination methods, we will also examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance."
            },
            "slug": "Text-Chunking-by-System-Combination-Sang",
            "title": {
                "fragments": [],
                "text": "Text Chunking by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A system-internal combination of memory-based learning classifiers is applied to the CoNLL-2000 shared task: finding base chunks to examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687164"
                        ],
                        "name": "Christer Johansson",
                        "slug": "Christer-Johansson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christer Johansson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "eren; Kudoh and Matsumoto. Vilain and Day (2000) approached the shared task in three di\ufb00erent ways. The most successful was an application of the Alembic parser which uses transformation-based rules. Johansson (2000) uses context-sensitive and contextfree rules for transforming part-of-speech (POS) tag sequences to chunk tag sequences. D\u00b4ejean (2000) has applied the theory re\ufb01nement system ALLiS to the shared tas"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Johansson (2000) uses context-sensitive and contextfree rules for transforming part-of-speech (POS) tag sequences to chunk tag sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9576372,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "cbfdf73deec0350aa286ce2ac8bf149ad4710f7c",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In Brill's (1994) groundbreaking work on parts-of-speech tagging, the starting point was to assign each word its most common tag. An extension to this first step is to utilize the lexical context (i.e., words and punctuation) surrounding the word. This approach could obviously be used for ordering tags into higher order units (referred to as chunks) using chunk labels."
            },
            "slug": "A-Context-Sensitive-Maximum-Likelihood-Approach-to-Johansson",
            "title": {
                "fragments": [],
                "text": "A Context Sensitive Maximum Likelihood Approach to Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "In Brill's (1994) groundbreaking work on parts-of-speech tagging, the starting point was to assign each word its most common tag, so this approach could obviously be used for ordering tags into higher order units (referred to as chunks) using chunk labels."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2200382"
                        ],
                        "name": "M. Vilain",
                        "slug": "M.-Vilain",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Vilain",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vilain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786957"
                        ],
                        "name": "David S. Day",
                        "slug": "David-S.-Day",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Day",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Day"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10835278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f706c5f9524fb5c80c6bf6df9d886ee9ea5a1d0",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For several years, chunking has been an integral part of MITRE's approach to information extraction. Our work exploits chunking in two principal ways. First, as part of our extraction system (Alembic) (Aberdeen et al., 1995), the chunker delineates descriptor phrases for entity extraction. Second, as part of our ongoing research in parsing, chunks provide the first level of a stratified approach to syntax - the second level is defined by grammatical relations, much as in the SPARKLE effort (Carroll et al., 1997)."
            },
            "slug": "Phrase-Parsing-with-Rule-Sequence-Processors:-an-to-Vilain-Day",
            "title": {
                "fragments": [],
                "text": "Phrase Parsing with Rule Sequence Processors: an Application to the Shared CoNLL Task"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work exploits chunking in two principal ways: first, as part of the authors' extraction system (Alembic) (Aberdeen et al., 1995), the chunker delineates descriptor phrases for entity extraction; and, as a part of the ongoing research in parsing, chunks provide the first level of a stratified approach to syntax."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299876"
                        ],
                        "name": "R. Koeling",
                        "slug": "R.-Koeling",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Koeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koeling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "speech tag. a memory-based learning process. The two other statistical systems use maximum-entropy based methods. Osborne (2000) trained Ratnaparkhi\u2019s maximum-entropy POS tagger to output chunk tags. Koeling (2000) used a standard maximum-entropy learner for generating chunk tags from words and POS tags. Both have tested di\ufb00erent feature combinations before \ufb01nding an optimal one and their \ufb01nal results are close"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Koeling (2000) used a standard maximum-entropy learner for generating chunk tags from words and POS tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6553227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2be4a6cfe8228b6f4f648ff10dbf1e62fc4562f7",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper I discuss a first attempt to create a text chunker using a Maximum Entropy model. The first experiments, implementing classifiers that tag every word in a sentence with a phrase-tag using very local lexical information, part-of-speech tags and phrase tags of surrounding words, give encouraging results."
            },
            "slug": "Chunking-with-Maximum-Entropy-Models-Koeling",
            "title": {
                "fragments": [],
                "text": "Chunking with Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A first attempt to create a text chunker using a Maximum Entropy model is discussed, implementing classifiers that tag every word in a sentence with a phrase-tag using very local lexical information, part-of-speech tags and phrase tags of surrounding words."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726527"
                        ],
                        "name": "N. Fakotakis",
                        "slug": "N.-Fakotakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Fakotakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fakotakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60501450,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b3c17ccfc354c01a53abb4671367aa4cb1f6247b",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The abundance of data often hinders computational efficiency in information retrie val, information extraction and parsing. Several data reduction techniques have been proposed t o v rcome this problem. A recently blooming approach to data reduction in computational linguist ics chunking. In (Daelemans, Buchholz, and Veenstra, 1999), we describe experiments on NP and V P (noun and verb phrase) chunking using Memory-Based Learning ( MBL ) carried out in the context of subject/object identification. In this paper we will show that NP, VP and PP ( prepositional phrase) chunking is possible with a precision and recall around 94-95%."
            },
            "slug": "Memory-Based-Text-Chunking-Veenstra-Fakotakis",
            "title": {
                "fragments": [],
                "text": "Memory-Based Text Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that NP, VP and PP ( prepositional phrase) chunking is possible with a precision and recall around 94-95% in the context of subject/object identification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782178"
                        ],
                        "name": "S. Buchholz",
                        "slug": "S.-Buchholz",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Buchholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buchholz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2117334,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "5db4b1405a1e77311a4a78414fe7eedc7712c4ed",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder."
            },
            "slug": "Cascaded-Grammatical-Relation-Assignment-Buchholz-Veenstra",
            "title": {
                "fragments": [],
                "text": "Cascaded Grammatical Relation Assignment"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The effect of adding several levels to this cascaded classifier was studied and it was found that even the less performing chunkers enhanced the performance of the relation finder."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3131784"
                        ],
                        "name": "Taku Kudoh",
                        "slug": "Taku-Kudoh",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 128
                            }
                        ],
                        "text": "We have chosen to work with a corpus with parse information, the\nWall Street Journal (WSJ) part of the Penn Treebank II corpus (Marcus et al., 1993), and to extract chunk information from the parse trees in this corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6953360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling."
            },
            "slug": "Use-of-Support-Vector-Learning-for-Chunk-Kudoh-Matsumoto",
            "title": {
                "fragments": [],
                "text": "Use of Support Vector Learning for Chunk Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper investigates how SVMs with a very large number of features perform with the classification task of chunk labelling, CoNLL-2000 shared task, chunk identification."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145977875"
                        ],
                        "name": "Antal van den Bosch",
                        "slug": "Antal-van-den-Bosch",
                        "structuredName": {
                            "firstName": "Antal",
                            "lastName": "van den Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antal van den Bosch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15295006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f1b336efaa17811800d8737b20f6f61d23f90e0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In the shared task for CoNLL-2000, words and tags form the basic multi-valued features for predicting a rich phrase segmentation code. While the tag features, containing WSJ part-of-speech tags (Marcus et al., 1993), have about 45 values, the word features have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a meta-learning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following:1. The generalization accuracy of TiMBL with default settings (multi-valued features, overlap metric, feature weighting).2. The usage of MVDM (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) (Section 2), which should work well on word value pairs with a medium or high frequency, but may work badly on word value pairs with low frequency.3. The straightforward unpacking of feature values into binary features. On some tasks we have found that splitting multi-valued features into several binary features can enhance performance of the classifier.4. A heuristic search for complex features on the basis of all unpacked feature values, and using these complex features for the classification task."
            },
            "slug": "Single-Classifier-Memory-Based-Phrase-Chunking-Veenstra-Bosch",
            "title": {
                "fragments": [],
                "text": "Single-Classifier Memory-Based Phrase Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This study has looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle multi-valued features for predicting a rich phrase segmentation code."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2131960"
                        ],
                        "name": "Herv\u00e9 D\u00e9jean",
                        "slug": "Herv\u00e9-D\u00e9jean",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "D\u00e9jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Herv\u00e9 D\u00e9jean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5615449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399775bd029d31366e9d276d2b7d8a4f77581824",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the result of a symbolic machine learning system for the CoNLL-2000 shared task. This system, ALLiS, is based on theory refinement. In this paper we want to show that XML format not only offers a good framework to annotate texts, but also provides a good formalism and tools in order to learn (syntactic) structures."
            },
            "slug": "Learning-Syntactic-Structures-with-XML-D\u00e9jean",
            "title": {
                "fragments": [],
                "text": "Learning Syntactic Structures with XML"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is shown that XML format not only offers a good framework to annotate texts, but also provides a good formalism and tools in order to learn (syntactic) structures."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144300985"
                        ],
                        "name": "Ferran Pl\u00e0",
                        "slug": "Ferran-Pl\u00e0",
                        "structuredName": {
                            "firstName": "Ferran",
                            "lastName": "Pl\u00e0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferran Pl\u00e0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621529"
                        ],
                        "name": "Antonio Molina",
                        "slug": "Antonio-Molina",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145387377"
                        ],
                        "name": "N. Prieto",
                        "slug": "N.-Prieto",
                        "structuredName": {
                            "firstName": "Natividad",
                            "lastName": "Prieto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Prieto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6583976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "861138f881ec752592cd69320ef14cceaf2971c1",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we present a stochastic approach to shallow parsing. Most of the current approaches to shallow parsing have a common characteristic: they take the sequence of lexical tags proposed by a POS tagger as input for the chunking process. Our system produces tagging and chunking in a single process using an Integrated Language Model (ILM) formalized as Markov Models. This model integrates several knowledge sources: lexical probabilities, a contextual Language Model (LM) for every chunk, and a contextual LM for the sentences. We have extended the ILM by adding lexical information to the contextual LMs. We have applied this approach to the CoNLL-2000 shared task improving the performance of the chunker."
            },
            "slug": "Improving-Chunking-by-Means-of-Lexical-Contextual-Pl\u00e0-Molina",
            "title": {
                "fragments": [],
                "text": "Improving Chunking by Means of Lexical-Contextual Information in Statistical Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work produces tagging and chunking in a single process using an Integrated Language Model formalized as Markov Models that integrates several knowledge sources: lexical probabilities, a contextual Language Model for every chunk, and a contextual LM for the sentences."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165139"
                        ],
                        "name": "H. V. Halteren",
                        "slug": "H.-V.-Halteren",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Halteren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Halteren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 4
                            }
                        ],
                        "text": "Van Halteren (2000) used Weighted Probability Distribution Voting (WPDV) for combining the results of four WPDV chunk taggers and a memory-based chunk tagger."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13382016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2b16a6f2154fd2814f50d9f96283a76c2959da0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper I describe the application of the WPDV algorithm to the CoNLL-2000 shared task, the identification of base chunks in English text (Tjong Kim Sang and Buchholz, 2000). For this task, I use a three-stage architecture: I first run five different base chunkers, then combine them and finally try to correct some recurring errors. Except for one base chunker, which uses the memory-based machine learning system TiMBL, all modules are based on WPDV models (van Halteren, 2000a)."
            },
            "slug": "Chunking-with-WPDV-Models-Halteren",
            "title": {
                "fragments": [],
                "text": "Chunking with WPDV Models"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper describes the application of the WPDV algorithm to the CoNLL-2000 shared task, the identification of base chunks in English text, using a three-stage architecture."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143740945"
                        ],
                        "name": "Guodong Zhou",
                        "slug": "Guodong-Zhou",
                        "structuredName": {
                            "firstName": "Guodong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guodong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144538026"
                        ],
                        "name": "J. Su",
                        "slug": "J.-Su",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280751"
                        ],
                        "name": "TongGuan Tey",
                        "slug": "TongGuan-Tey",
                        "structuredName": {
                            "firstName": "TongGuan",
                            "lastName": "Tey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "TongGuan Tey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10591000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b19f0e891bf6ab9c1b2584b710fbb2bc832dc5c",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries. Finally, memory-based learning is adopted to further improve the performance of the chunk tagger."
            },
            "slug": "Hybrid-Text-Chunking-Zhou-Su",
            "title": {
                "fragments": [],
                "text": "Hybrid Text Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry and an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependentLexical entries."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ation about dividing sentences into chunks of words of arbitrary types. We have chosen to work with a corpus with parse information, the Wall Street Journal (WSJ) part of the Penn Treebank II corpus (Marcus et al., 1993), and to extract chunk information from the parse trees in this corpus. We will give a global description of the various chunk types in the next section. 3 Chunk Types The chunk types are based on the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 128
                            }
                        ],
                        "text": "We have chosen to work with a corpus with parse information, the\nWall Street Journal (WSJ) part of the Penn Treebank II corpus (Marcus et al., 1993), and to extract chunk information from the parse trees in this corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Osborne (2000) trained Ratnaparkhi\u2019s maximum-entropy POS tagger to output chunk tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 109854,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a6268f7ec37bccab46cb8b6d8ed7300f90699799",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches. Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94.88%, with an overall FB1 score of 91.94%. The individual FB1 scores for NPs were 92.19%, VPs 92.70% and PPs 96.69%."
            },
            "slug": "Shallow-Parsing-as-Part-of-Speech-Tagging-Osborne",
            "title": {
                "fragments": [],
                "text": "Shallow Parsing as Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches, using the CoNLL 2000 training and testing material."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "There has been a large interest in recognizing non-overlapping noun phrases (Ramshaw and Marcus (1995) and follow-up papers) but relatively little has been written about identifying phrases of other syntactic categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 12
                            }
                        ],
                        "text": "Ramshaw and Marcus (1995) approached chunking by using a machine learning method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 58
                            }
                        ],
                        "text": "Our NP chunks are very similar to the ones of Ramshaw and Marcus (1995)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 27
                            }
                        ],
                        "text": "In contrast to Ramshaw and Marcus (1995), predicative adjectives of the verb are not part of the VP chunk, e.g. in \u201c[NP they ] [VP are ] [ADJP unhappy ]\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 261
                            }
                        ],
                        "text": "\u2026text chunking data set is available at http://lcgwww.uia.ac.be/conll2000/chunking/\nB-X first word of a chunk of type X I-X non-initial word in an X chunk O word outside of any chunk\nThis representation type is based on a representation proposed by Ramshaw and Marcus (1995) for noun phrase chunks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 168
                            }
                        ],
                        "text": "For the CoNLL shared task, we have chosen to work with the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition (Ramshaw and Marcus, 1995): WSJ sections 15\u201318 of the Penn Treebank as training material and section 20 as test material3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "As Ramshaw and Marcus (1995) already noted: \u201cWhile this automatic derivation process introduced a small percentage of errors on its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": true,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "By then, Church (1988) had already reported on recognition of base noun phrases with statistical methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Ratnaparkhi (1998) has recognized arbitrary chunks as part of a parsing task but did not report on the chunking performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2600845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy. \nWe discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages: \nState-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources. \nKnowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or \"knowledge-poor\", but yet succeed in approximating complex linguistic relationships. \nReusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis. \nThe experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."
            },
            "slug": "Maximum-entropy-models-for-natural-language-Ratnaparkhi-Marcus",
            "title": {
                "fragments": [],
                "text": "Maximum entropy models for natural language ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 93
                            }
                        ],
                        "text": "Additionally, a partof-speech (POS) tag was assigned to each token by a standard POS tagger (Brill (1994) trained on the Penn Treebank)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12309040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "733234e097dceb9011baa8914930861996eb0b5e",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "slug": "Some-Advances-in-Transformation-Based-Part-of-Brill",
            "title": {
                "fragments": [],
                "text": "Some Advances in Transformation-Based Part of Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for expressing lexical relations in tagging that stochastic taggers are currently unable to express is described and how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3212973"
                        ],
                        "name": "Ann Bies",
                        "slug": "Ann-Bies",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Bies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann Bies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054785529"
                        ],
                        "name": "Mark Ferguson",
                        "slug": "Mark-Ferguson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ferguson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Ferguson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065702818"
                        ],
                        "name": "Karen Katz",
                        "slug": "Karen-Katz",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Katz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Katz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33353168"
                        ],
                        "name": "R. MacIntyre",
                        "slug": "R.-MacIntyre",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "MacIntyre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. MacIntyre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59752771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b723d87aa184064f752ed70dcb47face44c1581d",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bracketing-Guidelines-For-Treebank-II-Style-Penn-Bies-Ferguson",
            "title": {
                "fragments": [],
                "text": "Bracketing Guidelines For Treebank II Style Penn Treebank Project"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "In the early nineties, Abney (1991) proposed to approach parsing by starting with finding related chunks of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsing by chunks. In Principle-Based Parsing. Kluwer Academic Pub- lishers"
            },
            "venue": {
                "fragments": [],
                "text": "Parsing by chunks. In Principle-Based Parsing. Kluwer Academic Pub- lishers"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 97
                            }
                        ],
                        "text": "And third, the F\u03b2=1 rate which is equal to (\u03b22+1)*precision*recall / (\u03b22*precision+recall) with \u03b2=1 (van Rijsbergen, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "/ormation Retrieval. Buttersworth"
            },
            "venue": {
                "fragments": [],
                "text": "/ormation Retrieval. Buttersworth"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bracket Guidelines /or Treebank H Style Penn Treebank Project. Penn Treebank II cdrom"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 93
                            }
                        ],
                        "text": "Additionally, a partof-speech (POS) tag was assigned to each token by a standard POS tagger (Brill (1994) trained on the Penn Treebank)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some advances in rule-based part of speech tagging"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94)"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building a large"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 97
                            }
                        ],
                        "text": "And third, the F\u03b2=1 rate which is equal to (\u03b22+1)*precision*recall / (\u03b22*precision+recall) with \u03b2=1 (van Rijsbergen, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Buttersworth"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval. Buttersworth"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "Part of the Sparkle project has concentrated on finding various sorts of chunks for the different languages (Carroll et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phrasal Parsing Software. Sparkle Work Package 3, Deliverable D3.2"
            },
            "venue": {
                "fragments": [],
                "text": "Phrasal Parsing Software. Sparkle Work Package 3, Deliverable D3.2"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "Part of the Sparkle project has concentrated on finding various sorts of chunks for the different languages (Carroll et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phrasal Parsing Software"
            },
            "venue": {
                "fragments": [],
                "text": "Sparkle Work Package"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Text chunking consists of dividing a text into phrases in such a way that syntactically related words become member of the same phrase."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bracket Guidelines /or Treebank H Style Penn Treebank Project"
            },
            "venue": {
                "fragments": [],
                "text": "Bracket Guidelines /or Treebank H Style Penn Treebank Project"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 9,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Introduction-to-the-CoNLL-2000-Shared-Task-Chunking-Sang-Buchholz/9e85832b04cc3700c2c26d6ba93fdeae39cac04a?sort=total-citations"
}