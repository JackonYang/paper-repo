{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34961461"
                        ],
                        "name": "Andrew L. Maas",
                        "slug": "Andrew-L.-Maas",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Maas",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew L. Maas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119816047"
                        ],
                        "name": "Raymond E. Daly",
                        "slug": "Raymond-E.-Daly",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Daly",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond E. Daly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061523260"
                        ],
                        "name": "Peter T. Pham",
                        "slug": "Peter-T.-Pham",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Pham",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter T. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110408720"
                        ],
                        "name": "Dan Huang",
                        "slug": "Dan-Huang",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922861"
                        ],
                        "name": "Christopher Potts",
                        "slug": "Christopher-Potts",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Potts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Potts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 63
                            }
                        ],
                        "text": "IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 64
                            }
                        ],
                        "text": "IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4\nAthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphics,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1428702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "649d03490ef72c5274e3bccd03d7a299d2f8da91",
            "isKey": false,
            "numCitedBy": 3009,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area."
            },
            "slug": "Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly",
            "title": {
                "fragments": [],
                "text": "Learning Word Vectors for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content, and finds it out-performs several previously introduced methods for sentiment classification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 206
                            }
                        ],
                        "text": "Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": ", 2010) RAE: Recursive Autoencoders (Socher et al., 2011)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3116311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfa2646776405d50533055ceb1b7f050e9014dcb",
            "isKey": false,
            "numCitedBy": 1245,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines."
            },
            "slug": "Semi-Supervised-Recursive-Autoencoders-for-Socher-Pennington",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions that outperform other state-of-the-art approaches on commonly used datasets, without using any pre-defined sentiment lexica or polarity shifting rules."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40037218"
                        ],
                        "name": "T. Nakagawa",
                        "slug": "T.-Nakagawa",
                        "structuredName": {
                            "firstName": "Tetsuji",
                            "lastName": "Nakagawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nakagawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3040648"
                        ],
                        "name": "Kentaro Inui",
                        "slug": "Kentaro-Inui",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Inui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kentaro Inui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795664"
                        ],
                        "name": "S. Kurohashi",
                        "slug": "S.-Kurohashi",
                        "structuredName": {
                            "firstName": "Sadao",
                            "lastName": "Kurohashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurohashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 178
                            }
                        ],
                        "text": "Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Nakagawa et al., 2010) used a SVM with secondorder polynomial kernel and additional features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "Our SVM-uni results are consistent with BoFnoDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 116
                            }
                        ],
                        "text": "These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 65
                            }
                        ],
                        "text": "CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 177
                            }
                        ],
                        "text": "Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 66
                            }
                        ],
                        "text": "CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2\nMPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3\nSubj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoencoders (Socher et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5935641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da5cd00115f7ec108de8eebf071c5f3f19807df4",
            "isKey": true,
            "numCitedBy": 362,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."
            },
            "slug": "Dependency-Tree-based-Sentiment-Classification-CRFs-Nakagawa-Inui",
            "title": {
                "fragments": [],
                "text": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Experimental results of sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables showed that the method performs better than other methods based on bag-of-features."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40378713"
                        ],
                        "name": "Justin Martineau",
                        "slug": "Justin-Martineau",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Martineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Martineau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144121212"
                        ],
                        "name": "Timothy W. Finin",
                        "slug": "Timothy-W.-Finin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Finin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy W. Finin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This interpolation can be seen as a form of regularization: trust NB unless the SVM is very confident."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Mining opinions and sentiment from social networking sites is a popular application for social media systems. Common approaches use a machine learning system with a bag of words feature set. We present Delta TFIDF, an intuitive general purpose technique to efficiently weight word scores before classification. Delta TFIDF is easy to compute, implement, and understand. We use Support Vector Machines to show that Delta TFIDF significantly improves accuracy for sentiment analysis problems using three well known data sets."
            },
            "slug": "Delta-TFIDF:-An-Improved-Feature-Space-for-Analysis-Martineau-Finin",
            "title": {
                "fragments": [],
                "text": "Delta TFIDF: An Improved Feature Space for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Delta TFIDF is presented, an intuitive general purpose technique to efficiently weight word scores before classification to significantly improves accuracy for sentiment analysis problems using three well known data sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICWSM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 70
                            }
                        ],
                        "text": "Details of the equivalent probabilistic formulations are presented in (McCallum and Nigam, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7311285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size."
            },
            "slug": "A-comparison-of-event-models-for-naive-bayes-text-McCallum-Nigam",
            "title": {
                "fragments": [],
                "text": "A comparison of event models for naive bayes text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi -variateBernoulli model at any vocabulary size."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5652683"
                        ],
                        "name": "L. Shih",
                        "slug": "L.-Shih",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Shih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113253"
                        ],
                        "name": "J. Teevan",
                        "slug": "J.-Teevan",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Teevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Teevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743286"
                        ],
                        "name": "D. Karger",
                        "slug": "D.-Karger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Karger",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Karger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "Compared to the excellent performance of MNB on snippet datasets, the many poor assumptions of MNB pointed out in (Rennie et al., 2003) become more crippling for these longer documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13606541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7fab2f72ebbf4e98def1daf8c29ffcfe91183bf",
            "isKey": false,
            "numCitedBy": 1089,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive Bayes is often used as a baseline in text classification because it is fast and easy to implement. Its severe assumptions make such efficiency possible but also adversely affect the quality of its results. In this paper we propose simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model. We find that our simple corrections result in a fast algorithm that is competitive with state-of-the-art text classification algorithms such as the Support Vector Machine."
            },
            "slug": "Tackling-the-Poor-Assumptions-of-Naive-Bayes-Text-Rennie-Shih",
            "title": {
                "fragments": [],
                "text": "Tackling the Poor Assumptions of Naive Bayes Text Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "167e1359943b96b9e92ee73db1df69a1f65d731d",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "slug": "A-Sentimental-Education:-Sentiment-Analysis-Using-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel machine-learning method is proposed that applies text-categorization techniques to just the subjective portions of the document, which greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3264224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
            "isKey": false,
            "numCitedBy": 2290,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."
            },
            "slug": "Seeing-Stars:-Exploiting-Class-Relationships-for-to-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A meta-algorithm is applied, based on a metric labeling formulation of the rating-inference problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081253"
                        ],
                        "name": "Casey Whitelaw",
                        "slug": "Casey-Whitelaw",
                        "structuredName": {
                            "firstName": "Casey",
                            "lastName": "Whitelaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Casey Whitelaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2838371"
                        ],
                        "name": "Navendu Garg",
                        "slug": "Navendu-Garg",
                        "structuredName": {
                            "firstName": "Navendu",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navendu Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1619039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0470303953de1d19765423f719d8314e3cb91278",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Little work to date in sentiment analysis (classifying texts by `positive' or `negative' orientation) has attempted to use fine-grained semantic distinctions in features used for classification. We present a new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny''. An appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on Appraisal Theory. Semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers. We classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%. In addition, we find that some types of appraisal appear to be more significant for sentiment classification than others."
            },
            "slug": "Using-appraisal-groups-for-sentiment-analysis-Whitelaw-Garg",
            "title": {
                "fragments": [],
                "text": "Using appraisal groups for sentiment analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny'' is presented, based on several task-independent semantic taxonomies based on Appraisal Theory."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722180"
                        ],
                        "name": "Ryan P. Adams",
                        "slug": "Ryan-P.-Adams",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Adams",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan P. Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4491618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e12a485325776d3c23eae2b488d4812d86b4052",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The restricted Boltzmann machine (RBM) is a flexible model for complex data. However, using RBMs for high-dimensional multinomial observations poses significant computational difficulties. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundred thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K-way softmax visible units during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue with a more general class of Markov chain Monte Carlo operators on the visible units, yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible with RBMs and by using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter."
            },
            "slug": "Training-Restricted-Boltzmann-Machines-on-Word-Dahl-Adams",
            "title": {
                "fragments": [],
                "text": "Training Restricted Boltzmann Machines on Word Observations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The success of this approach is demonstrated by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible with RBMs and by using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388317459"
                        ],
                        "name": "S. Lacoste-Julien",
                        "slug": "S.-Lacoste-Julien",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lacoste-Julien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lacoste-Julien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 59
                            }
                        ],
                        "text": "Table 4: On 3 20-newsgroup subtasks, we compare to DiscLDA (Lacoste-Julien et al., 2008) and ActiveSVM (Schohn and Cohn, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6500077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding a reduced dimensionality representation. Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics."
            },
            "slug": "DiscLDA:-Discriminative-Learning-for-Dimensionality-Lacoste-Julien-Sha",
            "title": {
                "fragments": [],
                "text": "DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents DiscLDA, a discriminative variation on Latent Dirichlet Allocation in which a class-dependent linear transformation is introduced on the topic mixture proportions, and obtains a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761866"
                        ],
                        "name": "Greg Schohn",
                        "slug": "Greg-Schohn",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Schohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Schohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50742419"
                        ],
                        "name": "David A. Cohn",
                        "slug": "David-A.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2008) and ActiveSVM (Schohn and Cohn, 2000)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1713753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "609e5cc1da126d7f760d1444b43b4fae41602841",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classification tasks. We observe a number of benefits, the most surprising of which is that a SVM trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data."
            },
            "slug": "Less-is-More:-Active-Learning-with-Support-Vector-Schohn-Cohn",
            "title": {
                "fragments": [],
                "text": "Less is More: Active Learning with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A simple active learning heuristic is described which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classification tasks and frequently does so in less time than the naive approach of training on all available data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081253"
                        ],
                        "name": "Casey Whitelaw",
                        "slug": "Casey-Whitelaw",
                        "structuredName": {
                            "firstName": "Casey",
                            "lastName": "Whitelaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Casey Whitelaw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Appraisal Taxonomy: (Whitelaw et al., 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14516398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f85eee93f76aaacece1ad3ae6b4a8a39a5bb937",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have seen a growing interest in non-topical text analysis, in which characterizations are sought of the opinions, feelings, and attitudes expressed in a text, rather than just the facts. A key problem in this area is sentiment classification, in which a document is labelled as a positive (\u2018thumbs up\u2019) or negative (\u2019thumbs down\u2019) evaluation of a target object (film, book, product, etc.). Immediate applications include data and web mining, market research, and customer relationship management."
            },
            "slug": "Using-Appraisal-Taxonomies-for-Sentiment-Analysis-Whitelaw",
            "title": {
                "fragments": [],
                "text": "Using Appraisal Taxonomies for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Non-topical text analysis, in which characterizations are sought of the opinions, feelings, and attitudes expressed in a text, rather than just the facts, is seen as a growing interest."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34827767"
                        ],
                        "name": "Alistair Kennedy",
                        "slug": "Alistair-Kennedy",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Kennedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This interpolation can be seen as a form of regularization: trust NB unless the SVM is very confident."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16414995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14609a3a6c6f8ef3269d3e0728f88da57826698",
            "isKey": false,
            "numCitedBy": 774,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two methods for determining the sentiment expressed by a movie review. The semantic orientation of a review can be positive, negative, or neutral. We examine the effect of valence shifters on classifying the reviews. We examine three types of valence shifters: negations, intensifiers, and diminishers. Negations are used to reverse the semantic polarity of a particular term, while intensifiers and diminishers are used to increase and decrease, respectively, the degree to which a term is positive or negative. The first method classifies reviews based on the number of positive and negative terms they contain. We use the General Inquirer to identify positive and negative terms, as well as negation terms, intensifiers, and diminishers. We also use positive and negative terms from other sources, including a dictionary of synonym differences and a very large Web corpus. To compute corpus\u2010based semantic orientation values of terms, we use their association scores with a small group of positive and negative terms. We show that extending the term\u2010counting method with contextual valence shifters improves the accuracy of the classification. The second method uses a Machine Learning algorithm, Support Vector Machines. We start with unigram features and then add bigrams that consist of a valence shifter and another word. The accuracy of classification is very high, and the valence shifter bigrams slightly improve it. The features that contribute to the high accuracy are the words in the lists of positive and negative terms. Previous work focused on either the term\u2010counting method or the Machine Learning method. We show that combining the two methods achieves better results than either method alone."
            },
            "slug": "SENTIMENT-CLASSIFICATION-of-MOVIE-REVIEWS-USING-Kennedy-Inkpen",
            "title": {
                "fragments": [],
                "text": "SENTIMENT CLASSIFICATION of MOVIE REVIEWS USING CONTEXTUAL VALENCE SHIFTERS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that extending the term\u2010counting method with contextual valence shifters improves the accuracy of the classification, and combining the two methods achieves better results than either method alone."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47340898"
                        ],
                        "name": "Karo Moilanen",
                        "slug": "Karo-Moilanen",
                        "structuredName": {
                            "firstName": "Karo",
                            "lastName": "Moilanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karo Moilanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 94
                            }
                        ],
                        "text": "Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "(Moilanen and Pulman, 2007) suggests that while \u201cstatistical methods\u201d work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based system is necessary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32599709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9b0190b06ac7270e9052895f8592beb4959ccfd",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment classification of grammatical constituents can be explained in a quasicompositional way. The classification of a complex constituent is derived via the classification of its component constituents and operations on these that resemble the usual methods of compositional semantic analysis. This claim is illustrated with a description of sentiment propagation, polarity reversal, and polarity conflict resolution within various linguistic constituent types at various grammatical levels. We propose a theoretical composition model, evaluate a lexical dependency parsing post-process implementation, and estimate its impact on general NLP pipelines."
            },
            "slug": "Sentiment-Composition-Moilanen-Pulman",
            "title": {
                "fragments": [],
                "text": "Sentiment Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A theoretical composition model is proposed, a lexical dependency parsing post-process implementation is evaluated, and its impact on general NLP pipelines is estimated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5024,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 296750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90929a6aa901ba958eb4960aeeb594c752e08369",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation\u2014which is borne out in repeated experiments\u2014that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster."
            },
            "slug": "On-Discriminative-vs.-Generative-Classifiers:-A-of-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270033"
                        ],
                        "name": "V. Metsis",
                        "slug": "V.-Metsis",
                        "structuredName": {
                            "firstName": "Vangelis",
                            "lastName": "Metsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Metsis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752430"
                        ],
                        "name": "Ion Androutsopoulos",
                        "slug": "Ion-Androutsopoulos",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Androutsopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Androutsopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4738873"
                        ],
                        "name": "G. Paliouras",
                        "slug": "G.-Paliouras",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Paliouras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Paliouras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18749628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f5ce28afc0c2eafd4a6ef711e399bee4056c3b8",
            "isKey": false,
            "numCitedBy": 563,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five dierent versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the tempo- ral order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of person- alized spam filters, and we plot roc curves that allow us to compare the dierent versions of nb over the entire tradeo between true positives and true negatives."
            },
            "slug": "Spam-Filtering-with-Naive-Bayes-Which-Naive-Bayes-Metsis-Androutsopoulos",
            "title": {
                "fragments": [],
                "text": "Spam Filtering with Naive Bayes - Which Naive Bayes?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An experimental procedure that emulates the incremental training of person- alized spam filters is adopted, and roc curves that allow us to compare the dierent versions of nb over the entire tradeo between true positives and true negatives are plotted."
            },
            "venue": {
                "fragments": [],
                "text": "CEAS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1118305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6",
            "isKey": false,
            "numCitedBy": 957,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections."
            },
            "slug": "A-Markov-random-field-model-for-term-dependencies-Metzler-Croft",
            "title": {
                "fragments": [],
                "text": "A Markov random field model for term dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data, and significant improvements are possible by modeling dependencies, especially on the larger web collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509504"
                        ],
                        "name": "A. D. Vries",
                        "slug": "A.-D.-Vries",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Vries",
                            "middleNames": [
                                "P.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785829"
                        ],
                        "name": "T. Roelleke",
                        "slug": "T.-Roelleke",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Roelleke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Roelleke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6080670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bc16530a3e3727823d03bae8f91d725c3e92ff9",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "When investigating alternative estimates for term discriminativeness, we discovered that relevance information and idf are much closer related than formulated in classical literature. Therefore, we revisited the justification of idf as it follows from the binary independent retrieval (BIR) model. The main result is a formal framework uncovering the close relationship of a generalised idf and the BIR model. The framework makes explicit how to incorporate relevance information into any retrieval function that involves an idf-component.In addition to the idf-based formulation of the BIR model, we propose Poisson-based estimates as an alternative to the classical estimates, this being motivated by the superiority of Poisson-based estimates for the within-document term frequencies. The main experimental finding is that a Poisson-based idf is superior to the classical idf, where the superiority is particularly evident for long queries."
            },
            "slug": "Relevance-information:-a-loss-of-entropy-but-a-gain-Vries-Roelleke",
            "title": {
                "fragments": [],
                "text": "Relevance information: a loss of entropy but a gain for IDF?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The main result is a formal framework uncovering the close relationship of a generalised idf and the BIR model, and a Poisson-based idf is superior to the classical idf, where the superiority is particularly evident for long queries."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19355717"
                        ],
                        "name": "Minqing Hu",
                        "slug": "Minqing-Hu",
                        "structuredName": {
                            "firstName": "Minqing",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minqing Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655430"
                        ],
                        "name": "Bing Liu",
                        "slug": "Bing-Liu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207155218,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "cdcf7cb29f37ac0546961ea8a076075b9cc1f992",
            "isKey": false,
            "numCitedBy": 7040,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques."
            },
            "slug": "Mining-and-summarizing-customer-reviews-Hu-Liu",
            "title": {
                "fragments": [],
                "text": "Mining and summarizing customer reviews"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This research aims to mine and to summarize all the customer reviews of a product, and proposes several novel techniques to perform these tasks."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143869012"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207670589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d986dac610e20441adb9161e5466c88932626e9",
            "isKey": false,
            "numCitedBy": 1345,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to \"explain\" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data."
            },
            "slug": "A-study-of-smoothing-methods-for-language-models-to-Zhai-Lafferty",
            "title": {
                "fragments": [],
                "text": "A study of smoothing methods for language models applied to information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to or better than the best results achieved using a single smoothing methods and exhaustive parameter search on the test data."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798723"
                        ],
                        "name": "Mandar Mitra",
                        "slug": "Mandar-Mitra",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5991216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5604bbf9fd2083cd61f0beb98662986292cab72b",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Most casual users of IR systems type short queries. Recent research has shown that adding new words to these queries via odhoc feedback improves the retrieval effectiveness of such queries. We investigate ways to improve this query expansion process by refining the set of documents used in feedback. We start by using manually formulated Boolean filters along with proximity constraints. Our approach is similar to the one proposed by Hearst[l2]. Next, we investigate a completely automatic method that makes use of term cooccurrence information to estimate word correlation. Experimental results show that refining the set of documents used in query expansion often prevents the query drift caused by blind expansion and yields substantial improvements in retrieval effectiveness, both in terms of average precision and precision in the top twenty documents. More importantly, the fully automatic approach developed in this study performs competitively with the best manual approach and requires little computational overhead."
            },
            "slug": "Improving-automatic-query-expansion-Mitra-Singhal",
            "title": {
                "fragments": [],
                "text": "Improving automatic query expansion"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results show that refining the set of documents used in query expansion often prevents the query drift caused by blind expansion and yields substantial improvements in retrieval effectiveness, both in terms of average precision and precision in the top twenty documents."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2618428"
                        ],
                        "name": "Timothy G. Armstrong",
                        "slug": "Timothy-G.-Armstrong",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Armstrong",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy G. Armstrong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774250"
                        ],
                        "name": "William Webber",
                        "slug": "William-Webber",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Webber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Webber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8057389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4841c8edcf42b3d4bc2a2d3fb639cc1dc6e8ed5b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation forums such as TREC allow systematic measurement and comparison of information retrieval techniques. The goal is consistent improvement, based on reliable comparison of the effectiveness of different approaches and systems. In this paper we report experiments to determine whether this goal has been achieved. We ran five publicly available search systems, in a total of seventeen different configurations, against nine TREC adhoc-style collections, spanning 1994 to 2005. These runsets were then used as a benchmark for reassessing the relative effectiveness of the original TREC runs for those collections. Surprisingly, there appears to have been no overall improvement in effectiveness for either median or top-end TREC submissions, even after allowing for several possible confounds. We therefore question whether the effectiveness of adhoc information retrieval has improved over the past decade and a half."
            },
            "slug": "Has-adhoc-retrieval-improved-since-1994-Armstrong-Moffat",
            "title": {
                "fragments": [],
                "text": "Has adhoc retrieval improved since 1994?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "There appears to have been no overall improvement in effectiveness for either median or top-end TREC submissions, even after allowing for several possible confounds, and it is questioned whether the effectiveness of adhoc information retrieval has improved over the past decade and a half."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108590949"
                        ],
                        "name": "Shuang Liu",
                        "slug": "Shuang-Liu",
                        "structuredName": {
                            "firstName": "Shuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642131"
                        ],
                        "name": "Clement T. Yu",
                        "slug": "Clement-T.-Yu",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Yu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement T. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104096"
                        ],
                        "name": "Chaojing Sun",
                        "slug": "Chaojing-Sun",
                        "structuredName": {
                            "firstName": "Chaojing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaojing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170732244"
                        ],
                        "name": "Fang Liu",
                        "slug": "Fang-Liu",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38699354"
                        ],
                        "name": "W. Meng",
                        "slug": "W.-Meng",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Meng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1761326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac52fdb033f3114d7f642400a561f5190dd6ab30",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been shown that using phrases properly in the document retrieval leads to higher retrieval effectiveness. In this paper, we define four types of noun phrases and present an algorithm for recognizing these phrases in queries. The strengths of several existing tools are combined for phrase recognition. Our algorithm is tested using a set of 500 web queries from a query log, and a set of 238 TREC queries. Experimental results show that our algorithm yields high phrase recognition accuracy. We also use a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries. A document retrieval experiment is conducted using the TREC queries (1) without any phrases, (2) with the phrases recognized from a baseline noun phrase recognition algorithm, and (3) with the phrases recognized from our algorithm respectively. The retrieval effectiveness of (3) is better than that of (2), which is better than that of (1). This demonstrates that utilizing phrases in queries does improve the retrieval effectiveness, and better noun phrase recognition yields higher retrieval performance."
            },
            "slug": "Recognition-and-classification-of-noun-phrases-in-Zhang-Liu",
            "title": {
                "fragments": [],
                "text": "Recognition and classification of noun phrases in queries for effective retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper defines four types of noun phrases and presents an algorithm for recognizing these phrases in queries and uses a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47481818"
                        ],
                        "name": "Theresa Wilson",
                        "slug": "Theresa-Wilson",
                        "structuredName": {
                            "firstName": "Theresa",
                            "lastName": "Wilson",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Theresa Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2\nMPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3\nSubj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 51
                            }
                        ],
                        "text": "MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 382842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cff7cc15555c38607016aaba24059e76b160adb",
            "isKey": false,
            "numCitedBy": 1752,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented."
            },
            "slug": "Annotating-Expressions-of-Opinions-and-Emotions-in-Wiebe-Wilson",
            "title": {
                "fragments": [],
                "text": "Annotating Expressions of Opinions and Emotions in Language"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Lang. Resour. Evaluation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108590949"
                        ],
                        "name": "Shuang Liu",
                        "slug": "Shuang-Liu",
                        "structuredName": {
                            "firstName": "Shuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642131"
                        ],
                        "name": "Clement T. Yu",
                        "slug": "Clement-T.-Yu",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Yu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement T. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38699354"
                        ],
                        "name": "W. Meng",
                        "slug": "W.-Meng",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Meng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14203644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "187634752d16ad97193dd6ab3cd41a4ed13dfe5a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to determine the senses of words in queries by using WordNet. In our approach, noun phrases in a query are determined first. For each word in the query, information associated with it, including its synonyms, hyponyms, hypernyms, definitions of its synonyms and hyponyms, and its domains, can be used for word sense disambiguation. By comparing these pieces of information associated with the words which form a phrase, it may be possible to assign senses to these words. If the above disambiguation fails, then other query words, if exist, are used, by going through exactly the same process. If the sense of a query word cannot be determined in this manner, then a guess of the sense of the word is made, if the guess has at least 50% chance of being correct. If no sense of the word has 50% or higher chance of being used, then we apply a Web search to assist in the word sense disambiguation process. Experimental results show that our approach has 100% applicability and 90% accuracy on the most recent robust track of TREC collection of 250 queries. We combine this disambiguation algorithm to our retrieval system to examine the effect of word sense disambiguation in text retrieval. Experimental results show that the disambiguation algorithm together with other components of our retrieval system yield a result which is 13.7% above that produced by the same system but without the disambiguation, and 9.2% above that produced by using Lesk's algorithm. Our retrieval effectiveness is 7% better than the best reported result in the literature."
            },
            "slug": "Word-sense-disambiguation-in-queries-Liu-Yu",
            "title": {
                "fragments": [],
                "text": "Word sense disambiguation in queries"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A new approach to determine the senses of words in queries by using WordNet is presented, which has 100% applicability and 90% accuracy on the most recent robust track of TREC collection of 250 queries and the retrieval effectiveness is 7% better than the best reported result in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18532232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "471cb4c2e5039bdaacb0274fee70c7fe2e93493e",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "A range of methods for measuring the effectiveness of information retrieval systems has been proposed. These are typically intended to provide a quantitative single-value summary of a document ranking relative to a query. However, many of these measures have failings. For example, recall is not well founded as a measure of satisfaction, since the user of an actual system cannot judge recall. Average precision is derived from recall, and suffers from the same problem. In addition, average precision lacks key stability properties that are needed for robust experiments. In this article, we introduce a new effectiveness metric, rank-biased precision, that avoids these problems. Rank-biased pre-cision is derived from a simple model of user behavior, is robust if answer rankings are extended to greater depths, and allows accurate quantification of experimental uncertainty, even when only partial relevance judgments are available."
            },
            "slug": "Rank-biased-precision-for-measurement-of-retrieval-Moffat-Zobel",
            "title": {
                "fragments": [],
                "text": "Rank-biased precision for measurement of retrieval effectiveness"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new effectiveness metric, rank-biased precision, is introduced that is derived from a simple model of user behavior, is robust if answer rankings are extended to greater depths, and allows accurate quantification of experimental uncertainty, even when only partial relevance judgments are available."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751287"
                        ],
                        "name": "C. Clarke",
                        "slug": "C.-Clarke",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Clarke",
                            "middleNames": [
                                "L.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732541"
                        ],
                        "name": "F. Scholer",
                        "slug": "F.-Scholer",
                        "structuredName": {
                            "firstName": "Falk",
                            "lastName": "Scholer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scholer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144526707"
                        ],
                        "name": "I. Soboroff",
                        "slug": "I.-Soboroff",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Soboroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Soboroff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15412217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6626cb6139312a6c3ee1b525ace3f2347cd90052",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Terabyte Track explores how retrieval and evaluation techniques can scale to terabyte-sized collections, examining both efficiency and effectiveness issues. TREC 2005 is the second year for the track. The track was introduced as part of TREC 2004, with a single adhoc retrieval task. That year, 17 groups submitted 70 runs in total. This year, the track consisted of three experimental tasks: an adhoc retrieval task, an efficiency task and a named page finding task. 18 groups submitted runs to the adhoc retrieval task, 13 groups submitted runs to the efficiency task, and 13 groups submitted runs to the named page finding task. This report provides an overview of each task, summarizes the results and discusses directions for the future. Further background information on the development of the track can be found in last year\u2019s track report [4]."
            },
            "slug": "The-TREC-2005-Terabyte-Track-Clarke-Scholer",
            "title": {
                "fragments": [],
                "text": "The TREC 2005 Terabyte Track"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The Terabyte Track explores how retrieval and evaluation techniques can scale to terabyte-sized collections, examining both efficiency and effectiveness issues and discusses directions for the future."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691929"
                        ],
                        "name": "D. Hiemstra",
                        "slug": "D.-Hiemstra",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Hiemstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hiemstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430625"
                        ],
                        "name": "S. Robertson",
                        "slug": "S.-Robertson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robertson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833561"
                        ],
                        "name": "H. Zaragoza",
                        "slug": "H.-Zaragoza",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Zaragoza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zaragoza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 585924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bedd7d91ec8cb7ad68075e1d973cbdbb79b0597",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We systematically investigate a new approach to estimating the parameters of language models for information retrieval, called parsimonious language models. Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing. As such, they need fewer (non-zero) parameters to describe the data. We apply parsimonious models at three stages of the retrieval process: 1) at indexing time; 2) at search time; 3) at feedback time. Experimental results show that we are able to build models that are significantly smaller than standard models, but that still perform at least as well as the standard approaches."
            },
            "slug": "Parsimonious-language-models-for-information-Hiemstra-Robertson",
            "title": {
                "fragments": [],
                "text": "Parsimonious language models for information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing, and need fewer (non-zero) parameters to describe the data."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985957"
                        ],
                        "name": "Trevor Strohman",
                        "slug": "Trevor-Strohman",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Strohman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Strohman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118116642"
                        ],
                        "name": "Yun Zhou",
                        "slug": "Yun-Zhou",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yun Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2369011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73cd8396c778cde6a78e04dc6c37fabd622f21a9",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This work details the experiments carried out using the Indri search engine during the TREC 2005 Terabyte Track. Results are presented for each of the three tasks, including eciency , ad hoc, and named page nding. Our eciency runs focused on query optimization techniques, our ad hoc runs look at the importance of term proximity and document quality, and our named-page nding runs investigate the use of document priors and document structure."
            },
            "slug": "Indri-at-TREC-2005:-Terabyte-Track-Metzler-Strohman",
            "title": {
                "fragments": [],
                "text": "Indri at TREC 2005: Terabyte Track"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work details the experiments carried out using the Indri search engine during the TREC 2005 Terabyte Track, including eciency, ad hoc, and named page nding, which focused on query optimization techniques and the use of document priors and document structure."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2143884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1168df8436f10b279594a440cbde69f4bef0583",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overview-of-the-Second-Text-REtrieval-Conference-Harman",
            "title": {
                "fragments": [],
                "text": "Overview of the Second Text REtrieval Conference (TREC-2)"
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849128"
                        ],
                        "name": "Rong-En Fan",
                        "slug": "Rong-En-Fan",
                        "structuredName": {
                            "firstName": "Rong-En",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong-En Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144799660"
                        ],
                        "name": "Xiang-Rui Wang",
                        "slug": "Xiang-Rui-Wang",
                        "structuredName": {
                            "firstName": "Xiang-Rui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang-Rui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 23
                            }
                        ],
                        "text": "The LIBLINEAR library (Fan et al., 2008) is used here."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3116168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "268a4f8da15a42f3e0e71691f760ff5edbf9cec8",
            "isKey": false,
            "numCitedBy": 7765,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."
            },
            "slug": "LIBLINEAR:-A-Library-for-Large-Linear-Fan-Chang",
            "title": {
                "fragments": [],
                "text": "LIBLINEAR: A Library for Large Linear Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "LIBLINEAR is an open source library for large-scale linear classification that supports logistic regression and linear support vector machines and provides easy-to-use command-line tools and library calls for users and developers."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689089"
                        ],
                        "name": "M. Smucker",
                        "slug": "M.-Smucker",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Smucker",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Smucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750995"
                        ],
                        "name": "Ben Carterette",
                        "slug": "Ben-Carterette",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Carterette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Carterette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5893582,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3bf42fdbe24fe5aaa491266006d89bae53e99552",
            "isKey": false,
            "numCitedBy": 682,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Information retrieval (IR) researchers commonly use three tests of statistical significance: the Student's paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher's randomization (permutation) test as non-parametric significance tests for IR but these tests have seen little use. For each of these five tests, we took the ad-hoc retrieval runs submitted to TRECs 3 and 5-8, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means."
            },
            "slug": "A-comparison-of-statistical-significance-tests-for-Smucker-Allan",
            "title": {
                "fragments": [],
                "text": "A comparison of statistical significance tests for information retrieval evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is discovered that there is little practical difference between the randomization, bootstrap, and t tests and their use should be discontinued for measuring the significance of a difference between means."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144721996"
                        ],
                        "name": "M. Sanderson",
                        "slug": "M.-Sanderson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sanderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sanderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13309327,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4314bbc2a62c37a9b66d759fc35ae6f7607344e0",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The effectiveness of information retrieval systems is measured by comparing performance on a common set of queries and documents. Significance tests are often used to evaluate the reliability of such comparisons. Previous work has examined such tests, but produced results with limited application. Other work established an alternative benchmark for significance, but the resulting test was too stringent. In this paper, we revisit the question of how such tests should be used. We find that the t-test is highly reliable (more so than the sign or Wilcoxon test), and is far more reliable than simply showing a large percentage difference in effectiveness measures between IR systems. Our results show that past empirical work on significance tests over-estimated the error of such tests. We also re-consider comparisons between the reliability of precision at rank 10 and mean average precision, arguing that past comparisons did not consider the assessor effort required to compute such measures. This investigation shows that assessor effort would be better spent building test collections with more topics, each assessed in less detail."
            },
            "slug": "Information-retrieval-system-evaluation:-effort,-Sanderson-Zobel",
            "title": {
                "fragments": [],
                "text": "Information retrieval system evaluation: effort, sensitivity, and reliability"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is found that the t-test is highly reliable (more so than the sign or Wilcoxon test), and is far more reliable than simply showing a large percentage difference in effectiveness measures between IR systems."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750485"
                        ],
                        "name": "T. Lynam",
                        "slug": "T.-Lynam",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lynam",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lynam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751287"
                        ],
                        "name": "C. Clarke",
                        "slug": "C.-Clarke",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Clarke",
                            "middleNames": [
                                "L.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3114123"
                        ],
                        "name": "G. Cormack",
                        "slug": "G.-Cormack",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Cormack",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cormack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1670462,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1999d96a9949f33cddff251c97c822b9e16300f0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems. Each system demonstrated improved effectiveness with the use of <i>blind feedback</i>, in which the results of a preliminary retrieval step were used to augment the efficacy of a secondary retrieval step. The hybrid combination of primary and secondary retrieval steps from different systems in a number of cases yielded better effectiveness than either of the constituent systems alone. This positive combining effect was observed when entire documents were passed between the two retrieval steps, but not when only the expansion terms were passed. Several combinations of primary and secondary retrieval steps were fused using the CombMNZ algorithm; all yielded significant effectiveness improvement over the individual systems, with the best yielding a an improvement of 13% (<i>p</i> = 10<sup>-6</sup>) over the best individual system and an improvement of 4% (<i>p</i> = 10<sup>-5</sup>) over a simple fusion of the eight systems."
            },
            "slug": "A-multi-system-analysis-of-document-and-term-for-Lynam-Buckley",
            "title": {
                "fragments": [],
                "text": "A multi-system analysis of document and term selection for blind feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems and each system demonstrated improved effectiveness with the use of <i>blind feedback</i>, in which the results of a preliminary retrieval step were used to augment the efficacy of a secondary retrieval step."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35260669"
                        ],
                        "name": "J. Gonz\u00e1lez",
                        "slug": "J.-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": [
                                "Luis",
                                "Vicedo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gonz\u00e1lez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143802306"
                        ],
                        "name": "Jaime G\u00f3mez",
                        "slug": "Jaime-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "G\u00f3mez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaime G\u00f3mez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33367421,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "88fbf478fd5d0baa2c1d5cc6d0b7daea38893a89",
            "isKey": false,
            "numCitedBy": 1010,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Any books that you read, no matter how you got the sentences that have been read from the books, surely they will give you goodness. But, we will show you one of recommendation of the book that you need to read. This trec experiment and evaluation in information retrieval is what we surely mean. We will show you the reasonable reasons why you need to read this book. This book is a kind of precious book written by an experienced author."
            },
            "slug": "TREC:-Experiment-and-evaluation-in-information-Gonz\u00e1lez-G\u00f3mez",
            "title": {
                "fragments": [],
                "text": "TREC: Experiment and evaluation in information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "One of recommendation of the book that you need to read is shown, which is a kind of precious book written by an experienced author and the reasonable reasons why you should read this book are shown."
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 204
                            }
                        ],
                        "text": "The TREC effort collects sizeable document corpuses, formulates topics, and undertakes relevance assessments, creating judgment sets that are tolerably comprehensive even for multi-gigabyte document sets [Zobel, 1998]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14804938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "150a31a1d38d90acefb560c2a42efed1ae67f7f7",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Two stages in measurement of techniques for informationretrieval are gathering of documents for relevance assessment anduse of the assessments to numerically evaluate effectiveness. Weconsider both of these stages in the context of the TRECexperiments, to determine whether they lead to measurements thatare trustworthy and fair. Our detailed empirical investigation ofthe TREC results shows that the measured relative performance ofsystems appears to be reliable, but that recall is overestimated:it is likely that many relevant documents have not been found. Wepropose a new pooling strategy that can significantly in- creasethe number of relevant documents found for given effort, withoutcompromising fairness."
            },
            "slug": "How-reliable-are-the-results-of-large-scale-Zobel",
            "title": {
                "fragments": [],
                "text": "How reliable are the results of large-scale information retrieval experiments?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A detailed empirical investigation of the TREC results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145344526"
                        ],
                        "name": "H. Fang",
                        "slug": "H.-Fang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736467"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14779811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e59c231e2f68b99efd9c7174fbdd4daa744073a8",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A common limitation of many retrieval models, including the recently proposed axiomatic approaches, is that retrieval scores are solely based on exact (i.e., syntactic) matching of terms in the queries and documents, without allowing distinct but semantically related terms to match each other and contribute to the retrieval score. In this paper, we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of terms. We define several desirable retrieval constraints for semantic term matching and use such constraints to extend the axiomatic model to directly support semantic term matching based on the mutual information of terms computed on some document set. We show that such extension can be efficiently implemented as query expansion. Experiment results on several representative data sets show that, with mutual information computed over the documents in either the target collection for retrieval or an external collection such as the Web, our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model. As a pseudo feedback method, our method also outperforms a state-of-the-art language modeling feedback method."
            },
            "slug": "Semantic-term-matching-in-axiomatic-approaches-to-Fang-Zhai",
            "title": {
                "fragments": [],
                "text": "Semantic term matching in axiomatic approaches to information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of terms, and shows that such extension can be efficiently implemented as query expansion."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2540550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d9340b8cee37a2be8034a4b2e8aaf02b3644ff6",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The eighth Text REtrieval Conference TREC was held at the National Institute of Standards and Tech nology NIST on November The conference was co sponsored by NIST and the Information Technology O ce of the Defense Advanced Research Projects Agency DARPA TREC is the latest in a series of workshops designed to foster research in text retrieval For analyses of the results of previous workshops see Tague Sutcli e and Blustein Harman and Sparck Jones In addition the overview paper in each of the previous TREC proceedings summarizes the results of that TREC The TREC workshop series has the following goals"
            },
            "slug": "Overview-of-the-Eighth-Text-REtrieval-Conference-Voorhees-Harman",
            "title": {
                "fragments": [],
                "text": "Overview of the Eighth Text REtrieval Conference (TREC-8)"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The eighth Text REtrieval Conference TREC was held at the National Institute of Standards and Tech nology NIST on November and outlined the goals of the series of workshops designed to foster research in text retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774250"
                        ],
                        "name": "William Webber",
                        "slug": "William-Webber",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Webber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Webber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 121
                            }
                        ],
                        "text": "If different researchers perform experiments on different, privately-formed collections, comparing scores is problematic [Webber et al., 2008]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2158484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b17626c1908e97391c1c38b137a22b54559c48",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of system evaluation in information retrieval has always been to determine which of a set of systems is superior on a given collection. The tool used to determine system ordering is an evaluation metric such as average precision, which computes relative, collection-specific scores. We argue that a broader goal is achievable. In this paper we demonstrate that, by use of standardization, scores can be substantially independent of a particular collection, allowing systems to be compared even when they have been tested on different collections. Compared to current methods, our techniques provide richer information about system performance, improved clarity in outcome reporting, and greater simplicity in reviewing results from disparate sources."
            },
            "slug": "Score-standardization-for-inter-collection-of-Webber-Moffat",
            "title": {
                "fragments": [],
                "text": "Score standardization for inter-collection comparison of retrieval systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that, by use of standardization, scores can be substantially independent of a particular collection, allowing systems to be compared even when they have been tested on different collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222817"
                        ],
                        "name": "C. Cleverdon",
                        "slug": "C.-Cleverdon",
                        "structuredName": {
                            "firstName": "Cyril",
                            "lastName": "Cleverdon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cleverdon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59960705,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8aebe1773c62f13d3670fe5c4fc917fec5cdd056",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The investigation dealt with the effect which different devices have on the performance of index languages. It appeared that the most important consideration was the specificity of the index terms; within the context of the conditions existing in this test, single\u2010word terms were more effective than concept terms or a controlled vocabulary."
            },
            "slug": "The-Cranfield-tests-on-index-language-devices-Cleverdon",
            "title": {
                "fragments": [],
                "text": "The Cranfield tests on index language devices"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It appeared that the most important consideration was the specificity of the index terms; within the context of the conditions existing in this test, single\u2010word terms were more effective than concept terms or a controlled vocabulary."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222817"
                        ],
                        "name": "C. Cleverdon",
                        "slug": "C.-Cleverdon",
                        "structuredName": {
                            "firstName": "Cyril",
                            "lastName": "Cleverdon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cleverdon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1066940,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3ccdda9e2b4e270dd4b3b5db4aaefa5724c9fb1e",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "s Co F().w"
            },
            "slug": "The-significance-of-the-Cranfield-tests-on-index-Cleverdon",
            "title": {
                "fragments": [],
                "text": "The significance of the Cranfield tests on index languages"
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '91"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "EvaluatIR: Measurement and certification of IR systems"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 32nd Ann. Int. ACM SIGIR Conf. on Research and Development in Information Retrieval,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 6
                            }
                        ],
                        "text": "While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On discriminative vs"
            },
            "venue": {
                "fragments": [],
                "text": "generative classifiers: A comparison of logistic regression and naive bayes. In Proceedings of NIPS, volume 2, pages 841\u2013848."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valence Shifter: (Kennedy and Inkpen, 2006). tf"
            },
            "venue": {
                "fragments": [],
                "text": "idf: (Martineau and Finin,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tf. idf: (Martineau and Finin"
            },
            "venue": {
                "fragments": [],
                "text": "Valence Shifter: (Kennedy and Inkpen,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "N+, N are the number of positive and negative training cases"
            },
            "venue": {
                "fragments": [],
                "text": "However, as in (Metsis et al.,"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 7,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 49,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Baselines-and-Bigrams:-Simple,-Good-Sentiment-and-Wang-Manning/dc0975ae518a5b30e60fde23a41c74bafd7c6f8c?sort=total-citations"
}