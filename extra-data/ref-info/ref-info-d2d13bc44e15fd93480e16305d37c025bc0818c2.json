{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 777816,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee177aacf6b3697d079579ce558cdb2ee58cee39",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite-dimensional unit ball in feature space into a finite-dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence, we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines."
            },
            "slug": "Generalization-performance-of-regularization-and-of-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks, are derived by obtaining new bounds on their covering numbers by using the eigenvalues of an integral operator induced by the kernel function used by the machine."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "This approach was also sketched in [90]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11652139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0366ce5be03f003f8b28078f8e154a79baa80987",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem."
            },
            "slug": "On-a-Kernel-Based-Method-for-Pattern-Recognition,-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion is presented, adopting a regularization-theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": "Algorithmica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "It is possible to show that for a given constant C in problem (5.4), there exist C and in problem (5.1) such that the solutions of the two problems are the same, up to a constant factor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Using the fact that yi \u2208 {\u22121,+1} it is easy to see that our formulation (equation (5.10)) is equivalent to the following quadratic programming problem, originally proposed by Cortes and Vapnik [22]:\nProblem 5.4.\nmin f\u2208H,\u03be\n\u03a6(f, \u03be) = C\nl l\u2211 i=1 \u03bei + 1 2 \u2016f\u20162K\nsubject to the constraints:\nyif(xi)\u2265 1 \u2212\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "It is possible to show that for a given constant C in problem (5.4), there exist C and in problem (5.1) such that the solutions of the two problems are the same, up to a constant factor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Classical regularization theory, as we will consider here formulates the regression problem as a variational problem of finding the function f that minimizes the functional\nmin f\u2208H H[f ] = 1 l l\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162K (1.1)\nwhere \u2016f\u20162K is a norm in a Reproducing Kernel Hilbert Space H defined\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Consider the classical regularization case\nmin f\u2208H H[f ] = 1 l l\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162K (7.1)\nFollowing Girosi et al. [39] let us define:\n1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Jaakkola and Haussler [47] consider the case in which prior information is available in terms of a parametric probabilistic model P (x, y) of the process generating the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "22 Further distribution dependent results have been derived recently - see [47,16,34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1888591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c35cc80fe8c6cdea742d4fa1af1f2e698d41aba7",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of exible conditional probability models and techniques for classi cation regression problems Many existing methods such as generalized linear models and support vector machines are subsumed under this class The exibility of this class of techniques comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models"
            },
            "slug": "Probabilistic-kernel-regression-models-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Probabilistic kernel regression models"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A class of exible conditional probability models and techniques for classi cation regression problems that comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "Notice that in problem (5.1) each example has to satisfy two inequalities (which come out of using the ILF), while in problem (5.4) each example has to satisfy one inequality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6082464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27c7569fdbcbb57ff511f5293e32b547acca7b3",
            "isKey": true,
            "numCitedBy": 572,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory."
            },
            "slug": "An-Equivalence-Between-Sparse-Approximation-and-Girosi",
            "title": {
                "fragments": [],
                "text": "An Equivalence Between Sparse Approximation and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "If the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124737808,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "97368838bb3c6093a23f76d2d6f56b61aa97ab70",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the relation between support vector machines (SVMs) for regression (SVMR) and SVM for classication (SVMC). We show that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters. In particular our result is that for suciently close to one, the optimal hyperplane and threshold for the SVMC problem with regularization parameter Cc are equal to 1 1 times the optimal hyperplane and threshold for SVMR with regularization parameter Cr =( 1 )Cc. A direct consequence of this result is that SVMC can be seen as a special case of SVMR."
            },
            "slug": "From-Regression-to-Classication-in-Support-Vector-Pontil-Rifkin",
            "title": {
                "fragments": [],
                "text": "From Regression to Classication in Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters, and SVMC can be seen as a special case of SVMR."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3733,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8ad28637ff067070d7abbeb5b79dbb1b328ddd93",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the relation between support vector machines (SVMs) for regression (SVMR) and SVM for classification (SVMC). We show that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters. In particular our result is that for $\\epsilon$ sufficiently close to one, the optimal hyperplane and threshold for the SVMC problem with regularization parameter C_c are equal to (1-\\epsilon)^{- 1} times the optimal hyperplane and threshold for SVMR with regularization parameter C_r = (1-\\epsilon)C_c. A direct consequence of this result is that SVMC can be seen as a special case of SVMR."
            },
            "slug": "From-regression-to-classification-in-support-vector-Pontil-Rifkin",
            "title": {
                "fragments": [],
                "text": "From regression to classification in support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters, and SVMC can be seen as a special case of SVMR."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Notice that in problem (5.1) each example has to satisfy two inequalities (which come out of using the ILF), while in problem (5.4) each example has to satisfy one inequality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "of many so-called neural networks and Gaussian processes and the fact that they work quite well (see [ 55 ] and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116281095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d3b01a9ce510c80c72a31595045bb40844e404a",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and classification problems. From a Bayesian perspective, a choice of a neural network model can be viewed as defining a prior probability distribution over non-linear functions, and the neural network's learning process can be interpreted in terms of the posterior probability distribution over the unknown function. (Some learning algorithms search for the function with maximum posterior probability and other Monte Carlo methods draw samples from this posterior probability). In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over non-linear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes. The hyperparameters of the neural network model determine the characteristic length scales of the Gaussian process. Neal's observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes. Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process. In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997), Barber and Williams (1997) and Gibbs and MacKay (1997), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded."
            },
            "slug": "Introduction-to-Gaussian-processes-Mackay",
            "title": {
                "fragments": [],
                "text": "Introduction to Gaussian processes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter will assess whether the feedforward network has been superceded, for supervised regression and classification tasks, and will review work on this idea by Williams and Rasmussen (1996), Neal (1997), Barber and Williams (1997) and Gibbs and MacKay (1997)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675811"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Then we will discuss a result due to Girosi [38] that shows an equivalence between SVMs and a particular SA technique."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 0
                            }
                        ],
                        "text": "Recently, Pontil, Mukherjee and Girosi [75] have derived a noise model corresponding to Vapnik\u2019s -insensitive loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "where \u03c7[\u2212\u01eb,\u01eb](t) is 1 for t \u2208 [\u2212\u01eb, \u01eb], 0 otherwise, For the derivation see [75]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "As noticed by Girosi et al. [39], functionals of\nthe type (7.3) are common in statistical physics [67], where the stabilizer (here \u2016f\u20162K) plays the role of an energy functional."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Recently, Pontil, Mukherjee and Girosi [75] have derived a noise model corresponding to Vapnik\u2019s \u01eb-insensitive loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 102
                            }
                        ],
                        "text": "Consider the classical regularization case\nmin f\u2208H H[f ] = 1 l l\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162K (7.1)\nFollowing Girosi et al. [39] let us define:\n1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6244043,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1f078c4e39c21e94eec26a20a6efc0ee67aa436d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines Regression (SVMR) is a learning technique where the goodness of fit is measured not by the usual quadratic loss function (the mean square error), but by a different loss function called the \u0190-Insensitive Loss Function (ILF), which is similar to loss functions used in the field of robust statistics. The quadratic loss function is well justified under the assumption of Gaussian additive noise. However, the noise model underlying the choice of the ILF is not clear. In this paper the use of the ILF is justified under the assumption that the noise is additive and Gaussian, where the variance and mean of the Gaussian are random variables. The probability distributions for the variance and mean will be stated explicitly. While this work is presented in the framework of SVMR, it can be extended to justify nonquadratic loss functions in any Maximum Likelihood or Maximum AP osteriori approach. It applies not only to the ILF, but to a much broader class of loss functions."
            },
            "slug": "On-the-Noise-Model-of-Support-Vector-Machines-Pontil-Mukherjee",
            "title": {
                "fragments": [],
                "text": "On the Noise Model of Support Vector Machines Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work can be extended to justify nonquadratic loss functions in any Maximum Likelihood or Maximum AP osteriori approach, and applies not only to the ILF, but to a much broader class of loss functions."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44864234,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "aa6011f1b05e85fe91fb72310b1fe214dd092aef",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a computation of the V\u03b3 dimension for regression in bounded subspaces of Reproducing Kernel Hilbert Spaces (RKHS) for the Support Vector Machine (SVM) regression \u0190-insensitive loss function L\u0190, and general Lp loss functions. Finiteness of the V\u03b3 dimension is shown, which also proves uniform convergence in probability for regression machines in RKHS subspaces that use the L\u0190 or general Lp loss functions. This paper presents a novel proof of this result. It also presents a computation of an upper bound of the V\u03b3 dimension under some conditions, that leads to an approach for the estimation of the empirical V\u03b3 dimension given a set of training data."
            },
            "slug": "On-the-Vgamma-Dimension-for-Regression-in-Kernel-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "On the Vgamma Dimension for Regression in Reproducing Kernel Hilbert Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Finiteness of the V\u03b3 dimension is shown, which also proves uniform convergence in probability for regression machines in RKHS subspaces that use the L\u0190 or general Lp loss functions."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "22 Further distribution dependent results have been derived recently - see [47,16,34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 793899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbc78c2669eb595c988ab69a6dd4cbabc2421043",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space. It is shown that using these functionals, one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter."
            },
            "slug": "Model-Selection-for-Support-Vector-Machines-Chapelle-Vapnik",
            "title": {
                "fragments": [],
                "text": "Model Selection for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space and it is shown that using these functionals one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402412793"
                        ],
                        "name": "Luis P\u00e9rez-Breva",
                        "slug": "Luis-P\u00e9rez-Breva",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "P\u00e9rez-Breva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis P\u00e9rez-Breva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "22 Further distribution dependent results have been derived recently - see [47,16,34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6229829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4c697c5c67d7f2a38c4310b4713df43ae4fc418",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning using combinations of machines. In particular we present new theoretical bounds on the generalization performance of voting ensembles of kernel machines. Special cases considered are bagging and support vector machines. We present experimental results supporting the theoretical bounds, and describe characteristics of kernel machines ensembles suggested from the experimental findings. We also show how such ensembles can be used for fast training with very large datasets."
            },
            "slug": "Bounds-on-the-Generalization-Performance-of-Kernel-Evgeniou-P\u00e9rez-Breva",
            "title": {
                "fragments": [],
                "text": "Bounds on the Generalization Performance of Kernel Machine Ensembles"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "New theoretical bounds on the generalization performance of voting ensembles ofkernel machines of kernel machines are presented and it is shown how such ensembleles can be used for fast training with very large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2253520,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "69dfe8ec813cf609a5cf91a88c1995a2839cd137",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the V\u03b3 dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived."
            },
            "slug": "A-Note-on-the-Generalization-Performance-of-Kernel-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "A Note on the Generalization Performance of Kernel Classifiers with Margin"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin of Support Vector Machine classifiers stem out of this class of machines."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "Notice that in the case of infinite dimensional RKHS asymptotic results ensuring consistency are available (see [27], theorem 29.8) but depend on several conditions that are not automatically satisfied in the case we are considering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16804370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a27406efbf76f1d3538643135bcb13248567a41",
            "isKey": true,
            "numCitedBy": 187,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward networks together with their training algorithms are a class of regression techniques that can be used to learn to perform some task from a set of examples. The question of generalization of network performance from a finite training set to unseen data is clearly of crucial importance. In this article we first show that the generalization error can be decomposed into two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error, due to insufficient information about the target function because of the finite number of samples. We then consider the problem of learning functions belonging to certain Sobolev spaces with gaussian radial basis functions. Using the above-mentioned decomposition we bound the generalization error in terms of the number of basis functions and number of examples. While the bound that we derive is specific for radial basis functions, a number of observations deriving from it apply to any approximation technique. Our result also sheds light on ways to choose an appropriate network architecture for a particular problem and the kinds of problems that can be effectively solved with finite resources, i.e., with a finite number of parameters and finite amounts of data."
            },
            "slug": "On-the-Relationship-between-Generalization-Error,-Niyogi-Girosi",
            "title": {
                "fragments": [],
                "text": "On the Relationship between Generalization Error, Hypothesis Complexity, and Sample Complexity for Radial Basis Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article shows that the generalization error can be decomposed into two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error,due to insufficient information about the target function because of the finite number of samples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122396506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be866f9e928956545fb05ce9047a53b3c096f17d",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of constructing universal networks capable of approximating all functions having bounded derivatives is discussed. It is demonstrated that, using standard ideas from the theory of spline approximation, it is possible to construct such networks to provide localized approximation. The networks can be used to implement multivariate analogues of the Chui-Wang wavelets (1990) and also for the simultaneous approximation of a function and its derivative. The number of neurons required to yield the desired approximation at any point does not depend upon the degree of accuracy desired.<<ETX>>"
            },
            "slug": "Neural-networks-for-localized-approximation-of-real-Mhaskar",
            "title": {
                "fragments": [],
                "text": "Neural networks for localized approximation of real functions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is demonstrated that, using standard ideas from the theory of spline approximation, it is possible to construct such networks to provide localized approximation and can be used for the simultaneous approximation of a function and its derivative."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing III - Proceedings of the 1993 IEEE-SP Workshop"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716824"
                        ],
                        "name": "A. Verri",
                        "slug": "A.-Verri",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Verri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Verri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15223500,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1aa5d36b8b178b7caa5a6d85ee44d66f512b90e7",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "When training Support Vector Machines (SVMs) over nonseparable data sets, one sets the threshold b using any dual cost coefficient that is strictly between the bounds of 0 and C. We show that there exist SVM training problems with dual optimal solutions with all coefficients at bounds, but that all such problems are degenerate in the sense that the \"optimal separating hyperplane\" is given by w = 0, and the resulting (degenerate) SVM will classify all future points identically (to the class that supplies more training data). We also derive necessary and sufficient conditions on the input data for this to occur. Finally, we show that an SVM training problem can always be made degenerate by the addition of a single data point belonging to a certain unbounded polyhedron, which we characterize in terms of its extreme points and rays."
            },
            "slug": "A-Note-on-Support-Vector-Machine-Degeneracy-Rifkin-Pontil",
            "title": {
                "fragments": [],
                "text": "A Note on Support Vector Machine Degeneracy"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an SVM training problem can always be made degenerate by the addition of a single data point belonging to a certain unbounded polyhedron, which is characterized in terms of its extreme points and rays."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444268"
                        ],
                        "name": "R. Lordo",
                        "slug": "R.-Lordo",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lordo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lordo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 222
                            }
                        ],
                        "text": "where \u2016f\u2016(2)K is a norm in a Reproducing Kernel Hilbert Space H defined by the positive definite function K, l is the number of data points or examples (the 1 There is a large literature on the subject: useful reviews are [44,19,102,39], [96] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20604466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b993cfb0321fc546ca6265dcd7859c0c72e2ee25",
            "isKey": false,
            "numCitedBy": 995,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This book consists of a collection of new wavelet techniques that can be used to explore several types of statistical problems, including but not limited to nonparametric regression, density estimation, and change-point problems. It provides an easy but solid introduction to available wavelet tools from an applied point of view. The main focus of this book seems to be development of the applied aspects of statistical functional estimation using a variety of wavelet methods. This book is intended as a reference for advanced undergraduateto graduate-level courses. It relies exclusively on several worked-out examples and provides step-by-step methods for most of the illustrated examples. This is de\u008e nitely a plus point for readers who want to get their hands dirty with wavelet tools. In fact, the reader may want to download the S-PLUS codes from the Web site cited by the author. The introductory chapters provide an overview of essential theory of wavelets, and these results are used throughout the text. Among the basic topics covered in this book are dyadic wavelets, time-frequency localization, wavelet transforms, frames, spline wavelets, orthonormal wavelet bases, and wavelet packets. In addition, the author presents generalizations and extensions to twodimensional wavelets and translation-invariant wavelet smoothing. The book requires a background in undergraduate calculus, linear algebra, and basic statistical theory. The \u201cmeat\u201d part of the book lies in its Chapter 4, in which the author presents several \u201cwavelet features and examples.\u201d The essential theory on wavelet decomposition and reconstruction is presented in a manner that is easy to follow and does not require substantial knowledge of advanced theory of functional analysis. The author then presents fundamental concepts of \u008e lter representation and time-frequency localization. Finally, all of the aforementioned topics are then illustrated via several wavelet examples. Chapters 6 and 7 provide essential concepts for any researcher who is interested in statistical inference for wavelet-based models but is not necessarily an expert in either. The book has achieved its goal of presenting basic wavelet concepts in an understandable way to an audience familiar with the basic theory of statistics. The central theme of the book seems to focus on analyzing several statistical models using ready-made wavelet tools. The material covered in each chapter sometimes seems limited; emphasis on geometrical appeal to wavelets is not addressed. In summary, Essential Wavelets for Statistical Applications and Data Analysis does a good job of presenting wavelet tools to explain various aspects of statistical modeling. A very nice aspect of this book is that it provides a Web site reference that contains most additional resources, such as S-PLUS codes that were used to generate graphics used in the book. However, the book lacks the elegant geometrical approach of wavelet methods, but that is partially the nature of the material. Such fundamental geometrical concepts might turn out to be dif\u008e cult to follow for the beginners. Unfortunately, most books on wavelets are primarily accessible to research statisticians. This book presents basic and advanced concepts of wavelets in a way that is accessible to anyone with only a fundamental knowledge of statistical and mathematical theory. The reader may also want to read a book by Vidakovic (1999) that also presents ideas similar to those developed in this one. I liked the book very much and would not hesitate to recommend its usage in a classroom as reference book for a course on statistical inference based on wavelet models."
            },
            "slug": "Learning-from-Data:-Concepts,-Theory,-and-Methods-Lordo",
            "title": {
                "fragments": [],
                "text": "Learning from Data: Concepts, Theory, and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book consists of a collection of new wavelet techniques that can be used to explore several types of statistical problems, including but not limited to nonparametric regression, density estimation, and change-point problems."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Regularization, Radial Basis Functions, Support Vector Machines, Reproducing Kernel Hilbert Space, Structural Risk Minimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This connection between ICA and sparsity has also been studied in [64]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15214722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69d713c63575947e4ef78b4dc04ed3a686cc06a4",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work (Olshausen \\& Field 1996), an algorithm was described for learning linear sparse codes which, when trained on natural images, produces a set of basis functions that are spatially localized, oriented, and bandpass (i.e., wavelet-like). This note shows how the algorithm may be interpreted within a maximum-likelihood framework. Several useful insights emerge from this connection: it makes explicit the relation to statistical independence (i.e., factorial coding), it shows a formal relationship to the algorithm of Bell and Sejnowski (1995), and it suggests how to adapt parameters that were previously fixed."
            },
            "slug": "Learning-linear,-sparse,-factorial-codes-Olshausen",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This note shows how the algorithm for learning linear sparse codes may be interpreted within a maximum-likelihood framework and makes explicit the relation to statistical independence and shows a formal relationship to Bell and Sejnowski's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "Various methods for ICA have been developed in recent years [3,9,63,53,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2426066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9a1048d0f5356f2e08b182dc1a125423d827072",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Overcomplete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component analysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcomplete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency."
            },
            "slug": "Learning-Nonlinear-Overcomplete-Representations-for-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Nonlinear Overcomplete Representations for Efficient Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The utility of overcomplete representations on natural speech is demonstrated and it is shown that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26373808"
                        ],
                        "name": "R. DeVore",
                        "slug": "R.-DeVore",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "DeVore",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. DeVore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26373808"
                        ],
                        "name": "R. DeVore",
                        "slug": "R.-DeVore",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "DeVore",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. DeVore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "We do not discuss these results here and refer the reader to [54,26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15288042,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9f6058c289adb7a91b1ddcc904ff23094edaa92f",
            "isKey": false,
            "numCitedBy": 922,
            "numCiting": 182,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey of nonlinear approximation, especially that part of the subject which is important in numerical computation. Nonlinear approximation means that the approximants do not come from linear spaces but rather from nonlinear manifolds. The central question to be studied is what, if any, are the advantages of nonlinear approximation over the simpler, more established, linear methods. This question is answered by studying the rate of approximation which is the decrease in error versus the number of parameters in the approximant. The number of parameters usually correlates well with computational effort. It is shown that in many settings the rate of nonlinear approximation can be characterized by certain smoothness conditions which are significantly weaker than required in the linear theory. Emphasis in the survey will be placed on approximation by piecewise polynomials and wavelets as well as their numerical implementation. Results on highly nonlinear methods such as optimal basis selection and greedy algorithms (adaptive pursuit) are also given. Applications to image processing, statistical estimation, regularity for PDEs, and adaptive algorithms are discussed."
            },
            "slug": "Nonlinear-approximation-DeVore-DeVore",
            "title": {
                "fragments": [],
                "text": "Nonlinear approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This is a survey of nonlinear approximation, especially that part of the subject which is important in numerical computation, and emphasis will be placed on approximation by piecewise polynomials and wavelets as well as their numerical implementation."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "It is in fact well know that there is a close relation between Gaussian processes and RKHS [58,40,72]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12841311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "188bcf4e56b6437098c612c063e0f97d4e8d6e9c",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new general representation for a function as a linear combination of local correlation kernels at optimal sparse locations (and scales) and characterize its relation to principal component analysis, regularization, sparsity principles, and support vector machines."
            },
            "slug": "A-Sparse-Representation-for-Function-Approximation-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "A Sparse Representation for Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new general representation for a function is derived as a linear combination of local correlation kernels at optimal sparse locations (and scales) and its relation to principal component analysis, regularization, sparsity principles, and support vector machines is characterized."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Following the notation in [4], the problem can be formulated as finding at any time t both the n (n predefined) sources x(t) = (x1(t), ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": false,
            "numCitedBy": 2730,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16707671,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4dde852a9852580b982f2dc5a833e733b79ac36c",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of results have bounded generalization of a classifier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization. Freund and Schapire have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. Algorithms arising from the approach are related to those of Cortes and Vapnik. We generalise the basic result to function classes with bounded fat- shattering dimension and the 1-norm of the slack variables which gives rise to Vapnik's box constraint algorithm. We also extend the results to the regression case and obtain bounds on the probability that a randomly chosen test point will have error greater than a given value. The bounds apply to the $\\epsilon$-insensitive loss function proposed by Vapnik for Support Vector Machine regression. A special case of this bound gives a bound on the probabilities in terms of the least squares error on the training set showing a quadratic decline in probability with margin."
            },
            "slug": "Robust-Bounds-on-Generalization-from-the-Margin-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Robust Bounds on Generalization from the Margin Distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "Various methods for ICA have been developed in recent years [3,9,63,53,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37365552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11656f389fabe0c8ab987ded90372d06c6591008",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-nonlinear-PCA-learning-rule-in-independent-Oja",
            "title": {
                "fragments": [],
                "text": "The nonlinear PCA learning rule in independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155551370"
                        ],
                        "name": "Shaobing Chen",
                        "slug": "Shaobing-Chen",
                        "structuredName": {
                            "firstName": "Shaobing",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaobing Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122514140"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Donoho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [18,17] the authors use the L1 norm as an approximation of the L0 norm, obtaining an approximation scheme that they call Basis Pursuit De-Noising (BPDN) which consists of minimizing:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 96447294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30f3567eeb13079a1a02ac1342f610cbd95df3bc",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.<<ETX>>"
            },
            "slug": "Basis-pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Basis pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements, which has several advantages over matching pursuit and best ortho basis, including super-resolution and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "To get a necessary condition we need a slight extension of the VC-dimension that has been developed (among others) in [50,2], known as the V\u03b3\u2013dimension(11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085069"
                        ],
                        "name": "C. Rabut",
                        "slug": "C.-Rabut",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Rabut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rabut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121343367,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3b2627c636e72b807fa626df22ef601511a9c540",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AN-INTRODUCTION-TO-SCHOENBERG'S-APPROXIMATION-Rabut",
            "title": {
                "fragments": [],
                "text": "AN INTRODUCTION TO SCHOENBERG'S APPROXIMATION"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145627013"
                        ],
                        "name": "J. Stewart",
                        "slug": "J.-Stewart",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stewart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [91] it is reported that the positivity of the operator associated to K is equivalent to the statement that the kernel K is positive definite, that is the matrix Kij = K(xi,xj) is positive definite 15 We remind the reader that positive definite operators in L2 are self-adjoint operators such that < Kf, f > \u2265 0 for all f \u2208 L2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120093770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400ca5d045588b887ec8949662dff5436bbc2461",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "2 fi COS Xi + i X 6 S i n i = \u00b0i = l ' ' i = l ' Likewise it is easily verified directly that e is p.d. for real \\ , but it is not so straightforward to see that such functions as e~H e~*, and (1 4x)\" ? e p.d. These and other examples are discussed in \u00a7 3. Positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics since the beginning of this century. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, embedding problems, information theory, and other areas. Their history constitutes a good illustration of the words of Hobson [51, p. 290] : \"Not only are special results, obtained independently of one another, frequently seen to be really included in"
            },
            "slug": "Positive-definite-functions-and-generalizations,-an-Stewart",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations, an historical survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "Various methods for ICA have been developed in recent years [3,9,63,53,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "(Alon et al. , 1993 ) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Notice that an approximation of the regression function using a mean square error criterion places more emphasis on the most probable data points and not on the most \u201cimportant\u201d ones which are the ones near the separating boundary."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "(Alon et al. , 1993 ) Let A \u2264 V (y, f(x))) \u2264 B, f \u2208 F , F be a set of bounded functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This shows that apparently \u201cglobal\u201d approximation schemes can be regarded as local, memory-based techniques (see equation 4.18) [59]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8347198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e07c3df9d8d53c3be8cb9e982da98a4471322d90",
            "isKey": true,
            "numCitedBy": 412,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>"
            },
            "slug": "Scale-sensitive-dimensions,-uniform-convergence,-Alon-Ben-David",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive dimensions, uniform convergence, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire, and shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [51,102,100,73,58,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4346156,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4c6bd3b8d35c4fc14360160efc9c66727abac9df",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data. Computational vision aims to understand how such descriptions can be obtained from inherently ambiguous and noisy data. A recent development in this field sees early vision as a set of ill-posed problems, which can be solved by the use of regularization methods. These lead to algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain."
            },
            "slug": "Computational-vision-and-regularization-theory-Poggio-Torre",
            "title": {
                "fragments": [],
                "text": "Computational vision and regularization theory"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data and algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10656220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4cb568fa8bbab1afcc87b62538f6052c770a660",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training-set size. We show that in some contexts this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well motivated. The process of creating virtual examples in real-world pattern recognition tasks is highly nontrivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea."
            },
            "slug": "Incorporating-prior-information-in-machine-learning-Niyogi-Girosi",
            "title": {
                "fragments": [],
                "text": "Incorporating prior information in machine learning by creating virtual examples"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that in some contexts this idea of using prior knowledge by creating virtual examples and thereby expanding the effective training-set size is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well motivated."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737063"
                        ],
                        "name": "I. Daubechies",
                        "slug": "I.-Daubechies",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Daubechies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Daubechies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58524360,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "7e63bf9af3f70abd5771c06d459a0d3fbfbb2909",
            "isKey": false,
            "numCitedBy": 15558,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Preliminaries and notation The what, why, and how of wavelets The continuous wavelet transform Discrete wavelet transforms: Frames Time-frequency density and orthonormal bases Orthonormal bases of wavelets and multiresolutional analysis Orthonormal bases of compactly supported wavelets More about the regularity of compactly supported wavelets Symmetry for compactly supported wavelet bases Characterization of functional spaces by means of wavelets Generalizations and tricks for orthonormal wavelet bases References Indexes."
            },
            "slug": "Ten-Lectures-on-Wavelets-Daubechies",
            "title": {
                "fragments": [],
                "text": "Ten Lectures on Wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a meta-analyses of the wavelet transforms of Coxeter\u2019s inequality and its applications to multiresolutional analysis and orthonormal bases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11074771,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2c598e80a7c70cf974fe1e57b5741aeb2fecbd00",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Given n noisy observations g i of the same quantity f, i t i s c o m m o n use to give an estimate of f by minimizing the function P n i=1 (g i ;f) 2. From a statistical point of view this corresponds to computing the Maximum Likelihood estimate, under the assumption of Gaussian noise. Howeve r , i t i s w ell known that this choice leads to results that are very sensitive to the presence of outliers in the data. For this reason it has been proposed to minimize functions of the form P n i=1 V (g i ; f), where V is a function that increases less rapidly than the square. Several choices for V have been proposed and successfully used to obtain \\robust\" estimates. In this paper we show that, for a class of functions V , using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V ."
            },
            "slug": "Models-of-Noise-and-Robust-Estimates-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, for a class of functions V, using robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [18,17] the authors use the L1 norm as an approximation of the L0 norm, obtaining an approximation scheme that they call Basis Pursuit De-Noising (BPDN) which consists of minimizing:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9740,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6059616"
                        ],
                        "name": "G. F. Harpur",
                        "slug": "G.-F.-Harpur",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Harpur",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Harpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13911187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0590940043b88992c4417d239c01c282cd31309",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an unsupervised neural network which exhibits competition between units via inhibitory feedback. The operation is such as to minimize reconstruction error, both for individual patterns, and over the entire training set. A key difference from networks which perform principal components analysis, or one of its variants, is the ability to converge to non-orthogonal weight values. We discuss the network's operation in relation to the twin goals of maximizing information transfer and minimizing code entropy, and show how the assignment of prior probabilities to network outputs can help to reduce entropy. We present results from two binary coding problems, and from experiments with image coding."
            },
            "slug": "Development-of-low-entropy-coding-in-a-recurrent-Harpur-Prager",
            "title": {
                "fragments": [],
                "text": "Development of low entropy coding in a recurrent network."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised neural network which exhibits competition between units via inhibitory feedback is presented, and it is shown how the assignment of prior probabilities to network outputs can help to reduce entropy."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "Various methods for ICA have been developed in recent years [3,9,63,53,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "In fact iterative methods where at every iteration A is fixed and the sources are found, and then for fixed sources, A is updated using a learning rule have been developed in [65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Notice that the variance has a unimodal distribution that does not depend on , and the mean has a distribution which is uniform in the interval [\u2212 , ], (except for two delta functions at \u00b1 , which ensures that the mean has not zero probability to be equal to \u00b1 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Various methods for ICA have been developed in recent years [3,9,53,63,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Niyogy et al. [62] showed how several invariances can be embedded in the stabilizer or, equivalently, in virtual examples (see for a related work on tangent distance [89] and [ 84 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15109515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51c1519a57a65351a713a3d74f8d477105df0ec3",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions."
            },
            "slug": "Prior-Knowledge-in-Support-Vector-Kernels-Sch\u00f6lkopf-Simard",
            "title": {
                "fragments": [],
                "text": "Prior Knowledge in Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions by exploring methods for incorporating prior knowledge in Support Vector learning machines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709398"
                        ],
                        "name": "M. Wickerhauser",
                        "slug": "M.-Wickerhauser",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wickerhauser",
                            "middleNames": [
                                "Victor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wickerhauser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 546882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5478a91c183c3a460bd4098acb8927bfc671367c",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals. It permits efficient compression of a variety of signals, such as sound and images. The predefined libraries of modulated waveforms include orthogonal wavelet-packets and localized trigonometric functions, and have reasonably well-controlled time-frequency localization properties. The idea is to build out of the library functions an orthonormal basis relative to which the given signal or collection of signals has the lowest information cost. The method relies heavily on the remarkable orthogonality properties of the new libraries: all expansions in a given library conserve energy and are thus comparable. Several cost functionals are useful; one of the most attractive is Shannon entropy, which has a geometric interpretation in this context. >"
            },
            "slug": "Entropy-based-algorithms-for-best-basis-selection-Coifman-Wickerhauser",
            "title": {
                "fragments": [],
                "text": "Entropy-based algorithms for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals, and relies heavily on the remarkable orthogonality properties of the new libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117653567,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cf6226fa2d10f9837d56c99e50b1464ba5812b3",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractFor a radial-basis function\u03d5\u2236\u211b\u2192\u211b we consider interpolation on an infinite regular lattice, tof\u2236\u211bn\u2192\u211b, whereh is the spacing between lattice points and the cardinal function, satisfiesX(j)=\u03b4oj for allj\u2208\u2112n. We prove existence and uniqueness of such cardinal functionsX, and we establish polynomial precision properties ofIh for a class of radial-basis functions which includes\n$$\\varphi (r) = r^{2q + 1} $$\n,\n$$\\varphi (r) = r^{2q} \\log r,\\varphi (r) = \\sqrt {r^2 + c^2 } $$\n, and\n$$\\varphi (r) = 1/\\sqrt {r^2 + c^2 } $$\n whereq\u2208\u2112+. We also deduce convergence orders ofIhf to sufficiently differentiable functionsf whenh\u21920."
            },
            "slug": "Multivariate-cardinal-interpolation-with-functions-Buhmann",
            "title": {
                "fragments": [],
                "text": "Multivariate cardinal interpolation with radial-basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8852,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "\u2026MA 02139 USA E-mail: theos@ai.mit.edu, pontil@ai.mit.edu, tp@ai.mit.edu\nRegularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular the regression problem of approximating a multivariate function from sparse data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14285485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "31b5a06273e75f159d5d9e42bc5bdfd7fd4b625e",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Mathematical results on ill-posed and ill-conditioned problems are reviewed and the formal aspects of regularization theory in the linear case are introduced. Specific topics in early vision and their regularization are then analyzed rigorously, characterizing existence, uniqueness, and stability of solutions. A fundamental difficulty that arises in almost every vision problem is scale, that is, the resolution at which to operate. Methods that have been proposed to deal with the problem include scale-space techniques that consider the behavior of the result across a continuum of scales. From the point of view of regulation theory, the concept of scale is related quite directly to the regularization parameter lambda . It suggested that methods used to obtained the optimal value of lambda may provide, either directly or after suitable modification, the optimal scale associated with the specific instance of certain problems. >"
            },
            "slug": "Ill-posed-problems-in-early-vision-Bertero-Poggio",
            "title": {
                "fragments": [],
                "text": "Ill-posed problems in early vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770452"
                        ],
                        "name": "L. Goddard",
                        "slug": "L.-Goddard",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Goddard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goddard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "We do not discuss these results here and refer the reader to [54,26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4192258,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6bef91d0c32549f07c969322cb6d28f1fd96ec9d",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Theory of Approximation of Functions of a Real VariableBy A. F. Timan. Translated by J. Berry. English translation edited and editorial preface by J. Cossar. (International Series of Monographs on Pure and Applied Mathematics, Vol. 34.) Pp. xii + 631. (London and New York: Pergamon Press, 1963.) 100s. net."
            },
            "slug": "Approximation-of-Functions-Goddard",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2630782"
                        ],
                        "name": "J. Marroqu\u00edn",
                        "slug": "J.-Marroqu\u00edn",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Marroqu\u00edn",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marroqu\u00edn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689724"
                        ],
                        "name": "S. Mitter",
                        "slug": "S.-Mitter",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Mitter",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mitter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "It is in fact well know that there is a close relation between Gaussian processes and RKHS [58,40,72]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [51,102,100,73,58,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "4) One of the several possible estimates [58] of the function f from the probability distribution (7."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 115
                            }
                        ],
                        "text": "Gaussian processes, regularization and SVM The very close relation between Gaussian processes and RN is well known [58,102]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "In many cases, the average of f = \u222b fdP [f |Dl] may make more sense24 (see [58])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14692859,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "97c12f5ddfdf036eb6cf935a679d1e4ba2fe535f",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate several problems in early vision as inverse problems. Among the solution methods we review standard regularization theory, discuss its limitations, and present new stochastic (in particular, Bayesian) techniques based on Markov Random Field models for their solution. We derive efficient algorithms and describe parallel implementations on digital parallel SIMD architectures, as well as a new class of parallel hybrid computers that mix digital with analog components."
            },
            "slug": "Probabilistic-Solution-of-Ill-Posed-Problems-in-Marroqu\u00edn-Mitter",
            "title": {
                "fragments": [],
                "text": "Probabilistic Solution of Ill-Posed Problems in Computational Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work derives efficient algorithms and describes parallel implementations on digital parallel SIMD architectures, as well as a new class of parallel hybrid computers that mix digital with analog components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3) [37,40,90]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122927965,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ae6ad5780d3562b0fb4af3fa4e17998fef53a2bc",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Least squares estimators are very common in statistics, but they lead to results that are very sensitive to outliers, and it has been proposed to minimize other measures of error, that lead to ``robust'''' estimates. In this paper we show that using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "slug": "Models-of-Noise-and-Robust-Estimation-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that using robust least squares estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378798"
                        ],
                        "name": "L. Schumaker",
                        "slug": "L.-Schumaker",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Schumaker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schumaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The functions Bn are piecewise polynomials of degree n, whose exact definition can be found in [85]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123077071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fa7e5d7ef68525b53a2de82c7b7dfdfeb781511",
            "isKey": false,
            "numCitedBy": 2927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This classic work continues to offer a comprehensive treatment of the theory of univariate and tensor-product splines. It will be of interest to researchers and students working in applied analysis, numerical analysis, computer science, and engineering. The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences. This new edition includes a supplement outlining some of the major advances in the theory since 1981, and some 250 new references. It can be used as the main or supplementary text for courses in splines, approximation theory or numerical analysis."
            },
            "slug": "Spline-Functions:-Basic-Theory-Schumaker",
            "title": {
                "fragments": [],
                "text": "Spline Functions: Basic Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Notice that an approximation of the regression function using a mean square error criterion places more emphasis on the most probable data points and not on the most \u201cimportant\u201d ones which are the ones near the separating boundary."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2551295,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fc48a9403b5d01a2d1724d4e04218a4a9b78cb3a",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning real-valued functions from random examples when the function values are corrupted with noise. With mild conditions on independent observation noise, we provide characterizations of the learnability of a real-valued function class in terms of a generalization of the Vapnik-Chervonenkis dimension, the fat shattering function, introduced by Kearns and Schapire. We show that, given some restrictions on the noise, a function class is learnable in our model if and only if its fat-shattering function is finite. With different (also quite mild) restrictions, satisfied for example by gaussian noise, we show that a function class is learnable from polynomially many examples if and only if its fat-shattering function grows polynomially. We prove analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "slug": "Fat-shattering-and-the-learnability-of-real-valued-Bartlett-Long",
            "title": {
                "fragments": [],
                "text": "Fat-shattering and the learnability of real-valued functions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, given some restrictions on the noise, a function class is learnable in this model if and only if its fat-shattering function is finite, and analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Recently, Pontil, Mukherjee and Girosi [75] have derived a noise model corresponding to Vapnik\u2019s -insensitive loss function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We will characterize the form of the solution and then show that SVM for binary pattern classification can be derived as a special case of the regression formulation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119733144,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69e535bb138886d10cdba8a53d0b297a5444a86a",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere une approche spline pour la regression non parametrique et l'estimation de courbe. On montre que dans un certain sens, le lissage spline correspond approximativement au lissage par une methode du noyau avec une largeur de bande dependant de la densite locale des points du plan de regression"
            },
            "slug": "Spline-Smoothing:-The-Equivalent-Variable-Kernel-Silverman",
            "title": {
                "fragments": [],
                "text": "Spline Smoothing: The Equivalent Variable Kernel Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121204061,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac168e8a779572c2f789d6f6094046d5e772bab8",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract It has been known since 1987 that quasi-interpolation with radial functions on the integer grid can be exact for certain order polynomials. If, however, we require that the basis functions of the quasi-interpolants be finite linear combinations of translates of the radial functions, then this can be done only in spaces whose dimension has a prescribed parity. In this paper we show how infinite linear combinations of translates of a given radial function can be found that provide polynomial exactness in spaces whose dimensions do not have this prescribed parity. These infinite linear combinations are of a simple form. They are, in particular, easier to find than the cardinal functions of radial basis function interpolation, which provide polynomial exactness in all dimensions. The techniques that are used in this work also give rise to some remarks about interpolation with radial functions both on the integers and on the nonnegative integers."
            },
            "slug": "On-quasi-interpolation-with-radial-basis-functions-Buhmann",
            "title": {
                "fragments": [],
                "text": "On quasi-interpolation with radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "[62] showed how several invariances can be embedded in the stabilizer or, equivalently, in virtual examples (see for a related work on tangent distance [89] and [84])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51378744"
                        ],
                        "name": "J. Lamperti",
                        "slug": "J.-Lamperti",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lamperti",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lamperti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "If, for any number N , it is possible to find N points x1, . . . ,xN that can be separated in all the 2N possible ways, we will say that the VC-dimension of the set is infinite."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37016743,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f274a4cee19d93218c209aa07f47abea0598ae91",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "It is clear that for given I,un } and t, the better theorem of this kind would be the one in which (2) is proved for the larger class of functions f. In this paper we shall show that certain known \"invariance principles\" can under some hypotheses be improved by considerably enlarging the class of functions for which (2) holds. This will be done by considering spaces S other than the customary ones. For example, in studying convergence to the Wiener process, it is usual to let S be the space (denoted e) of continuous functions with the uniform topology. However, this choice does not fully exploit the pleasant properties of the Wiener path-functions, which are not only continuous but also Holder continuous of any order up to 1/2. Therefore we shall attempt to use spaces Lip5 in place of e as the function-space S. When weak convergence can be established using such spaces, the class of functionals for which (2) is known to hold becomes much larger than before. To carry out the idea sketched above it is necessary to have a criterion which guarantees that the sample functions of a stochastic process are a.s."
            },
            "slug": "ON-CONVERGENCE-OF-STOCHASTIC-PROCESSES-Lamperti",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE OF STOCHASTIC PROCESSES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706122"
                        ],
                        "name": "N. Dyn",
                        "slug": "N.-Dyn",
                        "structuredName": {
                            "firstName": "Nira",
                            "lastName": "Dyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30912435"
                        ],
                        "name": "I. Jackson",
                        "slug": "I.-Jackson",
                        "structuredName": {
                            "firstName": "Irh",
                            "lastName": "Jackson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343509"
                        ],
                        "name": "D. Levin",
                        "slug": "D.-Levin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428225"
                        ],
                        "name": "A. Ron",
                        "slug": "A.-Ron",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15138913,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "de7216b19e67edd11613f504edbda2c15774e661",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximation properties of the dilations of the integer translates of a smooth function, with some derivatives vanishing at infinity, are studied. The results apply to fundamental solutions of homogeneous elliptic operators and to \u201cshifted\u201d fundamental solutions of the iterated Laplacian. Following the approach from spline theory, the question of polynomial reproduction by quasi-interpolation is addressed first. The analysis makes an essential use of the structure of the generalized Fourier transform of the basis function. In contrast with spline theory, polynomial reproduction is not sufficient for the derivation of exact order of convergence by dilated quasi-interpolants. These convergence orders are established by a careful and quite involved examination of the decay rates of the basis function. Furthermore, it is shown that the same approximation orders are obtained with quasi-interpolants defined on a bounded domain."
            },
            "slug": "On-multivariate-approximation-by-integer-translates-Dyn-Jackson",
            "title": {
                "fragments": [],
                "text": "On multivariate approximation by integer translates of a basis function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083605154"
                        ],
                        "name": "D. M. Allen",
                        "slug": "D.-M.-Allen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Allen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Allen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Let N be the dimensionality of a RKHS R with kernel K. Assume our input space X is bounded and let R be the radius of the sphere where our data x belong to, in the feature space induced by kernel K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "It is interesting to notice that the same analysis can be used for the problem of classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121351491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c08cae9bd11e903e301e1f69ef28eb3cba663fb2",
            "isKey": false,
            "numCitedBy": 1252,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that data augmentation provides a rather general formulation for the study of biased prediction techniques using multiple linear regression. Variable selection is a limiting case, and Ridge regression is a special case of data augmentation. We propose a way to obtain predictors given a credible criterion of good prediction."
            },
            "slug": "The-Relationship-Between-Variable-Selection-and-and-Allen",
            "title": {
                "fragments": [],
                "text": "The Relationship Between Variable Selection and Data Agumentation and a Method for Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that data augmentation provides a rather general formulation for the study of biased prediction techniques using multiple linear regression and a way to obtain predictors given a credible criterion of good prediction is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1544847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c00831037152d852814f0474f184d89cd404a08",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Unifying-Information-Theoretic-Framework-for-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "A Unifying Information-Theoretic Framework for Independent Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145884505"
                        ],
                        "name": "V. Cherkassky",
                        "slug": "V.-Cherkassky",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Cherkassky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cherkassky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Figure (7.4) gives a preliminary empirical demonstration that in the case of SVMR the \u201dBayesian\u201d dependence of \u03bb as \u03b1l may not be correct."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "A direct implication of this result is that one can solve any SVMC problem through the SVMR formulation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "It is possible that theorem 5.5 may help to extend them to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "As we mentioned in section 5, the connection between SVMC and SVMR outlined in that section may suggest how to extend such results to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "We now address the following question: what happens if we apply the SVMR formulation given by problem (5.1) to the binary pattern classification case, i.e. the case where yi take values {\u22121, 1}, treating classification as a regression on binary data?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 238
                            }
                        ],
                        "text": "First, as we discussed in section 2, SRM using the V\u03b3 dimension is practically difficult because we do not have tight bounds to use in order to pick the optimal Fn\u2217(l) (combining theorems 6.2 and 2.11, bounds on the expected risk of RN and SVMR machines of the form (6.1) can be derived, but these bounds are not practically useful)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Thus both RNs and SVMR are consistent in HA for any A   \u221e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "We point out that the SVM technique has first been proposed for binary pattern classification problems and then extended to the general regression problem [95]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Similar ideas have been explored by others (see [95,96] for a summary)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Moreover, the V\u03b3 dimension is finite for \u2200 \u03b3 > 0; therefore, according to theorem (2.10), ERM uniformly converges in HA for any A < \u221e, both for RN and for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "On the other hand, if we had the optimal Lagrange multiplier \u03bbn\u2217(l), we could simply solve the unconstrained minimization problem:\n1 l l\u2211 i=1 V (yi, f(xi)) + \u03bbn\u2217(l)||f ||2K (6.3)\nboth for RN and for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "It is an open question what theoretical implications theorem 5.5 may have about SVMC and SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "In particular in section 6 we will discuss some recent theoretical results on SVMC that have not yet been extended to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Summarizing, both the RN and the SVMR methods discussed in sections 4 and 5 can be seen as approximations of the extended SRM method using the V\u03b3 dimension, with nested hypothesis spaces being of the form HA = {f \u2208 R : ||f ||K \u2264 A}, R being a RKHS defined by kernel K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Finally,\nit is an open question whether similar results exist for the case of SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "The method of SVMR [96] corresponds to the following functional\nH[f ] = 1 l l\u2211 i=1 |yi \u2212 f(xi)| + \u03bb\u2016f\u20162K (5.1)\nwhich is a special case of equation (4.4) and where\nV (x) = |x| \u2261 {\n0 if |x|   |x| \u2212 otherwise, (5.2)\nis the \u2212Insensitive Loss Function (ILF) (also noted with L )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "(SVMC) correspond to the minimization of H in equation (1.3) for different choices of V :\n\u2022 Classical (L2) Regularization Networks (RN)\nV (yi, f(xi)) = (yi \u2212 f(xi))2 (1.4) \u2022 Support Vector Machines Regression (SVMR)\nV (yi, f(xi)) = |yi \u2212 f(xi)| (1.5) \u2022 Support Vector Machines Classification (SVMC)\nV (yi, f(xi)) = |1 \u2212 yif(xi)|+ (1.6)\nwhere | \u00b7 | is Vapnik\u2019s epsilon-insensitive norm (see later), |x|+ = x if x is positive and zero otherwise, and yi is a real number in RN and SVMR, whereas it takes values \u22121, 1 in SVMC. Loss function (1.6) is also called the soft margin loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For both RN and SVMR\nthe V\u03b3 dimension of the loss function V in HA is finite for \u2200 \u03b3 > 0, so the ERM method uniformly converges in HA for any A < \u221e, and we can use the extended SRM method outlined in section 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "In the case of RN, V is the L2 loss function, whereas in the case of SVMR it is the -insensitive loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "So the VC-dimension cannot be used directly neither for RN nor for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "In this section, after stating the formulation of SVM for binary pattern classification (SVMC) as developed by Cortes and Vapnik [22], we discuss a connection between SVMC and SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "In this section we discuss the technique of Support Vector Machines (SVM) for Regression (SVMR) [95,96] in terms of the SVM functional."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "We will describe how classical regularization and Support Vector Machines [96] for both regression (SVMR) and classification\n2 The method of quasi-solutions of Ivanov and the equivalent Tikhonov\u2019s regularization technique were developed to solve ill-posed problems of the type Af = F , where A is a (linear) operator, f is the desired solution in a metric space E1, and F are the \u201cdata\u201d in a metric space E2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "We have just seen that minimization of both the RN and the SVMR functionals can be interpreted as corresponding to the MAP estimate of the posterior probability of f given the data, for certain models of the noise and for a specific Gaussian prior on the space of functions f ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "This theorem, combined with the theorems on V\u03b3 dimension summarized in section 2, can be used for a distribution independent analysis of SVMC (of the form (6.1)) like that of SVMR and RN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206755547,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "e64fecbaf4d75e0dd6711f8f335c8a53da9fd360",
            "isKey": true,
            "numCitedBy": 3182,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "If you really want to be smarter, reading can be one of the lots ways to evoke and realize. Many people who like reading will have more knowledge and experiences. Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others. As one of the part of book categories, the nature of statistical learning theory always becomes the most wanted book. Many people are absolutely searching for this book. It means that many love to read this kind of book."
            },
            "slug": "The-Nature-Of-Statistical-Learning-Theory-Cherkassky",
            "title": {
                "fragments": [],
                "text": "The Nature Of Statistical Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "As one of the part of book categories, the nature of statistical learning theory always becomes the most wanted book."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706122"
                        ],
                        "name": "N. Dyn",
                        "slug": "N.-Dyn",
                        "structuredName": {
                            "firstName": "Nira",
                            "lastName": "Dyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343509"
                        ],
                        "name": "D. Levin",
                        "slug": "D.-Levin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47204630"
                        ],
                        "name": "S. Rippa",
                        "slug": "S.-Rippa",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Rippa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rippa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "It is often possible to write the solution f(x) as a linear combination of SVs in a number of different ways (for example in case that the feature space induced by the kernel K has dimensionality lower than the number of SVs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121300866,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "79332b153d9a94868f0f6d84897fd7f8f8e75dfc",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications one encounters the problem of approximating surfaces from data given on a set of scattered points in a two-dimensional domain. The global interpolation methods with Duchon's \u201cthin plate splines\u201d and Hardy's multiquadrics are considered to be of high quality; however, their application is limited, due to computational difficulties, to $ \\sim 150$ data points. In this work we develop some efficient iterative schemes for computing global approximation surfaces interpolating a given smooth data. The suggested iterative procedures can, in principle, handle any number of data points, according to computer capacity. These procedures are extensions of a previous work by Dyn and Levin on iterative methods for computing thin-plate spline interpolants for data given on a square grid. Here the procedures are improved significantly and generalized to the case of data given in a general configuration.The major theme of this work is the development of an iterative scheme for the construction of a smooth surface, presented by global basis functions, which approximates only the smooth components of a set of scattered noisy data. The novelty in the suggested method is in the construction of an iterative procedure for low-pass filtering based on detailed spectral properties of a preconditioned matrix. The general concepts of this approach can also be used in designing iterative computation procedures for many other problems.The interpolation and smoothing procedures are tested, and the theoretical results are verified, by many numerical experiments."
            },
            "slug": "Numerical-Procedures-for-Surface-Fitting-of-Data-by-Dyn-Levin",
            "title": {
                "fragments": [],
                "text": "Numerical Procedures for Surface Fitting of Scattered Data by Radial Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The major theme of this work is the development of an iterative scheme for the construction of a smooth surface, presented by global basis functions, which approximates only the smooth components of a set of scattered noisy data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2634385"
                        ],
                        "name": "W. Madych",
                        "slug": "W.-Madych",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Madych",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Madych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758670"
                        ],
                        "name": "S. Nelson",
                        "slug": "S.-Nelson",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Nelson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Let Rl be the radius of the smallest hypersphere in the feature space induced by kernel K containing all essential SVs, \u2016f\u20162K(l) the norm of the solution of SVMC, and \u03c1(l) = 1\u2016f\u20162K(l)\nthe margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Any of the SVs for which 0   \u03b1j   Cl (and therefore \u03bej = 0) can be used to compute the parameter b."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 296
                            }
                        ],
                        "text": "With respect to the new variable ci problem (5.2) becomes:\nProblem 5.3.\nmin c E[c] = 1 2 l\u2211 i,j=1 cicjK(xi,xj) \u2212 l\u2211 i=1 ciyi + l\u2211 i=1 |ci|\nsubject to the constraints\nl\u2211 i=1 ci = 0,\n\u2212C l \u2264 ci \u2264 C l , i = 1, . . . , l.\n18 In degenerate cases however, it can happen that points whose error is equal to are not SVs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "It is often possible to write the solution f(x) as a linear combination of SVs in a number of different ways (for example in case that the feature space induced by the kernel K has dimensionality lower than the number of SVs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "The input data points xi for which \u03b1i is different from zero are called, as in the case of regression, support vectors (SVs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 342,
                                "start": 339
                            }
                        ],
                        "text": "The solutions of problems (5.1) and (5.2) are related by the Kuhn-Tucker conditions:\n\u03b1i(f(xi) \u2212 yi \u2212 \u2212 \u03bei) = 0 i = 1, . . . , l (5.6) \u03b1\u2217i (yi \u2212 f(xi) \u2212 \u2212 \u03be\u2217i ) = 0 i = 1, . . . , l (5.7)\n( C\nl \u2212 \u03b1i)\u03bei = 0 i = 1, . . . , l (5.8)\n( C\nl \u2212 \u03b1\u2217i )\u03be\u2217i = 0 i = 1, . . . , l. (5.9)\nThe input data points xi for which \u03b1i or \u03b1\u2217i are different from zero are called support vectors (SVs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SVs that appear in all these linear combinations are called essential support vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "A consequence of this fact is that if the SVM were run again on the new data set consisting of only the SVs the same solution would be found."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SVs are those data points xi at which the error is either greater or equal to 18."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122338574,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d47d29417ba4e516ca5a4889180536a4497825d2",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polyharmonic-cardinal-splines:-a-minimization-Madych-Nelson",
            "title": {
                "fragments": [],
                "text": "Polyharmonic cardinal splines: a minimization property"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [51,102,100,73,58,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53645805,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "db5beea6b9dfcbda12cd9b7b44c94c8170938133",
            "isKey": false,
            "numCitedBy": 735,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. Let there be given a sequence of ordinates \n \n$$ \\left\\{ {{y_n}} \\right\\}\\quad \\left( {n = 0, \\pm 1 \\pm 2, \\ldots } \\right), $$ \n \ncorresponding to all integral values of the variable x = n. If these ordinates are the values of a known analytic function F(x), then the problem of interpolation between these ordinates has an obvious and precise meaning: we are required to compute intermediate values F(x) to the same accuracy to which the ordinates are known. Undoubtedly, the most convenient tool for the solution of this problem is the polynomial central interpolation method. It uses the polynomial of degree k \u2014 1, interpolating k successive ordinates, as an approximation to F(x) only within a unit interval in x, centrally located with respect to its k defining ordinates. Assuming k fixed, successive approximating arcs for F(x) are thus obtained which present discontinuities on passing from one arc to the next if k is odd, or discontinuities in their first derivatives if k is even (see section 2.121). Actually these discontinuities are irrelevant in our present case of an analytic function F(x). Indeed, if the interpolated values obtained are sufficiently accurate, these discontinuities will be apparent only if we force the computation beyond the intrinsic accuracy of the y n."
            },
            "slug": "Contributions-to-the-Problem-of-Approximation-of-by-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Contributions to the Problem of Approximation of Equidistant Data by Analytic Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "This result does not use the V\u03b3 or VC dimensions, which, as we mentioned in section 2, are used only for distribution independent analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": false,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144083823"
                        ],
                        "name": "V. Hutson",
                        "slug": "V.-Hutson",
                        "structuredName": {
                            "firstName": "Vivian",
                            "lastName": "Hutson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hutson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 125
                            }
                        ],
                        "text": "The mathematical details (such as the convergence or not of certain series) can be found in the theory of integral equations [45,20,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "In fact, it follows from Mercer\u2019s theorem [45] that any function K(x,y) which is the kernel of a positive operator 15 in L2(\u03a9) has an expansion of the form (3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4298975,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "22388b80c859a567548052bba5787d0df4676bab",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Integral Equations and their ApplicationsVol. 1. By W. Pogorzelski. (International Series of Monographs in Pure and Applied Mathematics, Vol. 88.) Pp. xiii + 714. (London and New York: Pergamon Press Ltd.; Warszawa: PWN-Polish Scientific Publishers, 1966.) 120s."
            },
            "slug": "Integral-Equations-Hutson",
            "title": {
                "fragments": [],
                "text": "Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Vapnik [96], in comparing the empirical risk minimization principle with the Minimum Description Length principle [81], derives a bound on the generalization error as a function of the compression coefficient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "This is consistent with the Minimum Description Length (MDL) principle proposed by Rissanen [81] to measure the complexity of a hypothesis in terms of the bit length needed to encode it."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358061"
                        ],
                        "name": "V. Ivanov",
                        "slug": "V.-Ivanov",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Ivanov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ivanov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "In this section, after stating the formulation of SVM for binary pattern classification (SVMC) as developed by Cortes and Vapnik [22], we discuss a connection between SVMC and SVMR."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117565361,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "875f918d2b4437a68ebc1ecb7767b7a64ef3f58f",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "almost periodic coefficients. The author developed a method of reducing these equations to integro-differential equations and a simplified means of obtaining stability criteria. The treatment in the book is analytical, but the results find wide applications in technology and science. After an extensive introduction, Chapters 1-3, related to the analytical methods, analysis of vibratory motion, effects of damping and stability criteria, the book deals with the concept of resonances and their stability, in Chapter 4. In Chapters 5 and 6 the author presents combination resonances. Application to beams and columns are found in Chapter 7; torsional vibrations of shafts in Chapter 8 and vibration of plates and shells in Chapter 9. Chapter 10 contains topics related to rigid body motion, and flow motion encountered in physics and technology. The book is expertly written and constitutes a milestone in the world literature in mechanics. It is an indispensible tool for a person dealing with up-to-date problems in vibrations."
            },
            "slug": "The-Theory-of-Approximate-Methods-and-Their-to-the-Ivanov",
            "title": {
                "fragments": [],
                "text": "The Theory of Approximate Methods and Their Application to the Numerical Solution of Singular Integr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "12 This is like -learnability in the PAC model [93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik and Chervonenkis 1971) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions and h the VC-dimension of V in F ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Vapnik and Chervonenkis [ 97 ,98] studied under what conditions uniform convergence of the empirical risk to expected risk takes place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "3) with various values of \u03bb and then picking the best \u03bb using techniques such as cross-validation [1,100,101,49], Generalized Cross Validation, Finite Prediction Error and the MDL criteria (see [96] for a review and comparison)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14461054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d700e611ee7ffdf54873684a9e8883d3da0bcd7",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
            },
            "slug": "Interpolation-of-scattered-data:-Distance-matrices-Micchelli",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: Distance matrices and conditionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085069"
                        ],
                        "name": "C. Rabut",
                        "slug": "C.-Rabut",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Rabut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rabut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115392048,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "11e314cd16608c458f9220d1af750c70c729039f",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-to-Build-Quasi-Interpolants:-Application-to-Rabut",
            "title": {
                "fragments": [],
                "text": "How to Build Quasi-Interpolants: Application to Polyharmonic B-Splines"
            },
            "venue": {
                "fragments": [],
                "text": "Curves and Surfaces"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 222
                            }
                        ],
                        "text": "where \u2016f\u2016(2)K is a norm in a Reproducing Kernel Hilbert Space H defined by the positive definite function K, l is the number of data points or examples (the 1 There is a large literature on the subject: useful reviews are [44,19,102,39], [96] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9900,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259351"
                        ],
                        "name": "C. D. Boor",
                        "slug": "C.-D.-Boor",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Boor",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Boor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16203596,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "db3cf1fb1036cc13ef4c6a64b96f2ca899df774a",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "The determination of the approximation power of spaces of multivariate splines with the aid of quasiinterpolants is reviewed. In the process, a streamlined description of the existing quasiinterpolant theory is given."
            },
            "slug": "Quasiinterpolants-and-Approximation-Power-of-Boor",
            "title": {
                "fragments": [],
                "text": "Quasiinterpolants and Approximation Power of Multivariate Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658455"
                        ],
                        "name": "E. Gin\u00e9",
                        "slug": "E.-Gin\u00e9",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Gin\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gin\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715480"
                        ],
                        "name": "J. Zinn",
                        "slug": "J.-Zinn",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Zinn",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zinn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "If, for any number N , it is possible to find N points x1, . . . ,xN that can be separated in all the 2N possible ways, we will say that the VC-dimension of the set is infinite."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122862998,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e650cf667f6c0a69c7228fb25768d47df6a404d0",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractA class\n$$\\widetilde{F}$$\n of measurable functions on a probability space is called a Glivenko-Cantelli class if the empirical measuresPn converge to the trueP uniformly over\n$$\\widetilde{F}$$\n almost surely.\n$$\\widetilde{F}$$\n is a universal Glivenko-Cantelli class if it is a Glivenko-Cantelli Cantelli class for all lawsP on a measurable space, and a uniform Glivenko-Cantelli class if the convergence is also uniform inP. We give general sufficient conditions for the Glivenko-Cantelli and universal Glivenko-Cantelli properties and examples to show that some stronger conditions are not necessary. The uniform Glivenko-Cantelli property is characterized, under measurability assumptions, by an entropy condition."
            },
            "slug": "Uniform-and-universal-Glivenko-Cantelli-classes-Dudley-Gin\u00e9",
            "title": {
                "fragments": [],
                "text": "Uniform and universal Glivenko-Cantelli classes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390055667"
                        ],
                        "name": "Dr. M. G. Worster",
                        "slug": "Dr.-M.-G.-Worster",
                        "structuredName": {
                            "firstName": "Dr.",
                            "lastName": "Worster",
                            "middleNames": [
                                "M.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dr. M. G. Worster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 125
                            }
                        ],
                        "text": "The mathematical details (such as the convergence or not of certain series) can be found in the theory of integral equations [45,20,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4079036,
            "fieldsOfStudy": [
                "Education",
                "Physics"
            ],
            "id": "757dd5fbc67d8f2591eb2077180af74c1797fcd1",
            "isKey": false,
            "numCitedBy": 2430,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following people have maintained these notes."
            },
            "slug": "Methods-of-Mathematical-Physics-Worster",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102992139"
                        ],
                        "name": "Vasiliy Yakovlevich Arsenin",
                        "slug": "Vasiliy-Yakovlevich-Arsenin",
                        "structuredName": {
                            "firstName": "Vasiliy",
                            "lastName": "Arsenin",
                            "middleNames": [
                                "Yakovlevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasiliy Yakovlevich Arsenin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "It also leads to bounds on the performance of SVMC that (unlike the distribution independent ones) can be useful in practice22."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "It is unclear whether SVMC is consistent in terms of misclassification error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "A direct implication of this result is that one can solve any SVMC problem through the SVMR formulation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "Let Rl be the radius of the smallest hypersphere in the feature space induced by kernel K containing all essential SVs, \u2016f\u20162K(l) the norm of the solution of SVMC, and \u03c1(l) = 1\u2016f\u20162K(l)\nthe margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "As we mentioned in section 5, the connection between SVMC and SVMR outlined in that section may suggest how to extend such results to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We will not discuss the theory of SVMC here; we refer the reader to [96]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "We close this section with a brief reference to a recent distribution dependent result on the generalization error of SVMC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Then for a fixed kernel and for a fixed value of the SVMC parameter C the following theorem holds:\nTheorem 6.4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Using the fact that yi \u2208 {\u22121,+1} it is easy to see that our formulation (equation (5.10)) is equivalent to the following quadratic programming problem, originally proposed by Cortes and Vapnik [22]:\nProblem 5.4.\nmin f\u2208H,\u03be\n\u03a6(f, \u03be) = C\nl l\u2211 i=1 \u03bei + 1 2 \u2016f\u20162K\nsubject to the constraints:\nyif(xi)\u2265 1 \u2212\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SVMC can be formulated as the problem of minimizing:\nH(f) = 1 l l\u2211 i |1 \u2212 yif(xi)|+ + 1 2C \u2016f\u20162K , (5.10)\nwhich is again of the form (1.3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "It is an open question what theoretical implications theorem 5.5 may have about SVMC and SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "This theorem can also be used to justify the current formulation of SVMC, since minimizing ||f ||2K(l) affects the bound of theorem (6.4)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "In particular in section 6 we will discuss some recent theoretical results on SVMC that have not yet been extended to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 1
                            }
                        ],
                        "text": "(SVMC) correspond to the minimization of H in equation (1.3) for different choices of V :\n\u2022 Classical (L2) Regularization Networks (RN)\nV (yi, f(xi)) = (yi \u2212 f(xi))2 (1.4) \u2022 Support Vector Machines Regression (SVMR)\nV (yi, f(xi)) = |yi \u2212 f(xi)| (1.5) \u2022 Support Vector Machines Classification (SVMC)\nV (yi, f(xi)) = |1 \u2212 yif(xi)|+ (1.6)\nwhere | \u00b7 | is Vapnik\u2019s epsilon-insensitive norm (see later), |x|+ = x if x is positive and zero otherwise, and yi is a real number in RN and SVMR, whereas it takes values \u22121, 1 in SVMC. Loss function (1.6) is also called the soft margin loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "For a given training set of size l, let us define SVl to be the number of essential support vectors of SVMC, (as we defined them in section 5 - see eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "In fact, the norm of f is related to the notion of margin, an important idea for SVMC for which we refer the reader to [96,15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "For SVMC, we will also discuss two other loss functions:\n\u2022 The hard margin loss function: V (yi, f(x)) = \u03b8(1 \u2212 yif(xi)) (1.7)\n\u2022 The misclassification loss function: V (yi, f(x)) = \u03b8(\u2212yif(xi)) (1.8)\nWhere \u03b8(\u00b7) is the Heaviside function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "This implies that, as discussed at the beginning of this section, it cannot be used to study the expected misclassification error of SVMC in terms of the empirical one."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "In this section, after stating the formulation of SVM for binary pattern classification (SVMC) as developed by Cortes and Vapnik [22], we discuss a connection between SVMC and SVMR."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Notice that SVMC does not minimize the misclassification error, and instead minimizes the empirical error using the soft margin loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "However, a direct application of theorems 6.3 and 2.11 leads to a bound on the expected soft margin error of the SVMC solution, instead of a more interesting bound on the expected misclassification error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Thus, using the same approach outlined above for the soft margin, we can get a bound on the misclassification error of SVMC in terms of \u2211l i=1(\u03bei)\u03c3, which, for \u03c3 near 0, is close to the margin error used in [8] (for more information we refer the reader to [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "This theorem, combined with the theorems on V\u03b3 dimension summarized in section 2, can be used for a distribution independent analysis of SVMC (of the form (6.1)) like that of SVMR and RN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122072756,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc14819e745cd7af37efd09ea29773dc0065119e",
            "isKey": true,
            "numCitedBy": 7884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-of-ill-posed-problems-Tikhonov-Arsenin",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Figure (7.4) gives a preliminary empirical demonstration that in the case of SVMR the \u201dBayesian\u201d dependence of \u03bb as \u03b1l may not be correct."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "4) to hold true in probability, or more precisely, for the empirical risk minimization principle to be non-trivially consistent (see [96] for a discussion about consistency versus non-trivial consistency), the following uniform law of large numbers (which \u201ctranslates\u201d to one-sided uniform convergence in probability of empirical risk to expected risk in F) is a necessary and sufficient condition:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "We will describe how classical regularization and Support Vector Machines [96] for both regression (SVMR) and classification (SVMC) correspond to the minimization of H in equation (1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "A direct implication of this result is that one can solve any SVMC problem through the SVMR formulation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "It is possible that theorem 5.5 may help to extend them to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "As we mentioned in section 5, the connection between SVMC and SVMR outlined in that section may suggest how to extend such results to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "We now address the following question: what happens if we apply the SVMR formulation given by problem (5.1) to the binary pattern classification case, i.e. the case where yi take values {\u22121, 1}, treating classification as a regression on binary data?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 238
                            }
                        ],
                        "text": "First, as we discussed in section 2, SRM using the V\u03b3 dimension is practically difficult because we do not have tight bounds to use in order to pick the optimal Fn\u2217(l) (combining theorems 6.2 and 2.11, bounds on the expected risk of RN and SVMR machines of the form (6.1) can be derived, but these bounds are not practically useful)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Thus both RNs and SVMR are consistent in HA for any A   \u221e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Moreover, the V\u03b3 dimension is finite for \u2200 \u03b3 > 0; therefore, according to theorem (2.10), ERM uniformly converges in HA for any A < \u221e, both for RN and for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "On the other hand, if we had the optimal Lagrange multiplier \u03bbn\u2217(l), we could simply solve the unconstrained minimization problem:\n1 l l\u2211 i=1 V (yi, f(xi)) + \u03bbn\u2217(l)||f ||2K (6.3)\nboth for RN and for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "It is an open question what theoretical implications theorem 5.5 may have about SVMC and SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "In particular in section 6 we will discuss some recent theoretical results on SVMC that have not yet been extended to SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Summarizing, both the RN and the SVMR methods discussed in sections 4 and 5 can be seen as approximations of the extended SRM method using the V\u03b3 dimension, with nested hypothesis spaces being of the form HA = {f \u2208 R : ||f ||K \u2264 A}, R being a RKHS defined by kernel K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Finally,\nit is an open question whether similar results exist for the case of SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "The method of SVMR [96] corresponds to the following functional\nH[f ] = 1 l l\u2211 i=1 |yi \u2212 f(xi)| + \u03bb\u2016f\u20162K (5.1)\nwhich is a special case of equation (4.4) and where\nV (x) = |x| \u2261 {\n0 if |x|   |x| \u2212 otherwise, (5.2)\nis the \u2212Insensitive Loss Function (ILF) (also noted with L )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "4) holds in probability (see, for example, [96])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "(SVMC) correspond to the minimization of H in equation (1.3) for different choices of V :\n\u2022 Classical (L2) Regularization Networks (RN)\nV (yi, f(xi)) = (yi \u2212 f(xi))2 (1.4) \u2022 Support Vector Machines Regression (SVMR)\nV (yi, f(xi)) = |yi \u2212 f(xi)| (1.5) \u2022 Support Vector Machines Classification (SVMC)\nV (yi, f(xi)) = |1 \u2212 yif(xi)|+ (1.6)\nwhere | \u00b7 | is Vapnik\u2019s epsilon-insensitive norm (see later), |x|+ = x if x is positive and zero otherwise, and yi is a real number in RN and SVMR, whereas it takes values \u22121, 1 in SVMC. Loss function (1.6) is also called the soft margin loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For both RN and SVMR\nthe V\u03b3 dimension of the loss function V in HA is finite for \u2200 \u03b3 > 0, so the ERM method uniformly converges in HA for any A < \u221e, and we can use the extended SRM method outlined in section 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "convergence (a discussion can be found in [96]), and from now on we concentrate on the two-sided uniform convergence in probability, which we simply refer to as uniform convergence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "In the case of RN, V is the L2 loss function, whereas in the case of SVMR it is the -insensitive loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "So the VC-dimension cannot be used directly neither for RN nor for SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Under 1 There is a large literature on the subject: useful reviews are [19,39,44,96,102] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The last three kernels were proposed by Vapnik [96], originally for SVM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "In this section, after stating the formulation of SVM for binary pattern classification (SVMC) as developed by Cortes and Vapnik [22], we discuss a connection between SVMC and SVMR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "In this section we discuss the technique of Support Vector Machines (SVM) for Regression (SVMR) [95,96] in terms of the SVM functional."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik, 1998) The expected misclassification risk of the SVM trained on m data points sampled from X \u00d7 Y according to a probability distri-\n21 All these bounds are not tight enough in practice."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "We will describe how classical regularization and Support Vector Machines [96] for both regression (SVMR) and classification\n2 The method of quasi-solutions of Ivanov and the equivalent Tikhonov\u2019s regularization technique were developed to solve ill-posed problems of the type Af = F , where A is a (linear) operator, f is the desired solution in a metric space E1, and F are the \u201cdata\u201d in a metric space E2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "It can be shown (see, for example, [96]) that in order for the limits in equation (2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "We have just seen that minimization of both the RN and the SVMR functionals can be interpreted as corresponding to the MAP estimate of the posterior probability of f given the data, for certain models of the noise and for a specific Gaussian prior on the space of functions f ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "It can be shown (variation of [96]) that \u03b5-uniform convergence in probability implies that"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Using these definitions we can now state three important results of statistical learning theory [96]: \u2022 For a given probability distribution P (x, y): 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "This theorem, combined with the theorems on V\u03b3 dimension summarized in section 2, can be used for a distribution independent analysis of SVMC (of the form (6.1)) like that of SVMR and RN."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Learning Theory (Wiley"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 77
                            }
                        ],
                        "text": "1) are a special form of regularization theory developed by Tikhonov, Ivanov [46,92] and others to solve ill-posed problems and in particular to solve the problem of approximating the functional relation between x and y given a finite number of examples D = {xi, yi}i=1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 268
                            }
                        ],
                        "text": "In this section we consider the approximation scheme that arises from the minimization of the quadratic functional\nmin f\u2208H H[f ] = 1 l l\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162K (4.1)\nfor a fixed \u03bb. Formulations like equation (4.1) are a special form of regularization theory developed by Tikhonov, Ivanov [92,46] and others to solve ill-posed problems and in particular to solve the problem of approximating the functional relation between x and y given a finite number of examples D = {xi, yi}li=1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [11,51,58,73,100,102]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 127
                            }
                        ],
                        "text": "The problem of approximating a function from sparse data is ill-posed and a classical way to solve it is regularization theory [10,11,92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 200
                            }
                        ],
                        "text": "In classical regularization the data term is an L2 loss function for the empirical risk, whereas the second term \u2013 called stabilizer \u2013 is usually written as a functional \u03a9(f ) with certain properties [39,69,92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arsenin, Solutions of Ill-posed Problems (W.H"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "The purpose of this paper is to present a theoretical framework for the problem of learning from examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 73
                            }
                        ],
                        "text": "It is often possible to write the solution f(x) as a linear combination of SVs in a number of different ways (for example in case that the feature space induced by the kernel K has dimensionality lower than the number of SVs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The method of SVMR [96] corresponds to the following functional\nH[f ] = 1 l l\u2211 i=1 |yi \u2212 f(xi)| + \u03bb\u2016f\u20162K (5.1)\nwhich is a special case of equation (4.4) and where\nV (x) = |x| \u2261 {\n0 if |x|   |x| \u2212 otherwise, (5.2)\nis the \u2212Insensitive Loss Function (ILF) (also noted with L )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 161
                            }
                        ],
                        "text": "Notice that an approximation of the regression function using a mean square error criterion places more emphasis on the most probable data points and not on the most \u201cimportant\u201d ones which are the ones near the separating boundary."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "Then, there exists a value a \u2208 (0, 1) such that for \u2200 \u2208 [a, 1), if the regression problem (5.1) is solved with parameter (1 \u2212 )C, the optimal solution will be (1 \u2212 )f ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195857038,
            "fieldsOfStudy": [],
            "id": "b7294ac444ae927f71ac442372903ff5e2a763e1",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik and Chervonenkis 1971) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions and h the VC-dimension of V in F ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and Chervonenkis [97,98] studied under what conditions uniform convergence of the empirical risk to expected risk takes place."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 36
                            }
                        ],
                        "text": "An important outcome of the work of Vapnik and Chervonenkis is that the uniform deviation between empirical risk and expected risk in a hypothesis space can be bounded in terms of the VC-dimension, as shown in the following theorem:\nTheorem 2.7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "Uniform Convergence and the Vapnik-Chervonenkis bound Vapnik and Chervonenkis [97,98] studied under what conditions uniform convergence of the empirical risk to expected risk takes place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis"
            },
            "venue": {
                "fragments": [],
                "text": "The necessary and sufficient conditions for the uniform convergence of averages to their expected values. Teoriya Veroyatnostei i Ee Primeneniya, 26(3):543\u2013564,"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "(Alon et al. , 1993 ) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 44
                            }
                        ],
                        "text": "Unfortunately, existing bounds of that type [2,7] are not tight."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "(among others) in [2,50], known as the V\u03b3-dimension(11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "The reader can refer to [2,7] for an in-depth discussion on this topic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "(Alon et al. , 1993 ) Let A \u2264 V (y, f(x))) \u2264 B, f \u2208 F , F be a set of bounded functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": ", if hi is the V\u03b3-dimension of space Hi, then 13 Closed forms of G can be derived (see, for example, [2]) but we do not present them here for simplicity of notation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive-dimensions"
            },
            "venue": {
                "fragments": [],
                "text": "uniform convergence, and learnability, in: Symposium on Foundations of Computer Science "
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik and Chervonenkis 1971) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions and h the VC-dimension of V in F ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 24
                            }
                        ],
                        "text": "Vapnik and Chervonenkis [97,98] studied under what conditions uniform convergence of the empirical risk to expected risk takes place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 36
                            }
                        ],
                        "text": "An important outcome of the work of Vapnik and Chervonenkis is that the uniform deviation between empirical risk and expected risk in a hypothesis space can be bounded in terms of the VC-dimension, as shown in the following theorem:\nTheorem 2.7."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis, The necessary and sufficient conditions for the uniform convergence of averages to their expected values, Teor. Veroyatn. i Primenen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Classical regularization theory, as we will consider here formulates the regression problem as a variational problem of finding the function f that minimizes the functional\nmin f\u2208H H[f ] = 1 l l\u2211 i=1 (yi \u2212 f(xi))2 + \u03bb\u2016f\u20162K (1.1)\nwhere \u2016f\u20162K is a norm in a Reproducing Kernel Hilbert Space H defined\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "venue": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125923957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "731ecc3d6cde71cfde719ae809e2bfb1a852a1fd",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contributions-to-the-problem-of-approximation-of-by-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Contributions to the problem of approximation of equidistant data by analytic functions. Part A. On the problem of smoothing or graduation. A first class of analytic approximation formulae"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1946
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122552054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8cff1bb84a84c047a0157fbe078738760a06a2e1",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cardinal-interpolation-and-spline-functions-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Cardinal interpolation and spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69335340"
                        ],
                        "name": "J. A. Cochran",
                        "slug": "J.-A.-Cochran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cochran",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Cochran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 125
                            }
                        ],
                        "text": "The mathematical details (such as the convergence or not of certain series) can be found in the theory of integral equations [20,23,45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120984150,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e3736b70c8a204972e8fc05479df72498c21bdcc",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-analysis-of-linear-integral-equations-Cochran",
            "title": {
                "fragments": [],
                "text": "The analysis of linear integral equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 205
                            }
                        ],
                        "text": "The VC-dimension of a set {\u03b8(f(x)), f \u2208 F}, of indicator functions is the maximum number h of vectors x1, . . . ,xh that can be separated into two classes in all 2h possible ways using functions of the set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120994616,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3e211ab5f8fe33612149727fb3e5b1ad4af65d34",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-course-on-empirical-processes-Dudley",
            "title": {
                "fragments": [],
                "text": "A course on empirical processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1926176"
                        ],
                        "name": "J. Jerome",
                        "slug": "J.-Jerome",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Jerome",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jerome"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117905636,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07197b04d511b70a61b796d064a81e2ed769a3ae",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review:-Larry-L.-Schumaker,-Spline-functions:-Basic-Jerome",
            "title": {
                "fragments": [],
                "text": "Review: Larry L. Schumaker, Spline functions: Basic theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 127
                            }
                        ],
                        "text": "The problem of approximating a function from sparse data is ill-posed and a classical way to solve it is regularization theory [92,10,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117703131,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bd7efb0ea1893522220b4cf54d71269b77e80deb",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Regularization-methods-for-linear-inverse-problems-Bertero",
            "title": {
                "fragments": [],
                "text": "Regularization methods for linear inverse problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61853586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "623888e2350c0abc08d36a71fcd9024f19039994",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-Performance-of-Support-Vector-and-Bartlett-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Generalization Performance of Support Vector Machines and Other Pattern Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59746611,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "78ecaabe915ba7df950671d36f92678192802df4",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-Dependences-Based-on-Empirical-Data:-Vapnik",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences Based on Empirical Data: Springer Series in Statistics (Springer Series in Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Approximation theory can be used to bound this difference [61]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53667116,
            "fieldsOfStudy": [],
            "id": "44d9b606cb76473f07c2b372b2f0f93f7a7ab965",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Relationship-between-Generalization-Error-,-Niyogi-Girosi",
            "title": {
                "fragments": [],
                "text": "On the Relationship between Generalization Error , Hypothesis NG 1879 Complexity , and Sample Complexity for Radial Basis Functions N 00014-92-J-1879 6"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644344103"
                        ],
                        "name": "J. C. BurgesChristopher",
                        "slug": "J.-C.-BurgesChristopher",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "BurgesChristopher",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. BurgesChristopher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215966761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6716697767fc601efc7690f40820d9ea7a7bf57c",
            "isKey": false,
            "numCitedBy": 13527,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w..."
            },
            "slug": "A-Tutorial-on-Support-Vector-Machines-for-Pattern-BurgesChristopher",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Support Vector Machines for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This tutorial starts with an overview of the concepts of VC dimension and structural risk minimization and describes linear Support Vector Machines (SVMs) for separable and non-separable data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "We do not discuss these results here and refer the reader to [26,54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions (Chelsea"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to gaussian processes. 1997. (available at the URL"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to gaussian processes. 1997. (available at the URL"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evgeniou et al / Regularization Networks and Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Evgeniou et al / Regularization Networks and Support Vector Machines"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ten Lectures on Wavelets, CBMS-NSF Regional Conferences Series in Applied Mathematics (SIAM"
            },
            "venue": {
                "fragments": [],
                "text": "Ten Lectures on Wavelets, CBMS-NSF Regional Conferences Series in Applied Mathematics (SIAM"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning, in: Foundations of Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Networks for approximation and learning, in: Foundations of Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "It can be shown [28] that, if V in equation (4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene Analysis (Wiley"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "It can be shown [94] that for the chosen solution f\u0302n\u2217(l),l inequalities (2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data (Springer, Berlin,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matching Pursuit in a time-frequency dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 77
                            }
                        ],
                        "text": "More precisely, we have the following theorem (for a proof see, for example, [36,103]):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "More precisely, the following theorem holds (for a proof see [36]):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the v-gamma-dimension for regression in reproducing kernel Hilbert spaces"
            },
            "venue": {
                "fragments": [],
                "text": "A.I. Memo, MIT Artificial Intelligence Lab. "
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "These techniques go under the name of Sparse Approximations (SAs) [18,17,65,42,24,57,21,26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matching Pursuit in a time-frequency dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Under 1 There is a large literature on the subject: useful reviews are [19,39,44,96,102] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation (Macmillan"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Regularization, Radial Basis Functions, Support Vector Machines, Reproducing Kernel Hilbert Space, Structural Risk Minimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990, in: Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "venue": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990, in: Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "Various methods for ICA have been developed in recent years [3,9,63,53,65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An informationb maximization approach to blind separation and blind deconvolutio"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [51,102,100,73,58,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "3) with various values of \u03bb and then picking the best \u03bb using techniques such as cross-validation [1,100,101,49], Generalized Cross Validation, Finite Prediction Error and the MDL criteria (see [96] for a review and comparison)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline bases, regularization, and generalized cross-validation for solving approximation problems with large quantities of noisy data"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Approximation theory in honour of George Lorenz"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [91] it is reported that the positivity of the operator associated to K is equivalent to the statement that the kernel K is positive definite, that is the matrix Kij = K(xi, xj) is positive definite for all choices of distinct points xi \u2208 X."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations"
            },
            "venue": {
                "fragments": [],
                "text": "an historical survey, Rocky Mountain J. Math. 6 "
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 132
                            }
                        ],
                        "text": "17), and the basis functions bi(x) are called the equivalent kernels, because of the similarity with the kernel smoothing technique [88,41,43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applied nonparametric regression, volume 19 of Econometric Society Monographs"
            },
            "venue": {
                "fragments": [],
                "text": "Applied nonparametric regression, volume 19 of Econometric Society Monographs"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The functions Bn are piecewise polynomials of degree n, whose exact definition can be found in [85]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Functions: Basic Theory (Wiley"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Regularization, Radial Basis Functions, Support Vector Machines, Reproducing Kernel Hilbert Space, Structural Risk Minimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of networks for approximation and learning, A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence Laboratory Massachusetts Institute of Technology"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik and Chervonenkis 1971) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions and h the VC-dimension of V in F ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequences of events to their probabilities, Theory Probab"
            },
            "venue": {
                "fragments": [],
                "text": "Appl"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 73
                            }
                        ],
                        "text": "Regularization is the approach we have taken in earlier work on learning [39,69,77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990"
            },
            "venue": {
                "fragments": [],
                "text": "in: Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions, ed. W.A. Light "
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 1
                            }
                        ],
                        "text": "(Vapnik and Chervonenkis 1971) Let A \u2264 V (y, f(x)) \u2264 B, f \u2208 F , F be a set of bounded functions and h the VC-dimension of V in F ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The idea of SRM is to define a nested sequence of hypothesis spaces H1 \u2282 H2 \u2282 . . . \u2282 Hn(l) with n(l) a non-decreasing integer function of l, where each hypothesis space Hi has VC-dimension finite and larger than that of all previous sets, i.e. if hi is the VC-dimension of space Hi, then h1 \u2264 h2 \u2264\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequences of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Th. Prob. and its Applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "It is often possible to write the solution f(x) as a linear combination of SVs in a number of different ways (for example in case that the feature space induced by the kernel K has dimensionality lower than the number of SVs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolation and approximation by radial and related functions"
            },
            "venue": {
                "fragments": [],
                "text": "Interpolation and approximation by radial and related functions"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the noise model of support vector machine regression. A.I. Memo"
            },
            "venue": {
                "fragments": [],
                "text": "On the noise model of support vector machine regression. A.I. Memo"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "The idea of SRM is to define a nested sequence of hypothesis spaces H1 \u2282 H2 \u2282 . . . \u2282 Hn(l) with n(l) a non-decreasing integer function of l, where each hypothesis space Hi has VC-dimension finite and larger than that of all previous sets, i.e. if hi is the VC-dimension of space Hi, then h1 \u2264 h2 \u2264\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis. The necessary and sufficient conditions for the uniform convergence of averages to their expected values"
            },
            "venue": {
                "fragments": [],
                "text": "Teoriya Veroyatnostei i Ee Primeneniya"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "[62] showed how several invariances can be embedded in the stabilizer or, equivalently, in virtual examples (see for a related work on tangent distance [89] and [84])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prior knowledge in suport vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 9"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to build quasi-interpolants"
            },
            "venue": {
                "fragments": [],
                "text": "Applications to polyharmonic B-splines, in: Curves and Surfaces, eds. P.-J. Laurent, A. Le M\u00e9haut\u00e9 and L.L. Schumaker "
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "1) can be derived not only in the context of functional analysis [92], but also in a probabilistic framework [11,51,58,73,100,102]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "3) with various values of \u03bb and then picking the best \u03bb using techniques such as cross-validation [1,49,100,101], Generalized Cross Validation, Finite Prediction Error and the MDL criteria (see [96] for a review and comparison)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline bases"
            },
            "venue": {
                "fragments": [],
                "text": "regularization, and generalized cross-validation for solving approximation problems with large quantities of noisy data, in: Proceedings of the International Conference on Approximation Theory in Honour of George Lorenz, eds. J. Ward and E. Cheney, Austin, TX January 8\u201310, 1980 "
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quasi-interpolants and approximation power of multivariate splines, in: Computation of Curves and Surfaces"
            },
            "venue": {
                "fragments": [],
                "text": "Quasi-interpolants and approximation power of multivariate splines, in: Computation of Curves and Surfaces"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to build quasi-interpolants. Applications to polyharmonic B-splines, in: Curves and Surfaces"
            },
            "venue": {
                "fragments": [],
                "text": "How to build quasi-interpolants. Applications to polyharmonic B-splines, in: Curves and Surfaces"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes. A.I. Memo 1580"
            },
            "venue": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes. A.I. Memo 1580"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "A review of the methods can be found in [52]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A unifying information-theoretical framework for independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. on Math. and Comp. Mod"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "25 Ironically, it is only recently that the neural network community seems to have realized the equivalence of many so-called neural networks and Gaussian processes and the fact that they work quite well (see [55] and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Gaussian processes (1997) (available at the URL: http://wol.ra. phy.cam.ac.uk/mackay)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the v-gamma dimension for regression in reproducing kernel hilbert spaces. A.i. memo, MIT Artificial Intelligence Lab"
            },
            "venue": {
                "fragments": [],
                "text": "On the v-gamma dimension for regression in reproducing kernel hilbert spaces. A.i. memo, MIT Artificial Intelligence Lab"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "\u2026MA 02139 USA E-mail: theos@ai.mit.edu, pontil@ai.mit.edu, tp@ai.mit.edu\nRegularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular the regression problem of approximating a multivariate function from sparse data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arsenin, Solutions of Ill-posed Problems"
            },
            "venue": {
                "fragments": [],
                "text": "Arsenin, Solutions of Ill-posed Problems"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolation and approximation by radial and related functions, in: Approximation Theory VI"
            },
            "venue": {
                "fragments": [],
                "text": "Interpolation and approximation by radial and related functions, in: Approximation Theory VI"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Under 1 There is a large literature on the subject: useful reviews are [19,39,44,96,102] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from Data: Concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Theory, and Methods "
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis, The necessary and sufficient conditions for consistency in the empirical risk minimization method, Pattern Recognition and Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Preliminary experiments indicate that this strategy may give better results than other regression approaches [66]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sparse correlation kernel based signal reconstruction"
            },
            "venue": {
                "fragments": [],
                "text": "Sparse correlation kernel based signal reconstruction"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evgeniou et al / Regularization Networks and Support Vector Machines 51"
            },
            "venue": {
                "fragments": [],
                "text": "Evgeniou et al / Regularization Networks and Support Vector Machines 51"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Radial Basis Functions methods for multivariate approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Radial Basis Functions methods for multivariate approximation"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis. The necessary and sufficient conditions for consistency in the empirical risk minimization method"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition and Image Analysis"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Vapnik and Chervonenkis [97,98] studied under what conditions uniform convergence of the empirical risk to expected risk takes place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The necessary and sufficient conditions for the uniform convergence of averages to their expected values"
            },
            "venue": {
                "fragments": [],
                "text": "Teor. Veroyatn. i Primenen"
            },
            "year": 1981
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 65,
            "methodology": 36,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 144,
        "totalPages": 15
    },
    "page_url": "https://www.semanticscholar.org/paper/Regularization-Networks-and-Support-Vector-Machines-Evgeniou-Pontil/d2d13bc44e15fd93480e16305d37c025bc0818c2?sort=total-citations"
}