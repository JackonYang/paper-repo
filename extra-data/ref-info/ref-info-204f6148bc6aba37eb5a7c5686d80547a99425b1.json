{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2798755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "slug": "Hidden-Markov-Model}-Induction-by-Bayesian-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Model} Induction by Bayesian Model Merging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is compared with the Baum-Welch method of estimating fixed-size models, and it is found that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 519430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f64041c7eba31d40e7edd3e16751f8e46b9d012f",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out. In a continuous density hidden Markov model (CDHMM) framework, Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering, and corrective training. The goal of this study is to enhance model robustness in a CDHMM-based speech recognition system so as to improve performance. Our approach is to use Bayesian learning to incorporate prior knowledge into the CDHMM training process in the form of prior densities of the HMM parameters. The theoretical basis for this procedure is presented and preliminary results applying to HMM parameter smoothing, speaker adaptation, and speaker clustering are given.Performance improvements were observed on tests using the DARPA RM task. For speaker adaptation, under a supervised learning mode with 2 minutes of speaker-specific training data, a 31% reduction in word error rate was obtained compared to speaker-independent results. Using Baysesian learning for HMM parameter smoothing and sex-dependent modeling, a 21% error reduction was observed on the FEB91 test."
            },
            "slug": "Bayesian-Learning-of-Gaussian-Mixture-Densities-for-Gauvain-Lee",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning of Gaussian Mixture Densities for Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out and preliminary results applying to HMM parameter smoothing, speaker adaptation, and speaker clustering are given."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 263
                            }
                        ],
                        "text": "Back-o models (where a second model is consulted if, and only if, the rst one returnsprobability zero) do not yield consistent probabilities unless they are combined with `dis-counting' of probabilities to ensure that the total probability mass sums up to unity (Katz1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61058350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d82e058a5c40954b8f5db170a298a889a254c37",
            "isKey": false,
            "numCitedBy": 1409,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing."
            },
            "slug": "Connectionist-Speech-Recognition:-A-Hybrid-Approach-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist Speech Recognition: A Hybrid Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32588087"
                        ],
                        "name": "R. Redner",
                        "slug": "R.-Redner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Redner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Redner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145221576"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Walker",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2611600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54323bf565cea5d2aaee88a03ec9d1d3444a9bfd",
            "isKey": false,
            "numCitedBy": 2829,
            "numCiting": 158,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating the parameters which determine a mixture density has been the subject of a large, diverse body of literature spanning nearly ninety years. During the last two decades, the method of maximum likelihood has become the most widely followed approach to this problem, thanks primarily to the advent of high speed electronic computers. Here, we first offer a brief survey of the literature directed toward this problem and review maximum-likelihood estimation for it. We then turn to the subject of ultimate interest, which is a particular iterative procedure for numerically approximating maximum-likelihood estimates for mixture density problems. This procedure, known as the EM algorithm, is a specialization to the mixture density context of a general algorithm of the same name used to approximate maximum-likelihood estimates for incomplete data problems. We discuss the formulation and theoretical and practical properties of the EM algorithm for mixture densities, focussing in particular on ..."
            },
            "slug": "Mixture-densities,-maximum-likelihood,-and-the-EM-Redner-Walker",
            "title": {
                "fragments": [],
                "text": "Mixture densities, maximum likelihood, and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work discusses the formulation and theoretical and practical properties of the EM algorithm, a specialization to the mixture density context of a general algorithm used to approximate maximum-likelihood estimates for incomplete data problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49497622"
                        ],
                        "name": "K. Lari",
                        "slug": "K.-Lari",
                        "structuredName": {
                            "firstName": "Kaveh",
                            "lastName": "Lari",
                            "middleNames": [
                                "Sookhak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 189
                            }
                        ],
                        "text": "The struc-ture nding problem in this domain is even more severe, as standard EM-based estimationmethods have great di culty when presented with unstructured, fully parameterized gram-mars (Lari & Young 1990; Pereira & Schabes 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53736294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-stochastic-context-free-grammars-Lari-Young",
            "title": {
                "fragments": [],
                "text": "Applications of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906655"
                        ],
                        "name": "T. Hunkapiller",
                        "slug": "T.-Hunkapiller",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hunkapiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hunkapiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35016244"
                        ],
                        "name": "M. A. McClure",
                        "slug": "M.-A.-McClure",
                        "structuredName": {
                            "firstName": "Marcella",
                            "lastName": "McClure",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. McClure"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 135
                            }
                        ],
                        "text": "Recent applications include part-of-speech tagging (Cutting et al. 1992) and proteinclassi cation and alignment (Haussler et al. 1992; Baldi et al. 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9055893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da757ead286e8c41ab55a70f7a403e17d177fa45",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) can be applied to several important problems in molecular biology. We introduce a new convergent learning algorithm for HMMs that, unlike the classical Baum-Welch algorithm is smooth and can be applied on-line or in batch mode, with or without the usual Viterbi most likely path approximation. Left-right HMMs with insertion and deletion states are then trained to represent several protein families including immunoglobulins and kinases. In all cases, the models derived capture all the important statistical properties of the families and can be used efficiently in a number of important tasks such as multiple alignment, motif detection, and classification."
            },
            "slug": "Hidden-Markov-Models-in-Molecular-Biology:-New-and-Baldi-Chauvin",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models in Molecular Biology: New Algorithms and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new convergent learning algorithm for HMMs that, unlike the classical Baum-Welch algorithm is smooth and can be applied on-line or in batch mode, with or without the usual Viterbi most likely path approximation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47640603"
                        ],
                        "name": "P. Freeman",
                        "slug": "P.-Freeman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Freeman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 210
                            }
                        ],
                        "text": "Minimum Description Length Especially in the domain of discrete structures, it isuseful to remember the standard duality between the Bayesian approach and inference byMinimum Description Length (Rissanen 1983; Wallace & Freeman 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118095811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04eb446825da7a4c2ab3fa6df7ebd377baa66ebe",
            "isKey": false,
            "numCitedBy": 598,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random. The encoded form has two parts. The first states the inferred estimates of the unknown parameters in the model, the second states the data using an optimal code based on the data probability distribution implied by those parameter estimates. Choosing the model and the estimates that give the most compact coding leads to an interesting general inference procedure. In its strict form it has great generality and several nice properties but is computationally infeasible. An approximate form is developed and its relation to other methods is explored."
            },
            "slug": "Estimation-and-Inference-by-Compact-Coding-Wallace-Freeman",
            "title": {
                "fragments": [],
                "text": "Estimation and Inference by Compact Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735952"
                        ],
                        "name": "D. Ron",
                        "slug": "D.-Ron",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 120
                            }
                        ],
                        "text": "Asolution to this problem might be the addition of a complementary state splitting operator,possibly along the lines of Ron et al. (1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8202564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11376222763d16f39a5be1a5151d55556997236e",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long memory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text. Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states. We also show how the algorithm can be applied to intergenic E. coli DNA base prediction with results comparable to HMM based methods."
            },
            "slug": "The-Power-of-Amnesia-Ron-Singer",
            "title": {
                "fragments": [],
                "text": "The Power of Amnesia"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The algorithm is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small and using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1386567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "310c5457dbb1133908a9e51cda843be5fcc1b9a8",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Best-first model merging\" is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting. It is applicable to both learning and recognition tasks and often generalizes significantly better than fixed structures. We demonstrate the approach applied to the tasks of choosing radial basis functions for function learning, choosing local affine models for curve and constraint surface modelling, and choosing the structure of a balltree or bumptree to maximize efficiency of access."
            },
            "slug": "Best-First-Model-Merging-for-Dynamic-Learning-and-Omohundro",
            "title": {
                "fragments": [],
                "text": "Best-First Model Merging for Dynamic Learning and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The best-first model merging approach is demonstrated to be applicable to the tasks of choosing radial basis functions for function learning, choosing local affine models for curve and constraint surface modelling, and choosing the structure of a balltree or bumptree to maximize efficiency of access."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122569569,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae7beb7920485aca9c252ce3ecc3972c52eb3c37",
            "isKey": false,
            "numCitedBy": 1833,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated realvalued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision. 1. Introduction. In this paper we study estimation based upon the principle of minimizing the total number of binary digits required to rewrite the observed data, when each observation is given with some precision. Instead of attempting at an absolutely shortest description, which would be futile, we look for the optimum relative to a class of parametrically given distributions. This Minimum Description Length (MDL) principle, which we introduced in a less comprehensive form in [25], turns out to degenerate to the more familiar Maximum Likelihood (ML) principle in case the number of parameters in the models is fixed, so that the description length of the parameters themselves can be ignored. In another extreme case, where the parameters determine the data, it similarly degenerates to Jaynes's principle of maximum entropy, [14]. But the main power of the new criterion is that it permits estimates of the entire model, its parameters, their number, and even the way the parameters appear in the model; i.e., the model structure. Hence, there will be no need to supplement the estimated parameters with a separate hypothesis test to decide whether a model is adequately parameterized or, perhaps, over parameterized."
            },
            "slug": "A-UNIVERSAL-PRIOR-FOR-INTEGERS-AND-ESTIMATION-BY-Rissanen",
            "title": {
                "fragments": [],
                "text": "A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50512000"
                        ],
                        "name": "S. Porat",
                        "slug": "S.-Porat",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Porat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Porat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165973"
                        ],
                        "name": "J. Feldman",
                        "slug": "J.-Feldman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Feldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5197355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "059f6743b405234807adaf381e88354029a886c5",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist learning models have had considerable empirical success, but it is hard to characterize exactly what they learn. The learning of finite-state languages (FSL) from example strings is a domain which has been extensively studied and might provide an opportunity to help understand connectionist learning. A majot problem is that traditional FSL learning assumes the storage of all examples and thus violates connectionist principles. This paper presents a provably correct algorithm for inferring any minimum-state deterministic finite state automata (FSA) from a complete ordered sample using limited total storage and without storing example strings. The algorithm is an iterative strategy that uses at each stage a current encoding of the data considered so far, and one single sample string. One of the crucial advantages of our algorithm is that the total amount of space used in the course of learning for encoding any finite prefix of the sample is polynomial in the size of the inferred minimum state deterministic FSA. The algorithm is also relatively efficient in time and has been implemented. More importantly, there is a connectionist version of the algorithm that preserves these properties. The connectionist version requires much more structure than the usual models and has been implemented using the Rochester Connectionist Simulator. We also show that no machine with finite working storage can iteratively identify the FSL from arbitrary presentations."
            },
            "slug": "Learning-automata-from-ordered-examples-Porat-Feldman",
            "title": {
                "fragments": [],
                "text": "Learning automata from ordered examples"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A provably correct algorithm for inferring any minimum-state deterministic finite state automata (FSA) from a complete ordered sample using limited total storage and without storing example strings is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152517614"
                        ],
                        "name": "K. Sjolander",
                        "slug": "K.-Sjolander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sjolander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sjolander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13054203,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5c785b107aca104dcad6cdf3138c9f20259a6e2e",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors apply hidden Markov models to the problem of statistical modeling and multiple sequence alignment of protein families. A variant of the expectation maximization algorithm known as the Viterbi algorithm is used to obtain the statistical model from the unaligned sequences. In a detailed series of experiments, they have taken 400 unaligned globin sequences, and produced a statistical model entirely automatically from the primary sequences. The authors used no prior knowledge of globin structure. Using this model, a multiple alignment of the 400 sequences and 225 other globin sequences was obtained that agrees almost perfectly with a structural alignment by D. Bashford et al. (1987). This model can also discriminate all these 625 globins from nonglobin protein sequences with greater than 99% accuracy, and can thus be used for database searches.<<ETX>>"
            },
            "slug": "Protein-modeling-using-hidden-Markov-models:-of-Haussler-Krogh",
            "title": {
                "fragments": [],
                "text": "Protein modeling using hidden Markov models: analysis of globins"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A variant of the expectation maximization algorithm known as the Viterbi algorithm is used to obtain the statistical model from the unaligned sequences, and a multiple alignment of the 400 sequences and 225 other globin sequences was obtained that agrees almost perfectly with a structural alignment by D Bashford et al. (1987)."
            },
            "venue": {
                "fragments": [],
                "text": "[1993] Proceedings of the Twenty-sixth Hawaii International Conference on System Sciences"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 165
                            }
                        ],
                        "text": "However, as we will see below, the state-based priors by themselves produce a tendency towards reducing the number of states as a result of Bayesian 'Occam factors' (Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 116
                            }
                        ],
                        "text": "The ratio between the allowable range of a model's parameters a posterior and a prioriis known as the Occam factor (Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 116
                            }
                        ],
                        "text": "The ratio between the allowable range of a model's parameters a posterior and a priori is known as the Occam factor (Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 164
                            }
                        ],
                        "text": "However, as we will see below, the state-based priors by themselvesproduce a tendency towards reducing the number of states as a result of Bayesian `Occamfactors' (Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117915279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6272baf82e2e442edab4fb613ef2b7186bf5f1fb",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of Bayesian reasoning are reviewed and applied to problems of inference from data sampled from Poisson, Gaussian and Cauchy distributions. Probability distributions (priors and likelihoods) are assigned in appropriate hypothesis spaces using the Maximum Entropy Principle, and then manipulated via Bayes\u2019 Theorem. Bayesian hypothesis testing requires careful consideration of the prior ranges of any parameters involved, and this leads to a quantitive statement of Occam\u2019s Razor. As an example of this general principle we offer a solution to an important problem in regression analysis; determining the optimal number of parameters to use when fitting graphical data with a set of basis functions."
            },
            "slug": "Bayesian-Inductive-Inference-and-Maximum-Entropy-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Inductive Inference and Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17279285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61039fd2773a00e111d2121a63982a7b7d0b9f92",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statistics. This introduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to Quinlan's information gain, while smoothing and averaging replace pruning. Comparative experiments with reimplementations of a minimum encoding approach,c4 (Quinlanet al., 1987) andcart (Breimanet al., 1984), show that the full Bayesian algorithm can produce more accurate predictions than versions of these other approaches, though pays a computational price."
            },
            "slug": "Learning-classification-trees-Buntine",
            "title": {
                "fragments": [],
                "text": "Learning classification trees"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces Bayesian techniques for splitting, smoothing, and tree averaging, which are similar to Quinlan's information gain, while smoothing and averaging replace pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428168"
                        ],
                        "name": "M. Riley",
                        "slug": "M.-Riley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Riley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62108437,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68b23b20b82c5d96a640962029e859a14d5a390e",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods to predict detailed phonetic pronunciations from a coarse phonemic transcription are described. The phonemic base forms, obtainable from orthographic text by dictionary lookup and other means, do not specify fine phonetic detail such as flapping, glottal stop insertion, or the formation of syllabic nasals and liquids. These phenomena depend on the phonetic context (often spanning word boundaries), stress environment, speaking rate, and dialect. A procedure is presented that builds decision trees, trained on the TIMIT database, using some of these features to predict pronunciation alternatives. The resulting phonetic network predicts the correct pronunciation of a phoneme on test data from the same corpus approximately 83% of the time and the correct phone was in the top five guesses 99% of the time.<<ETX>>"
            },
            "slug": "A-statistical-model-for-generating-pronunciation-Riley",
            "title": {
                "fragments": [],
                "text": "A statistical model for generating pronunciation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure is presented that builds decision trees, trained on the TIMIT database, using some of these features to predict pronunciation alternatives, and the resulting phonetic network predicts the correct pronunciation of a phoneme on test data from the same corpus approximately 83% of the time."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705919"
                        ],
                        "name": "Chuck Wooters",
                        "slug": "Chuck-Wooters",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Wooters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuck Wooters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 166
                            }
                        ],
                        "text": ": : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4618 Hybrid MLP/HMM training/merging procedure used in the BeRP speech understand-ing system (Wooters 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 279
                            }
                        ],
                        "text": "\u2026Merging)\nForced Viterbi Alignment\nSentence Strings\nLabels Pronunciations\nProbabilities\nTraining Data\nTIMIT MLP\nForward Pass w/\nLabels Pronunciations\nTask Independent Word Models\nFigure 18: Hybrid MLP/HMM training/merging procedure used in the BeRP speech un-derstanding system (Wooters 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026discussion of ancillaryissues and a graphical HMM representation of the pronunciation for the 50 most commonwords in the BeRP corpus can be found in Wooters (1993).7 Conclusions and Further ResearchOur evaluations indicate that the HMM merging approach is a promising new way to induceprobabilistic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 191
                            }
                        ],
                        "text": "\u2026a larger value, until further increases reduce generalization on the cross-validation data.6.3 Multiple pronunciation word models for speech recognitionAs part of his dissertation research, Wooters (1993) has used HMM merging extensivelyin the context of the Berkeley Restaurant Project (BeRP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 162343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67f052a08bc272130d3e4554ca9e00a190a1cb98",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past 40 years, significant progress has been made in the fields of speech recognition and speech understanding. Current state-of-the-art speech recognition systems are capable of achieving word-level accuracies of 90% to 95% on continuous speech recognition tasks using 5000 words. Even larger systems, capable of recognizing 20,000 words are just now being developed. Speech understanding systems have recently been developed that perform fairly well within a restricted domain. \nWhile the size and performance of modern speech recognition and understanding systems are impressive, it is evident to anyone who has used these systems that the technology is primitive compared to our own human ability to understand speech. Some of the difficulties hampering progress in the fields of speech recognition and understanding stem from the many sources of variation that occur during human communication. \nOne of the sources of variation that occurs in human communication is the different ways that words can be pronounced. There are many causes of pronunciation variation, such as: the phonetic environment in which the word occurs, the dialect of the speaker, the speaker's age, the speaker's gender, and the speaking rate. Some researchers have shown improvements in speech recognition performance on a read-speech task through the use of explicit pronunciation modeling, while others have not shown any significant improvements. \nThis thesis presents an algorithm for the construction of models that attempt to capture the variation that occurs in the pronunciations of words in spontaneous (i.e., non-read) speech. A technique for developing alternate pronunciations of words and then estimating the probabilities of the alternate pronunciations is presented. Additionally, we describe the development and implementation of a spoken-language understanding system called the Berkeley Restaurant Project (BeRP). Multiple pronunciation word models constructed using the algorithm proposed in this thesis are evaluated within the context of the BeRP system. The results of this evaluation show that the explicit modeling of variation in the pronunciation of words improves the performance of both the speech recognition and the speech understanding components of the BeRP system."
            },
            "slug": "Multiple-pronunciation-lexical-modeling-in-a-speech-Wooters-Stolcke",
            "title": {
                "fragments": [],
                "text": "Multiple-pronunciation lexical modeling in a speaker independent speech understanding system"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm for the construction of models that attempt to capture the variation that occurs in the pronunciations of words in spontaneous speech, which improves the performance of both the speech recognition and the speech understanding components of the BeRP system."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "\u2026discussed in this section can be found in Bayesian approaches to theinduction of graph-based models in other domains (e.g., Bayesian networks (Cooper &Herskovits 1992; Buntine 1991) and decision trees (Buntine 1992)).3.4.1 Structural vs. parameter priorsAn HMM can be described in two stages:1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2124212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011fc271a69a3aa4cf2683099a5abcdc03317e26",
            "isKey": true,
            "numCitedBy": 756,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-Refinement-on-Bayesian-Networks-Buntine",
            "title": {
                "fragments": [],
                "text": "Theory Refinement on Bayesian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 696805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c0eab87d4855c42ae6395bf2e27eefe55003b4a",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus to achieve faster convergence and better modelling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133664"
                        ],
                        "name": "D. Cutting",
                        "slug": "D.-Cutting",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Cutting",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314752"
                        ],
                        "name": "Penelope Sibun",
                        "slug": "Penelope-Sibun",
                        "structuredName": {
                            "firstName": "Penelope",
                            "lastName": "Sibun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Penelope Sibun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "Recent applications include part-of-speech tagging (Cutting et al. 1992) and proteinclassi cation and alignment (Haussler et al. 1992; Baldi et al. 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7617879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c2263ceef50530996f4807da8d9a0e835905e8",
            "isKey": false,
            "numCitedBy": 785,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment."
            },
            "slug": "A-Practical-Part-of-Speech-Tagger-Cutting-Kupiec",
            "title": {
                "fragments": [],
                "text": "A Practical Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An implementation of a part-of-speech tagger based on a hidden Markov model that enables robust and accurate tagging with few resource requirements and accuracy exceeds 96%."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52089885"
                        ],
                        "name": "Wray L. BuntineRIACS",
                        "slug": "Wray-L.-BuntineRIACS",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "BuntineRIACS",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. BuntineRIACS"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "\u2026discussed in this section can be found in Bayesian approaches to theinduction of graph-based models in other domains (e.g., Bayesian networks (Cooper &Herskovits 1992; Buntine 1991) and decision trees (Buntine 1992)).3.4.1 Structural vs. parameter priorsAn HMM can be described in two stages:1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13593309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bdb1db3a2ba290cd69c665674c1d205ecd8523a",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Theory reenement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory reenement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incre-mental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally reened from data. Algorithms for reenement of Bayesian networks are presented to illustrate what is meant by \\partial theory\", \\alternative theory repre-sentation\", etc. The algorithms are an incre-mental variant of batch learning algorithms from the literature so can work well in batch and incremental mode."
            },
            "slug": "Theory-Reenement-on-Bayesian-Networks-BuntineRIACS",
            "title": {
                "fragments": [],
                "text": "Theory Reenement on Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Algorithms for reenement of Bayesian networks are presented to illustrate what is meant by partial theory, and are an incre-mental variant of batch learning algorithms from the literature so can work well in batch and incremental mode."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 35
                            }
                        ],
                        "text": "Somewhat surprisingly, the work by Brown et al. (1992) on the construction of class-based n-gram models for language modeling can also be viewed as a special case of HMMmerging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "The incremental word clustering algorithm given in(Brown et al. 1992) then becomes an instance of HMM merging, albeit one that is entirelybased on likelihoods.86 EvaluationWe have evaluated the HMM merging algorithm experimentally in a series of applications."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026to assess empirically the basic soundness of the merging heuristic and8Furthermore, after becoming aware of their work, we realized that the scheme Brown et al. (1992) areusing for e cient recomputation of likelihoods after merging is essentially the same as the one we were usingfor recomputing\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": true,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35709566"
                        ],
                        "name": "Francine R. Chen",
                        "slug": "Francine-R.-Chen",
                        "structuredName": {
                            "firstName": "Francine",
                            "lastName": "Chen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francine R. Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 250
                            }
                        ],
                        "text": "This is in contrast to other approaches where one rst induces rules for pronunciations ofindividual phonemes based on their contexts (e.g., using decision tree induction), whichcan then be concatenated into networks representing word pronunciations (Chen 1990;Riley 1991)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 61572357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c60a13f1bcb3bae012cafc0e2b83509a9267115",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A data-intensive, semiautomatic method is presented for identifying subsets of contextual factors which are useful for predicting the allophonic realizations of dictionary phonemes. The method organizes contextual descriptions of phonological variation into context trees. Context trees are computed using a combination of decision tree induction for factor selection and hierarchical clustering for forming natural groups of factor values. How the resulting context trees can be used to provide allophones in creating pronunciation networks is described. A phoneme-level representation with a flexible context description is used. It allows modeling of effects extending across syllables and word boundaries.<<ETX>>"
            },
            "slug": "Identification-of-contextual-factors-for-networks-Chen",
            "title": {
                "fragments": [],
                "text": "Identification of contextual factors for pronunciation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A data-intensive, semiautomatic method is presented for identifying subsets of contextual factors which are useful for predicting the allophonic realizations of dictionary phonemes and which can be used to provide allophones in creating pronunciation networks."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29942829"
                        ],
                        "name": "A. Cleeremans",
                        "slug": "A.-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cleeremans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63246537,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9127b58567e80a75e09805c5641ea221314709f0",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, I examine implicit learning--\"the process by which knowledge about the rule-governed complexities of the stimulus environment are acquired independently of conscious attempts to do so\" (Reber, 1989)--from an information processing perspective. I argue that most work on implicit learning has been distinctly atheoretical, and that progress in this field requires mechanistic models of the underlying processes to be developed. \nI first review experimental evidence and recent information processing models of implicit learning. I propose a general framework for thinking about implicit learning processes, based on a series of principles constrained by theoretical and empirical considerations. Next, I present a detailed PDP model (the \"SRN\" model) of implicit learning performance in a complex sequence acquisition paradigm, and two experiments exploring subject's ability to encode temporal information in this paradigm. The results indicate that subjects become progressively sensitive to the temporal structure of the material, despite their being unaware of the relevant contingencies. The model is successful both in accounting for human behavior and in instantiating general processing principles characterizing implicit learning mechanisms. I then explore how this model and the empirical work may be extended to cover additional phenomena and theoretical issues pertaining to sequence learning. I report on simulations of the effects of attention and of explicit knowledge on sequence learning performance, describe the performance of an amnesic patient, and explore how the SRN model may be applied to explicit prediction tasks. Together, this set of simulations provides additional support for the model. In a third experiment and accompanying simulations, I explore how well subjects can maintain information about remote context. The results do not fully support the SRN model: It appears that even simpler, decay-based mechanisms, may successfully account for the data. I discuss the implications of this finding for further research. The Appendix contains a reprint of a paper by Cleeremans, Servan-Schreiber and McClelland, in which we report on the computational characteristics of the SRN model. We show that the SRN is an instantiation of a new class of computational objects that we call \"Graded State Machines\"."
            },
            "slug": "Mechanisms-of-implicit-learning:-a-parallel-model-Cleeremans",
            "title": {
                "fragments": [],
                "text": "Mechanisms of implicit learning: a parallel distributed processing model of sequence acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results indicate that subjects become progressively sensitive to the temporal structure of the material, despite their being unaware of the relevant contingencies, and a general framework for thinking about implicit learning processes is proposed, based on a series of principles constrained by theoretical and empirical considerations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35414283"
                        ],
                        "name": "A. Reber",
                        "slug": "A.-Reber",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Reber",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Reber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145282756,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "43485c50f9f51d13e9759a831c835861e0b29600",
            "isKey": false,
            "numCitedBy": 1720,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Implicit-learning-of-artificial-grammars-Reber",
            "title": {
                "fragments": [],
                "text": "Implicit learning of artificial grammars"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700257"
                        ],
                        "name": "M. Thomason",
                        "slug": "M.-Thomason",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Thomason",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thomason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700008"
                        ],
                        "name": "E. Granum",
                        "slug": "E.-Granum",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Granum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Granum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "The idea of shortcutting the merging of samples into the existing model could be pursuedfurther along the lines of Thomason & Granum (1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 100
                            }
                        ],
                        "text": "A probabilistic approach to HMM structure induction that is probably closest to ours isdescribed by Thomason & Granum (1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14928800,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "0a641e615333cbad1e88f34398ae231f109d4134",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Inference of Markov networks from finite sets of sample strings is formulated using dynamic programming. Strings are installed in a network sequentially via optimal string-to-network alignments computed with a dynamic programming matrix, the cost function of which uses relative frequency estimates of transition probabilities to emphasize landmark substrings common to the sample set. Properties of an inferred network are described and the method is illustrated with artificial data and with data representing banded human chromosomes."
            },
            "slug": "Dynamic-Programming-Inference-of-Markov-Networks-of-Thomason-Granum",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming Inference of Markov Networks from Finite Sets of Sample Strings"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Inference of Markov networks from finite sets of sample strings is formulated using dynamic programming and the method is illustrated with artificial data and with data representing banded human chromosomes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705919"
                        ],
                        "name": "Chuck Wooters",
                        "slug": "Chuck-Wooters",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Wooters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuck Wooters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918411"
                        ],
                        "name": "G. Tajchman",
                        "slug": "G.-Tajchman",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Tajchman",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tajchman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069154640"
                        ],
                        "name": "Jonathan Segal",
                        "slug": "Jonathan-Segal",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 186
                            }
                        ],
                        "text": "BeRP is medium vocabulary,speaker-independent spontaneous continuous speech understanding system that functionsas a consultant for nding restaurants in the city of Berkeley, California (Jurafsky et al.1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 102
                            }
                        ],
                        "text": "cm p-\nlg /9\n40 50\n17\n09 M\nay 1\n99 4\nINTERNATIONAL COMPUTER SCIENCE INSTITUTE I1947 Center St. Suite 600 Berkeley, California 94704-1198 (510) 643-9153 FAX (510) 643-7684Best- rst Model Merging forHidden Markov Model InductionAndreas Stolcke Stephen M. OmohundroyTR-94-003January 1994Revised April 1994AbstractThis report describes a new technique for inducing the structure of Hidden MarkovModels from data which is based on the general `modelmerging' strategy (Omohundro1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8241008,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6237bf81e36284ed74fca2803169964dca1b96b6",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the architecture and performance of the Berkeley Restaurant Project (BeRP), a medium-vocabulary, speaker-independent, spontaneous continuous speech understanding system currently under development at ICSI. BeRP serves as a testbed for a number of our speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiplepronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading. In addition, it has proved quite usable in its function as a database frontend, even though many of our subjects are non-native speakers of English."
            },
            "slug": "The-berkeley-restaurant-project-Jurafsky-Wooters",
            "title": {
                "fragments": [],
                "text": "The berkeley restaurant project"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "BeRP serves as a testbed for a number of speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiplepronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113380562"
                        ],
                        "name": "James Kelly",
                        "slug": "James-Kelly",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308823"
                        ],
                        "name": "Matthew Self",
                        "slug": "Matthew-Self",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Self",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Self"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114766082"
                        ],
                        "name": "Will Taylor",
                        "slug": "Will-Taylor",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059655854"
                        ],
                        "name": "Don Freeman",
                        "slug": "Don-Freeman",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Freeman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "One important reason for the use of the Dirichlet prior in the case of multinomial param-eters (Cheeseman et al. 1988; Cooper & Herskovits 1992; Buntine 1992) is its mathematicalexpediency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "Our merging algorithm becomes applicable to suchmodels provided that one has a prior for such densities, which should be straightforward(Cheeseman et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 340806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f48079c278b02decf14f71b9f94c6ce1756940b",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AutoClass:-A-Bayesian-Classification-System-Cheeseman-Kelly",
            "title": {
                "fragments": [],
                "text": "AutoClass: A Bayesian Classification System"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42796,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has also been applied to the induction of non-probabilisticautomata (Angluin & Smith 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3209224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a great deal of theoretical and experimental work in computer science on inductive inference systems, that is, systems that try to infer general rules from examples. However, a complete and applicable theory of such systems is still a distant goal. This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations. 154 references."
            },
            "slug": "Inductive-Inference:-Theory-and-Methods-Angluin-Smith",
            "title": {
                "fragments": [],
                "text": "Inductive Inference: Theory and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 40
                            }
                        ],
                        "text": "The most likely, or Viterbi path (after Viterbi(1967)) is the one that maximizes the summand in equation (1):V (xjM) = argmaxq1:::q`2Q` p(qI ! q1)p(q1 \" x1)p(q1 ! q2) : : :p(q` \" x`)p(q` ! qF ) (4)Let Vi(xjM) denote the ith state in V (xjM), with V0(xjM) = qI and V`(xjM) = qF forconvenience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 613410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84dae6a2870c68005732b9db6890f375490f2d4e",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inferring-Decision-Trees-Using-the-Minimum-Length-Quinlan-Rivest",
            "title": {
                "fragments": [],
                "text": "Inferring Decision Trees Using the Minimum Description Length Principle"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Thisiterative procedure is a special case of the general EM (expectation-maximization) methodfor estimating distributions with hidden parameters (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706504"
                        ],
                        "name": "J. Hopcroft",
                        "slug": "J.-Hopcroft",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopcroft",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopcroft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31901407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a88a490d7ba9e383ecb16c4290083413a08258",
            "isKey": false,
            "numCitedBy": 13820,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Automata-Theory,-Languages-and-Hopcroft-Ullman",
            "title": {
                "fragments": [],
                "text": "Introduction to Automata Theory, Languages and Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48722988"
                        ],
                        "name": "R. J. Nelson",
                        "slug": "R.-J.-Nelson",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Nelson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120704005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19d5965db5de8e37c4122528bad34594a7c49565",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Automata-Nelson",
            "title": {
                "fragments": [],
                "text": "Introduction to Automata"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60349410,
            "fieldsOfStudy": [
                "Education",
                "Economics"
            ],
            "id": "09b99be3591c423fee4937477534f1eec22dcbc2",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: speech Reference EPFL-CONF-82487 Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "Connectionist-speech-recognition-Bourlard",
            "title": {
                "fragments": [],
                "text": "Connectionist speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Speech Reference EPFL-CONF-82487 describes the \u201cpolitics of language\u201d in the developing world and some of the challenges faced by speech interpreters and interpreters in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 166
                            }
                        ],
                        "text": ": : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4618 Hybrid MLP/HMM training/merging procedure used in the BeRP speech understand-ing system (Wooters 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 279
                            }
                        ],
                        "text": "\u2026Merging)\nForced Viterbi Alignment\nSentence Strings\nLabels Pronunciations\nProbabilities\nTraining Data\nTIMIT MLP\nForward Pass w/\nLabels Pronunciations\nTask Independent Word Models\nFigure 18: Hybrid MLP/HMM training/merging procedure used in the BeRP speech un-derstanding system (Wooters 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026discussion of ancillaryissues and a graphical HMM representation of the pronunciation for the 50 most commonwords in the BeRP corpus can be found in Wooters (1993).7 Conclusions and Further ResearchOur evaluations indicate that the HMM merging approach is a promising new way to induceprobabilistic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 191
                            }
                        ],
                        "text": "\u2026a larger value, until further increases reduce generalization on the cross-validation data.6.3 Multiple pronunciation word models for speech recognitionAs part of his dissertation research, Wooters (1993) has used HMM merging extensivelyin the context of the Berkeley Restaurant Project (BeRP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical Modeling in a Speaker Independent Speech Under standing System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102329511"
                        ],
                        "name": "George W. Soules",
                        "slug": "George-W.-Soules",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Soules",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George W. Soules"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063108982"
                        ],
                        "name": "Norman Weiss",
                        "slug": "Norman-Weiss",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norman Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 163
                            }
                        ],
                        "text": "\u2026\" x`)p(q` ! qF ) (1)1Where possible we try to keep the notation consistent with Bourlard & Morgan (1993).3\n2.4 HMM estimationThe Baum-Welch estimation method for HMMs (Baum et al. 1970) assumes a certaintopology and adjusts the parameters so as to maximize the model likelihood on the givensamples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122568650,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4",
            "isKey": true,
            "numCitedBy": 4551,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Maximization-Technique-Occurring-in-the-Analysis-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 163
                            }
                        ],
                        "text": "\u2026\" x`)p(q` ! qF ) (1)1Where possible we try to keep the notation consistent with Bourlard & Morgan (1993).3\n2.4 HMM estimationThe Baum-Welch estimation method for HMMs (Baum et al. 1970) assumes a certaintopology and adjusts the parameters so as to maximize the model likelihood on the givensamples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. The Annals of Mathematical Statistics 41"
            },
            "venue": {
                "fragments": [],
                "text": "A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. The Annals of Mathematical Statistics 41"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 20
                            }
                        ],
                        "text": ", Bayesian networks (Cooper & Herskovits 1992; Buntine 1991) and decision trees (Buntine 1992))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "This `empirical Bayes' approach is similar to the setting of prior class probability means in Buntine (1992). We are already working on the assumption that transitions and emissions are a priori independent of each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "\u2026discussed in this section can be found in Bayesian approaches to theinduction of graph-based models in other domains (e.g., Bayesian networks (Cooper &Herskovits 1992; Buntine 1991) and decision trees (Buntine 1992)).3.4.1 Structural vs. parameter priorsAn HMM can be described in two stages:1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory re nement of Bayesian networks"
            },
            "venue": {
                "fragments": [],
                "text": "Seventh Conference"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "One important reason for the use of the Dirichlet prior in the case of multinomial param-eters (Cheeseman et al. 1988; Cooper & Herskovits 1992; Buntine 1992) is its mathematicalexpediency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 203
                            }
                        ],
                        "text": "\u2026discussed in this section can be found in Bayesian approaches to theinduction of graph-based models in other domains (e.g., Bayesian networks (Cooper &Herskovits 1992; Buntine 1991) and decision trees (Buntine 1992)).3.4.1 Structural vs. parameter priorsAn HMM can be described in two stages:1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning classiication trees"
            },
            "venue": {
                "fragments": [],
                "text": "Artiicial Intelligence Frontiers in Statistics: AI and Statistics III"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 111
                            }
                        ],
                        "text": "This holding-out of training data makes the mixture model approach similar to the deletedinterpolation method (Jelinek & Mercer 1980)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789075"
                        ],
                        "name": "J. Horning",
                        "slug": "J.-Horning",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Horning",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Horning"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60515006,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ec350b29e088e3335d4c16dee51ef95cb2eddb93",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-study-of-grammatical-inference-Horning",
            "title": {
                "fragments": [],
                "text": "A study of grammatical inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of finite automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 4th Annual Conference of the Cognitive Science Society"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of finite automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 4th Annual Conference of the Cognitive Science Society, 105-108, Ann Arbor, Mich."
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Getting Started with the DARPA TIMIT CD-ROM: an Acoustic Phonetic Continuous Speech Database"
            },
            "venue": {
                "fragments": [],
                "text": "National Institute of Standards and Technology (NIST)"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Thisiterative procedure is a special case of the general EM (expectation-maximization) methodfor estimating distributions with hidden parameters (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelipood from incomplete data via the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian learning of Gaussian mixture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 176
                            }
                        ],
                        "text": "The TIMIT (Texas Instruments-MIT) database is a collection of hand-labeled speech samples compiled for the purpose of training speaker-independent phonetic recognition systems (Garofolo 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "The TIMIT (Texas Instruments-MIT) database is a collectionof hand-labeled speech samples compiled for the purpose of training speaker-independentphonetic recognition systems (Garofolo 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Getting Started with the DARPA TIMIT CD-ROM: an Acoustic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model induction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 168
                            }
                        ],
                        "text": "12This test model was inspired by nite-state models with similar characteristics that have been the subject of investigations into human language learning capabilities (Reber 1969; Cleeremans 1991) 30"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 214
                            }
                        ],
                        "text": "\u2026and [ is thedisjunction (set union) operator.12This test model was inspired by nite-state models with similar characteristics that have been the subjectof investigations into human language learning capabilities (Reber 1969; Cleeremans 1991)30\nAlternatively, a sample of 20 random strings was used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Implicit learning of arti cal grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Verbal Learning"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "David Stoutamire, and Jerry Feldman for helpful discussions of the issues in this paper, as well as numerous members of the ICSI AI and Speech groups for valuable comments on drafts"
            },
            "venue": {
                "fragments": [],
                "text": "David Stoutamire, and Jerry Feldman for helpful discussions of the issues in this paper, as well as numerous members of the ICSI AI and Speech groups for valuable comments on drafts"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2091,
                                "start": 222
                            }
                        ],
                        "text": "Minimum Description Length Especially in the domain of discrete structures, it is useful to remember the standard duality between the Bayesian approach and inference by Minimum Description Length (Rissanen 1983; Wallace & Freeman 1987). Brie y, the maximization ofP (M;X) = P (M)P (X jM) implicit in Bayesian model inference is equivalent to minimizing log P (M;X) = logP (M) logP (X jM) : This in turn can be interpreted as minimizing the coding or description lengths of the data X together with an underlying coding model M . Here log P (M) is the optimal encoding length of the model under the prior, whereas the negative log likelihood logP (X jM) corresponds to an optimal code for the data using M as a probabilistic model. The structural prior (12) above corresponds to a HMM coding scheme in which each transition is encoded by log pt bits, and each emission with log pe bits. Potential transitions and emissions that are missing each take up log(1 pt) and log(1 pe) respectively. Description Length priors Conversely, any (pre x-free) coding scheme for models that assigns M a code length `(M) can be used to induce a prior distribution over models with P (M) / e `(M) : We can take advantage of this fact to design `natural' priors for many domains. For example, a natural way to encode the transitions and emissions in an HMM is to simply enumerate them. Each transition can be encoded using log(jQj+ 1) bits, since there are jQj possible transitions, plus a special `end' marker which allows us not to encode the missing transitions explicitly. The total description length for all transitions from state q is thus n(q) t log(jQj+1). Similarly, all emissions from q can be coded using n(q) e log(j j+ 1) bits.5 5The basic idea of encoding transitions and emissions by enumeration has various more sophisticated variants. For example, one could base the enumeration of transitions on a canonical ordering of states, such that only log(n+ 1) + log n + + log(n nt + 1) bits are required. Or one could use the k-out-of-n-bit integer coding scheme described in Cover & Thomas (1991) and used for MDL inference in Quinlan & Rivest (1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2145,
                                "start": 222
                            }
                        ],
                        "text": "Minimum Description Length Especially in the domain of discrete structures, it is useful to remember the standard duality between the Bayesian approach and inference by Minimum Description Length (Rissanen 1983; Wallace & Freeman 1987). Brie y, the maximization ofP (M;X) = P (M)P (X jM) implicit in Bayesian model inference is equivalent to minimizing log P (M;X) = logP (M) logP (X jM) : This in turn can be interpreted as minimizing the coding or description lengths of the data X together with an underlying coding model M . Here log P (M) is the optimal encoding length of the model under the prior, whereas the negative log likelihood logP (X jM) corresponds to an optimal code for the data using M as a probabilistic model. The structural prior (12) above corresponds to a HMM coding scheme in which each transition is encoded by log pt bits, and each emission with log pe bits. Potential transitions and emissions that are missing each take up log(1 pt) and log(1 pe) respectively. Description Length priors Conversely, any (pre x-free) coding scheme for models that assigns M a code length `(M) can be used to induce a prior distribution over models with P (M) / e `(M) : We can take advantage of this fact to design `natural' priors for many domains. For example, a natural way to encode the transitions and emissions in an HMM is to simply enumerate them. Each transition can be encoded using log(jQj+ 1) bits, since there are jQj possible transitions, plus a special `end' marker which allows us not to encode the missing transitions explicitly. The total description length for all transitions from state q is thus n(q) t log(jQj+1). Similarly, all emissions from q can be coded using n(q) e log(j j+ 1) bits.5 5The basic idea of encoding transitions and emissions by enumeration has various more sophisticated variants. For example, one could base the enumeration of transitions on a canonical ordering of states, such that only log(n+ 1) + log n + + log(n nt + 1) bits are required. Or one could use the k-out-of-n-bit integer coding scheme described in Cover & Thomas (1991) and used for MDL inference in Quinlan & Rivest (1989). Any reasonable Bayesian inference procedure should not be sensitive to such minor di erence in the prior, unless it is used with too little data."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "AutoClass: A Bayesian classi cation system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 214
                            }
                        ],
                        "text": "\u2026and [ is thedisjunction (set union) operator.12This test model was inspired by nite-state models with similar characteristics that have been the subjectof investigations into human language learning capabilities (Reber 1969; Cleeremans 1991)30\nAlternatively, a sample of 20 random strings was used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Implicit learning of artiical grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Verbal Learning and Verbal Behavior"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "The TIMIT (Texas Instruments-MIT) database is a collectionof hand-labeled speech samples compiled for the purpose of training speaker-independentphonetic recognition systems (Garofolo 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Getting Started with the DARPA TIMIT CD-ROM: an Acoustic Phonetic Continuous Speech Database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical Modeling in a Speaker Independent Speech Under\u00ad standing System"
            },
            "venue": {
                "fragments": [],
                "text": "Lexical Modeling in a Speaker Independent Speech Under\u00ad standing System"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "Recent applications include part-of-speech tagging (Cutting et al. 1992) and proteinclassi cation and alignment (Haussler et al. 1992; Baldi et al. 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "27\nHaussler et al. (1992) apply HMMs trained by the Baum-Welch method to the problemof protein primary structure alignment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SAIRA MIAN, & KIMMEN SJOLANDER"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report UCSC-CRL~92-23,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical Modeling in a Speaker Independent Speech Under"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical Modeling in a Speaker Independent Speech Under\u00ad standing System. Berkeley, CA: University of California dissertation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Bayesian method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 170
                            }
                        ],
                        "text": "12This test model was inspired by finite-state models with similar characteristics that have been the subject of investigations into human language learning capabilities (Reber 1969; Cleeremans 1991) '-';\u00ad"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 214
                            }
                        ],
                        "text": "\u2026and [ is thedisjunction (set union) operator.12This test model was inspired by nite-state models with similar characteristics that have been the subjectof investigations into human language learning capabilities (Reber 1969; Cleeremans 1991)30\nAlternatively, a sample of 20 random strings was used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Implicit learning of artifical grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Verbal Learning and Verbal Behavior 6.855-863."
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Implicit learning of artifical grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Verbal Learning and Verbal Behavior"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inside-outside reestimation from partially"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic programming inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "One important reason for the use of the Dirichlet prior in the case of multinomial param-eters (Cheeseman et al. 1988; Cooper & Herskovits 1992; Buntine 1992) is its mathematicalexpediency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "Our merging algorithm becomes applicable to suchmodels provided that one has a prior for such densities, which should be straightforward(Cheeseman et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "AutoClass: A Bayesian classiication system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 5th International Conference on Machine Learning"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of nite automata from examples using"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning classi cation trees"
            },
            "venue": {
                "fragments": [],
                "text": "Arti cial Intelligence Frontiers in"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the implementation more flexible and robust"
            },
            "venue": {
                "fragments": [],
                "text": "the implementation more flexible and robust"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mechanisms of Implicit Learning"
            },
            "venue": {
                "fragments": [],
                "text": "A Parallel Distributed Pro-"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Best- rst model merging for dynamic learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning automata from ordered examples. Machine Learning 7.109-138"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 58
                            }
                        ],
                        "text": "Still in the eld of non-probabilistic automata induction, Tomita (1982) has used a sim-ple hill-climbing procedure combined with a goodness measure based on positive/negativesamples to search the space of possible models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of nite automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 4th Annual Conference of the Cognitive Science Society"
            },
            "year": 1982
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 21,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 73,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Best-first-Model-Merging-for-Hidden-Markov-Model-Stolcke-Omohundro/204f6148bc6aba37eb5a7c5686d80547a99425b1?sort=total-citations"
}