{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183233"
                        ],
                        "name": "Andrew Borthwick",
                        "slug": "Andrew-Borthwick",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Borthwick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Borthwick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144360624"
                        ],
                        "name": "J. Sterling",
                        "slug": "J.-Sterling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sterling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 73
                            }
                        ],
                        "text": "More MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 79
                            }
                        ],
                        "text": "M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 248
                            }
                        ],
                        "text": "On the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances (Miller et al., 1998) (Borthwick et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17353402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "497355583d46f1bdd3487f4a59133fa5d246b798",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new system called \\Maximum Entropy Named Entity\" or \\MENE\" (pronounced \\meanie\") which was NYU's entrant in the MUC-7 named entity evaluation. By working within the framework of maximum entropy theory and utilizing a exible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features and features indicating the current type of text (i.e. headline or main body). It makes use of a broad array of dictionaries of useful single or multi-word terms such as rst names, company names, and corporate su xes. These dictionaries required no manual editing and were either downloaded from the web or were simply \\obvious\" lists entered by hand."
            },
            "slug": "NYU:-Description-of-the-MENE-Named-Entity-System-as-Borthwick-Sterling",
            "title": {
                "fragments": [],
                "text": "NYU: Description of the MENE Named Entity System as Used in MUC-7"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes a new system called \\Maximum Entropy Named Entity\" or \\MENE\" (pronounced \\meanie\") which was NYU's entrant in the MUC-7 named entity evaluation and is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734560"
                        ],
                        "name": "Hiroyuki Shinnou",
                        "slug": "Hiroyuki-Shinnou",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Shinnou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroyuki Shinnou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 214
                            }
                        ],
                        "text": "The Viterbi search finds the highest probability path in which there are no two tokens in which the second one cannot follow the first, as defined by a table of all such invalid transitions (a similar approach to (Sekine et al., 1998))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "This approach is essentially the same as (Sekine et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 333513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78fcbe93ece4c5cfa2b03f99948f8953aa37c8f9",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system which uses a decision tree to find and classify names in Japanese texts. The decision tree uses part-of-speech, character type, and special dictionary information to determine the probability that a particular type of name opens or closes at a given position in the text. The output is generated from the consistent sequence of name opens and name closes with the highest probability. This system does not require any human adjustment. Experiments indicate good accuracy with a small amount of training data, and demonstrate the system's portability. The issues of training data size and domain dependency are discussed. 1 I n t r o d u c t i o n For some NLP applications, it is important to identify, \"named entities\" (NE), such as person names, organization names, time, date, or money expressions in the text. For example, in information extraction systems, it is crucial to identify them in order to provide the knowledge to be extracted, and in machine translation systems, they are useful for creating translations of unknown words or for disambiguation. However, it is not easy to identify these names, because they involve unknown words, and hence the strategy of listing candidates won' t work. Also, it is sometimes hard to determine the category of proper nouns, like distinguishing a person name from a company name. These phenomena are often different from domain to domain. One domain may use a special pattern which is not found in other domains. In this paper, we will present a supervised learning system whicil finds and classifies named entities in Japanese newspaper texts. Recently, several systems have been proposed for this task, but many of them use hand-coded patterns. Cre171 ating these patterns is laborious work, and when we adapt these systems to a new domain or a new definition of named entities, it is likely to need a large amount of additional work. On the other hand, in a supervised learning system, what is needed to adapt the system is to make new training data.. While this is also not a very easy task, it would be easier than creating complicated rules. For example, based on our experience, 100 training articles can be created in a day. There also have been several machine learning systems applied to this task. However, these either 1) partially need hand-made rules, 2) have parameters which must be adjusted by hand, or 3) do not perform well by fully automatic means. Our system does not work fully automatically and also needs special dictionaries, but performs welt and does not have parameters to be adjusted by hand. We will discuss one of the related systems in a later section. The issue of training data size will be discussed based on experiments using different sizes of training data. In order to demonstrate the portability of our system, we ran the system on a new domain with a new type of named entity. The experiment shows that the portability of the system is quite good and the performance is satisfactory."
            },
            "slug": "A-Decision-Tree-Method-for-Finding-and-Classifying-Sekine-Grishman",
            "title": {
                "fragments": [],
                "text": "A Decision Tree Method for Finding and Classifying Names in Japanese Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A supervised learning system which uses a decision tree to find and classify names in Japanese texts, which does not work fully automatically and also needs special dictionaries, but performs well and does not have parameters to be adjusted by hand."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@COLING/ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123937952"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40594158"
                        ],
                        "name": "Michael Crystal",
                        "slug": "Michael-Crystal",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Crystal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Crystal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20866393"
                        ],
                        "name": "Heidi Fox",
                        "slug": "Heidi-Fox",
                        "structuredName": {
                            "firstName": "Heidi",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heidi Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057335437"
                        ],
                        "name": "R. Stone",
                        "slug": "R.-Stone",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 286537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1b63980781545e76e4b74eca493ca4b4fe07b9f",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : For MUC-7, BBN has for the first time fielded a fully-trained system for NE, TE, and TR; results are all the output of statistical language models trained on annotated data, rather than programs executing handwritten rules. Such trained systems have some significant advantages: 1. They can be easily ported to new domains by simply annotating data with semantic answers. 2. The complex interactions that make rule-based systems difficult to develop and maintain can here be learned automatically from the training data. We believe that the results in this evaluation are evidence that such trained systems, even at their current level of development, can perform roughly on a par with rules hand-tailored by experts. Since MUC-3, BBN has been steadily increasing the proportion of the information extraction process that is statistically trained. Already in MET-1, our name-finding results were the output of a fully statistical, HMM-based model, and that statistical Identifinder(trademark) model was also used for the NE task in MUC-7. For the MUC-7 TE and TR tasks, BBN developed SIFT, a new model that represents a significant further step along this path, replacing PLUM, a system requiring handwritten patterns, with SIFT, a single integrated trained model."
            },
            "slug": "BBN:-Description-of-the-SIFT-System-as-Used-for-Miller-Crystal",
            "title": {
                "fragments": [],
                "text": "BBN: Description of the SIFT System as Used for MUC-7"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "For MUC-7, BBN has for the first time fielded a fully-trained system for NE, TE, and TR; results are all the output of statistical language models trained on annotated data, rather than programs executing handwritten rules."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15366907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c43d3fbca6086a5c096ea69675928d8dbcff5a4b",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems in natural language processing can be viewed as lin guistic classi cation problems in which linguistic contexts are used to pre dict linguistic classes Maximum entropy models o er a clean way to com bine diverse pieces of contextual evidence in order to estimate the proba bility of a certain linguistic class occurring with a certain linguistic con text This report demonstrates the use of a particular maximum entropy model on an example problem and then proves some relevant mathemat ical facts about the model in a simple and accessible manner This report also describes an existing procedure called Generalized Iterative Scaling which estimates the parameters of this particular model The goal of this report is to provide enough detail to re implement the maximum entropy models described in Ratnaparkhi Reynar and Ratnaparkhi Ratnaparkhi and also to provide a simple explanation of the max imum entropy formalism Introduction Many problems in natural language processing NLP can be re formulated as statistical classi cation problems in which the task is to estimate the probability of class a occurring with context b or p a b Contexts in NLP tasks usually include words and the exact context depends on the nature of the task for some tasks the context b may consist of just a single word while for others b may consist of several words and their associated syntactic labels Large text corpora usually contain some information about the cooccurrence of a s and b s but never enough to completely specify p a b for all possible a b pairs since the words in b are typically sparse The problem is then to nd a method for using the sparse evidence about the a s and b s to reliably estimate a probability model p a b Consider the Principle of Maximum Entropy Jaynes Good which states that the correct distribution p a b is that which maximizes en tropy or uncertainty subject to the constraints which represent evidence i e the facts known to the experimenter Jaynes discusses its advan tages in making inferences on the basis of partial information we must use that probability distribution which has maximum entropy sub ject to whatever is known This is the only unbiased assignment we can make to use any other would amount to arbitrary assumption of information which by hypothesis we do not have More explicitly if A denotes the set of possible classes and B denotes the set of possible contexts p should maximize the entropy H p X"
            },
            "slug": "A-Simple-Introduction-to-Maximum-Entropy-Models-for-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Simple Introduction to Maximum Entropy Models for Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The goal of this report is to provide enough detail to re implement the maximum entropy models described in Reynar and Ratnaparkhi and also to provide a simple explanation of the max imum entropy formalism."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 168
                            }
                        ],
                        "text": "He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi , 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6204420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6edceaf0fada3588ee5f036e944c1a00661df77a",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Identifying-Sentence-Reynar-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Identifying Sentence Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A trainable model for identifying sentence boundaries in raw text that can be trained easily on any genre of English, and should be trainable on any other Romanalphabet language."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1735632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b26fa1b848ed808a0511db34bce2426888f0b68",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge. In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy. Most existing statistical language models exploit the immediate past only. To extract information from further back in the document's history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from many sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, I apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the NE solution. Language modeling, Adaptive language modeling, Statistical language modeling, Maximum entropy, Speech recognition."
            },
            "slug": "Adaptive-Statistical-Language-Modeling;-A-Maximum-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Adaptive Statistical Language Modeling; A Maximum Entropy Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis views language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary), and applies the principle of Maximum Entropy to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c846ee00c6132d70429cc279e8577f63ed05e4",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "slug": "A-Linear-Observed-Time-Statistical-Parser-Based-on-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A statistical parser for natural language that obtains a parsing accuracy that surpasses the best previously published results on the Wall St. Journal domain, and it is shown that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2359882"
                        ],
                        "name": "George R. Krupka",
                        "slug": "George-R.-Krupka",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Krupka",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George R. Krupka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49386109"
                        ],
                        "name": "K. Hausman",
                        "slug": "K.-Hausman",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Hausman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hausman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "Also note that, when combined under MENE, the three weakest systems , MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 71
                            }
                        ],
                        "text": "All three of these elements are present in systems such as IsoQuest's (Krupka and Hausman, 1998), and their absence from MENE probably explains much of the reason why the MENE-only system failed to perform at the state-of-the-art."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "In addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18067056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fbdee94032da667c252eebfaa7b554f5f0379a2",
            "isKey": true,
            "numCitedBy": 105,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "IsoQuest used its commercial software product, NetOwl Extractor, for the MUC-7 Named Entity task. The product consists of a high-speed C engine that analyzes text based on a configuration file containing a pattern rule base and lexicon. IsoQuest used the NameTag Configuration to recognize proper names and other key phrases in text, and mapped the product\u2019s extraction tags to the MUC-7 NE tags. NetOwl Extractor provides access to the extracted information through a flexible API, and IsoQuest used a small application program to process the documents and write the SGML output."
            },
            "slug": "IsoQuest-Inc.:-Description-of-the-NetOwl\u2122-Extractor-Krupka-Hausman",
            "title": {
                "fragments": [],
                "text": "IsoQuest Inc.: Description of the NetOwl\u2122 Extractor System as Used for MUC-7"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "IsoQuest used its commercial software product, NetOwl Extractor, for the MUC-7 Named Entity task, and used the NameTag Configuration to recognize proper names and other key phrases in text, and mapped the product\u2019s extraction tags to the M UC-7 NE tags."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932753"
                        ],
                        "name": "A. Kehler",
                        "slug": "A.-Kehler",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Kehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kehler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12055395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a1dfcef6506b12a35cd1e1cd1ddcee42e9e8a9",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Certain applications require that the output of an information extraction system be probabilistic, so that a downstream system can reliably fuse the output with possibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI\u2019s FASTUS information extraction system."
            },
            "slug": "Probabilistic-Coreference-in-Information-Extraction-Kehler",
            "title": {
                "fragments": [],
                "text": "Probabilistic Coreference in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions and presents the results of initial experiments with several approaches to estimating such distributions in an application using SRI\u2019s FASTUS information extraction system."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 181
                            }
                        ],
                        "text": "More complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in (Berger et al., 1996) and (Della Pietra et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "Clearly a more sophisticated feature selection routine such as the ones in (Berger et al., 1996), or (Berger and Printz, 1998 ) would be required in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "This allows us to compute the conditional probability as follows (Berger et al., 1996): The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "More complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in (Berger et al., 1996) and (Della Pietra et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": true,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5914287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a574e320d899e7e82e341eb64baef7dfe8a24642",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems"
            },
            "slug": "A-Maximum-Entropy-Model-for-Part-Of-Speech-Tagging-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Part-Of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy and discusses the corpus consistency problems discovered during the implementation of these features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023469"
                        ],
                        "name": "D. Bikel",
                        "slug": "D.-Bikel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bikel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bikel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123937952"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 115174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9292ba3230f01b9f6990362fdf06783b9347bf6",
            "isKey": false,
            "numCitedBy": 919,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach."
            },
            "slug": "Nymble:-a-High-Performance-Learning-Name-finder-Bikel-Miller",
            "title": {
                "fragments": [],
                "text": "Nymble: a High-Performance Learning Name-finder"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 20
                            }
                        ],
                        "text": "For Proteus and the Manitoba system, this number seems to be'about 80 articles, because they show a degradation of performance at 40."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "Also note that, when combined under MENE, the three weakest systems , MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 71
                            }
                        ],
                        "text": "In addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17454561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39df62f0f1b87ec1685cf28ce795dc5c26e112fe",
            "isKey": true,
            "numCitedBy": 61,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Our main objective in participating MUC-7 is to investigate and experiment with the use of collocation statistics in information extraction. A collocation is a habitual word combination, such as \\weather a storm\", \\ le a lawsuit\", and \\the falling yen\". Collocation statistics refers to the frequency counts of the collocational relations extracted from a parsed corpus. For example, out of 6577 instances of \\addition\" in a corpus, 5190 was used as the object of \\in\". Out of 3214 instances of \\hire\", 12 of them take \\alien\" as the object."
            },
            "slug": "Using-Collocation-Statistics-in-Information-Lin",
            "title": {
                "fragments": [],
                "text": "Using Collocation Statistics in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The main objective in participating MUC-7 is to investigate and experiment with the use of collocation statistics in information extraction, which refers to the frequency counts of the collocational relations extracted from a parsed corpus."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270385"
                        ],
                        "name": "H. Printz",
                        "slug": "H.-Printz",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Printz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Printz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2699330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ab84dd5239a393ee5749a8ec772b5b7f27f82fb",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study the gain, a naturally-arising statistic from the theory of MEMD modeling [2], as a figure of merit for selecting features for an MEMD language model. We compare the gain with two popular alternatives-empirical activation and mutual information-and argue that the gain is the preferred statistic, on the grounds that it directly measures a feature's contribution to improving upon the base modeL"
            },
            "slug": "A-Comparison-of-Criteria-for-Maximum-Entropy/-Berger-Printz",
            "title": {
                "fragments": [],
                "text": "A Comparison of Criteria for Maximum Entropy/ Minimum Divergence Feature Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is argued that the gain is the preferred statistic, on the grounds that it directly measures a feature's contribution to improving upon the base modeL of an MEMD language model."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10820476,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d5bfbc7a36b3a39a0fa70ad09a83fbec052f7cef",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past five MUCs, New York University has clung faithfully to the idea that information extraction should begin with a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead."
            },
            "slug": "The-NYU-system-for-MUC-6-or-where's-the-syntax-Grishman",
            "title": {
                "fragments": [],
                "text": "The NYU system for MUC-6 or where's the syntax?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Over the past five MUCs, New York University has clung faithfully to the idea that information extraction should begin with a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950361"
                        ],
                        "name": "E. Marsh",
                        "slug": "E.-Marsh",
                        "structuredName": {
                            "firstName": "Elaine",
                            "lastName": "Marsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marsh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254427"
                        ],
                        "name": "D. Perzanowski",
                        "slug": "D.-Perzanowski",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Perzanowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Perzanowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 227
                            }
                        ],
                        "text": "On a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 (Marsh and Perzanowski, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13595867,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "2e5b428cc51d93201ac74afc6a645dd9133bf26f",
            "isKey": true,
            "numCitedBy": 152,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MUC-7-Evaluation-of-IE-Technology:-Overview-of-Marsh-Perzanowski",
            "title": {
                "fragments": [],
                "text": "MUC-7 Evaluation of IE Technology: Overview of Results"
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 153
                            }
                        ],
                        "text": "MENE's flexibility is due to its object-based treatment of the three essential cOmponents of a maximum entropy system: histories, futures, and features (Borthwick et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The system architecture of MENE was strongly influenced by the architecture of a maximum entropy language model jointly developed by these three authors at IBM Watson Labs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "lsoquest: Description of the netowl(tm) extractor system as used in muc-7"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Message Understanding Conference"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to identifying sentence structures"
            },
            "venue": {
                "fragments": [],
                "text": "Fifth Conference on Applied Natural Language Processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ltg: Description of the ne recognition system used for muc-7"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Message Understanding Conference (MUC-7)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy modeling toolkit, release 1.6 beta. http://www.mnemonic.com/software/memt, February. Includes documentation which has an overview of MaxEnt modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum entropy modeling toolkit, release 1.6 beta. http://www.mnemonic.com/software/memt, February. Includes documentation which has an overview of MaxEnt modeling"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Includes documentation which has an overview of MaxEnt modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 4,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Exploiting-Diverse-Knowledge-Sources-via-Maximum-in-Borthwick-Sterling/4df361d65a15ca9a7fc27c58c38b04d1f41e6f62?sort=total-citations"
}