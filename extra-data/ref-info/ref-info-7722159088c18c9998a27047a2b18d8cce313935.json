{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364013"
                        ],
                        "name": "A. O. Balan",
                        "slug": "A.-O.-Balan",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Balan",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 49
                            }
                        ],
                        "text": "This is consistent with the results reported in (Balan et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "We make no attempts to fit the shape of the limbs to the image measurements (Balan et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "This is discussed in greater detail in [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 200
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 423,
                                "start": 418
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 39
                            }
                        ],
                        "text": "As a convention from previous methods (Balan et al. 2005; Lan and Huttenlocher 2005; Sigal et al. 2004; Sigal and Black 2006) that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 132
                            }
                        ],
                        "text": "Alternative error measures that compute lower-bounds for sample- or kernel-based representations of the posterior are discussed in (Balan et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "Previous work (Balan et al. 2005; Deutscher and Reid 2005) has shown S performs well when combined with the edge likelihood E using (18), which we denote by E+S."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 440,
                                "start": 435
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 206
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "This error measure w as first introduced for 3D pose estimation and tracking in [74] and later extended in [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "This is consistent with the results reported in [7]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "This is discussed in greater detail in (Balan et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Alternative error measures that compute lower-bounds for ample- or kernel-based representations of the posteri r are discussed in [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "This error measure was first introduced for 3D pose estimation and tracking in (Sigal et al. 2004) and later extended in (Balan et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "L. Sigal ( ) University of Toronto, Dept. of Computer Science, 6 King\u2019s College Rd, Toronto, ON M5S 3H5, Canada e-mail: ls@cs.toronto.edu\nA.O. Balan \u00b7 M.J. Black Brown University, Dept. of Computer Science, 115 Waterman St, Box 1910, Providence, RI 02912, USA\nA.O. Balan e-mail: alb@cs.brown.edu\nM.J. Black e-mail: black@cs.brown.edu\nfor evaluating both 2D and 3D pose estimation and tracking algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 39
                            }
                        ],
                        "text": "As a convention from previous m ethods [7, 36, 74, 75] that have already used this error measure, we compute the 3D error in millimeters ( mm) and the 2D error directly in the image in pixels ( pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4608636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50f8292c2531457c2a3ad83307d8394bc0e9379d",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian estimation of 3D human motion from video sequences is quantitatively evaluated using synchronized, multi-camera, calibrated video and 3D ground truth poses acquired with a commercial motion capture system. While many methods for human pose estimation and tracking have been proposed, to date there has been no quantitative comparison. Our goal is to evaluate how different design choices influence tracking performance. Toward that end, we independently implemented two fairly standard Bayesian person trackers using two variants of particle filtering and propose an evaluation measure appropriate for assessing the quality of probabilistic tracking methods. In the Bayesian framework we compare various image likelihood functions and prior models of human motion that have been proposed in the literature. Our results suggest that in constrained laboratory environments, current methods perform quite well. Multiple cameras and background subtraction, however, are required to achieve reliable tracking suggesting that many current methods may be inappropriate in more natural settings. We discuss the implications of the study and the directions for future research that it entails"
            },
            "slug": "A-Quantitative-Evaluation-of-Video-based-3D-Person-Balan-Sigal",
            "title": {
                "fragments": [],
                "text": "A Quantitative Evaluation of Video-based 3D Person Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The Bayesian estimation of 3D human motion from video sequences is quantitatively evaluated using synchronized, multi-camera, calibrated video and 3D ground truth poses acquired with a commercial motion capture system to suggest that in constrained laboratory environments, current methods perform quite well."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "While no \u201cstandard\u201d algorithm exists in the community, we implemented a fairly common Bayesian filtering method based on the methods of Deutscher and Reid (2005) and Sidenbladh et al. (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 66
                            }
                        ],
                        "text": "Weaker implicit priors that utilize motion capture data directly (Sidenbladh et al. 2002) have also been effective."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 56
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 198
                            }
                        ],
                        "text": "The use of strong15 prior motion models are common with early work concentrating on switching dynamical models (Pavolvic et al. 1999) and eigen-models of cyclic motions (Ormoneit et al. 2000, 2001; Sidenbladh et al. 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 236
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u20262006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 227
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46737148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a39ac21415d028402c9e83443188b613a16961d",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic method for tracking 3D articulated human figures in monocular image sequences is presented. Within a Bayesian framework, we define a generative model of image appearance, a robust likelihood function based on image graylevel differences, and a prior probability distribution over pose and joint angles that models how humans move. The posterior probability distribution over model parameters is represented using a discrete set of samples and is propagated over time using particle filtering. The approach extends previous work on parameterized optical flow estimation to exploit a complex 3D articulated motion model. It also extends previous work on human motion tracking by including a perspective camera model, by modeling limb self occlusion, and by recovering 3D motion from a monocular sequence. The explicit posterior probability distribution represents ambiguities due to image matching, model singularities, and perspective projection. The method relies only on a frame-to-frame assumption of brightness constancy and hence is able to track people under changing viewpoints, in grayscale image sequences, and with complex unknown backgrounds."
            },
            "slug": "Stochastic-Tracking-of-3D-Human-Figures-Using-2D-Kjellstr\u00f6m-Black",
            "title": {
                "fragments": [],
                "text": "Stochastic Tracking of 3D Human Figures Using 2D Image Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A probabilistic method for tracking 3D articulated human figures in monocular image sequences that relies only on a frame-to-frame assumption of brightness constancy and hence is able to track people under changing viewpoints, in grayscale image sequences, and with complex unknown backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5697345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc9b263c1af95ea803c4f5c8888ef8e37f0cef80",
            "isKey": false,
            "numCitedBy": 818,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a vision system for the 3-D model-based tracking of unconstrained human movement. Using image sequences acquired simultaneously from multiple views, we recover the 3-D body pose at each time instant without the use of markers. The pose-recovery problem is formulated as a search problem and entails finding the pose parameters of a graphical human model whose synthesized appearance is most similar to the actual appearance of the real human in the multi-view images. The models used for this purpose are acquired from the images. We use a decomposition approach and a best-first technique to search through the high dimensional pose parameter space. A robust variant of chamfer matching is used as a fast similarity measure between synthesized and real edge images. We present initial tracking results from a large new Humans-in-Action (HIA) database containing more than 2500 frames in each of four orthogonal views. They contain subjects involved in a variety of activities, of various degrees of complexity, ranging from the more simple one-person hand waving to the challenging two-person close interaction in the Argentine Tango."
            },
            "slug": "3-D-model-based-tracking-of-humans-in-action:-a-Gavrila-Davis",
            "title": {
                "fragments": [],
                "text": "3-D model-based tracking of humans in action: a multi-view approach"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A vision system for the 3-D model-based tracking of unconstrained human movement and initial tracking results from a large new Humans-in-Action database containing more than 2500 frames in each of four orthogonal views are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775672"
                        ],
                        "name": "Dirk Ormoneit",
                        "slug": "Dirk-Ormoneit",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Ormoneit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk Ormoneit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2217697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3470656ac8e8cd50661bf908e94e1125d49134cd",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into \"cycles\". Then the mean and the principal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth transitions between cycles. The learned temporal model provides a prior probability distribution over human motions that can be used in a Bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3D motion."
            },
            "slug": "Learning-and-Tracking-Cyclic-Human-Motion-Ormoneit-Kjellstr\u00f6m",
            "title": {
                "fragments": [],
                "text": "Learning and Tracking Cyclic Human Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The learned temporal model provides a prior probability distribution over human motions that can be used in a Bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3D motion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197309"
                        ],
                        "name": "A. Kanaujia",
                        "slug": "A.-Kanaujia",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Kanaujia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kanaujia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34131475"
                        ],
                        "name": "Zhiguo Li",
                        "slug": "Zhiguo-Li",
                        "structuredName": {
                            "firstName": "Zhiguo",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiguo Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "\u2026statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 129
                            }
                        ],
                        "text": "Another form of inspection involves applying the estimated motion to a virtual character to see if the movements appear natural (Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "\u2026reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 53
                            }
                        ],
                        "text": "These approaches can be combined (Sigal et al. 2004; Sminchisescu et al. 2005), such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 25
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 223
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 139
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 212405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dffd149be34e7533d4c5663b7e0b819bb4537749",
            "isKey": true,
            "numCitedBy": 280,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a mixture density propagation algorithm to estimate 3D human motion in monocular video sequences based on observations encoding the appearance of image silhouettes. Our approach is discriminative rather than generative, therefore it does not require the probabilistic inversion of a predictive observation model. Instead, it uses a large human motion capture data-base and a 3D computer graphics human model in order to synthesize training pairs of typical human configurations together with their realistically rendered 2D silhouettes. These are used to directly learn to predict the conditional state distributions required for 3D body pose tracking and thus avoid using the generative 3D model for inference (the learned discriminative predictors can also be used, complementary, as importance samplers in order to improve mixing or initialize generative inference algorithms). We aim for probabilistically motivated tracking algorithms and for models that can represent complex multivalued mappings common in inverse, uncertain perception inferences. Our paper has three contributions: (1) we establish the density propagation rules for discriminative inference in continuous, temporal chain models; (2) we propose flexible algorithms for learning multimodal state distributions based on compact, conditional Bayesian mixture of experts models; and (3) we demonstrate the algorithms empirically on real and motion capture-based test sequences and compare against nearest-neighbor and regression methods."
            },
            "slug": "Discriminative-density-propagation-for-3D-human-Sminchisescu-Kanaujia",
            "title": {
                "fragments": [],
                "text": "Discriminative density propagation for 3D human motion estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The density propagation rules for discriminative inference in continuous, temporal chain models are established and flexible algorithms for learning multimodal state distributions based on compact, conditional Bayesian mixture of experts models are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32830733"
                        ],
                        "name": "S. Bhatia",
                        "slug": "S.-Bhatia",
                        "structuredName": {
                            "firstName": "Sidharth",
                            "lastName": "Bhatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhatia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 120
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 248
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 265
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 133
                            }
                        ],
                        "text": "It has since been also used for 3D tracking in (Li et al. 2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 523
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "As a convention from previous methods (Balan et al. 2005; Lan and Huttenlocher 2005; Sigal et al. 2004; Sigal and Black 2006) that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 34
                            }
                        ],
                        "text": "These approaches can be combined (Sigal et al. 2004; Sminchisescu et al. 2005), such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "This error measure w as first introduced for 3D pose estimation and tracking in [74] and later extended in [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "These approach es an be combined [74, 76], such that tracking benefits from automatic initialization and failure recovery in the f orm of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": "This error measure was first introduced for 3D pose estimation and tracking in (Sigal et al. 2004) and later extended in (Balan et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "L. Sigal ( ) University of Toronto, Dept. of Computer Science, 6 King\u2019s College Rd, Toronto, ON M5S 3H5, Canada e-mail: ls@cs.toronto.edu\nA.O. Balan \u00b7 M.J. Black Brown University, Dept. of Computer Science, 115 Waterman St, Box 1910, Providence, RI 02912, USA\nA.O. Balan e-mail: alb@cs.brown.edu\nM.J. Black e-mail: black@cs.brown.edu\nfor evaluating both 2D and 3D pose estimation and tracking algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 39
                            }
                        ],
                        "text": "As a convention from previous m ethods [7, 36, 74, 75] that have already used this error measure, we compute the 3D error in millimeters ( mm) and the 2D error directly in the image in pixels ( pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "The work of L. Sigal was conducted at Brown University."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14806670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc39b61661bc57c8239cd2678a09248c8d98e88f",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We pose the problem of 3D human tracking as one of inference in a graphical model. Unlike traditional kinematic tree representations, our model of the body is a collection of loosely-connected limbs. Conditional probabilities relating the 3D pose of connected limbs are learned from motion-captured training data. Similarly, we learn probabilistic models for the temporal evolution of each limb (forward and backward in time). Human pose and motion estimation is then solved with non-parametric belief propagation using a variation of particle filtering that can be applied over a general loopy graph. The loose-limbed model and decentralized graph structure facilitate the use of low-level visual cues. We adopt simple limb and head detectors to provide \"bottom-up\" information that is incorporated into the inference process at every time-step; these detectors permit automatic initialization and aid recovery from transient tracking failures. We illustrate the method by automatically tracking a walking person in video imagery using four calibrated cameras. Our experimental apparatus includes a marker-based motion capture system aligned with the coordinate frame of the calibrated cameras with which we quantitatively evaluate the accuracy of our 3D person tracker."
            },
            "slug": "Tracking-loose-limbed-people-Sigal-Bhatia",
            "title": {
                "fragments": [],
                "text": "Tracking loose-limbed people"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The problem of 3D human tracking as one of inference in a graphical model that is a collection of loosely-connected limbs and non-parametric belief propagation using a variation of particle filtering that can be applied over a general loopy graph is posed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 145
                            }
                        ],
                        "text": "\u20262004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "For 2D human pose/motion e stimation, quantitative evaluation is more common and typically uses hand-labeled data [30, 58, 59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 145
                            }
                        ],
                        "text": "\u20262004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 347
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 152
                            }
                        ],
                        "text": "For 2D human pose/motion estimation, quantitative evaluation is more common and typically uses hand-labeled data (Hua et al. 2005; Ramanan et al. 2005; Ramanan and Forsyth 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 117
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 481,
                                "start": 474
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 427,
                                "start": 423
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15039233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "768b9d18ebfc5ad2de18ab613d7baa0500239de8",
            "isKey": true,
            "numCitedBy": 324,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a tracker that can track moving people in long sequences without manual initialization. Moving people are modeled with the assumption that, while configuration can vary quite substantially from frame to frame, appearance does not. This leads to an algorithm that firstly builds a model of the appearance of the body of each individual by clustering candidate body segments, and then uses this model to find all individuals in each frame. Unusually, the tracker does not rely on a model of human dynamics to identify possible instances of people; such models are unreliable, because human motion is fast and large accelerations are common. We show our tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net. Experiments on video of real scenes demonstrate that this tracker can (a) count distinct individuals; (b) identify and track them; (c) recover when it loses track, for example, if individuals are occluded or briefly leave the view; (d) identify the configuration of the body largely correctly; and (e) is not dependent on particular models of human motion."
            },
            "slug": "Finding-and-tracking-people-from-the-bottom-up-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Finding and tracking people from the bottom up"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A tracker that can track moving people in long sequences without manual initialization is described and it is shown the tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070926374"
                        ],
                        "name": "M. Dimitrijevic",
                        "slug": "M.-Dimitrijevic",
                        "structuredName": {
                            "firstName": "Miodrag",
                            "lastName": "Dimitrijevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dimitrijevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2185019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3d02cc840a037a0c97a3c995972eee93b6f69bd",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-body-pose-detection-using-Bayesian-templates-Dimitrijevic-Lepetit",
            "title": {
                "fragments": [],
                "text": "Human body pose detection using Bayesian spatio-temporal templates"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956884"
                        ],
                        "name": "R\u00f3mer Rosales",
                        "slug": "R\u00f3mer-Rosales",
                        "structuredName": {
                            "firstName": "R\u00f3mer",
                            "lastName": "Rosales",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00f3mer Rosales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12126519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fc6c277261a004b378f2feec3310fa86bc0b91b",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach for estimating articulated body posture and motion from monocular video sequences is proposed. Human pose is defined as the instantaneous two dimensional configuration (i.e. the projection onto the image plane) of a single articulated body in terms of the position of a predetermined sets of joints. First, statistical segmentation of the human bodies from the background is performed and low-level visual features are found given the segmented body shape. The goal is to be able to map these generally low level visual features to body configurations. The system estimates different mappings, each one with a specific cluster in the visual feature space. Given a set of body motion sequences for training, unsupervised clustering is obtained via the Expectation Maximization algorithm. For each of the clusters, a function is estimated to build the mapping between low-level features to 2D pose. Given new visual features, a mapping from each cluster is performed to yield a set of possible poses. From this set, the system selects the most likely pose given the learned probability distribution and the visual feature of the proposed approach is characterized using real and artificially generated body postures, showing promising results."
            },
            "slug": "Inferring-body-pose-without-tracking-body-parts-Rosales-Sclaroff",
            "title": {
                "fragments": [],
                "text": "Inferring body pose without tracking body parts"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel approach for estimating articulated body posture and motion from monocular video sequences is proposed, characterized using real and artificially generated body postures, showing promising results."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database [27], initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms [20, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15480005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90a5a8d682a8b6fcb59b37edd70a3c6458ee5f19",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a motion exemplar approach for finding body configuration in monocular videos. A motion correlation technique is employed to measure the motion similarity at various space-time locations between the input video and stored video templates. These observations are used to predict the conditional state, distributions of exemplars and joint positions. Exemplar sequence selection and joint position estimation are then solved with approximate inference using Gibbs sampling and gradient ascent. The presented approach is able to find joint positions accurately for people with textured clothing. Results are presented on a dataset containing slow, fast and incline walk videos of various people from different view angles. The results demonstrate an overall improvement compared to previous methods."
            },
            "slug": "Human-Pose-Estimation-using-Motion-Exemplars-Fathi-Mori",
            "title": {
                "fragments": [],
                "text": "Human Pose Estimation using Motion Exemplars"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The presented approach is able to find joint positions accurately for people with textured clothing and demonstrates an overall improvement compared to previous methods."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12548071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76b8e3d11b5e3bff846ae606cbcb4aab1bf096ed",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Tracking body poses of multiple persons in monocular video is a challenging problem due to the high dimensionality of the state space and issues such as inter-occlusion of the persons' bodies. We proposed a three-stage approach with a multi-level state representation that enables a hierarchical estimation of 3D body poses. At the first stage, humans are tracked as blobs. In the second stage, parts such as face, shoulders and limbs are estimated and estimates are combined by grid-based belief propagation to infer 2D joint positions. The derived belief maps are used as proposal functions in the third stage to infer the 3D pose using data-driven Markov chain Monte Carlo. Experimental results on realistic indoor video sequences show that the method is able to track multiple persons during complex movement such as turning movement with inter-occlusion."
            },
            "slug": "Human-Pose-Tracking-Using-Multi-level-Structured-Lee-Nevatia",
            "title": {
                "fragments": [],
                "text": "Human Pose Tracking Using Multi-level Structured Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Experimental results on realistic indoor video sequences show that the proposed three-stage approach with a multi-level state representation that enables a hierarchical estimation of 3D body poses is able to track multiple persons during complex movement such as turning movement with inter-occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150925453"
                        ],
                        "name": "Rui Li",
                        "slug": "Rui-Li",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095482"
                        ],
                        "name": "T. Tian",
                        "slug": "T.-Tian",
                        "structuredName": {
                            "firstName": "Tai-Peng",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 232
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 55
                            }
                        ],
                        "text": "2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "It has since been also used for 3D tracking in (Li et al. 2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 27
                            }
                        ],
                        "text": "2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 173
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 182
                            }
                        ],
                        "text": "\u2026later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 292
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15504504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae3c6c70f2bdb24f86edadb80bec32331080268c",
            "isKey": true,
            "numCitedBy": 76,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Filtering based algorithms have become popular in tracking human body pose. Such algorithms can suffer the curse of dimensionality due to the high dimensionality of the pose state space; therefore, efforts have been dedicated to either smart sampling or reducing the dimensionality of the original pose state space. In this paper, a novel formulation that employs a dimensionality reduced state space for multi-hypothesis tracking is proposed. During off-line training, a mixture of factor analyzers is learned. Each factor analyzer can be thought of as a \u201clocal dimensionality reducer\u201d that locally approximates the pose manifold. Global coordination between local factor analyzers is achieved by learning a set of linear mixture functions that enforces agreement between local factor analyzers. The formulation allows easy bidirectional mapping between the original body pose space and the low-dimensional space. During online tracking, the clusters of factor analyzers are utilized in a multiple hypothesis tracking algorithm. Experiments demonstrate that the proposed algorithm tracks 3D body pose efficiently and accurately , even when self-occlusion, motion blur and large limb movements occur. Quantitative comparisons show that the formulation produces more accurate 3D pose estimates over time than those that can be obtained via a number of previously-proposed particle filtering based tracking algorithms."
            },
            "slug": "Monocular-Tracking-of-3D-Human-Motion-with-a-of-Li-Yang",
            "title": {
                "fragments": [],
                "text": "Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Quantitative comparisons show that the formulation produces more accurate 3D pose estimates over time than those that can be obtained via a number of previously-proposed particle filtering based tracking algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726358"
                        ],
                        "name": "A. Bissacco",
                        "slug": "A.-Bissacco",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bissacco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bissacco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15051824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f3e3bf38cd18692f68f78707de82735637d4598",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of estimating human pose in video sequences, where rough location has been determined. We exploit both appearance and motion information by defining suitable features of an image and its temporal neighbors, and learning a regression map to the parameters of a model of the human body using boosting techniques. Our algorithm can be viewed as a fast initialization step for human body trackers, or as a tracker itself. We extend gradient boosting techniques to learn a multi-dimensional map from (rotated and scaled) Haar features to the entire set of joint angles representing the full body pose. We test our approach by learning a map from image patches to body joint angles from synchronized video and motion capture walking data. We show how our technique enables learning an efficient real-time pose estimator, validated on publicly available datasets."
            },
            "slug": "Fast-Human-Pose-Estimation-using-Appearance-and-via-Bissacco-Yang",
            "title": {
                "fragments": [],
                "text": "Fast Human Pose Estimation using Appearance and Motion via Multi-Dimensional Boosting Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This work addresses the problem of estimating human pose in video sequences, where rough location has been determined, by defining suitable features of an image and its temporal neighbors, and learning a regression map to the parameters of a model of the human body using boosting techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984136"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 26
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used [1, 2, 26, 69, 76] for quantitative evaluatio n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 205
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 147
                            }
                        ],
                        "text": "\u2026measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 146
                            }
                        ],
                        "text": "\u2026as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 26
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 173
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 58
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7563707,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "954db9aa9f141ce9719d9ffc76c7677b27d9e45f",
            "isKey": true,
            "numCitedBy": 109,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a sparse Bayesian regression method for recovering 3D human body motion directly from silhouettes extracted from monocular video sequences. No detailed body shape model is needed, and realism is ensured by training on real human motion capture data. The tracker estimates 3D body pose by using Relevance Vector Machine regression to combine a learned autoregressive dynamical model with robust shape descriptors extracted automatically from image silhouettes. We studied several different combination methods, the most effective being to learn a nonlinear observation-update correction based on joint regression with respect to the predicted state and the observations. We demonstrate the method on a 54-parameter full body pose model, both quantitatively using motion capture based test sequences, and qualitatively on a test video sequence."
            },
            "slug": "Learning-to-track-3D-human-motion-from-silhouettes-Agarwal-Triggs",
            "title": {
                "fragments": [],
                "text": "Learning to track 3D human motion from silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A sparse Bayesian regression method for recovering 3D human body motion directly from silhouettes extracted from monocular video sequences, and demonstrates the method on a 54-parameter full body pose model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364013"
                        ],
                        "name": "A. O. Balan",
                        "slug": "A.-O.-Balan",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Balan",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111092372"
                        ],
                        "name": "James Davis",
                        "slug": "James-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108198574"
                        ],
                        "name": "H. Haussecker",
                        "slug": "H.-Haussecker",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Haussecker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Haussecker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "We make no attempts to fit the shape of the limbs to the image measurements (Balan et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "\u2026of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13181540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616c488dd3bfdc78d85137365d59cb29f75b20da",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of the research on video-based human motion capture assumes the body shape is known a priori and is represented coarsely (e.g. using cylinders or superquadrics to model limbs). These body models stand in sharp contrast to the richly detailed 3D body models used by the graphics community. Here we propose a method for recovering such models directly from images. Specifically, we represent the body using a recently proposed triangulated mesh model called SCAPE which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies. Previous work showed that the parameters of the SCAPE model could be estimated from marker-based motion capture data. Here we go further to estimate the parameters directly from image data. We define a cost function between image observations and a hypothesized mesh and formulate the problem as optimization over the body shape and pose parameters using stochastic search. Our results show that such rich generative models enable the automatic recovery of detailed human shape and pose from images."
            },
            "slug": "Detailed-Human-Shape-and-Pose-from-Images-Balan-Sigal",
            "title": {
                "fragments": [],
                "text": "Detailed Human Shape and Pose from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work represents the body using a recently proposed triangulated mesh model called SCAPE which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3330331"
                        ],
                        "name": "Marek Vondrak",
                        "slug": "Marek-Vondrak",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Vondrak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marek Vondrak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792217"
                        ],
                        "name": "O. Jenkins",
                        "slug": "O.-Jenkins",
                        "structuredName": {
                            "firstName": "Odest",
                            "lastName": "Jenkins",
                            "middleNames": [
                                "Chadwicke"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Jenkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "Lastly, priors based on abstracted (Brubaker et al. 2007) or full-body (Vondrak et al. 2008) physical simulations recently have been proposed for specific classes of motions (e.g. walking)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Lastly, priors based on abstracted [12] or full-body [88] physical simulations recently have been pro posed for specific classes of motions ( e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1321,
                                "start": 1317
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 258
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 350,
                                "start": 343
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "\u202610 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 782,
                                "start": 775
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8914159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f241981e3e7b1e4053284078e5c25592664794f",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Human motion tracking is an important problem in computer vision. Most prior approaches have concentrated on efficient inference algorithms and prior motion models; however, few can explicitly account for physical plausibility of recovered motion. The primary purpose of this work is to enforce physical plausibility in the tracking of a single articulated human subject. Towards this end, we propose a full-body 3D physical simulation-based prior that explicitly incorporates motion control and dynamics into the Bayesian filtering framework. We consider the humanpsilas motion to be generated by a ldquocontrol looprdquo. In this control loop, Newtonian physics approximates the rigid-body motion dynamics of the human and the environment through the application and integration of forces. Collisions generate interaction forces to prevent physically impossible hypotheses. This allows us to properly model human motion dynamics, ground contact and environment interactions. For efficient inference in the resulting high-dimensional state space, we introduce exemplar-based control strategy to reduce the effective search space. As a result we are able to recover the physically-plausible kinematic and dynamic state of the body from monocular and multi-view imagery. We show, both quantitatively and qualitatively, that our approach performs favorably with respect to standard Bayesian filtering methods."
            },
            "slug": "Physical-simulation-for-probabilistic-motion-Vondrak-Sigal",
            "title": {
                "fragments": [],
                "text": "Physical simulation for probabilistic motion tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A full-body 3D physical simulation-based prior that explicitly incorporates motion control and dynamics into the Bayesian filtering framework is proposed and is able to recover the physically-plausible kinematic and dynamic state of the body from monocular and multi-view imagery."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 56
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 55
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2751624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy."
            },
            "slug": "Tracking-people-with-twists-and-exponential-maps-Bregler-Malik",
            "title": {
                "fragments": [],
                "text": "Tracking people with twists and exponential maps"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences, and is the first computer vision based system able to process such challenging footage and recover complex motions with such high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364013"
                        ],
                        "name": "A. O. Balan",
                        "slug": "A.-O.-Balan",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Balan",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 293
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "\u2026templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14429389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23076c313612d4435f92de71cd05bfc1e845d52a",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and tracking of three-dimensional human body models has progressed rapidly but successful approaches typically rely on accurate foreground silhouettes obtained using background segmentation. There are many practical applications where such information is imprecise. Here we develop a new image likelihood function based on the visual appearance of the subject being tracked. We propose a robust, adaptive, appearance model based on the Wandering-Stable-Lost framework extended to the case of articulated body parts. The method models appearance using a mixture model that includes an adaptive template, frame-to-frame matching and an outlier process. We employ an annealed particle filtering algorithm for inference and take advantage of the 3D body model to predict selfocclusion and improve pose estimation accuracy. Quantitative tracking results are presented for a walking sequence with a 180 degree turn, captured with four synchronized and calibrated cameras and containing significant appearance changes and self-occlusion in each view."
            },
            "slug": "An-Adaptive-Appearance-Model-Approach-for-Object-Balan-Black",
            "title": {
                "fragments": [],
                "text": "An Adaptive Appearance Model Approach for Model-based Articulated Object Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A robust, adaptive, appearance model based on the Wandering-Stable-Lost framework extended to the case of articulated body parts is proposed, using a mixture model that includes an adaptive template, frame-to-frame matching and an outlier process."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6017383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7776d0a6bf3cdf9b3cd18b13d32c6babed84614b",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of probabilistically modeling 3D human motion for synthesis and tracking. Given the high dimensional nature of human motion, learning an explicit probabilistic model from available training data is currently impractical. Instead we exploit methods from texture synthesis that treat images as representing an implicit empirical distribution. These methods replace the problem of representing the probability of a texture pattern with that of searching the training data for similar instances of that pattern. We extend this idea to temporal data representing 3D human motion with a large database of example motions. To make the method useful in practice, we must address the problem of efficient search in a large training set; efficiency is particularly important for tracking. Towards that end, we learn a low dimensional linear model of human motion that is used to structure the example motion database into a binary tree. An approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time. This probabilistic tree search returns a particular sample human motion with probability approximating the true distribution of human motions in the database. This sampling method is suitable for use with particle filtering techniques and is applied to articulated 3D tracking of humans within a Bayesian framework. Successful tracking results are presented, along with examples of synthesizing human motion using the model."
            },
            "slug": "Implicit-Probabilistic-Models-of-Human-Motion-for-Kjellstr\u00f6m-Black",
            "title": {
                "fragments": [],
                "text": "Implicit Probabilistic Models of Human Motion for Synthesis and Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A low dimensional linear model of human motion is learned that is used to structure the example motion database into a binary tree and an approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917469"
                        ],
                        "name": "Shanon X. Ju",
                        "slug": "Shanon-X.-Ju",
                        "structuredName": {
                            "firstName": "Shanon",
                            "lastName": "Ju",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shanon X. Ju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5170789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e3b20fb94803d71910043059f402554aa5137b2",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the work of Black and Yacoob (1995) on the tracking and recognition of human facial expressions using parametrized models of optical flow to deal with the articulated motion of human limbs. We define a \"card-board person model\" in which a person's limbs are represented by a set of connected planar patches. The parametrized image motion of these patches in constrained to enforce articulated motion and is solved for directly using a robust estimation technique. The recovered motion parameters provide a rich and concise description of the activity that can be used for recognition. We propose a method for performing view-based recognition of human activities from the optical flow parameters that extends previous methods to cope with the cyclical nature of human motion. We illustrate the method with examples of tracking human legs of long image sequences."
            },
            "slug": "Cardboard-people:-a-parameterized-model-of-image-Ju-Black",
            "title": {
                "fragments": [],
                "text": "Cardboard people: a parameterized model of articulated image motion"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A method for performing view-based recognition of human activities from the optical flow parameters that extends previous methods to cope with the cyclical nature of human motion is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312794"
                        ],
                        "name": "X. Lan",
                        "slug": "X.-Lan",
                        "structuredName": {
                            "firstName": "Xiangyang",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "It has since been also used for 3D tracking in (Li et al. 2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 461,
                                "start": 458
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "As a convention from previous methods (Balan et al. 2005; Lan and Huttenlocher 2005; Sigal et al. 2004; Sigal and Black 2006) that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 421,
                                "start": 418
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10366469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46acb59184b8847d864a4c72d3d8c4d358e86392",
            "isKey": true,
            "numCitedBy": 64,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Tracking articulated objects in image sequences remains a challenging problem, particularly in terms of the ability to localize the individual parts of an object given self-occlusions and changes in viewpoint. In this paper we propose a two-dimensional spatio-temporal modeling approach that handles both self-occlusions and changes in viewpoint. We use a Bayesian framework to combine pictorial structure spatial models with hidden Markov temporal models. Inference for these combined models can be performed using dynamic programming and sampling methods. We demonstrate the approach for the problem of tracking a walking person, using silhouette data taken from a single camera viewpoint. Walking provides both strong spatial (kinematic) and temporal (dynamic) constraints, enabling the method to track limb positions in spite of simultaneous self-occlusion and viewpoint change."
            },
            "slug": "A-unified-spatio-temporal-articulated-model-for-Lan-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "A unified spatio-temporal articulated model for tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A two-dimensional spatio-temporal modeling approach that handles both self-occlusions and changes in viewpoint, using a Bayesian framework to combine pictorial structure spatial models with hidden Markov temporal models is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "Another form of inspection involves applying the estimated motion to a virtual character to see if the movements appear natural (Sminchisescu et al. 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "For example, hybr id Monte Carlo sampling [57], partitioned sampling [43], or covariance-scaled sampling [79] are all promising alter natives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 238
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 88
                            }
                        ],
                        "text": "We check for angles exceeding joint angle bounds and producing inter-penetrating limbs (Sminchisescu and Triggs 2003b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "These approaches can be combined (Sigal et al. 2004; Sminchisescu et al. 2005), such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 145
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "We check for angles exceeding joint angle bounds and producing inter-penetrating limbs [79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 223
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 146
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 545,
                                "start": 537
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14452190,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "84faf1c8950219da1df6d4050a92d00be015f53f",
            "isKey": true,
            "numCitedBy": 301,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for recovering three-dimensional (3D) human body motion from monocular video sequences based on a robust image matching metric, incorporation of joint limits and non-self-intersection constraints, and a new sample-and-refine search strategy guided by rescaled cost-function covariances. Monocular 3D body tracking is challenging: besides the difficulty of matching an imperfect, highly flexible, self-occluding model to cluttered image features, realistic body models have at least 30 joint parameters subject to highly nonlinear physical constraints, and at least a third of these degrees of freedom are nearly unobservable in any given monocular image. For image matching we use a carefully designed robust cost metric combining robust optical flow, edge energy, and motion boundaries. The nonlinearities and matching ambiguities make the parameter-space cost surface multimodal, ill-conditioned and highly nonlinear, so searching it is difficult. We discuss the limitations of CONDENSATION-like samplers, and describe a novel hybrid search algorithm that combines inflated-covariance-scaled sampling and robust continuous optimization subject to physical constraints and model priors. Our experiments on challenging monocular sequences show that robust cost modeling, joint and self-intersection constraints, and informed sampling are all essential for reliable monocular 3D motion estimation."
            },
            "slug": "Estimating-Articulated-Human-Motion-with-Covariance-Sminchisescu-Triggs",
            "title": {
                "fragments": [],
                "text": "Estimating Articulated Human Motion with Covariance Scaled Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "These experiments on challenging monocular sequences show that robust cost modeling, joint and self-intersection constraints, and informed sampling are all essential for reliable monocular 3D motion estimation."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "For 2D human pose/motion e stimation, quantitative evaluation is more common and typically uses hand-labeled data [30, 58, 59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 313
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 347
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "For 2D human pose/motion estimation, quantitative evaluation is more common and typically uses hand-labeled data (Hua et al. 2005; Ramanan et al. 2005; Ramanan and Forsyth 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 117
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 481,
                                "start": 474
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5574410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14eacd0e48a160bfc935cd4d419772f0110b1a0f",
            "isKey": true,
            "numCitedBy": 364,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an algorithm for finding and kinematically tracking multiple people in long sequences. Our basic assumption is that people tend to take on certain canonical poses, even when performing unusual activities like throwing a baseball or figure skating. We build a person detector that quite accurately detects and localizes limbs of people in lateral walking poses. We use the estimated limbs from a detection to build a discriminative appearance model; we assume the features that discriminate a figure in one frame will discriminate the figure in other frames. We then use the models as limb detectors in a pictorial structure framework, detecting figures in unrestricted poses in both previous and successive frames. We have run our tracker on hundreds of thousands of frames, and present and apply a methodology for evaluating tracking on such a large scale. We test our tracker on real sequences including a feature-length film, an hour of footage from a public park, and various sports sequences. We find that we can quite accurately automatically find and track multiple people interacting with each other while performing fast and unusual motions."
            },
            "slug": "Strike-a-pose:-tracking-people-by-finding-stylized-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Strike a pose: tracking people by finding stylized poses"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A person detector that quite accurately detects and localizes limbs of people in lateral walking poses is built, and an algorithm for finding and kinematically tracking multiple people in long sequences is developed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Xinyu Xu",
                        "slug": "Xinyu-Xu",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913552"
                        ],
                        "name": "Baoxin Li",
                        "slug": "Baoxin-Li",
                        "structuredName": {
                            "firstName": "Baoxin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoxin Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 279
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1075,
                                "start": 1071
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "\u2026T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 304793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f0065f7522d4509edc21acac3aad4344f4ae978",
            "isKey": true,
            "numCitedBy": 56,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Inference in 3D articulated human body tracking is challenging due to the high dimensionality and nonlinearity of the parameter-space. We propose a particle filter with Rao-Blackwellisation which marginalizes part of the state variables by exploiting the correlation between the right-side and the left-side joint Euler angles. The correlation is naturally induced by the symmetric and repetitive patterns in specific human activities. A novel algorithm is proposed to learn the correlation from the training data using partial least square regression. The learned correlation is then used as motion prior in designing the Rao-Blackwellised particle filter, which estimates only one group of state variables using the Monte Carlo method, leaving the other group being exactly computed through an analytical filter that utilizes the learned motion correlation. We evaluate the effectiveness of the motion correlation for 3D articulated human body tracking. The accuracy of the proposed 3D tracker is quantitatively assessed based on the distance between the true and the estimated marker positions. Extensive experiments with multi-camera walking sequences from the HumanEva-I/II data set show that (i) the proposed tracker achieves significantly lower estimation error than both the annealed particle filter and the standard particle filter; and (ii) the learned motion correlation generalizes well to motion performed by subjects other than the training subject."
            },
            "slug": "Learning-Motion-Correlation-for-Tracking-Human-Body-Xu-Li",
            "title": {
                "fragments": [],
                "text": "Learning Motion Correlation for Tracking Articulated Human Body with a Rao-Blackwellised Particle Filter"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A particle filter with Rao-Blackwellisation which marginalizes part of the state variables by exploiting the correlation between the right-side and the left-side joint Euler angles is proposed, which achieves significantly lower estimation error than both the annealed particle filter and the standard particle filter."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089978"
                        ],
                        "name": "D. Damen",
                        "slug": "D.-Damen",
                        "structuredName": {
                            "firstName": "Dima",
                            "lastName": "Damen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Damen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14174818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32fad849f86bb99d824150e9373c352219edd4ed",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method for detecting objects such as bags carried by pedestrians depicted in short video sequences. In common with earlier work [1,2] on the same problem, the method starts by averaging aligned foreground regions of a walking pedestrian to produce a representation of motion and shape (known as a temporal template) that has some immunity to noise in foreground segmentations and phase of the walking cycle. Our key novelty is for carried objects to be revealed by comparing the temporal templates against view-specific exemplars generated offline for unencumbered pedestrians. A likelihood map obtained from this match is combined in a Markov random field with a map of prior probabilities for carried objects and a spatial continuity assumption, from which we obtain a segmentation of carried objects using the MAP solution. We have re-implemented the earlier state of the art method [1] and demonstrate a substantial improvement in performance for the new method on the challenging PETS2006 dataset [3]. Although developed for a specific problem, the method could be applied to the detection of irregularities in appearance for other categories of object that move in a periodic fashion."
            },
            "slug": "Detecting-Carried-Objects-in-Short-Video-Sequences-Damen-Hogg",
            "title": {
                "fragments": [],
                "text": "Detecting Carried Objects in Short Video Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new method for detecting objects such as bags carried by pedestrians depicted in short video sequences by comparing the temporal templates against view-specific exemplars generated offline for unencumbered pedestrians, which yields a segmentation of carried objects using the MAP solution."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144196926"
                        ],
                        "name": "S. Wachter",
                        "slug": "S.-Wachter",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Wachter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wachter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144721252"
                        ],
                        "name": "H. Nagel",
                        "slug": "H.-Nagel",
                        "structuredName": {
                            "firstName": "Hans-Hellmut",
                            "lastName": "Nagel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 104
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 18
                            }
                        ],
                        "text": "Kalman filtering (Wachter and Nagel 1999) is another alternative that may be appropriate for the applications where one can ensure that the likelihood and the dynamics are uni-modal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5916504,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a6fad775a27cc172479fdb6f291e361b35fd2f1e",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Quantitative geometric descriptions of the movements of persons are obtained by fitting the projection of a three-dimensional person model to consecutive frames of an image sequence. The kinematic of the person model is given by a homogeneous transformation tree and its body parts are modeled by right-elliptical cones. The values of a varying number of degrees of freedom (DOFs; body joints, position, and orientation of the person relative to the camera) can be determined according to the application and the kind of image sequence. The determination of the DOFs is understood as an estimation problem which is solved by an iterated extended Kalman filter (IEKF). For this purpose, the person model is augmented by a simple motion model of constant velocity for all DOFs which is used in the prediction step of the IEKF. In the update step, both region and edge information are used. Various experiments demonstrate the efficiency of our approach."
            },
            "slug": "Tracking-Persons-in-Monocular-Image-Sequences-Wachter-Nagel",
            "title": {
                "fragments": [],
                "text": "Tracking Persons in Monocular Image Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The person model is augmented by a simple motion model of constant velocity for all DOFs which is used in the prediction step of the IEKF and in the update step, both region and edge information are used."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706204"
                        ],
                        "name": "I. Kakadiaris",
                        "slug": "I.-Kakadiaris",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Kakadiaris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kakadiaris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 170
                            }
                        ],
                        "text": "To obtain some form of ground truth, previous approaches have resorted to custom action-specific schemes; e.g. motion of the arm along a circular path of known diameter (Kakadiaris and Metaxas 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "motion of the arm along a circular path of known diameter [34] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18748251,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d24117d83ad925d837ed0b7d6aa065140fb0248",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for the 3D model-based tracking of human body parts. To mitigate the difficulties arising due to occlusion among body parts, we employ multiple calibrated cameras in a mutually orthogonal configuration. In addition, we develop criteria for a time varying active selection of a set of cameras to track the motion of a particular human part. In particular, at every frame, each camera tracks a number of parts depending on the visibility of these parts and the observability of their predicted motion from the specific camera. To relate points on the occluding contours of the parts to points on their models we apply concepts from projective geometry. Then, within the physics-based framework we compute the generalized forces applied from the parts' occluding contours to model points of the body parts. These forces update the translational and rotational degrees of freedom of the model, such as to minimize the discrepancy between the sensory data and the estimated model state. We present initial tracking results from a series of experiments involving the recovery of complex 3D motions in the presence of significant occlusion."
            },
            "slug": "Model-based-estimation-of-3D-human-motion-with-on-Kakadiaris-Metaxas",
            "title": {
                "fragments": [],
                "text": "Model-based estimation of 3D human motion with occlusion based on active multi-viewpoint selection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Initial tracking results from a series of experiments involving the recovery of complex 3D motions in the presence of significant occlusion are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880628"
                        ],
                        "name": "Daniel Vlasic",
                        "slug": "Daniel-Vlasic",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Vlasic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Vlasic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34332381"
                        ],
                        "name": "Ilya Baran",
                        "slug": "Ilya-Baran",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Baran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Baran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752521"
                        ],
                        "name": "W. Matusik",
                        "slug": "W.-Matusik",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Matusik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Matusik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145492783"
                        ],
                        "name": "Jovan Popovi\u0107",
                        "slug": "Jovan-Popovi\u0107",
                        "structuredName": {
                            "firstName": "Jovan",
                            "lastName": "Popovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jovan Popovi\u0107"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 73
                            }
                        ],
                        "text": "Current research in non-marker-based methods for capturing human motion (Vlasic et al. 2008) may prove to be viable alternatives in a few years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 230280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af8990bc767da54a3bfb4c01e04f9bd1ed09396d",
            "isKey": false,
            "numCitedBy": 574,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Details in mesh animations are difficult to generate but they have great impact on visual quality. In this work, we demonstrate a practical software system for capturing such details from multi-view video recordings. Given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer, our system captures the motion of both the skeleton and the shape. The output mesh animation is enhanced with the details observed in the image silhouettes. For example, a performance in casual loose-fitting clothes will generate mesh animations with flowing garment motions. We accomplish this with a fast pose tracking method followed by nonrigid deformation of the template to fit the silhouettes. The entire process takes less than sixteen seconds per frame and requires no markers or texture cues. Captured meshes are in full correspondence making them readily usable for editing operations including texturing, deformation transfer, and deformation model learning."
            },
            "slug": "Articulated-mesh-animation-from-multi-view-Vlasic-Baran",
            "title": {
                "fragments": [],
                "text": "Articulated mesh animation from multi-view silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work demonstrates a practical software system for capturing details in mesh animations from multi-view video recordings given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700569"
                        ],
                        "name": "T. Moeslund",
                        "slug": "T.-Moeslund",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Moeslund",
                            "middleNames": [
                                "Baltzer"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moeslund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700008"
                        ],
                        "name": "E. Granum",
                        "slug": "E.-Granum",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Granum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Granum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 225
                            }
                        ],
                        "text": "A short summary of different approaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to (Gavrila 1999) and (Moeslund and Granum 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "A short summary of different ap proaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to [24] and [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14970359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee709de9c13034bcae1fff96b5abb7312bd097e",
            "isKey": false,
            "numCitedBy": 1978,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition. Each process is discussed and divided into subprocesses and/or categories of methods to provide a reference to describe and compare the more than 130 publications covered by the survey. References are included throughout the paper to exemplify important issues and their relations to the various methods. A number of general assumptions used in this research field are identified and the character of these assumptions indicates that the research field is still in an early stage of development. To evaluate the state of the art, the major application areas are identified and performances are analyzed in light of the methods presented in the survey. Finally, suggestions for future research directions are offered."
            },
            "slug": "A-Survey-of-Computer-Vision-Based-Human-Motion-Moeslund-Granum",
            "title": {
                "fragments": [],
                "text": "A Survey of Computer Vision-Based Human Motion Capture"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented, with a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6068544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "906fbff169dbb5eb774bdc43220c3c199c3b1b11",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Reliably recovering 3D human pose from monocular video requires models that bias the estimates towards typical human poses and motions. We construct priors for people tracking using the Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a recently introduced probabilistic dimensionality reduction model that combines the advantages of latent variable models\u2014a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction\u2014with those of spectral manifold learning methods\u2014no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM not only provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors."
            },
            "slug": "People-Tracking-with-the-Laplacian-Eigenmaps-Latent-Lu-Carreira-Perpi\u00f1\u00e1n",
            "title": {
                "fragments": [],
                "text": "People Tracking with the Laplacian Eigenmaps Latent Variable Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that LELVM not only provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779035"
                        ],
                        "name": "B. Rosenhahn",
                        "slug": "B.-Rosenhahn",
                        "structuredName": {
                            "firstName": "Bodo",
                            "lastName": "Rosenhahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rosenhahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068931"
                        ],
                        "name": "U. Kersting",
                        "slug": "U.-Kersting",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Kersting",
                            "middleNames": [
                                "Gustav"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Kersting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116584614"
                        ],
                        "name": "Andrew W. B. Smith",
                        "slug": "Andrew-W.-B.-Smith",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Smith",
                            "middleNames": [
                                "W.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew W. B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2227039"
                        ],
                        "name": "J. Gurney",
                        "slug": "J.-Gurney",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Gurney",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gurney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729664"
                        ],
                        "name": "R. Klette",
                        "slug": "R.-Klette",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Klette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Klette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 122
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 130
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 65
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion (Wang and Rehg 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 57
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in Sect."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 122
                            }
                        ],
                        "text": "2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u20261998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8813784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6e86db712c8b3ff6e1e6338c563f58bf1f76a7f",
            "isKey": true,
            "numCitedBy": 65,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this contribution we present a silhouette based human motion estimation system. The system components contain silhouette extraction based on level sets, a correspondence module, which relates image data to model data and a pose estimation module. Experiments are done in a four camera setup and we estimate the model components with 21 degrees of freedom in two frames per second. Finally, we perform a comparison of the motion estimation system with a marker based tracking system to perform a quantitative error analysis. The results show the applicability of the system for marker-less sports movement analysis."
            },
            "slug": "A-System-for-Marker-Less-Human-Motion-Estimation-Rosenhahn-Kersting",
            "title": {
                "fragments": [],
                "text": "A System for Marker-Less Human Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show the applicability of the system for marker-less sports movement analysis and a comparison of the motion estimation system with a marker based tracking system to perform a quantitative error analysis."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 756,
                                "start": 752
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u20262004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 317
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 484,
                                "start": 477
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 719,
                                "start": 712
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 293
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowled g ) introduced in [77] and later extended in [42]; Gaussian Processes Latent Variable Models [86], Gaussian Processes Dynamical Models [85] and Factor Analyzers [41] are popular and effective choices particularly for instances w here little training data is available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 227
                            }
                        ],
                        "text": "\u2026templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 230
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                }
            ],
            "corpusId": 518708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ced298a2a8e39127b8adecfa56256f5c693f3a25",
            "isKey": true,
            "numCitedBy": 518,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We advocate the use of Gaussian Process Dynamical Models (GPDMs) for learning human pose and motion priors for 3D people tracking. A GPDM provides a lowdimensional embedding of human motion data, with a density function that gives higher probability to poses and motions close to the training data. With Bayesian model averaging a GPDM can be learned from relatively small amounts of data, and it generalizes gracefully to motions outside the training set. Here we modify the GPDM to permit learning from motions with significant stylistic variation. The resulting priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and significant occlusions."
            },
            "slug": "3D-People-Tracking-with-Gaussian-Process-Dynamical-Urtasun-Fleet",
            "title": {
                "fragments": [],
                "text": "3D People Tracking with Gaussian Process Dynamical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work modifications the GPDM to permit learning from motions with significant stylistic variation, and the resulting priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and significant occlusions."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128317"
                        ],
                        "name": "David Knossow",
                        "slug": "David-Knossow",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Knossow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Knossow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898850"
                        ],
                        "name": "R\u00e9mi Ronfard",
                        "slug": "R\u00e9mi-Ronfard",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Ronfard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Ronfard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "The INRIA Perception Group also employed a similar approach for collection of ground truth data (Knossow et al. 2008), however,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 84
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "Typically hardware systems similar to the one proposed here have been employed (Knossow et al. 2008) where the video and motion capture data were captured either independently (and synchronized in software off-line) or with hardware synchronization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 97
                            }
                        ],
                        "text": "The INRIA Perception Group also employed a similar approach for collection of ground truth data (Knossow et al. 2008), however,\n2http://www.cc.gt.atl.ga.us/grads/w/Ping.Wang/Project/ FigureTracking.html.\nonly the multi-view video data is currently made available to the public."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 65
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion (Wang and Rehg 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 57
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in Sect."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2170101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3feb30eb10407c5525e0e656ac09e71636c32e55",
            "isKey": true,
            "numCitedBy": 60,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nThis paper addresses the problem of human motion tracking from multiple image sequences. The human body is described by five articulated mechanical chains and human body-parts are described by volumetric primitives with curved surfaces. If such a surface is observed with a camera, an extremal contour appears in the image whenever the surface turns smoothly away from the viewer. We describe a method that recovers human motion through a kinematic parameterization of these extremal contours. The method exploits the fact that the observed image motion of these contours is a function of both the rigid displacement of the surface and of the relative position and orientation between the viewer and the curved surface. First, we describe a parameterization of an extremal-contour point velocity for the case of developable surfaces. Second, we use the zero-reference kinematic representation and we derive an explicit formula that links extremal contour velocities to the angular velocities associated with the kinematic model. Third, we show how the chamfer-distance may be used to measure the discrepancy between predicted extremal contours and observed image contours; moreover we show how the chamfer distance can be used as a differentiable multi-valued function and how the tracker based on this distance can be cast into a continuous non-linear optimization framework. Fourth, we describe implementation issues associated with a practical human-body tracker that may use an arbitrary number of cameras. One great methodological and practical advantage of our method is that it relies neither on model-to-image, nor on image-to-image point matches. In practice we model people with 5 kinematic chains, 19 volumetric primitives, and 54 degrees of freedom; We observe silhouettes in images gathered with several synchronized and calibrated cameras. The tracker has been successfully applied to several complex motions gathered at 30\u00a0frames/second.\n"
            },
            "slug": "Human-Motion-Tracking-with-a-Kinematic-of\u00a0Extremal-Knossow-Ronfard",
            "title": {
                "fragments": [],
                "text": "Human Motion Tracking with a Kinematic Parameterization of\u00a0Extremal Contours"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a method that recovers human motion through a kinematic parameterization of an extremal-contour point velocity for the case of developable surfaces and describes implementation issues associated with a practical human-body tracker that may use an arbitrary number of cameras."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 180
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter responses [66, 70]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 236
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1255196,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c368eb34697350e2d9921a5354ddda76813408c3",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper address the problems of modeling the appearance of humans and distinguishing human appearance from the appearance of general scenes. We seek a model of appearance and motion that is generic in that it accounts for the ways in which people's appearance varies and, at the same time, is specific enough to be useful for tracking people in natural scenes. Given a 3D model of the person projected into an image we model the likelihood of observing various image cues conditioned on the predicted locations and orientations of the limbs. These cues are taken to be steered filter responses corresponding to edges, ridges, and motion-compensated temporal differences. Motivated by work on the statistics of natural scenes, the statistics of these filter responses for human limbs are learned from training images containing hand-labeled limb regions. Similarly, the statistics of the filter responses in general scenes are learned to define a \u201cbackground\u201d distribution. The likelihood of observing a scene given a predicted pose of a person is computed, for each limb, using the likelihood ratio between the learned foreground (person) and background distributions. Adopting a Bayesian formulation allows cues to be combined in a principled way. Furthermore, the use of learned distributions obviates the need for hand-tuned image noise models and thresholds. The paper provides a detailed analysis of the statistics of how people appear in scenes and provides a connection between work on natural image statistics and the Bayesian tracking of people."
            },
            "slug": "Learning-the-Statistics-of-People-in-Images-and-Kjellstr\u00f6m-Black",
            "title": {
                "fragments": [],
                "text": "Learning the Statistics of People in Images and Video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The paper provides a detailed analysis of the statistics of how people appear in scenes and provides a connection between work on natural image statistics and the Bayesian tracking of people."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2709174"
                        ],
                        "name": "L. M\u00fcndermann",
                        "slug": "L.-M\u00fcndermann",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "M\u00fcndermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M\u00fcndermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2876425"
                        ],
                        "name": "S. Corazza",
                        "slug": "S.-Corazza",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Corazza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Corazza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144886132"
                        ],
                        "name": "T. Andriacchi",
                        "slug": "T.-Andriacchi",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Andriacchi",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Andriacchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 92091,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1bfc9f74935035c595e95801513942eb771232b0",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach for accurate markerless motion capture combining a precise tracking algorithm with a database of articulated models is presented. The tracking approach employs an articulated iterative closest point algorithm with soft-joint constraints for tracking body segments in visual hull sequences. The database of articulated models is derived from a combination of human shapes and anthropometric data, contains a large variety of models and closely mimics variations found in the human population. The database provides articulated models that closely match the outer appearance of the visual hulls, e.g. matches overall height and volume. This information is paired with a kinematic chain enhanced through anthropometric regression equations. Deviations in the kinematic chain from true joint center locations are compensated by the soft-joint constraints approach. As a result accurate and a more anatomical correct outcome is obtained suitable for biomechanical and clinical applications. Joint kinematics obtained using this approach closely matched joint kinematics obtained from a marker based motion capture system."
            },
            "slug": "Accurately-measuring-human-movement-using-ICP-with-M\u00fcndermann-Corazza",
            "title": {
                "fragments": [],
                "text": "Accurately measuring human movement using articulated ICP with soft-joint constraints and a repository of articulated models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A novel approach for accurate markerless motion capture combining a precise tracking algorithm with a database of articulated models is presented, which closely matched joint kinematics obtained from a marker based motion capture system."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108130121"
                        ],
                        "name": "Jiayong Zhang",
                        "slug": "Jiayong-Zhang",
                        "structuredName": {
                            "firstName": "Jiayong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiayong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143980462"
                        ],
                        "name": "R. Collins",
                        "slug": "R.-Collins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Collins",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689241"
                        ],
                        "name": "Yanxi Liu",
                        "slug": "Yanxi-Liu",
                        "structuredName": {
                            "firstName": "Yanxi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanxi Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 303
                            }
                        ],
                        "text": "Available Yes No Yes No Yes Dataset Content Motion Walk Walk Dance Many Walk Jog Dance Exercise Throw/Catch Jumping Jacks Gesture Box Combo Appearance Natural Natural / Natural / MoCap Suit Natural MoCap Suit MoCap Suit Ground Truth Content 3D 2D None 3D 2D Source MoCap MoCap / None MoCap Manual Label [92] Manual Label"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database [27], initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms [20, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 207
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database (Gross and Shi 2001), initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms (Fathi et al. 2007; Zhang et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5064803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212425def3dc2c3024138e73e95db0a2c21906bb",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a 3-level hierarchical model for localizing human bodies in still images from arbitrary viewpoints. We first fit a simple tree-structured model defined on a small landmark set along the body contours by Dynamic Programming (DP). The output is a series of proposal maps that encode the probabilities of partial body configurations. Next, we fit a mixture of view-dependent models by Sequential Monte Carlo (SMC), which handles self-occlusion, anthropometric constraints, and large viewpoint changes. DP and SMC are designed to search in opposite directions such that the DP proposals are utilized effectively to initialize and guide the SMC inference. This hybrid strategy of combining deterministic and stochastic search ensures both the robustness and efficiency of DP, and the accuracy of SMC. Finally, we fit an expanded mixture model with increased landmark density through local optimization. The model hierarchy is trained on a large number of gait images. Extensive tests on cluttered images with varying poses including walking, dancing and various types of sports activities demonstrate the feasibility of the proposed approach."
            },
            "slug": "Body-Localization-in-Still-Images-Using-Models-and-Zhang-Luo",
            "title": {
                "fragments": [],
                "text": "Body Localization in Still Images Using Hierarchical Models and Hybrid Search"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A 3-level hierarchical model for localizing human bodies in still images from arbitrary viewpoints by Dynamic Programming, Sequential Monte Carlo, and an expanded mixture model with increased landmark density through local optimization is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779035"
                        ],
                        "name": "B. Rosenhahn",
                        "slug": "B.-Rosenhahn",
                        "structuredName": {
                            "firstName": "Bodo",
                            "lastName": "Rosenhahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rosenhahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145156858"
                        ],
                        "name": "H. Seidel",
                        "slug": "H.-Seidel",
                        "structuredName": {
                            "firstName": "Hans-Peter",
                            "lastName": "Seidel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seidel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1324590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26559d0860acadebf42035fd3bd8619ac7e63a98",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an approach to use prior knowledge in the particle filter framework for 3D tracking, i.e. estimating the state parameters such as joint angles of a 3D object. The probability of the object's states, including correlations between the state parameters, is learned a priori from training samples. We introduce a framework that integrates this knowledge into the family of particle filters and particularly into the annealed particle filter scheme. Furthermore, we show that the annealed particle filter also works with a variational model for level set based image segmentation that does not rely on background subtraction and, hence, does not depend on a static background. In our experiments, we use a four camera set-up for tracking the lower part of a human body by a kinematic model with 18 degrees of freedom. We demonstrate the increased accuracy due to the prior knowledge and the robustness of our approach to image distortions. Finally, we compare the results of our multi-view tracking system quantitatively to the outcome of an industrial marker based tracking system."
            },
            "slug": "Learning-for-Multi-view-3D-Tracking-in-the-Context-Gall-Rosenhahn",
            "title": {
                "fragments": [],
                "text": "Learning for Multi-view 3D Tracking in the Context of Particle Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An approach to use prior knowledge in the particle filter framework for 3D tracking, i.e. estimating the state parameters such as joint angles of a 3D object, is presented and a framework that integrates this knowledge into the family of particle filters and particularly into the annealed particle filter scheme is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ISVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403822242"
                        ],
                        "name": "Thomas F. El-Maraghi",
                        "slug": "Thomas-F.-El-Maraghi",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "El-Maraghi",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas F. El-Maraghi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 207
                            }
                        ],
                        "text": "\u2026templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11413173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c",
            "isKey": false,
            "numCitedBy": 1174,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework for learning robust, adaptive appearance models to be used for motion-based tracking of natural objects. The approach involves a mixture of stable image structure, learned over long time courses, along with 2-frame motion information and an outlier process. An online EM-algorithm is used to adapt the appearance model parameters over time. An implementation of this approach is developed for an appearance model based on the filter responses from a steerable pyramid. This model is used in a motion-based tracking algorithm to provide robustness in the face of image outliers, such as those caused by occlusions. It also provides the ability to adapt to natural changes in appearance, such as those due to facial expressions or variations in 3D pose. We show experimental results on a variety of natural image sequences of people moving within cluttered environments."
            },
            "slug": "Robust-online-appearance-models-for-visual-tracking-Jepson-Fleet",
            "title": {
                "fragments": [],
                "text": "Robust online appearance models for visual tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A framework for learning robust, adaptive appearance models to be used for motion-based tracking of natural objects and provides the ability to adapt to natural changes in appearance, such as those due to facial expressions or variations in 3D pose."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321919"
                        ],
                        "name": "Gr\u00e9gory Rogez",
                        "slug": "Gr\u00e9gory-Rogez",
                        "structuredName": {
                            "firstName": "Gr\u00e9gory",
                            "lastName": "Rogez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gr\u00e9gory Rogez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856452"
                        ],
                        "name": "Jonathan Rihan",
                        "slug": "Jonathan-Rihan",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rihan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Rihan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145686644"
                        ],
                        "name": "S. Ramalingam",
                        "slug": "S.-Ramalingam",
                        "structuredName": {
                            "firstName": "Srikumar",
                            "lastName": "Ramalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ramalingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398367150"
                        ],
                        "name": "C. Orrite-Uru\u00f1uela",
                        "slug": "C.-Orrite-Uru\u00f1uela",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Orrite-Uru\u00f1uela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Orrite-Uru\u00f1uela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 213
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13351972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ab12d3270214cc1c327260ac0b26be544923bd8",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses human pose recognition from video sequences by formulating it as a classification problem. Unlike much previous work we do not make any assumptions on the availability of clean segmentation. The first step of this work consists in a novel method of aligning the training images using 3D Mocap data. Next we define classes by discretizing a 2D manifold whose two dimensions are camera viewpoint and actions. Our main contribution is a pose detection algorithm based on random forests. A bottom-up approach is followed to build a decision tree by recursively clustering and merging the classes at each level. For each node of the decision tree we build a list of potentially discriminative features using the alignment of training images; in this paper we consider Histograms of Orientated Gradient (HOG). We finally grow an ensemble of trees by randomly sampling one of the selected HOG blocks at each node. Our proposed approach gives promising results with both fixed and moving cameras."
            },
            "slug": "Randomized-trees-for-human-pose-detection-Rogez-Rihan",
            "title": {
                "fragments": [],
                "text": "Randomized trees for human pose detection"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper addresses human pose recognition from video sequences by formulating it as a classification problem and considers Histograms of Orientated Gradient (HOG), a pose detection algorithm based on random forests."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988571"
                        ],
                        "name": "G. Hua",
                        "slug": "G.-Hua",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50118130"
                        ],
                        "name": "Ying Wu",
                        "slug": "Ying-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "For 2D human pose/motion e stimation, quantitative evaluation is more common and typically uses hand-labeled data [30, 58, 59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 103
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 114
                            }
                        ],
                        "text": "For 2D human pose/motion estimation, quantitative evaluation is more common and typically uses hand-labeled data (Hua et al. 2005; Ramanan et al. 2005; Ramanan and Forsyth 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 136
                            }
                        ],
                        "text": "\u2026et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7566190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9533193ce23a9aa79ec3aebf8ccd4b4fb6844994",
            "isKey": true,
            "numCitedBy": 133,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a statistical formulation for 2D human pose estimation from single images. The human body configuration is modeled by a Markov network and the estimation problem is to infer pose parameters from image cues such as appearance, shape, edge, and color. From a set of hand labeled images, we accumulate prior knowledge of 2D body shapes by learning their low-dimensional representations for inference of pose parameters. A data driven belief propagation Monte Carlo algorithm, utilizing importance sampling functions built from bottom-up visual cues, is proposed for efficient probabilistic inference. Contrasted to the few sequential statistical formulations in the literature, our algorithm integrates both top-down as well as bottom-up reasoning mechanisms, and can carry out the inference tasks in parallel. Experimental results demonstrate the potency and effectiveness of the proposed algorithm in estimating 2D human pose from single images."
            },
            "slug": "Learning-to-estimate-human-pose-with-data-driven-Hua-Yang",
            "title": {
                "fragments": [],
                "text": "Learning to estimate human pose with data driven belief propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A data driven belief propagation Monte Carlo algorithm, utilizing importance sampling functions built from bottom-up visual cues, is proposed for efficient probabilistic inference for 2D human pose estimation from single images."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153024876"
                        ],
                        "name": "J. P. Lewis",
                        "slug": "J.-P.-Lewis",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 278
                            }
                        ],
                        "text": "\u2026that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow (Baker et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 217
                            }
                        ],
                        "text": "It is worth noting that similar efforts have been made in related areas including the development of datasets for face detection [55, 56], human gait identification [27, 67], dense stereo vision [68] and optical flow [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 316800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804836b8ad86ef8042e3dcbd45442a52f031ee03",
            "isKey": false,
            "numCitedBy": 2377,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1)\u00a0sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2)\u00a0realistic synthetic sequences, (3)\u00a0high frame-rate video used to study interpolation error, and (4)\u00a0modified stereo sequences of static scenes. In addition to the average angular error used by Barron et\u00a0al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them."
            },
            "slug": "A-Database-and-Evaluation-Methodology-for-Optical-Baker-Scharstein",
            "title": {
                "fragments": [],
                "text": "A Database and Evaluation Methodology for Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms and analyzes the results obtained to date to draw a large number of conclusions."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984136"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 26
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used [1, 2, 26, 69, 76] for quantitative evaluatio n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 205
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 147
                            }
                        ],
                        "text": "\u2026measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 146
                            }
                        ],
                        "text": "\u2026as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 26
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 173
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 58
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 302682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3845d9e62540b8e2406343f801e026b562299ae0",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning based method for recovering 3D human body pose from single images and monocular image sequences. Our approach requires neither an explicit body model nor prior labelling of body pans in the image. Instead, it recovers pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes. For robustness against local silhouette segmentation errors, silhouette shape is encoded by histogram-of-shape-contexts descriptors. For the main regression, we evaluate both regularized least squares and relevance vector machine (RVM) regressors over both linear and kernel bases. The RVM's provide much sparser regressors without compromising performance, and kernel bases give a small but worthwhile improvement in performance. For realism and good generalization with respect to viewpoints, we train the regressors on images resynthesized from real human motion capture data, and test it both quantitatively on similar independent test data, and qualitatively on a real image sequence. Mean angular errors of 6-7 degrees are obtained - a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "slug": "3D-human-pose-from-silhouettes-by-relevance-vector-Agarwal-Triggs",
            "title": {
                "fragments": [],
                "text": "3D human pose from silhouettes by relevance vector regression"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work describes a learning based method for recovering 3D human body pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes, and results are a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963230"
                        ],
                        "name": "Huazhong Ning",
                        "slug": "Huazhong-Ning",
                        "structuredName": {
                            "firstName": "Huazhong",
                            "lastName": "Ning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huazhong Ning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143836295"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 195
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1496310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa988a20e88bf658a5ceb836f9ea786aa5a7b0ff",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of recovering 3D human pose from a single monocular image, using a discriminative bag-of-words approach. In previous work, the visual words are learned by unsupervised clustering algorithms. They capture the most common patterns and are good features for coarse-grain recognition tasks like object classification. But for those tasks which deal with subtle differences such as pose estimation, such representation may lack the needed discriminative power. In this paper, we propose to jointly learn the visual words and the pose regressors in a supervised manner. More specifically, we learn an individual distance metric for each visual word to optimize the pose estimation performance. The learned metrics rescale the visual words to suppress unimportant dimensions such as those corresponding to background. Another contribution is that we design an appearance and position context (APC) local descriptor that achieves both selectivity and invariance while requiring no background subtraction. We test our approach on both a quasi-synthetic dataset and a real dataset (HumanEva) to verify its effectiveness. Our approach also achieves fast computational speed thanks to the integral histograms used in APC descriptor extraction and fast inference of pose regressors."
            },
            "slug": "Discriminative-learning-of-visual-words-for-3D-pose-Ning-Xu",
            "title": {
                "fragments": [],
                "text": "Discriminative learning of visual words for 3D human pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes to jointly learn the visual words and the pose regressors in a supervised manner to optimize the pose estimation performance, and designs an appearance and position context (APC) local descriptor that achieves both selectivity and invariance while requiring no background subtraction."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577358"
                        ],
                        "name": "P. Srinivasan",
                        "slug": "P.-Srinivasan",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 447,
                                "start": 437
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1026,
                                "start": 1022
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 144
                            }
                        ],
                        "text": "\u2026N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6668959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "811ec5a6139466aa2195c4fe883597e7d7e80f7e",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing humans, estimating their pose and segmenting their body parts are key to high-level image understanding. Because humans are highly articulated, the range of deformations they undergo makes this task extremely challenging. Previous methods have focused largely on heuristics or pairwise part models in approaching this problem. We propose a bottom-up parsing of increasingly more complete partial body masks guided by a parse tree. At each level of the parsing process, we evaluate the partial body masks directly via shape matching with exemplars, without regard to how the parses are formed. The body is evaluated as a whole, not the sum of its constituent parses, unlike previous approaches. Multiple image segmentations are included at each of the levels of the parsing, to augment existing parses or to introduce ones. Our method yields both a pose estimate as well as a segmentation of the human. We demonstrate competitive results on this challenging task with relatively few training examples on a dataset of baseball players with wide pose variation. Our method is comparatively simple and could be easily extended to other objects."
            },
            "slug": "Bottom-up-Recognition-and-Parsing-of-the-Human-Body-Srinivasan-Shi",
            "title": {
                "fragments": [],
                "text": "Bottom-up Recognition and Parsing of the Human Body"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a bottom-up parsing of increasingly more complete partial body masks guided by a parse tree, and yields both a pose estimate as well as a segmentation of the human."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052391642"
                        ],
                        "name": "J. O'Rourke",
                        "slug": "J.-O'Rourke",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Rourke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Rourke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699200"
                        ],
                        "name": "N. Badler",
                        "slug": "N.-Badler",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Badler",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Badler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 172
                            }
                        ],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s (Hogg 1983; O\u2019Rourke and Badler 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 161
                            }
                        ],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s [28, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15680007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df0428c30b8aab4f7e6f367e70126efdfb8fc45",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "A system capable of analyzing image sequences of human motion is described. The system is structured as a feedback loop between high and low levels: predictions are made at the semantic level and verifications are sought at the image level. The domain of human motion lends itself to a model-driven analysis, and the system includes a detailed model of the human body. All information extracted from the image is interpreted through a constraint network based on the structure of the human model. A constraint propagation operator is defined and its theoretical properties outlined. An implementation of this operator is described, and results of the analysis system for short image sequences are presented."
            },
            "slug": "Model-based-image-analysis-of-human-motion-using-O'Rourke-Badler",
            "title": {
                "fragments": [],
                "text": "Model-based image analysis of human motion using constraint propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A system capable of analyzing image sequences of human motion is described, structured as a feedback loop between high and low levels: predictions are made at the semantic level and verifications are sought at the image level."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152672574"
                        ],
                        "name": "J. Deutscher",
                        "slug": "J.-Deutscher",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Deutscher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deutscher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The MSE between the predicted and observed silhouette values for these points is computed (Deutscher and Reid 2005):\n\u2212 logpf (yt |xt ) \u221d 1|{\u03befxt }| \u2211 j ( 1 \u2212 Mft ( \u03be f xt (j ) ))2 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "While no \u201cstandard\u201d algori thm exists in the community, we implemented a fairly common Bayesian filtering method based on the methods of Deut scher and Reid [17] and Sidenbladh et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 90
                            }
                        ],
                        "text": "A variation of SIR is the Annealed Particle Filter (APF) introduced for human tracking by Deutscher and Reid (2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 128
                            }
                        ],
                        "text": "5.1.3.1 Edge-based Likelihood Functions We detect edges using image gradients that have been thresholded to obtain binary maps (Deutscher and Reid 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "This can be thought of as representing the edg e probability [17] at a given pixel."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 113
                            }
                        ],
                        "text": "The simplest model applicable to generic motions assumes no change in state from one time to the next: x\u0304t = xt\u22121 (Deutscher and Reid 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "The reader is referred to (Deutscher and Reid 2005) for a more detailed discussion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 134
                            }
                        ],
                        "text": "While no \u201cstandard\u201d algorithm exists in the community, we implemented a fairly common Bayesian filtering method based on the methods of Deutscher and Reid (2005) and Sidenbladh et al. (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "Previous work (Balan et al. 2005; Deutscher and Reid 2005) has shown S performs well when combined with the edge likelihood E using (18), which we denote by E+S."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3 D body pose into the image (or set of images) and visually asses sing how the estimates explain the image [17, 21, 60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 230
                            }
                        ],
                        "text": "The noise is drawn from a multi-variate Gaussian with diagonal covariance where the sampling standard deviation of each body angle is set to equal the maximum absolute inter-frame angular difference for a particular motion style (Deutscher and Reid 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 156
                            }
                        ],
                        "text": "Many image features could be used, including appearance models and optical flow constraints, however, most common approaches rely on silhouettes and edges (Deutscher and Reid 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 160
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3D body pose into the image (or set of images) and visually assessing how the estimates explain the image (Deutscher and Reid 2005; Felzenszwalb and Huttenlocher 2005; Ren et al. 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "A variation of SIR is the Annealed Part icle Filter (APF) introduced for human tracking by Deutscher and Reid [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "We detect edges using image gradients that have been thresho ld d t obtain binary maps [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Many image features could be used, including appearance models and optical flow constraints, however, most common approaches rely on silho uettes and edges [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The reader is referred to [17] for a more detailed discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 61
                            }
                        ],
                        "text": "This can be thought of as representing the edge probability (Deutscher and Reid 2005) at a given pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9342230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38199c8b99acd210b3be834d911303056f593f24",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a modified particle filter which is shown to be effective at searching the high-dimensional configuration spaces (c. 30 + dimensions) encountered in visual tracking of articulated body motion. The algorithm uses a continuation principle, based on annealing, to introduce the influence of narrow peaks in the fitness function, gradually. The new algorithm, termed annealed particle filtering, is shown to be capable of recovering full articulated body motion efficiently. A mechanism for achieving a soft partitioning of the search space is described and implemented, and shown to improve the algorithm\u2019s performance. Likewise, the introduction of a crossover operator is shown to improve the effectiveness of the search for kinematic trees (such as a human body). Results are given for a variety of agile motions such as walking, running and jumping."
            },
            "slug": "Articulated-Body-Motion-Capture-by-Stochastic-Deutscher-Reid",
            "title": {
                "fragments": [],
                "text": "Articulated Body Motion Capture by Stochastic Search"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A modified particle filter is developed which is shown to be effective at searching the high-dimensional configuration spaces encountered in visual tracking of articulated body motion and to be capable of recovering full articulated bodymotion efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306925"
                        ],
                        "name": "Sudeep Sarkar",
                        "slug": "Sudeep-Sarkar",
                        "structuredName": {
                            "firstName": "Sudeep",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudeep Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152613893"
                        ],
                        "name": "Z. Liu",
                        "slug": "Z.-Liu",
                        "structuredName": {
                            "firstName": "Zongyi",
                            "lastName": "Liu",
                            "middleNames": [
                                "Joe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365346"
                        ],
                        "name": "Isidro Robledo Vega",
                        "slug": "Isidro-Robledo-Vega",
                        "structuredName": {
                            "firstName": "Isidro",
                            "lastName": "Vega",
                            "middleNames": [
                                "Robledo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isidro Robledo Vega"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136478"
                        ],
                        "name": "P. Grother",
                        "slug": "P.-Grother",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Grother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 188
                            }
                        ],
                        "text": "\u2026that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow (Baker et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7693282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb6b14397f69bbfbc52cbd1a04bc6302a8b938d7",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Identification of people by analysis of gait patterns extracted from video has recently become a popular research problem. However, the conditions under which the problem is \"solvable\" are not understood or characterized. To provide a means for measuring progress and characterizing the properties of gait recognition, we introduce the humanlD gait challenge problem. The challenge problem consists of a baseline algorithm, a set of 12 experiments, and a large data set. The baseline algorithm estimates silhouettes by background subtraction and performs recognition by temporal correlation of silhouettes. The 12 experiments are of increasing difficulty, as measured by the baseline algorithm, and examine the effects of five covariates on performance. The covariates are: change in viewing angle, change in shoe type, change in walking surface, carrying or not carrying a briefcase, and elapsed time between sequences being compared. Identification rates for the 12 experiments range from 78 percent on the easiest experiment to 3 percent on the hardest. All five covariates had statistically significant effects on performance, with walking surface and time difference having the greatest impact. The data set consists of 1,870 sequences from 122 subjects spanning five covariates (1.2 gigabytes of data). This infrastructure supports further development of gait recognition algorithms and additional experiments to understand the strengths and weaknesses of new algorithms. The experimental results are presented, the more detailed is the possible meta-analysis and greater is the understanding. It is this potential from the adoption of this challenge problem that represents a radical departure from traditional computer vision research methodology."
            },
            "slug": "The-humanID-gait-challenge-problem:-data-sets,-and-Sarkar-Phillips",
            "title": {
                "fragments": [],
                "text": "The humanID gait challenge problem: data sets, performance, and analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The humanlD gait challenge problem is introduced, to provide a means for measuring progress and characterizing the properties of gait recognition, and represents a radical departure from traditional computer vision research methodology."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144658464"
                        ],
                        "name": "V. Pavlovic",
                        "slug": "V.-Pavlovic",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Pavlovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pavlovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144530541"
                        ],
                        "name": "T. Cham",
                        "slug": "T.-Cham",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Cham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "The use of strong18 prior motion models are common with early work concentratin g o switching dynamical models [54] and 18By strong prior motion models here we mean models that bias in fere ce towards a particular pre-defined class of motions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 112
                            }
                        ],
                        "text": "The use of strong15 prior motion models are common with early work concentrating on switching dynamical models (Pavolvic et al. 1999) and eigen-models of cyclic motions (Ormoneit et al. 2000, 2001; Sidenbladh et al. 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 476872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "127e36764bb179174b24b614dc2b8263ee56944f",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. However most work on tracking and synthesizing figure motion has employed either simple, generic dynamic models or highly specific hand-tailored ones. Recently, a broad class of learning and inference algorithms for time-series models have been successfully cast in the framework of dynamic Bayesian networks (DBNs). This paper describes a novel DBN-based switching linear dynamic system (SLDS) model and presents its application to figure motion analysis. A key feature of our approach is an approximate Viterbi inference technique for overcoming the intractability of exact inference in mixed-state DBNs. We present experimental results for learning figure dynamics from video data and show promising initial results for tracking, interpolation, synthesis, and classification using learned models."
            },
            "slug": "A-dynamic-Bayesian-network-approach-to-figure-using-Pavlovic-Rehg",
            "title": {
                "fragments": [],
                "text": "A dynamic Bayesian network approach to figure tracking using learned dynamic models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel DBN-based switching linear dynamic system (SLDS) model that is an approximate Viterbi inference technique for overcoming the intractability of exact inference in mixed-state DBNs is described and its application to figure motion analysis is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 58
                            }
                        ],
                        "text": "2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 139
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 267
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 55
                            }
                        ],
                        "text": "2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 165
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 133
                            }
                        ],
                        "text": "It has since been also used for 3D tracking in (Li et al. 2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 104
                            }
                        ],
                        "text": "As a convention from previous methods (Balan et al. 2005; Lan and Huttenlocher 2005; Sigal et al. 2004; Sigal and Black 2006) that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 25
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 47
                            }
                        ],
                        "text": "2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1570800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46626dce354feb5e21fde1095cd436e2a7d0c03a",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Part-based tree-structured models have been widely used for 2D articulated human pose-estimation. These approaches admit efficient inference algorithms while capturing the important kinematic constraints of the human body as a graphical model. These methods often fail however when multiple body parts fit the same image region resulting in global pose estimates that poorly explain the overall image evidence. Attempts to solve this problem have focused on the use of strong prior models that are limited to learned activities such as walking. We argue that the problem actually lies with the image observations and not with the prior. In particular, image evidence for each body part is estimated independently of other parts without regard to self-occlusion. To address this we introduce occlusion-sensitive local likelihoods that approximate the global image likelihood using per-pixel hidden binary variables that encode the occlusion relationships between parts. This occlusion reasoning introduces interactions between non-adjacent body parts creating loops in the underlying graphical model. We deal with this using an extension of an approximate belief propagation algorithm (PAMPAS). The algorithm recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods."
            },
            "slug": "Measure-Locally,-Reason-Globally:-Articulated-Pose-Sigal-Black",
            "title": {
                "fragments": [],
                "text": "Measure Locally, Reason Globally: Occlusion-sensitive Articulated Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An extension of an approximate belief propagation algorithm (PAMPAS) that recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747779"
                        ],
                        "name": "Aaron Hertzmann",
                        "slug": "Aaron-Hertzmann",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Hertzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Hertzmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 165
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "\u2026(to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowled g ) introduced in [77] and later extended in [42]; Gaussian Processes Latent Variable Models [86], Gaussian Processes Dynamical Models [85] and Factor Analyzers [41] are popular and effective choices particularly for instances w here little training data is available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 912116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c15fe581f5521b16f04258689f8622b3a0019bd",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We advocate the use of scaled Gaussian process latent variable models (SGPLVM) to learn prior models of 3D human pose for 3D people tracking. The SGPLVM simultaneously optimizes a low-dimensional embedding of the high-dimensional pose data and a density function that both gives higher probability to points close to training data and provides a nonlinear probabilistic mapping from the low-dimensional latent space to the full-dimensional pose space. The SGPLVM is a natural choice when only small amounts of training data are available. We demonstrate our approach with two distinct motions, golfing and walking. We show that the SGPLVM sufficiently constrains the problem such that tracking can be accomplished with straightforward deterministic optimization."
            },
            "slug": "Priors-for-people-tracking-from-small-training-sets-Urtasun-Fleet",
            "title": {
                "fragments": [],
                "text": "Priors for people tracking from small training sets"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown that the SGPLVM sufficiently constrains the problem such that tracking can be accomplished with straightforward deterministic optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9177303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5d6d6f5d9caaba221d785f0b92d07ce2bfa3a48",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to detect a human figure image and localize his joints and limbs along with their associated pixel masks. In this work we attempt to tackle this problem in a general setting. The dataset we use is a collection of sports news photographs of baseball players, varying dramatically in pose and clothing. The approach that we take is to use segmentation to guide our recognition algorithm to salient bits of the image. We use this segmentation approach to build limb and torso detectors, the outputs of which are assembled into human figures. We present quantitative results on torso localization, in addition to shortlisted full body configurations."
            },
            "slug": "Recovering-human-body-configurations:-combining-and-Mori-Ren",
            "title": {
                "fragments": [],
                "text": "Recovering human body configurations: combining segmentation and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses segmentation to build limb and torso detectors, the outputs of which are assembled into human figures, and presents quantitative results on torso localization, in addition to shortlisted full body configurations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867160"
                        ],
                        "name": "F. D. L. Torre",
                        "slug": "F.-D.-L.-Torre",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Torre",
                            "middleNames": [
                                "De",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. D. L. Torre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 198
                            }
                        ],
                        "text": "The use of strong15 prior motion models are common with early work concentrating on switching dynamical models (Pavolvic et al. 1999) and eigen-models of cyclic motions (Ormoneit et al. 2000, 2001; Sidenbladh et al. 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 262
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u20262006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5476898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2dfa8d94e1d8fb771d9016a702ffdcd637e7da5",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a framework for constructing a linear subspace model of image appearance for complex articulated 3D figures such as humans and other animals. A commercial motion capture system provides 3D data that is aligned with images of subjects performing various activities. Portions of a limb's image appearance are seen from multiple views and for multiple subjects. From these partial views, weighted principal component analysis is used to construct a linear subspace representation of the \"unwrapped\" image appearance of each limb. The linear subspaces provide a generative model of the object appearance that is exploited in a Bayesian particle filtering tracking system. Results of tracking single limbs and walking humans are presented."
            },
            "slug": "A-framework-for-modeling-the-appearance-of-3D-Kjellstr\u00f6m-Torre",
            "title": {
                "fragments": [],
                "text": "A framework for modeling the appearance of 3D articulated figures"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A framework for constructing a linear subspace model of image appearance for complex articulated 3D figures such as humans and other animals and results of tracking single limbs and walking humans are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686665"
                        ],
                        "name": "A. Telea",
                        "slug": "A.-Telea",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Telea",
                            "middleNames": [
                                "Cristian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Telea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 66
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood [80, 81] that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 917172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e523ee43d4dc4fd4ecd9b0563ca484f11a99ea",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel similarity measure (likelihood) for estimating three-dimensional human pose from image silhouettes in model-based vision applications. One of the challenges in such approaches is the construction of a model-to-image likelihood that truly reflects the good configurations of the problem. This is hard, commonly due to the violation of consistency principle resulting in the introduction of spurious, unrelated peaks/minima that make the search for model localization difficult. We introduce an entirely continuous formulation which enforces model estimation consistency by means of an attraction/explanation silhouette-based term pair. We subsequently show how the proposed method provides significant consolidation and improved attraction zone around the desired likelihood configurations and elimination of some of the spurious ones. Finally, we present a skeleton-based smoothing method for the image silhouettes that stabilizes and accelerates the search process."
            },
            "slug": "Human-Pose-Estimation-from-Silhouettes-A-Consistent-Sminchisescu-Telea",
            "title": {
                "fragments": [],
                "text": "Human Pose Estimation from Silhouettes - A Consistent Approach Using Distance Level Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work introduces an entirely continuous formulation which enforces model estimation consistency by means of an attraction/explanation silhouette-based term pair and presents a novel similarity measure (likelihood) for estimating three-dimensional human pose from image silhouettes in model-based vision applications."
            },
            "venue": {
                "fragments": [],
                "text": "WSCG"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5283941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2069e28d8e1619093bfa2f00577f8b0d69f6837c",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A major difficulty for 3D (three-dimensional) human body tracking from monocular image sequences is the near nonobservability of kinematic degrees of freedom that generate motion in depth. For known link (body segment) lengths, the strict nonobservabilities reduce to twofold 'forwards/backwards flipping' ambiguities for each link. These imply 2/sup # links/ formal inverse kinematics solutions for the full model, and hence linked groups of O(2/sup # links/) local minima in the model-image matching cost function. Choosing the wrong minimum leads to rapid mistracking, so for reliable tracking, rapid methods of investigating alternative minima within a group are needed. Previous approaches to this have used generic search methods that do not exploit the specific problem structure. Here, we complement these by using simple kinematic reasoning to enumerate the tree of possible forwards/backwards flips, thus greatly speeding the search within each linked group of minima. Our methods can be used either deterministically, or within stochastic 'jump-diffusion' style search processes. We give experimental results on some challenging monocular human tracking sequences, showing how the new kinematic-flipping based sampling method improves and complements existing ones."
            },
            "slug": "Kinematic-jump-processes-for-monocular-3D-human-Sminchisescu-Triggs",
            "title": {
                "fragments": [],
                "text": "Kinematic jump processes for monocular 3D human tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work uses simple kinematic reasoning to enumerate the tree of possible forwards/backwards flips, thus greatly speeding the search within each linked group of minima in the model-image matching cost function."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898850"
                        ],
                        "name": "R\u00e9mi Ronfard",
                        "slug": "R\u00e9mi-Ronfard",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Ronfard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Ronfard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [ 63 ] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed for tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60,  63 , 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9478443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac7f973658b55563f4d56e5b763c9049dd1034e0",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting people in images is a key problem for video indexing, browsing and retrieval. The main difficulties are the large appearance variations caused by action, clothing, illumination, viewpoint and scale. Our goal is to find people in static video frames using learned models of both the appearance of body parts (head, limbs, hands), and of the geometry of their assemblies. We build on Forsyth & Fleck's general 'body plan' methodology and Felzenszwalb & Huttenlocher's dynamic programming approach for efficiently assembling candidate parts into 'pictorial structures'. However we replace the rather simple part detectors used in these works with dedicated detectors learned for each body part using SupportVector Machines (SVMs) or RelevanceVector Machines (RVMs). We are not aware of any previous work using SVMs to learn articulated body plans, however they have been used to detect both whole pedestrians and combinations of rigidly positioned subimages (typically, upper body, arms, and legs) in street scenes, under a wide range of illumination, pose and clothing variations. RVMs are SVM-like classifiers that offer a well-founded probabilistic interpretation and improved sparsity for reduced computation. We demonstrate their benefits experimentally in a series of results showing great promise for learning detectors in more general situations."
            },
            "slug": "Learning-to-Parse-Pictures-of-People-Ronfard-Schmid",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Pictures of People"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work builds on Forsyth & Fleck's general 'body plan' methodology and Felzenszwalb & Huttenlocher's dynamic programming approach for efficiently assembling candidate parts into 'pictorial structures' but replaces the rather simple part detectors used in these works with dedicated detectors learned for each body part using SupportVector Machines (SVMs) or Relevance Vector Machines (RVMs)."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 232
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9981819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef9b6db5e02f1db18069cf8f42cd39c16446a740",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative approaches to human pose inference involve mapping visual observations to articulated body configurations. Current probabilistic approaches to learn this mapping have been limited in their ability to handle domains with a large number of activities that require very large training sets. We propose an online probabilistic regression scheme for efficient inference of complex, high- dimensional, and multimodal mappings. Our technique is based on a local mixture of Gaussian processes, where locality is defined based on both appearance and pose, and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space. The mixture components are defined online in very small neighborhoods, so learning and inference is extremely efficient. When the mapping is one-to-one, we derive a bound on the approximation error of local regression (vs. global regression) for monotonically decreasing co- variance functions. Our method can determine when training examples are redundant given the rest of the database, and use this criteria for pruning. We report results on synthetic (Poser) and real (Humaneva) pose databases, obtaining fast and accurate pose estimates using training set sizes up to 105."
            },
            "slug": "Sparse-probabilistic-regression-for-human-pose-Urtasun-Darrell",
            "title": {
                "fragments": [],
                "text": "Sparse probabilistic regression for activity-independent human pose inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An online probabilistic regression scheme for efficient inference of complex, high- dimensional, and multimodal mappings, based on a local mixture of Gaussian processes, where locality is defined based on both appearance and pose, and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152344400"
                        ],
                        "name": "T. Roberts",
                        "slug": "T.-Roberts",
                        "structuredName": {
                            "firstName": "Timothy J.",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739987"
                        ],
                        "name": "I. Ricketts",
                        "slug": "I.-Ricketts",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Ricketts",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ricketts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4474264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd824e1187478016ee87fda50d44764420efb25a",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of human appearance is presented for efficient pose estimation from real-world images. In common with related approaches, a high-level model defines a space of configurations which can be associated with image measurements and thus scored. A search is performed to identify good configuration(s). Such an approach is challenging because the configuration space is high dimensional, the search is global, and the appearance of humans in images is complex due to background clutter, shape uncertainty and texture."
            },
            "slug": "Human-Pose-Estimation-Using-Learnt-Probabilistic-Roberts-McKenna",
            "title": {
                "fragments": [],
                "text": "Human Pose Estimation Using Learnt Probabilistic Region Similarities and Partial Configurations"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A model of human appearance is presented for efficient pose estimation from real-world images using a high-level model which defines a space of configurations which can be associated with image measurements and thus scored."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312794"
                        ],
                        "name": "X. Lan",
                        "slug": "X.-Lan",
                        "structuredName": {
                            "firstName": "Xiangyang",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 183
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 106
                            }
                        ],
                        "text": "It has since been also used for 3D tracking in (Li et al. 2006) and for 2D pose estimation evaluation in (Lan and Huttenlocher 2005; Sigal and Black 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 260
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 461,
                                "start": 458
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "As a convention from previous methods (Balan et al. 2005; Lan and Huttenlocher 2005; Sigal et al. 2004; Sigal and Black 2006) that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 150
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 93
                            }
                        ],
                        "text": "It has since been also used for 3D t acking in [41] and for 2D pose estimation evaluation in [36, 75]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 421,
                                "start": 418
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 39
                            }
                        ],
                        "text": "As a convention from previous m ethods [7, 36, 74, 75] that have already used this error measure, we compute the 3D error in millimeters ( mm) and the 2D error directly in the image in pixels ( pix)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 622540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a6af296b99e2c6cd58a49533b49f3c7cdab02c",
            "isKey": true,
            "numCitedBy": 190,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Tree structured models have been widely used for determining the pose of a human body, from either 2D or 3D data. While such models can effectively represent the kinematic constraints of the skeletal structure, they do not capture additional constraints such as coordination of the limbs. Tree structured models thus miss an important source of information about human body pose, as limb coordination is necessary for balance while standing, walking, or running, as well as being evident in other activities such as dancing and throwing. In this paper, we consider the use of undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs. We refer to these as common-factor models, since they are constructed by using factor analysis to identify additional correlations in limb position that are not accounted for by the kinematic tree structure. These common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation. We present some experimental results contrasting common-factor models with tree models, and quantify the improvement in pose estimation for 2D image data."
            },
            "slug": "Beyond-trees:-common-factor-models-for-2D-human-Lan-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Beyond trees: common-factor models for 2D human pose recovery"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs are considered, since these common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 95
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 94
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 506761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e32a0a459142efd847b0bb27e68d13fcca0d9552",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Many difficult visual perception problems, like 3D human motion estimation, can be formulated in terms of inference using complex generative models, defined over high-dimensional state spaces. Despite progress, optimizing such models is difficult because prior knowledge cannot be flexibly integrated in order to reshape an initially designed representation space. Nonlinearities, inherent sparsity of high-dimensional training sets, and lack of global continuity makes dimensionality reduction challenging and low-dimensional search inefficient. To address these problems, we present a learning and inference algorithm that restricts visual tracking to automatically extracted, non-linearly embedded, low-dimensional spaces. This formulation produces a layered generative model with reduced state representation, that can be estimated using efficient continuous optimization methods. Our prior flattening method allows a simple analytic treatment of low-dimensional intrinsic curvature constraints, and allows consistent interpolation operations. We analyze reduced manifolds for human interaction activities, and demonstrate that the algorithm learns continuous generative models that are useful for tracking and for the reconstruction of 3D human motion in monocular video."
            },
            "slug": "Generative-modeling-for-continuous-non-linearly-Sminchisescu-Jepson",
            "title": {
                "fragments": [],
                "text": "Generative modeling for continuous non-linearly embedded visual inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a learning and inference algorithm that restricts visual tracking to automatically extracted, non-linearly embedded, low-dimensional spaces, and produces a layered generative model with reduced state representation that can be estimated using efficient continuous optimization methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "\u2026error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 55
                            }
                        ],
                        "text": "2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 244
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 40
                            }
                        ],
                        "text": "2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8298534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd4a61eae6b7eb2501a53ed05ccb0313d62cb0d8",
            "isKey": true,
            "numCitedBy": 236,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an image-based approach to infer 3D structure parameters using a probabilistic \"shape+structure\" model. The 3D shape of an object class is represented by sets of contours from silhouette views simultaneously observed from multiple calibrated cameras, while structural features of interest on the object are denoted by a number of 3D locations. A prior density over the multiview shape and corresponding structure is constructed with a mixture of probabilistic principal components analyzers. Given a novel set of contours, we infer the unknown structure parameters from the new shape's Bayesian reconstruction. Model matching and parameter inference are done entirely in the image domain and require no explicit 3D construction. Our shape model enables accurate estimation of structure despite segmentation errors or missing views in the input silhouettes, and it works even with only a single input view. Using a training set of thousands of pedestrian images generated from a synthetic model, we can accurately infer the 3D locations of 19 joints on the body based on observed silhouette contours from real images."
            },
            "slug": "Inferring-3D-structure-with-a-statistical-shape-Grauman-Shakhnarovich",
            "title": {
                "fragments": [],
                "text": "Inferring 3D structure with a statistical image-based shape model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The shape model enables accurate estimation of structure despite segmentation errors or missing views in the input silhouettes, and it works even with only a single input view, using a training set of thousands of pedestrian images generated from a synthetic model."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575536"
                        ],
                        "name": "Marcus A. Brubaker",
                        "slug": "Marcus-A.-Brubaker",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Brubaker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus A. Brubaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747779"
                        ],
                        "name": "Aaron Hertzmann",
                        "slug": "Aaron-Hertzmann",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Hertzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Hertzmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "Lastly, priors based on abstracted (Brubaker et al. 2007) or full-body (Vondrak et al. 2008) physical simulations recently have been proposed for specific classes of motions (e.g. walking)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Lastly, priors based on abstracted [12] or full-body [88] physical simulations recently have been pro posed for specific classes of motions ( e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8121463,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "106874c75154d16e54d2d49cc98591ff2ff4f53d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a physics-based model for 3D person tracking. Based on a biomechanical characterization of lower-body dynamics, the model captures important physical properties of bipedal locomotion such as balance and ground contact, generalizes naturally to variations in style due to changes in speed, step-length, and mass, and avoids common problems such as footskate that arise with existing trackers. The model dynamics comprises a two degree-of-freedom representation of human locomotion with inelastic ground contact. A stochastic controller generates impulsive forces during the toe-off stage of walking and spring-like forces between the legs. A higher-dimensional kinematic observation model is then conditioned on the underlying dynamics. We use the model for tracking walking people from video, including examples with turning, occlusion, and varying gait."
            },
            "slug": "Physics-Based-Person-Tracking-Using-Simplified-Brubaker-Fleet",
            "title": {
                "fragments": [],
                "text": "Physics-Based Person Tracking Using Simplified Lower-Body Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A physics-based model for 3D person tracking that captures important physical properties of bipedal locomotion such as balance and ground contact, generalizes naturally to variations in style due to changes in speed, step-length, and mass, and avoids common problems such as footskate that arise with existing trackers."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2876425"
                        ],
                        "name": "S. Corazza",
                        "slug": "S.-Corazza",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Corazza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Corazza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2709174"
                        ],
                        "name": "L. M\u00fcndermann",
                        "slug": "L.-M\u00fcndermann",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "M\u00fcndermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M\u00fcndermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144886132"
                        ],
                        "name": "T. Andriacchi",
                        "slug": "T.-Andriacchi",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Andriacchi",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Andriacchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "The true gold standard in localizing the position of hip joints is still debated in the bio-mechan ics literature [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 114
                            }
                        ],
                        "text": "The true gold standard in localizing the position of hip joints is still debated in the bio-mechanics literature (Corazza et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24988564,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6aa61466829983634b48e16e69523ebb48974654",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-framework-for-the-functional-identification-of-Corazza-M\u00fcndermann",
            "title": {
                "fragments": [],
                "text": "A framework for the functional identification of joint centers using markerless motion capture, validation for the hip joint."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of biomechanics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50997742"
                        ],
                        "name": "Ping Wang",
                        "slug": "Ping-Wang",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 349,
                                "start": 345
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion\n(Wang and Rehg 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 275
                            }
                        ],
                        "text": "\u2026et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion\n(Wang and Rehg 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[89] where synchronized motion capture and monocular video was collected ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 275
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attemp ts [23, 35, 47, 65] to simultaneously capture video and ground truth 3D motion data (in the form of marker-based t racking); some groups were also able to capture 2D motion ground truth data in a similar fashion [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 803,
                                "start": 799
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The INRIA Perception Group also employed a similar approach for collection of ground truth data (Knossow et al. 2008), however,\n2http://www.cc.gt.atl.ga.us/grads/w/Ping.Wang/Project/ FigureTracking.html.\nonly the multi-view video data is currently made available to the public."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 262
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Wh ile this allowed some quantitative analysis of results [23, 35, 47, 65, 89], to our knowledge none of the synchronized dat a c ptured by these groups (with the exception of [89], discussed in Section 2) has been made available to the commun ity at large, making it hard for competing approaches to compare performance directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 257
                            }
                        ],
                        "text": "\u2026this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in Sect."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in Sect."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "\u2026clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Non-edge-based likelihood measures include optical flow [11, 73], flow occlusion/disocclusion b oundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18 ], principal component-based models of appearance [72], and robust on-line local [6, 85] and global appearance model s [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "INRIA Perception [35] CMU MoCap CMU MoBo Datasets [89] Multi-Cam Dataset Dataset [15] Dataset [27] # of Subjects 4 3 Unknown > 100 25 # of Frames \u2248 80, 000 \u2248 450 Unknown Unknown \u2248 200, 000 # of Sequences 56 4 13 2,605 100 Video Data # of Cameras 4/7 1 8/34 1 6 Calib."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 35
                            }
                        ],
                        "text": "A similar approach was employed by Wang and Rehg (2006) where synchronized motion capture and monocular video was collected."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15581754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b471b6c0d5ffae22d09b8f42d525dab780a9447f",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the first systematic empirical study of the particle filter (PF) algorithms for human figure tracking in video. Our analysis and evaluation follows a modular approach which is based upon the underlying statistical principles and computational concerns that govern the performance of PF algorithms. Based on our analysis, we propose a novel PF algorithm for figure tracking with superior performance called the Optimized Unscented PF. We examine the role of edge and template features, introduce computationally-equivalent sample sets, and describe a method for the automatic acquisition of reference data using standard motion capture hardware. The software and test data are made publicly-available on our project website."
            },
            "slug": "A-Modular-Approach-to-the-Analysis-and-Evaluation-Wang-Rehg",
            "title": {
                "fragments": [],
                "text": "A Modular Approach to the Analysis and Evaluation of Particle Filters for Figure Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a novel PF algorithm for figure tracking with superior performance called the Optimized Unscented PF, which examines the role of edge and template features, introduces computationally-equivalent sample sets, and describes a method for the automatic acquisition of reference data using standard motion capture hardware."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 363
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 144
                            }
                        ],
                        "text": "\u2026and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3 D body pose into the image (or set of images) and visually asses sing how the estimates explain the image [17, 21, 60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 221
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3D body pose into the image (or set of images) and visually assessing how the estimates explain the image (Deutscher and Reid 2005; Felzenszwalb and Huttenlocher 2005; Ren et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 524,
                                "start": 521
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3025856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a359e88ced86b7497b4a116a1c606a5266632b",
            "isKey": true,
            "numCitedBy": 252,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to recover human body configurations from static images. Without assuming a priori knowledge of scale, pose or appearance, this problem is extremely challenging and demands the use of all possible sources of information. We develop a framework which can incorporate arbitrary pairwise constraints between body parts, such as scale compatibility, relative position, symmetry of clothing and smooth contour connections between parts. We detect candidate body parts from bottom-up using parallelism, and use various pairwise configuration constraints to assemble them together into body configurations. To find the most probable configuration, we solve an integer quadratic programming problem with a standard technique using linear approximations. Approximate IQP allows us to incorporate much more information than the traditional dynamic programming and remains computationally efficient. 15 hand-labeled images are used to train the low-level part detector and learn the pairwise constraints. We show test results on a variety of images."
            },
            "slug": "Recovering-human-body-configurations-using-pairwise-Ren-Berg",
            "title": {
                "fragments": [],
                "text": "Recovering human body configurations using pairwise constraints between parts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A framework which can incorporate arbitrary pairwise constraints between body parts, such as scale compatibility, relative position, symmetry of clothing and smooth contour connections between parts is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 206
                            }
                        ],
                        "text": "A short summary of different approaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to (Gavrila 1999) and (Moeslund and Granum 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 206
                            }
                        ],
                        "text": "A short summary of different ap proaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to [24] and [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7788290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "isKey": false,
            "numCitedBy": 2210,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions."
            },
            "slug": "The-Visual-Analysis-of-Human-Movement:-A-Survey-Gavrila",
            "title": {
                "fragments": [],
                "text": "The Visual Analysis of Human Movement: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A number of promising applications are identified and an overview of recent developments in this domain is provided, including work on whole-body or hand motion and the various methodologies."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651486"
                        ],
                        "name": "Liefeng Bo",
                        "slug": "Liefeng-Bo",
                        "structuredName": {
                            "firstName": "Liefeng",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liefeng Bo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197309"
                        ],
                        "name": "A. Kanaujia",
                        "slug": "A.-Kanaujia",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Kanaujia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kanaujia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "\u2026Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 127626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed566a960b98611c399045d0f9c895b576c7b22f",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The potential success of discriminative learning approaches to 3D reconstruction relies on the ability to efficiently train predictive algorithms using sufficiently many examples that are representative of the typical configurations encountered in the application domain. Recent research indicates that sparse conditional Bayesian mixture of experts (cMoE) models (e.g. BME (Sminchisescu et al., 2005)) are adequate modeling tools that not only provide contextual 3D predictions for problems like human pose reconstruction, but can also represent multiple interpretations that result from depth ambiguities or occlusion. However, training conditional predictors requires sophisticated double-loop algorithms that scale unfavorably with the input dimension and the training set size, thus limiting their usage to 10,000 examples of less, so far. In this paper we present large-scale algorithms, referred to as fBME, that combine forward feature selection and bound optimization in order to train probabilistic, BME models, with one order of magnitude more data (100,000 examples and up) and more than one order of magnitude faster. We present several large scale experiments, including monocular evaluation on the HumanEva dataset (Sigal and Black, 2006), demonstrating how the proposed methods overcome the scaling limitations of existing ones."
            },
            "slug": "Fast-algorithms-for-large-scale-conditional-3D-Bo-Sminchisescu",
            "title": {
                "fragments": [],
                "text": "Fast algorithms for large scale conditional 3D prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Large-scale algorithms are presented, referred to as fBME, that combine forward feature selection and bound optimization in order to train probabilistic, BME models, with one order of magnitude more data (100,000 examples and up) and more than one orders of magnitude faster."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266063"
                        ],
                        "name": "Jing Wang",
                        "slug": "Jing-Wang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143941894"
                        ],
                        "name": "Bobby Bodenheimer",
                        "slug": "Bobby-Bodenheimer",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Bodenheimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Bodenheimer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 197018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f95bd7dc125d970a026ae8d5635d9b5dd8928d3",
            "isKey": false,
            "numCitedBy": 1488,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This article develops methods for determining visually appealing motion transitions using linear blending. Motion transitions are segues between two sequences of animation, and are important components for generating compelling animation streams in virtual environments and computer games. Methods involving linear blending are studied because of their efficiency, computational speed, and widespread use. Two methods of transition specification are detailed, center-aligned and start-end transitions. First, we compute a set of optimal weights for an underlying cost metric used to determine the transition points. We then evaluate the optimally weighted cost metric for generalizability, appeal, and robustness through a cross-validation and user study. Next, we develop methods for computing visually appealing blend lengths for two broad categories of motion. We empirically evaluate these results through user studies. Finally, we assess the importance of these techniques by determining the minimum sensitivity of viewers to transition durations, the just noticeable difference, for both center-aligned and start-end specifications."
            },
            "slug": "Synthesis-and-evaluation-of-linear-motion-Wang-Bodenheimer",
            "title": {
                "fragments": [],
                "text": "Synthesis and evaluation of linear motion transitions"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This article develops methods for determining visually appealing motion transitions using linear blending, and assess the importance of these techniques by determining the minimum sensitivity of viewers to transition durations, the just noticeable difference, for both center-aligned and start-end specifications."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698422"
                        ],
                        "name": "J. MacCormick",
                        "slug": "J.-MacCormick",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "MacCormick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacCormick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "For example, hybr id Monte Carlo sampling [57], partitioned sampling [43], or covariance-scaled sampling [79] are all promising alter natives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14382281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86efc15754e34254e5991c3a0fdcffbffbc47a3e",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Partitioned sampling is a technique which was introduced in [I7] for avoiding the high cost of particle filters when tracking more than one object. In fact this technique can reduce the curse of dimensionality in other situations too. This paper describes how to use partitioned sampling on articulated objects, obtaining results that would be impossible with standard sampling methods. Because partitioned sampling is the statistical analogue of a hierarchical search, it makes sense to use it on articulated objects, since links at the base of the object can be localised before moving on to search for subsequent links."
            },
            "slug": "Partitioned-Sampling,-Articulated-Objects,-and-Hand-MacCormick-Isard",
            "title": {
                "fragments": [],
                "text": "Partitioned Sampling, Articulated Objects, and Interface-Quality Hand Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes how to use partitioned sampling on articulated objects, obtaining results that would be impossible with standard sampling methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32990144"
                        ],
                        "name": "E. Poon",
                        "slug": "E.-Poon",
                        "structuredName": {
                            "firstName": "Eunice",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Poon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter responses [66, 70]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, hybrid Monte Carlo sampling [57], partitioned sampling [43], or covariance-scaled sampling [79] are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18213619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3efa4483cb537501db6e815a3c8ba436b01bdb25",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical inefficiency often limits the effectiveness of particle filters for high-dimensional Bayesian tracking problems. To improve sampling efficiency on continuous domains, we propose the use of a particle filter with hybrid Monte Carlo (HMC), an MCMC (Markov chain Monte Carlo) method that follows posterior gradients toward. high probability states, while ensuring a properly weighted approximation to the posterior. We use HMC filtering to infer the 3D shape and motion of people from natural, monocular image sequences. The approach currently uses an empirical, edge-based likelihood function, and a second-order dynamic model with soft biomechanical joint constraints."
            },
            "slug": "Hybrid-Monte-Carlo-filtering:-edge-based-people-Poon-Fleet",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo filtering: edge-based people tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work uses HMC filtering to infer the 3D shape and motion of people from natural, monocular image sequences by using an empirical, edge-based likelihood function, and a second-order dynamic model with soft biomechanical joint constraints."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Motion and Video Computing, 2002. Proceedings."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109425383"
                        ],
                        "name": "Chan-Su Lee",
                        "slug": "Chan-Su-Lee",
                        "structuredName": {
                            "firstName": "Chan-Su",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chan-Su Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159523"
                        ],
                        "name": "A. Elgammal",
                        "slug": "A.-Elgammal",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Elgammal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elgammal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9,  38 , 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7192536,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "990c0dbbbaaea26ed11c4649a84cf645acbb5b49",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider modeling data lying on multiple continuous manifolds. In particular, we model the shape manifold of a person performing a motion observed from different view points along a view circle at fixed camera height. We introduce a model that ties together the body configuration (kinematics) manifold and the visual manifold (observations) in a way that facilitates tracking the 3D configuration with continuous relative view variability. The model exploits the low dimensionality nature of both the body configuration manifold and the view manifold where each of them are represented separately."
            },
            "slug": "Modeling-View-and-Posture-Manifolds-for-Tracking-Lee-Elgammal",
            "title": {
                "fragments": [],
                "text": "Modeling View and Posture Manifolds for Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A model is introduced that ties together the body configuration (kinematics) manifold and the visual manifold (observations) in a way that facilitates tracking the 3D configuration with continuous relative view variability."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "This is the basis of th e Sequential Importance Resampling (SIR) algorithm, or Condensation [3, 31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 117
                            }
                        ],
                        "text": "This is the basis of the Sequential Importance Resampling (SIR) algorithm, or Condensation (Arulampalam et al. 2002; Isard and Blake 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6821810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "963dddc907f56bd1d6c98dd40f560eb8786e49ea",
            "isKey": false,
            "numCitedBy": 5523,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time."
            },
            "slug": "CONDENSATION\u2014Conditional-Density-Propagation-for-Isard-Blake",
            "title": {
                "fragments": [],
                "text": "CONDENSATION\u2014Conditional Density Propagation for Visual Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33731953"
                        ],
                        "name": "R. Gross",
                        "slug": "R.-Gross",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 where data from 7 synchronized video cameras is illustrated with an overlay of ground truth body pose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 36
                            }
                        ],
                        "text": "The CMU Graphics Lab Motion Capture Database (CMU) is by far the most extensive dataset of publicly available motion capture data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 168
                            }
                        ],
                        "text": "\u2026that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow (Baker et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords Articulated pose estimation \u00b7 Articulated tracking \u00b7 Motion capture \u00b7 Human tracking \u00b7 Datasets and evaluation"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 40
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database (Gross and Shi 2001), initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms (Fathi et al. 2007; Zhang et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17484648,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "599cc88971d2f5b375ff23b6342f17855e01791c",
            "isKey": true,
            "numCitedBy": 620,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database."
            },
            "slug": "The-CMU-Motion-of-Body-(MoBo)-Database-Gross-Shi",
            "title": {
                "fragments": [],
                "text": "The CMU Motion of Body (MoBo) Database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The capture setup, the collection procedure and the organization of the database are described, which contains 25 individuals walking on a treadmill in the CMU 3D room."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31589308"
                        ],
                        "name": "C. J. Taylor",
                        "slug": "C.-J.-Taylor",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Taylor",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11364178,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37056506546e89011cf69cd32cabb5993f5e65a7",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the problem of recovering information about the configuration of an articulated object, such as a human figure, from point correspondences in a single-image. Unlike previous approaches, the proposed reconstruction method does not assume that the imagery was acquired with a calibrated camera. An analysis is presented which demonstrates that there are a family of solutions to this reconstruction problem parameterized by a single variable. A simple and effective algorithm is proposed for recovering the entire set of solutions by considering the foreshortening of the segments of the model in the image. Results obtained by applying this algorithm to real images are presented."
            },
            "slug": "Reconstruction-of-articulated-objects-from-point-in-Taylor",
            "title": {
                "fragments": [],
                "text": "Reconstruction of articulated objects from point correspondences in a single uncalibrated image"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper investigates the problem of recovering information about the configuration of an articulated object, such as a human figure, from point correspondences in a single image by considering the foreshortening of the segments of the model in the image."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698422"
                        ],
                        "name": "J. MacCormick",
                        "slug": "J.-MacCormick",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "MacCormick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacCormick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7644136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4dcfabc9877d614b0aa472ba8597d8d7053adb6",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Tracking multiple targets is a challenging problem, especially when the targets are \u201cidentical\u201d, in the sense that the same model is used to describe each target. In this case, simply instantiating several independent 1-body trackers is not an adequate solution, because the independent trackers tend to coalesce onto the best-fitting target. This paper presents an observation density for tracking which solves this problem by exhibiting a probabilistic exclusion principle. Exclusion arises naturally from a systematic derivation of the observation density, without relying on heuristics. Another important contribution of the paper is the presentation of partitioned sampling, a new sampling method for multiple object tracking. Partitioned sampling avoids the high computational load associated with fully coupled trackers, while retaining the desirable properties of coupling."
            },
            "slug": "A-Probabilistic-Exclusion-Principle-for-Tracking-MacCormick-Blake",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Exclusion Principle for Tracking Multiple Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An observation density for tracking is presented which solves this problem by exhibiting a probabilistic exclusion principle, and is presented of partitioned sampling, a new sampling method for multiple object tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110979"
                        ],
                        "name": "H. V. Trees",
                        "slug": "H.-V.-Trees",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Trees",
                            "middleNames": [
                                "L.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Trees"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231264"
                        ],
                        "name": "K. Bell",
                        "slug": "K.-Bell",
                        "structuredName": {
                            "firstName": "Kristine",
                            "lastName": "Bell",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57688401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fc4a3f552f57f440b995663d961393735a2edfd",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or ?>particle ?>) representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example."
            },
            "slug": "A-Tutorial-on-Particle-Filters-for-Online-Bayesian-Trees-Bell",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Particle Filters for Online Nonlinear/NonGaussian Bayesian Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298395"
                        ],
                        "name": "M. Arulampalam",
                        "slug": "M.-Arulampalam",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Arulampalam",
                            "middleNames": [
                                "Sanjeev"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Arulampalam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731320"
                        ],
                        "name": "S. Maskell",
                        "slug": "S.-Maskell",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Maskell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maskell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144998829"
                        ],
                        "name": "N. Gordon",
                        "slug": "N.-Gordon",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102907269"
                        ],
                        "name": "T. Clapp",
                        "slug": "T.-Clapp",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Clapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Clapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "This is the basis of th e Sequential Importance Resampling (SIR) algorithm, or Condensation [3, 31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 115
                            }
                        ],
                        "text": "with a sensor Markov assumption p(yt|x1:t,y1:t\u22121) = p(yt|xt), a recursive formula for the posterior can be derived [3, 19]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 142
                            }
                        ],
                        "text": "\u2026|x1:t\u22121) = p(xt |xt\u22121), with a sensor Markov assumption\np(yt |x1:t ,y1:t\u22121) = p(yt |xt ), a recursive formula for the posterior can be derived (Arulampalam et al. 2002; Doucet et al. 2000):\np(xt |y1:t ) \u221d p(yt |xt ) \u222b\np(xt |xt\u22121)p(xt\u22121|y1:t\u22121) dxt\u22121, (7)\nwhere the integral in (7) computes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 92
                            }
                        ],
                        "text": "This is the basis of the Sequential Importance Resampling (SIR) algorithm, or Condensation (Arulampalam et al. 2002; Isard and Blake 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122803681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "763493cf694f3a7365c02676c139ee01cbac30b9",
            "isKey": true,
            "numCitedBy": 11118,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or \"particle\") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example."
            },
            "slug": "A-tutorial-on-particle-filters-for-online-Bayesian-Arulampalam-Maskell",
            "title": {
                "fragments": [],
                "text": "A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 180
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter responses [66, 70]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 218
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2050617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "948f87c41a32cbea499567e71963dd19dcbce485",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian methods for visual tracking model the likelihood of image measurements conditioned on a tracking hypothesis. Image measurements may, for example, correspond to various filter responses at multiple scales and orientations. Most tracking approaches exploit ad hoc likelihood models while those that exploit more rigorous, learned, models often make unrealistic assumptions about the underlying probabilistic model. Such assumptions cause problems for Bayesian inference when an unsound likelihood is combined with an a priori probability distribution. Errors in modeling the likelihood can lead to brittle tracking results, particularly when using non-parametric inference techniques such as particle filtering. We show how assumptions of conditional independence of filter responses are violated in common tracking scenarios, lead to incorrect likelihood models, and cause problems for Bayesian inference. We address the problem of modeling more principled likelihoods using Gibbs learning. The learned models are compared with naive Bayes methods which assume conditional independence of the filter responses. We show how these Gibbs models can be used as an effective image likelihood, and demonstrate them in the context of particle filter-based human tracking."
            },
            "slug": "Gibbs-likelihoods-for-Bayesian-tracking-Roth-Sigal",
            "title": {
                "fragments": [],
                "text": "Gibbs likelihoods for Bayesian tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how assumptions of conditional independence of filter responses are violated in common tracking scenarios, lead to incorrect likelihood models, and cause problems for Bayesian inference, and the problem of modeling more principled likelihoods using Gibbs learning is addressed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784015"
                        ],
                        "name": "R. Navaratnam",
                        "slug": "R.-Navaratnam",
                        "structuredName": {
                            "firstName": "Ramanan",
                            "lastName": "Navaratnam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Navaratnam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "\u202615 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 975,
                                "start": 971
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "Nevertheless, the video data has proved useful for the analysis of dis-\ncriminative methods that do not estimate 3D body location e.g. (Navaratnam et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 389,
                                "start": 379
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 199
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2365737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c536b7a77cdc07db357c9f5d58d0df706723723",
            "isKey": true,
            "numCitedBy": 104,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision tasks may be expressed as the problem of learning a mapping between image space and a parameter space. For example, in human body pose estimation, recent research has directly modelled the mapping from image features (z) to joint angles (thetas). Fitting such models requires training data in the form of labelled (z, thetas) pairs, from which are learned the conditional densities p(thetas\\z). Inference is then simple: given test image features z, the conditional p(thetas\\z) is immediately computed. However large amounts of training data are required to fit the models, particularly in the case where the spaces are high dimensional. We show how the use of unlabelled data-samples from the marginal distributions p(z) and p(thetas)-may be used to improve fitting. This is valuable because it is often significantly easier to obtain unlabelled than labelled samples. We use a Gaussian process latent variable model to learn the mapping from a shared latent low-dimensional manifold to the feature and parameter spaces. This extends existing approaches to (a) use unlabelled data, and (b) represent one-to-many mappings. Experiments on synthetic and real problems demonstrate how the use of unlabelled data improves over existing techniques. In our comparisons, we include existing approaches that are explicitly semi-supervised as well as those which implicitly make use of unlabelled examples."
            },
            "slug": "The-Joint-Manifold-Model-for-Semi-supervised-Navaratnam-Fitzgibbon",
            "title": {
                "fragments": [],
                "text": "The Joint Manifold Model for Semi-supervised Multi-valued Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A Gaussian process latent variable model is used to learn the mapping from a shared latent low-dimensional manifold to the feature and parameter spaces and Experiments on synthetic and real problems demonstrate how the use of unlabelled data improves over existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 147
                            }
                        ],
                        "text": "\u2026e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used [1, 2, 26, 69, 76] for quantitative evaluatio n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 299
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 252
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 112
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 487,
                                "start": 483
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2051403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e1556aea42601df3f457ad43dfb059498931a33",
            "isKey": true,
            "numCitedBy": 906,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images."
            },
            "slug": "Fast-pose-estimation-with-parameter-sensitive-Shakhnarovich-Viola",
            "title": {
                "fragments": [],
                "text": "Fast pose estimation with parameter-sensitive hashing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new algorithm is introduced that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task, and can rapidly and accurately estimate the articulated pose of human figures from a large database of example images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33731953"
                        ],
                        "name": "R. Gross",
                        "slug": "R.-Gross",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 36
                            }
                        ],
                        "text": "The CMU Graphics Lab Motion Capture Database (CMU) is by far the most extensive dataset of publicly available motion capture data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 168
                            }
                        ],
                        "text": "\u2026that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow (Baker et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "Keywords Articulated pose estimation \u00b7 Articulated tracking \u00b7 Motion capture \u00b7 Human tracking \u00b7 Datasets and evaluation"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database [27], initially devel oped for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorit hms [20, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 40
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database (Gross and Shi 2001), initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms (Fathi et al. 2007; Zhang et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 11
                            }
                        ],
                        "text": "2.2 Common Datasets\nWhile HUMANEVA is the most extensive dataset for evaluation of human pose and motion estimation, there have been several related efforts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 167
                            }
                        ],
                        "text": "It is worth noting that simi lar efforts have been made in related areas including the development of datasets for face detection [55, 56], human g ait identification [27, 67], dense stereo vision [68] and optical flow [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "INRIA Perception [35] CMU MoCap CMU MoBo Datasets [89] Multi-Cam Dataset Dataset [15] Dataset [27] # of Subjects 4 3 Unknown > 100 25 # of Frames \u2248 80, 000 \u2248 450 Unknown Unknown \u2248 200, 000 # of Sequences 56 4 13 2,605 100 Video Data # of Cameras 4/7 1 8/34 1 6 Calib."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14790768,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "26e890fa27a7b4a502fa65a117794a213cc4e0a6",
            "isKey": true,
            "numCitedBy": 7,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In March 2001 we started to collect the CMU Motion of Body (MoB o) database. To date the database contains 25 individuals walking on a tre admill in the CMU 3D room. The subjects perform four different walk patterns: sl ow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this te chnical report we describe the capture setup, the collection procedure and the organiz ation of the database."
            },
            "slug": "The-CMU-Motion-of-Body-(-MoBo-)-Gross-Shi",
            "title": {
                "fragments": [],
                "text": "The CMU Motion of Body ( MoBo )"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The capture setup, the collection procedure and the organiz ation of the database of the CMU Motion of Body database are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 147
                            }
                        ],
                        "text": "\u2026for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2983119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32510e7f88bc0767fbbc811397ba068dbc4cf549",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image. We operationalize this idea using an over-segmentation of an image into superpixels. The problem domain we explore is human body pose estimation from still images. The superpixels prove useful in two ways. First, we restrict the joint positions in our human body model to lie at centers of superpixels, which reduces the size of the model search space. In addition, accurate support masks for computing features on half-limbs of the body model are obtained by using agglomerations of superpixels as half limb segments. We present results on a challenging dataset of people in sports news images"
            },
            "slug": "Guiding-model-search-using-segmentation-Mori",
            "title": {
                "fragments": [],
                "text": "Guiding model search using segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image by using an over-segmentation of an image into superpixels."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 96
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8492723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d15d6ac8860d4afbd8bd2699e8e548fa38284fed",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents two novel likelihood terms for silhouettes and contours in model-based contexts. Despite the power of such formulations, building likelihoods that truly reflect the good configurations of the problem is by no means easy due to, most commonly, the violation of consistency principle resulting in the introduction of spurious, unrelated peaks/minima that make target localization difficult. We introduce an entirely continuous formulation which enforces consistency by means of an attraction/explanation pair for silhouettes. For contours, we address the search window vs. noise level dilemma by means of a combined robust estimation and feature coupling solution which builds a likelihood model not only in terms of individual contour responses but also \"Gestalt\" type higher-order couplings between matched configurations. We subsequently show how the proposed method provides significant consolidation and improved attraction zone around the true cost minima and elimination of some of the spurious ones."
            },
            "slug": "Consistency-and-coupling-in-human-model-likelihoods-Sminchisescu",
            "title": {
                "fragments": [],
                "text": "Consistency and coupling in human model likelihoods"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper introduces an entirely continuous formulation which enforces consistency by means of an attraction/explanation pair for silhouettes and contours in model-based contexts and addresses the search window vs. noise level dilemma."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 161
                            }
                        ],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s (Hogg 1983; O\u2019Rourke and Badler 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s [28, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Year First Author Model Type Parts Dim Type Evaluation Measure 1983 Hogg [28] Cylinders 14 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34873540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92f98b189cec1220d479e3079b942e71b244aa65",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-based-vision:-a-program-to-see-a-walking-Hogg",
            "title": {
                "fragments": [],
                "text": "Model-based vision: a program to see a walking person"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31628486"
                        ],
                        "name": "J. Potthoff",
                        "slug": "J.-Potthoff",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Potthoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Potthoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679944"
                        ],
                        "name": "C. Schn\u00f6rr",
                        "slug": "C.-Schn\u00f6rr",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Schn\u00f6rr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schn\u00f6rr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779035"
                        ],
                        "name": "B. Rosenhahn",
                        "slug": "B.-Rosenhahn",
                        "structuredName": {
                            "firstName": "Bodo",
                            "lastName": "Rosenhahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rosenhahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145156858"
                        ],
                        "name": "H. Seidel",
                        "slug": "H.-Seidel",
                        "structuredName": {
                            "firstName": "Hans-Peter",
                            "lastName": "Seidel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seidel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 58
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 645101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75a37ce81b68d135ba812867e22ab75a869c0162",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nInteracting and annealing are two powerful strategies that are applied in different areas of stochastic modelling and data analysis. Interacting particle systems approximate a distribution of interest by a finite number of particles where the particles interact between the time steps. In computer vision, they are commonly known as particle filters. Simulated annealing, on the other hand, is a global optimization method derived from statistical mechanics. A recent heuristic approach to fuse these two techniques for motion capturing has become known as annealed particle filter. In order to analyze these techniques, we rigorously derive in this paper two algorithms with annealing properties based on the mathematical theory of interacting particle systems. Convergence results and sufficient parameter restrictions enable us to point out limitations of the annealed particle filter. Moreover, we evaluate the impact of the parameters on the performance in various experiments, including the tracking of articulated bodies from noisy measurements. Our results provide a general guidance on suitable parameter choices for different applications.\n"
            },
            "slug": "Interacting-and-Annealing-Particle-Filters:-for-Gall-Potthoff",
            "title": {
                "fragments": [],
                "text": "Interacting and Annealing Particle Filters: Mathematics\u00a0and\u00a0a\u00a0Recipe for Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Two algorithms with annealing properties based on the mathematical theory of interacting particle systems are derived and evaluated to provide a general guidance on suitable parameter choices for different applications."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Mathematical Imaging and Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 58
                            }
                        ],
                        "text": "2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 185
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3D body pose into the image (or set of images) and visually assessing how the estimates explain the image (Deutscher and Reid 2005; Felzenszwalb and Huttenlocher 2005; Ren et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2424,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150925453"
                        ],
                        "name": "Rui Li",
                        "slug": "Rui-Li",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095482"
                        ],
                        "name": "T. Tian",
                        "slug": "T.-Tian",
                        "structuredName": {
                            "firstName": "Tai-Peng",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 173
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 196
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2473005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5e984bbf9c06d34695ef03620fa6147a338d7e0",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to learn a parsimonious and informative representation for high-dimensional time series. Conceptually, this comprises two distinct yet tightly coupled tasks: learning a low-dimensional manifold and modeling the dynamical process. These two tasks have a complementary relationship as the temporal constraints provide valuable neighborhood information for dimensionality reduction and conversely, the low-dimensional space allows dynamics to be learnt efficiently. Solving these two tasks simultaneously allows important information to be exchanged mutually. If nonlinear models are required to capture the rich complexity of time series, then the learning problem becomes harder as the nonlinearities in both tasks are coupled. The proposed solution approximates the nonlinear manifold and dynamics using piecewise linear models. The interactions among the linear models are captured in a graphical model. By exploiting the model structure, efficient inference and learning algorithms are obtained without oversimplifying the model of the underlying dynamical process. Evaluation of the proposed framework with competing approaches is conducted in three sets of experiments: dimensionality reduction and reconstruction using synthetic time series, video synthesis using a dynamic texture database, and human motion synthesis, classification and tracking on a benchmark data set. In all experiments, the proposed approach provides superior performance."
            },
            "slug": "Simultaneous-Learning-of-Nonlinear-Manifold-and-for-Li-Tian",
            "title": {
                "fragments": [],
                "text": "Simultaneous Learning of Nonlinear Manifold and Dynamical Models for High-dimensional Time Series"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The goal of this work is to learn a parsimonious and informative representation for high-dimensional time series by exploiting the model structure, efficient inference and learning algorithms are obtained without oversimplifying the model of the underlying dynamical process."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701800"
                        ],
                        "name": "A. Doucet",
                        "slug": "A.-Doucet",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Doucet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doucet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708961"
                        ],
                        "name": "S. Godsill",
                        "slug": "S.-Godsill",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Godsill",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Godsill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49630843"
                        ],
                        "name": "C. Andrieu",
                        "slug": "C.-Andrieu",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Andrieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Andrieu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 53
                            }
                        ],
                        "text": "a recursive formula for the posterior can be derived (Arulampalam et al. 2002; Doucet et al. 2000):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "\u2026with a sensor Markov assumption\np(yt |x1:t ,y1:t\u22121) = p(yt |xt ), a recursive formula for the posterior can be derived (Arulampalam et al. 2002; Doucet et al. 2000):\np(xt |y1:t ) \u221d p(yt |xt ) \u222b\np(xt |xt\u22121)p(xt\u22121|y1:t\u22121) dxt\u22121, (7)\nwhere the integral in (7) computes the prediction using the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16288401,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "58675ec2ecdcb1410deba60c7ccc26975d83d39a",
            "isKey": false,
            "numCitedBy": 4618,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we present an overview of methods for sequential simulation from posterior distributions. These methods are of particular interest in Bayesian filtering for discrete time dynamic models that are typically nonlinear and non-Gaussian. A general importance sampling framework is developed that unifies many of the methods which have been proposed over the last few decades in several different scientific disciplines. Novel extensions to the existing methods are also proposed. We show in particular how to incorporate local linearisation methods similar to those which have previously been employed in the deterministic filtering literature; these lead to very effective importance distributions. Furthermore we describe a method which uses Rao-Blackwellisation in order to take advantage of the analytic structure present in some important classes of state-space models. In a final section we develop algorithms for prediction, smoothing and evaluation of the likelihood in dynamic models."
            },
            "slug": "On-sequential-Monte-Carlo-sampling-methods-for-Doucet-Godsill",
            "title": {
                "fragments": [],
                "text": "On sequential Monte Carlo sampling methods for Bayesian filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An overview of methods for sequential simulation from posterior distributions for discrete time dynamic models that are typically nonlinear and non-Gaussian, and how to incorporate local linearisation methods similar to those which have previously been employed in the deterministic filtering literature are shown."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729041"
                        ],
                        "name": "J. Canny",
                        "slug": "J.-Canny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Canny",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Canny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
            "isKey": false,
            "numCitedBy": 27662,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."
            },
            "slug": "A-Computational-Approach-to-Edge-Detection-Canny",
            "title": {
                "fragments": [],
                "text": "A Computational Approach to Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2660633"
                        ],
                        "name": "V. Camomilla",
                        "slug": "V.-Camomilla",
                        "structuredName": {
                            "firstName": "Valentina",
                            "lastName": "Camomilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Camomilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2752350"
                        ],
                        "name": "A. Cereatti",
                        "slug": "A.-Cereatti",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Cereatti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cereatti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2564808"
                        ],
                        "name": "G. Vannozzi",
                        "slug": "G.-Vannozzi",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Vannozzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vannozzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3334582"
                        ],
                        "name": "A. Cappozzo",
                        "slug": "A.-Cappozzo",
                        "structuredName": {
                            "firstName": "Aurelio",
                            "lastName": "Cappozzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cappozzo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 154
                            }
                        ],
                        "text": "For example, hip joints are not well defined and can only be measured to about 2\u2013 10 (mm) accuracy given the marker protocol employed by the Vicon system (Camomilla et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16438846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b030d6827904c022a9a0e5c34f616701d25911f4",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-optimized-protocol-for-hip-joint-centre-using-Camomilla-Cereatti",
            "title": {
                "fragments": [],
                "text": "An optimized protocol for hip joint centre determination using the functional method."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of biomechanics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136478"
                        ],
                        "name": "P. Grother",
                        "slug": "P.-Grother",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Grother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3107704"
                        ],
                        "name": "R. Micheals",
                        "slug": "R.-Micheals",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Micheals",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Micheals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32889627"
                        ],
                        "name": "D. Blackburn",
                        "slug": "D.-Blackburn",
                        "structuredName": {
                            "firstName": "Duane",
                            "lastName": "Blackburn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blackburn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326261"
                        ],
                        "name": "Elham Tabassi",
                        "slug": "Elham-Tabassi",
                        "structuredName": {
                            "firstName": "Elham",
                            "lastName": "Tabassi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elham Tabassi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144505249"
                        ],
                        "name": "Mike Bone",
                        "slug": "Mike-Bone",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Bone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Bone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30512497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "491472dcde6783f5607f69b7922fb4dcc21f4bcb",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given. The face recognition vendor test (FRVT) 2002 is an independently administered technology evaluation of mature face recognition systems. FRVT 2002 provides performance measures for assessing the capability of face recognition systems to meet requirements for large-scale, real-world applications. Participation in FRVT 2002 was open to commercial and mature prototype systems from universities, research institutes, and companies. Ten companies submitted either commercial or prototype systems. FRVT 2002 computed performance statistics on an extremely large data set-121,589 operational facial images of 37,437 individuals. FRVT 2002 1) characterized identification and watch list performance as a function of database size, 2) estimated the variability in performance for different groups of people, 3) characterized performance as a function of elapsed time between enrolled and new images of a person, and 4) investigated the effect of demographics on performance. FRVT 2002 showed that recognition from indoor images has made substantial progress since FRVT 2000. Demographic results show that males are easier to recognize than females and that older people are easier to recognize than younger people. FRVT 2002 also assessed the impact of three new techniques for improving face recognition: three-dimensional morphable models, normalization of similarity scores, and face recognition from video sequences. Results show that three-dimensional morphable models and normalization increase performance and that face recognition from video sequences offers only a limited increase in performance over still images. A new XML-based evaluation protocol was developed for FRVT 2002. This protocol is flexible and supports evaluations of biometrics in general The FRVT 2002 reports can be found at http://www.frvt.org."
            },
            "slug": "Face-recognition-vendor-test-2002-Phillips-Grother",
            "title": {
                "fragments": [],
                "text": "Face recognition vendor test 2002"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Results show that recognition from indoor images has made substantial progress since FRVT 2000 and that three-dimensional morphable models and normalization increase performance and that face recognition from video sequences offers only a limited increase in performance over still images."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11112994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "isKey": false,
            "numCitedBy": 1212,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers."
            },
            "slug": "Annealed-importance-sampling-Neal",
            "title": {
                "fragments": [],
                "text": "Annealed importance sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler, which can be seen as a generalization of a recently-proposed variant of sequential importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638847"
                        ],
                        "name": "Hyeonjoon Moon",
                        "slug": "Hyeonjoon-Moon",
                        "structuredName": {
                            "firstName": "Hyeonjoon",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeonjoon Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958806"
                        ],
                        "name": "S. A. Rizvi",
                        "slug": "S.-A.-Rizvi",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Rizvi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. A. Rizvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 497801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "791e530f6a4098bb39696d1476032821a7a1c569",
            "isKey": false,
            "numCitedBy": 2335,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1199 individuals are included in the FERET database, which is divided into development and sequestered portions. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to (1) assess the state of the art, (2) identify future areas of research, and (3) measure algorithm performance on large databases."
            },
            "slug": "The-FERET-evaluation-methodology-for-algorithms-Phillips-Moon",
            "title": {
                "fragments": [],
                "text": "The FERET evaluation methodology for face-recognition algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 545,
                                "start": 538
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 276
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u20262005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Larning to parse pic  tures of people"
            },
            "venue": {
                "fragments": [],
                "text": " European Conference on Computer Vision (ECCV)  , vol. 4, pp. 700\u2013714,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 65
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion (Wang and Rehg 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 119
                            }
                        ],
                        "text": "Also, a more anatomically correct modeling of the DoF of the joints may be required for applications in bio-mechanics (Muendermann et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 57
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in Sect."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Accurately measuring human movement using articulated ICP with softjoint constraints and a repository of articulated models. In IEEE conference on computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "\u2026statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 129
                            }
                        ],
                        "text": "Another form of inspection involves applying the estimated motion to a virtual character to see if the movements appear natural (Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used [1, 2, 26, 69, 76] for quantitative evaluatio n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 421,
                                "start": 417
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 238
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "\u2026reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "We check for angles exceeding joint angle bounds and producing inter-penetrating limbs (Sminchisescu and Triggs 2003b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "average root-mean-squared (RMS) angular error [1, 2, 76], normalized error in joint ang les [69], silhouette overlap [58, 59], joint center distanc e [7, 26, 36, 39, 41, 74, 75], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 53
                            }
                        ],
                        "text": "These approaches can be combined (Sigal et al. 2004; Sminchisescu et al. 2005), such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "Another form of inspection involves applying the estimated motion to a virtual character to see if the movements appear natural [76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 223
                            }
                        ],
                        "text": "Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. Agarwal and Triggs 2004b; Navaratnam et al. 2007; Sminchisescu et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "These approach es an be combined [74, 76], such that tracking benefits from automatic initialization and failure recovery in the f orm of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 139
                            }
                        ],
                        "text": "Alternatively, synthetic data have been extensively used (Agarwal and Triggs 2004a, 2004b; Grauman et al. 2003; Shakhnarovich et al. 2003; Sminchisescu et al. 2005) for quantitative evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Disc  riminative density propagation for 3D human motion estimation.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Conference on Computer Vision and Pattern Recognition  (CVPR),"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3 D body pose into the image (or set of images) and visually asses sing how the estimates explain the image [17, 21, 60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 185
                            }
                        ],
                        "text": "This is usually achieved by projecting the estimated 3D body pose into the image (or set of images) and visually assessing how the estimates explain the image (Deutscher and Reid 2005; Felzenszwalb and Huttenlocher 2005; Ren et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 377,
                                "start": 365
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 155
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictorial struct  ures for object recognition"
            },
            "venue": {
                "fragments": [],
                "text": " International Journal of Computer Vision (IJCV) , 61(1):55-79, Jan."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attemp ts [23, 35, 47, 65] to simultaneously capture video and ground truth 3D motion data (in the form of marker-based t racking); some groups were also able to capture 2D motion ground truth data in a similar fashion [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 10
                            }
                        ],
                        "text": "The INRIA Perception Group also employed a similar approach for collection of ground truth data (Knossow et al. 2008), however,\n2http://www.cc.gt.atl.ga.us/grads/w/Ping.Wang/Project/ FigureTracking.html.\nonly the multi-view video data is currently made available to the public."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Wh ile this allowed some quantitative analysis of results [23, 35, 47, 65, 89], to our knowledge none of the synchronized dat a c ptured by these groups (with the exception of [89], discussed in Section 2) has been made available to the commun ity at large, making it hard for competing approaches to compare performance directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The INRIA Perception Group also employed a simila r approach for collection of ground truth data [35], however, only the multi-view video data is currently made av ailable to the public."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Typicall y hardware systems similar to the one proposed here have been employed [35] where the video and motion capture data we re captured either independently (and synchronized in software off-line) or with hardware synchronization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "INRIA Perception [35] CMU MoCap CMU MoBo Datasets [89] Multi-Cam Dataset Dataset [15] Dataset [27] # of Subjects 4 3 Unknown > 100 25 # of Frames \u2248 80, 000 \u2248 450 Unknown Unknown \u2248 200, 000 # of Sequences 56 4 13 2,605 100 Video Data # of Cameras 4/7 1 8/34 1 6 Calib."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human Motion Track ing with a Kinematic Parameterization of Extremal Contours.International Journal of Computer Vision (IJCV"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1268,
                                "start": 1264
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 232
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 317
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 484,
                                "start": 477
                            }
                        ],
                        "text": "Non-edgebased likelihood measures include optical flow (Bregler and Malik 1998; Sidenbladh et al. 2000), flow occlusion/disoc clusion boundaries (Sminchisescu and Triggs 2003b), segmented silhouettes based on level sets (Rosenhahn et al. 2006), image templates (Wang and Rehg 2006), spatiotemporal templates (Dimitrijevic et al. 2006), principal component-based models of appearance (Sidenbladh et al. 2000), and robust on-line local (Balan and Black 2006; Jepson et al. 2003; Urtasun et al. 2006) and global appearance models (Balan and Black 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 719,
                                "start": 712
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 208
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor Analyzers (Li et al. 2006) are popular and effective choices particularly for instances where little training data is available."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local Probabilistic Regress  ion for Activity-Independent Human Pose Inference"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition (CVP  R),"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been for mulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter r esponses [66, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Kalman filtering [90] is another alternative that m ay be appropriate for the applications where one can ensure tha t the likelihood and the dynamics are uni-modal."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 18
                            }
                        ],
                        "text": "Kalman filtering (Wachter and Nagel 1999) is another alternative that may be appropriate for the applications where one can ensure that the likelihood and the dynamics are uni-modal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tracking persons in monocula  r im ge sequences"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision and Image Understanding,  74(3):174-192,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1215,
                                "start": 1211
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 213
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 298
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 660,
                                "start": 655
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T  orr Randomized Trees for Human Pose Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo filtering: edge-based people tracking. It IEEE workshop on motion and video computing (pp. 151\u2013158)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "\u20262005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 145
                            }
                        ],
                        "text": "\u2026people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 221
                            }
                        ],
                        "text": "A variety of statistical (Agarwal and Triggs 2004a, 2004b; Balan et al. 2005; Deutscher and Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and Huttenlocher 2005; Hua et al. 2005; Lan and Huttenlocher 2005; Mori 2005, Mori et al. 2004; Ramanan et al. 2005; Ramanan and Forsyth 2003; Ren et al. 2005; Ronfard et al. 2002; Sigal and Black 2006) or multiple (Balan et al. 2005; Deutscher and Reid 2005; Grauman et al. 2003; Sigal et al. 2004) views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "15 3 T Qualitative(3) 2004 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER \u2020 2004 Deutscher [17] R-Elliptical Cones 15 3 T Qualitative 2004 Lan [37] Rectangles 10 2 T,P Qualitative 2004 Mori [46] Stick-Figure 9 3 P Qualitative 2004 Roberts [61] Prob."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recovering human b ody configurations: Combining segmentation and recognition.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Conference on Computer Vision and Pattern Recognition  (CVPR),"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1167,
                                "start": 1163
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 195
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 280
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 606,
                                "start": 602
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative Lear  ning of Visual Words for 3D Human Pose Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition  (CVPR),"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "While this allowed some quantitative analysis of results (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006; Wang and Rehg 2006), to our knowledge none of the synchronized data captured by these groups (with the exception of (Wang and Rehg 2006), discussed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Also, a more anatomically cor rect modeling of the DoF of the joints may be required for applications in bio-mechanics [47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attempts (Gall et al. 2006; Knossow et al. 2008; Muendermann et al. 2007; Rosenhahn et al. 2006) to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "In the last few years, there have been a few successful attemp ts [23, 35, 47, 65] to simultaneously capture video and ground truth 3D motion data (in the form of marker-based t racking); some groups were also able to capture 2D motion ground truth data in a similar fashion [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 918,
                                "start": 914
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 119
                            }
                        ],
                        "text": "Also, a more anatomically correct modeling of the DoF of the joints may be required for applications in bio-mechanics (Muendermann et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Wh ile this allowed some quantitative analysis of results [23, 35, 47, 65, 89], to our knowledge none of the synchronized dat a c ptured by these groups (with the exception of [89], discussed in Section 2) has been made available to the commun ity at large, making it hard for competing approaches to compare performance directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Accurate ly measuring human movement using articulated ICP with soft-joint constraints and a repository of articulate  d models.IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1213,
                                "start": 1209
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMANEVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and \u22c4 2007 Lee [38] Joint centers N/A 3 T HUMANEVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and \u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and \u22c4 2007 Xu [91] Cylinders 10 3 T HUMANEVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMANEVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMANEVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMANEVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMANEVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMANEVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 213
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 298
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 660,
                                "start": 655
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 196
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Randomized trees for human pose estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE conference on computer vision and pattern recognition ( CVPR )"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 862,
                                "start": 858
                            }
                        ],
                        "text": "Template 10 2 P Qualitative 2004 Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture \u22c6\u22c6 2005 Balan [7] R-Elliptical Cones 10 3 T Motion Capture \u22c6\u22c6 2005 Felzenszwalb [21] Rectangles 10 2 P Qualitative 2005 Hua [30] Quadrangular 10 2 P Hand Labeled \u266e 2005 Lan [36] Rectangles 10 2 P Motion Capture \u22c6 2005 Ramanan [58] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2005 Ren [60] Stick-Figure 9 2 P Qualitative 2005 Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER \u2020 2006 Gall [23] Mesh N/A 3 T Motion Capture \u2020 2006 Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled \u22c6\u22c6(4) 2006 Li [41] R-Elliptical Cones 10 3 T HUMAN EVA \u22c6\u22c6 2006 Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture \u2020 2006 Sigal [75] Quadrangular 10 2 P Motion Capture \u22c6 2006 Urtasun [85] Stick-figure 15 3 T Qualitative 2006 Wang [89] SPM + templates 10 2 T Motion Capture \u22c6 and\u22c4 2007 Lee [38] Joint centers N/A 3 T HUMAN EVA \u22c6\u22c6 2007 Mundermann [47] SCAPE 15 3 T Motion Capture \u22c6\u22c6 and\u22c4 2007 Navaratnam [48] Mesh N/A 3 P Motion Capture \u2020 2007 Srinivasan [82] Exemplars 6 2 P Hand Labeled \u22c6 and\u22c4 2007 Xu [91] Cylinders 10 3 T HUMAN EVA \u22c6\u22c6 2008 Bo [9] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Ning [50] Stick-figure 10 3 P HUMAN EVA \u2020 2008 Rogez [62] Joint centers 10 2/3 P HUMAN EVA \u22c6 2008 Urtasun [84] Joint centers N/A 3 P HUMAN EVA \u22c6\u22c6 2008 Vondrak [88] Ellipsoids + prisms 13 3 T HUMAN EVA \u22c6\u22c6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 488,
                                "start": 485
                            }
                        ],
                        "text": "Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error (Agarwal and Triggs 2004a, 2004b, Sminchisescu et al. 2005), normalized error in joint angles (Shakhnarovich et al. 2003), silhouette overlap (Ramanan et al. 2005; Ramanan and Forsyth 2003), joint center distance (Balan et al. 2005; Grauman et al. 2003; Lan and Huttenlocher 2005; Lee and Nevatia 2006; Li et al. 2006; Sigal et al. 2004; Sigal and Black 2006), etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 235
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 256
                            }
                        ],
                        "text": "While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007) Mesh N/A 3 P Motion Capture \u2020\n2007 Srinivasan and Shi (2007) Exemplars 6 2 P Hand Labeled and\n2007 Xu and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026ground\nTable 1 (Continued)\nYear Reference Model type Parts Dim Type Evaluation Measure\n2007 Balan et al. (2007) SCAPE 15 3 P Qualitative\n2007 Lee and Elgammal (2007) Joint centers N/A 3 T HUMANEVA\n2007 Muendermann et al. (2007) SCAPE 15 3 T Motion Capture and 2007 Navaratnam et al. (2007)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling View and Posture Man  ifold for Tracking.IEEE International Conference on Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917469"
                        ],
                        "name": "Shanon X. Ju",
                        "slug": "Shanon-X.-Ju",
                        "structuredName": {
                            "firstName": "Shanon",
                            "lastName": "Ju",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shanon X. Ju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "5 T Qualitative 1996 Ju [33] Patches 2 2 T Qualitative 1996 Kakadiaris [34] D Silhouettes 2 3 T Quantitative 1998 Bregler [11] Ellipsoids 10 3 T Qualitative* 2000 Rosales [64] Stick-Figure 10 3 P Synthetic \u22c6(2) 2000 Sidenbladh [73] Cylinders 2/10 3 T Qualitative 2002 Ronfard [63] Patches 15 2 P Hand Labeled 2002 Sidenbladh [71] Cylinders 2/10 3 T Qualitative 2003 Grauman [26] Mesh N/A 3 P Synthetic/POSER \u22c6 2003 Ramanan [59] Rectangles 10 2 T,P Hand Labeled \u22c4\u22c4 2003 Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER \u2021 2003 Sminchisescu [78, 79] Superquadric Ellip."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 108068634,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "dc132c04da4d6817a775efdac86372ae77f12b0f",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cardboard-people:-A-parametrized-model-of-motion-Ju-Black",
            "title": {
                "fragments": [],
                "text": "Cardboard people: A parametrized model of articulated motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089978"
                        ],
                        "name": "D. Damen",
                        "slug": "D.-Damen",
                        "structuredName": {
                            "firstName": "Dima",
                            "lastName": "Damen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Damen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 232
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords Articulated pose estimation \u00b7 Articulated tracking \u00b7 Motion capture \u00b7 Human tracking \u00b7 Datasets and evaluation"
                    },
                    "intents": []
                }
            ],
            "corpusId": 64711781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd116435b6f93e803e8db708ad4d0bce71499982",
            "isKey": false,
            "numCitedBy": 1554,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-and-Pattern-Recognition-(CVPR)-Damen-Hogg",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition (CVPR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775672"
                        ],
                        "name": "Dirk Ormoneit",
                        "slug": "Dirk-Ormoneit",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Ormoneit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk Ormoneit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9450153"
                        ],
                        "name": "H. Sidenbladh",
                        "slug": "H.-Sidenbladh",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Sidenbladh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sidenbladh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60142175,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "9bd63939c5e3cf2a39220f001a6a8af963463ed3",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-modeling-and-tracking-of-human-motion-Ormoneit-Sidenbladh",
            "title": {
                "fragments": [],
                "text": "Stochastic modeling and tracking of human motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35137706"
                        ],
                        "name": "J. Bouguet",
                        "slug": "J.-Bouguet",
                        "structuredName": {
                            "firstName": "J.-Y.",
                            "lastName": "Bouguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bouguet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54145653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b99e7d8237e2116f81bf65acc7ca631cb86581c5",
            "isKey": false,
            "numCitedBy": 2613,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Camera-calibration-toolbox-for-matlab-Bouguet",
            "title": {
                "fragments": [],
                "text": "Camera calibration toolbox for matlab"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 225
                            }
                        ],
                        "text": "A short summary of different approaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to (Gavrila 1999) and (Moeslund and Granum 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of computer visionbased human motion capture. Computer Vision and Image Understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 173
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database [27], initially devel oped for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorit hms [20, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human Pose Estimation using Motion  Exemplars.IEEE International Conference on Computer Vision (ICCV"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 58
                            }
                        ],
                        "text": "motion of the arm along a circular path of known diameter (Kakadiaris and Metaxas 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 170
                            }
                        ],
                        "text": "To obtain some form of ground truth, previous approaches have resorted to custom action-specific schemes; e.g. motion of the arm along a circular path of known diameter (Kakadiaris and Metaxas 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model-based estimation of 3D human motion with occlusion based on active multi-viewpoint selection. In IEEE conference on computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Randomized Trees for Human Pose Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 96
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likeli hood [80, 81] that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistency and Coupling in Human Mod  el Likelihoods.International Conference on Automatic Face and Gesture Recognition (FG)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "T }, was recovered using a Hough circle transform [29] that was manually initialized in the fi rst frame and subsequently tracked."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "T (2D)}, was recovered using a Hough circle transform (Hough 1962) that was manually initialized in the first frame and subsequently tracked."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Method and means for recognizing complex  atterns.U.S"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 130
                            }
                        ],
                        "text": "It is worth noting that simi lar efforts have been made in related areas including the development of datasets for face detection [55, 56], human g ait identification [27, 67], dense stereo vision [68] and optical flow [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 130
                            }
                        ],
                        "text": "It is worth noting that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition vendor test"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "These approaches can be combined (Sigal et al. 2004; Sminchisescu et al. 2005), such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 where data from 7 synchronized video cameras is illustrated with an overlay of ground truth body pose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CMU Motion Capture Database"
            },
            "venue": {
                "fragments": [],
                "text": "CMU Motion Capture Database"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "A variety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed for tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction of articulated objects from point correspondences in a single image"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision and Image Understanding, 80(3), 349\u2013363."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 182
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been for mulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter r esponses [66, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 218
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gibbs Likelihoods for B  ayesian Tracking"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  , vol. 1, pp. 886\u2013893,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 182
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been for mulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter r esponses [66, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 218
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gibbs Likelihoods for B  ayesian Tracking"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  , vol. 1, pp. 886\u2013893,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "T (2D)}, was recovered using a Hough circle transform (Hough 1962) that was manually initialized in the first frame and subsequently tracked."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Method and means for recognizing complex patterns"
            },
            "venue": {
                "fragments": [],
                "text": "U.S. Patent 3,069,654."
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 182
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been for mulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter r esponses [66, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 236
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning the statistics o  f people in images and video"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision (IJCV)  , 54(1\u20133):183\u2013209,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 232
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local probabilistic regression for activity-independent human pose inference. In IEEE conference on computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 170
                            }
                        ],
                        "text": "The use of strong15 prior motion models are common with early work concentrating on switching dynamical models (Pavolvic et al. 1999) and eigen-models of cyclic motions (Ormoneit et al. 2000, 2001; Sidenbladh et al. 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Le  arning and tracking cyclic human motion,  Advances in Neural Information Processing Systems 13  , pp"
            },
            "venue": {
                "fragments": [],
                "text": "894\u2013900,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 198
                            }
                        ],
                        "text": "In the short time that the dataset has been made available to the research comm unity, it has already helped with the development and evaluation of new approaches for articulated motion estima tion [8, 9, 38, 40, 41, 50, 62, 84, 88, 91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 173
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simultaneous Learnin  g of Nonlinear Manifold and Dynamical Models for High-dimensional Time Series"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 232
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008) Ellipsoids + prisms 13 3 T HUMANEVA\nMean squared distance in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local probabilistic regression for activityindependent human pose inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 130
                            }
                        ],
                        "text": "It is worth noting that simi lar efforts have been made in related areas including the development of datasets for face detection [55, 56], human g ait identification [27, 67], dense stereo vision [68] and optical flow [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 130
                            }
                        ],
                        "text": "It is worth noting that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FE  RET evaluation methodology for face-recognition algorithms.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Pattern Analysis and Machine Intellig  nce,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 303
                            }
                        ],
                        "text": "Available Yes No Yes No Yes Dataset Content Motion Walk Walk Dance Many Walk Jog Dance Exercise Throw/Catch Jumping Jacks Gesture Box Combo Appearance Natural Natural / Natural / MoCap Suit Natural MoCap Suit MoCap Suit Ground Truth Content 3D 2D None 3D 2D Source MoCap MoCap / None MoCap Manual Label [92] Manual Label"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 207
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database (Gross and Shi 2001), initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms (Fathi et al. 2007; Zhang et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 173
                            }
                        ],
                        "text": "The CMU Motion of Body (MoBo) Database [27], initially devel oped for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorit hms [20, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Body Localizatio n in Still Images Using Hierarchical Models and Hybrid Search.IEEE International Conference on Computer Vision and Patte  rn Recognition (CVPR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Triggs Kinematic jump processes for monocular 3 D human tracking"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human pose estimation from silhouettes a consistent approach using distance level sets. In International conference on computer graphics, visualization and computer vision (WSCG)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3 - D model - based tracking of humans in action : a multiview approach"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE conference on computer vision and pattern recognition ( CVPR )"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 170
                            }
                        ],
                        "text": "The use of strong15 prior motion models are common with early work concentrating on switching dynamical models (Pavolvic et al. 1999) and eigen-models of cyclic motions (Ormoneit et al. 2000, 2001; Sidenbladh et al. 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "St  ochastic modeling and tracking of human motion, Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Reid 2005; Hua et al. 2005; Sigal et al. 2004; Sigal and Black 2006; Sminchisescu et al. 2005) as well as deterministic methods (Mori et al. 2004; Taylor 2000;\nShakhnarovich et al. 2003) have been developed for tracking people from single (Agarwal and Triggs 2004a, 2004b; Felzenszwalb and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "A v ariety of statistical [1, 2, 7, 17, 30, 74, 75, 76] as well as deterministic methods [46, 83, 69] have been developed fo r tracking people from single [1, 2, 21, 30, 36, 45, 46, 58, 59, 60, 63, 75] or multiple [7, 17, 26, 74] views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction of articulated objects fr  om point correspondences in a single image"
            },
            "venue": {
                "fragments": [],
                "text": " Computer Vision and Image Understanding (CVIU)  , 80(3):349\u2013363,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 161
                            }
                        ],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s (Hogg 1983; O\u2019Rourke and Badler 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 161
                            }
                        ],
                        "text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980\u2019s [28, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Year First Author Model Type Parts Dim Type Evaluation Measure 1983 Hogg [28] Cylinders 14 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model-based vision: A program to see a walkin  g person.Image and Vision Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 213
                            }
                        ],
                        "text": "\u2026it has already helped with the development and evaluation of new approaches for articulated motion estimation (Bissacco et al. 2007; Bo et al. 2008; Lee and Elgammal 2007; Li et al. 2006, 2007; Ning et al. 2008; Rogez et al. 2008; Urtasun and Darrell 2008; Vondrak et al. 2008; Xu and Li 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and Li (2007) Cylinders 10 3 T HUMANEVA\n2008 Bo et al. (2008) Joint centers N/A 3 P HUMANEVA 2008 Ning et al. (2008) Stick-figure 10 3 P HUMANEVA \u2020\n2008 Rogez et al. (2008) Joint centers 10 2/3 P HUMANEVA\n2008 Urtasun and Darrell (2008) Joint centers N/A 3 P HUMANEVA 2008 Vondrak et al. (2008)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Randomized trees for human pose estimation. In IEEE conference on computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 130
                            }
                        ],
                        "text": "It is worth noting that similar efforts have been made in related areas including the development of datasets for face detection (Phillips et al. 2000, 2002), human gait identification (Gross and Shi 2001; Sarkar et al. 2005), dense stereo vision (Scharstein and Szeliski 2002) and optical flow\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition vendor test. http:// www.frvt.org"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "For example, more principled edge likelihoods have been formulated using measurable model edge segments (Wachter and Nagel 1999), phase information (Poon and Fleet 2002) and the learned statistics of filter responses\n(Roth et al. 2004; Sidenbladh and Black 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "For example, hybrid Monte Carlo sampling (Poon and Fleet 2002), partitioned sampling (MacCormick and Isard 2000), or covariance-scaled sampling (Sminchisescu and Triggs 2003b) are all promising alternatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo filtering : edgebased people tracking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowled g ) introduced in [77] and later extended in [42]; Gaussian Processes Latent Variable Models [86], Gaussian Processes Dynamical Models [85] and Factor Analyzers [41] are popular and effective choices particularly for instances w here little training data is available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 148
                            }
                        ],
                        "text": "Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in (Sminchisescu and Jepson 2004) and later extended in (Lu et al. 2007); Gaussian Processes Latent Variable Models (Urtasun et al. 2005), Gaussian Processes Dynamical Models (Urtasun et al. 2006) and Factor\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "People Tracki  ng with the Laplacian Eigenmaps Latent Variable Model.Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 170
                            }
                        ],
                        "text": "To obtain some form of ground truth, previous approaches have resorted to custom action-specific schemes; e.g. motion of the arm along a circular path of known diameter (Kakadiaris and Metaxas 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model - based estimation of 3 D human motion with occlusion based on active multiviewpoint selection"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE conference on computer vision and pattern recognition ( CVPR )"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likelihood (Sminchisescu and Telea 2002; Sminchisescu\n2002) that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "We can correct this by defining a symmetric silhouette likeli hood [80, 81] that penalizes non-overlapping regions for both silhouettes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human pose estimation fro  m silhouettes a consistent approach using distance level sets.International Conference on Computer Graphics, Visualiza  tion and Computer Vision (WSCG"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3-D model-based tracking of humans in action: a multi-view approach. In IEEE conference on computer vision and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 94,
            "methodology": 73,
            "result": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 142,
        "totalPages": 15
    },
    "page_url": "https://www.semanticscholar.org/paper/HumanEva:-Synchronized-Video-and-Motion-Capture-and-Sigal-Balan/7722159088c18c9998a27047a2b18d8cce313935?sort=total-citations"
}