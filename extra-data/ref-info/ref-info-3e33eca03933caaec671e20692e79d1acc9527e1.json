{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144515983"
                        ],
                        "name": "J. Sirat",
                        "slug": "J.-Sirat",
                        "structuredName": {
                            "firstName": "Jacques-Ariel",
                            "lastName": "Sirat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sirat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62625175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd7f63611d59ae32fb0a029f978d4b0c1168adf3",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors propose a new classifier based on neural network techniques. The \u2018network\u2019 consists of a set of perceptrons functionally organized in a binary tree (\u2018neural tree\u2019). The learning algorithm is inspired from a growth algorithm, the tiling algorithm, recently introduced for feedforward neural networks. As in the former case, this is a constructive algorithm, for which convergence is guaranteed. In the neural tree one distinguishes the structural organization from the functional organization: each neuron of a neural tree receives inputs from, and only from, the input layer; its output does not feed into any other neuron, but is used to propagate down a decision tree. The main advantage of this approach is due to the local processing in restricted portions of input space, during both learning and classification. Moreover, only a small subset of neurons have to be updated during the classification stage. Finally, this approach is easily and efficiently extended to classification in a multiclass probl..."
            },
            "slug": "Neural-trees:-a-new-tool-for-classification-Sirat-Nadal",
            "title": {
                "fragments": [],
                "text": "Neural trees: a new tool for classification"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new classifier based on neural network techniques, inspired from a growth algorithm, recently introduced for feedforward neural networks, which is easily and efficiently extended to classification in a multiclass probl..."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145630387"
                        ],
                        "name": "Ananth Sankar",
                        "slug": "Ananth-Sankar",
                        "structuredName": {
                            "firstName": "Ananth",
                            "lastName": "Sankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ananth Sankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872039"
                        ],
                        "name": "R. Mammone",
                        "slug": "R.-Mammone",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mammone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mammone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62716089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "416f95c878f2d2152ba3e37e98064a8c5e13123f",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks and decision trees are two common approaches to pattern recognition. In this paper, these approaches are combined to develop a new neural network architecture based on decision trees and a new learning rule to grow this architecture using neural network techniques. The resulting neural network is called a neural tree network (NTN). The NTN can be implemented very efficiently as compared to multilayer perceptrons (MLP). The learning algorithm is more efficient than the exhaustive search techniques used in standard decision tree methods. The algorithm also grows the network, thus finding the correct number of neurons as opposed to the backpropagation algorithm used to train MLPs in which the number of neurons and their interconnections must be known before learning can begin. Two different approaches are presented to grow the NTN based on self-organizing clustering techniques and a supervised learning rule. Simulation results are presented on a speaker-independent vowel recognition task which show the superiority of the NTN approach over both MLPs and decision trees.\u00a9 (1991) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."
            },
            "slug": "Combining-neural-networks-and-decision-trees-Sankar-Mammone",
            "title": {
                "fragments": [],
                "text": "Combining neural networks and decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Simulation results are presented on a speaker-independent vowel recognition task which show the superiority of the NTN approach over both MLPs and decision trees."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70164614"
                        ],
                        "name": "Wayne Ieee",
                        "slug": "Wayne-Ieee",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Ieee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Ieee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62345153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e5d49d10b56d3a48def156af8ab6293e86c4e9",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ".A multiple-layer artificial network (ANN) structure is capable of implementing arbitrary input-output mappings. Similarly, hierarchical classifiers, more commonly known as decision trees, possess the capabilites of generating arbitrarily complex decision boundaries in an n-dimensional space. Given a decision tree, it is possible to restructure it as a multilayered neural network. The objective of this paper is to show how this mapping of decision trees into a multilayer neural network structure can be exploited for the systematic design of a class of layered neural networks, called entropy nets, that have far fewer connections. Several important issues such as the automatic tree generation, incorporation of incremental learning, and the generalization of knowledge acquired during the treedesign phase are discussed. Finally, a two-step methodology for designing entropy networks is presented. The advantages of this methodology are that it specifies the number of neurons needed in each layer, alongwith thedesired output. This leads to a faster progressive training procedure that allows each layer to be trained separately. Two examples are presented to show the success of neural network design through decision tree mapping."
            },
            "slug": "Entropy-Nets:-From-Decision-Trees-to-Neural-Ieee",
            "title": {
                "fragments": [],
                "text": "Entropy Nets: From Decision Trees to Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how the mapping of decision trees into a multilayer neural network structure can be exploited for the systematic design of a class of layered neural networks, called entropy nets, that have far fewer connections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788202"
                        ],
                        "name": "M. Goudreau",
                        "slug": "M.-Goudreau",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Goudreau",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goudreau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14926063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95028f80968188df6bf673d1cc1547edcc0993b2",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "It is often difficult to predict the optimal neural network size for a particular application. Constructive or destructive methods that add or subtract neurons, layers, connections, etc. might offer a solution to this problem. We prove that one method, recurrent cascade correlation, due to its topology, has fundamental limitations in representation and thus in its learning capabilities. It cannot represent with monotone (i.e., sigmoid) and hard-threshold activation functions certain finite state automata. We give a \"preliminary\" approach on how to get around these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully-recurrent structure. We illustrate this approach by simulations which learn many examples of regular grammars that the recurrent cascade correlation method is unable to learn."
            },
            "slug": "Constructive-learning-of-recurrent-neural-networks:-Giles-Chen",
            "title": {
                "fragments": [],
                "text": "Constructive learning of recurrent neural networks: limitations of recurrent cascade correlation and a simple solution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proves that one method, recurrent cascade correlation, has fundamental limitations in representation and thus in its learning capabilities, and gives a \"preliminary\" approach on how to get around these limitations by devising a simple constructive training method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2938,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8785487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14a9a814a54dbab99388fafbd96a1c5fe249e376",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a study on LRAAM-based (Labeling Recursive Auto-Associative Memory)classification of symbolic recursive structures encoding terms. The results reported here have been obtained by combining an LRAAM network with an analog perceptron. The approach used was to interleave the development of representations (unsupervised learning of the LRAAM) with the learning of the classification task. In this way, the representations are optimized with respect to the classification task. The intended applications of the approach described in this paper are hybrid (symbolic/connectionist) systems, where the connectionist part has to solve logic-oriented inductive learning tasks similar to the term-classification problems used in our experiments. These problems range from the detection of a specific subterm to the satisfaction of a specific unification pattern, and they can get a very satisfactory solution by our approach."
            },
            "slug": "Learning-Distributed-Representations-for-the-of-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Learning Distributed Representations for the Classification of Terms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The intended applications of the approach described in this paper are hybrid (symbolic/connectionist) systems, where the connectionist part has to solve logic-oriented inductive learning tasks similar to the term-classification problems used in the experiments."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705299"
                        ],
                        "name": "L. Atlas",
                        "slug": "L.-Atlas",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Atlas",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Atlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33259680"
                        ],
                        "name": "J. Connor",
                        "slug": "J.-Connor",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Connor",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145958247"
                        ],
                        "name": "Dong-Chul Park",
                        "slug": "Dong-Chul-Park",
                        "structuredName": {
                            "firstName": "Dong-Chul",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong-Chul Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15845288"
                        ],
                        "name": "M. El-Sharkawi",
                        "slug": "M.-El-Sharkawi",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "El-Sharkawi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. El-Sharkawi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47398186"
                        ],
                        "name": "R. Marks",
                        "slug": "R.-Marks",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Marks",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144082528"
                        ],
                        "name": "A. Lippman",
                        "slug": "A.-Lippman",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lippman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lippman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8089863"
                        ],
                        "name": "R. Cole",
                        "slug": "R.-Cole",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Cole",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2007265"
                        ],
                        "name": "Y. Muthusamy",
                        "slug": "Y.-Muthusamy",
                        "structuredName": {
                            "firstName": "Yeshwant",
                            "lastName": "Muthusamy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Muthusamy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1920852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84d444d6c9d91879a887bbb0dcde087040173ea7",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer perceptrons and trained classification trees are two very different techniques which have recently become popular. Giving enough data and time, both methods are capable of performing arbitrary nonlinear classification. The two techniques have not previously been compared on real-world problems. The authors first consider the important differences between multilayer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. They then present results of a number of empirical tests on quite different problems in power system load forecasting and speaker-independent vowel identification. They compare the performance for classification and prediction in terms of accuracy outside the training set. In all cases, even with various sizes of training sets, the multilayer perceptron performed as well as or better than the trained classification trees. The authors are confident that the univariate version of the trained classification trees do not perform as well as the multilayer perceptron. More studies are needed, however, on the comparative performance of the linear combination version of the classification trees.<<ETX>>"
            },
            "slug": "A-performance-comparison-of-trained-multilayer-and-Atlas-Connor",
            "title": {
                "fragments": [],
                "text": "A performance comparison of trained multilayer perceptrons and trained classification trees"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "There is not enough theoretical basis for the clear-cut superiority of one technique over the other in terms of classification and prediction outside the training set, and the authors are confident that the univariate version of the trained classification trees do not perform as well as the multilayer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6422558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dfc50eeb234d3458b13c055a57b0119c5c9435",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Labeling recursive auto-associative memory (LRAAM) is an extension of the RAAM model by Pollack (1990) to obtain distributed reduced representations of labeled directed graphs. In this paper some mathematical properties of LRAAM are discussed. Specifically, sufficient conditions on the asymptotical stability of the decoding process along a cycle of the encoded structure are given. LRAAM can be transformed into an analog Hopfield network with hidden units and an asymmetric connections matrix by connecting the output units with the input units. In this architecture encoded data can be accessed by content and different access procedures can be defined depending on the access key. Each access procedure corresponds to a particular constrained version of the recurrent network. The authors give sufficient conditions under which the property of asymptotical stability of a fixed point in one particular constrained version of the recurrent network can be extended to related fixed points in different constrained versions of the network. An example of encoding of a labeled directed graph on which the theoretical results are applied is given and discussed."
            },
            "slug": "Stability-properties-of-labeling-recursive-memory-Sperduti",
            "title": {
                "fragments": [],
                "text": "Stability properties of labeling recursive auto-associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors give sufficient conditions under which the property of asymptotical stability of a fixed point in one particular constrained version of the recurrent network can be extended to related fixed points in different constrained versions of the network."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61657495,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c476197defeb0c66e66c25826d0707454ce527bb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for generating tree structured neural networks using a soft-competitive recursive partitioning rule is described. It is demonstrated that this algorithm grows robust, honest estimators. Preliminary results on a 10-class, 240-dimensional optical character recognition classification task show that the tree outperforms backpropagation. Arguments are made that suggest why this should be the case. The connection of the soft-competitive splitting rule to the twoing rule is described.<<ETX>>"
            },
            "slug": "A-soft-competitive-splitting-rule-for-adaptive-Perrone",
            "title": {
                "fragments": [],
                "text": "A soft-competitive splitting rule for adaptive tree-structured neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that this algorithm grows robust, honest estimators and it is shown that the tree outperforms backpropagation on a 10-class, 240-dimensional optical character recognition classification task."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2537503,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "24b9eebe49cf7e00cf50cf7b7d9243386a23fe7c",
            "isKey": false,
            "numCitedBy": 6274,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A model for a large network of \"neurons\" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed."
            },
            "slug": "Neurons-with-graded-response-have-collective-like-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neurons with graded response have collective computational properties like those of two-state neurons."
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A model for a large network of \"neurons\" with a graded response (or sigmoid input-output relation) is studied and collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 770011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a835df43fdc2f79126319f6fa033bb42147c6f6",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Distributed-Representations-Pollack",
            "title": {
                "fragments": [],
                "text": "Recursive Distributed Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686506"
                        ],
                        "name": "A. Atiya",
                        "slug": "A.-Atiya",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Atiya",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atiya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11168657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab033369446f669153b18b1fa7c907f2385cbdff",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper generalizes the backpropagation method to a general network containing feedback connections. The network model considered consists of interconnected groups of neurons, where each group could be fully interconnected (it could have feedback connections, with possibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent algorithm is applied, under a certain inequality constraint on each intra-group weight matrix which ensures for the network to possess a unique equilibrium state for every input."
            },
            "slug": "Learning-on-a-General-Network-Atiya",
            "title": {
                "fragments": [],
                "text": "Learning on a General Network"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The backpropagation method is generalized to a general network containing feedback connections, where each group could be fully interconnected (it could have feedback connections), but no loops between the groups are allowed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9861,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58997935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef634444b659fca4a0783d01cf46088bb0cd4695",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss a general method which allows to implement a Neural Tree in a high-order recurrent network, the Executor, using an extension of the RAAM model, the Labeling RAAM model (LRAAM). Neural Trees and LRAAMs are briefly reviewed and the Executor defined. A Neural Tree is encoded by a LRAAM, and the decoding part of the LRAAM used to control the dynamics of the Executor. The main aspect of this kind of technique is that the weights of the LRAAM can be considered as a neural code implementing the Neural Tree on the Executor. An example of the method is presented for the 8 bits parity problem."
            },
            "slug": "An-Example-of-Neural-Code:-Neural-Trees-Implemented-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "An Example of Neural Code: Neural Trees Implemented by LRAAMs"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A general method which allows to implement a Neural Tree in a high-order recurrent network, the Executor, using an extension of the RAAM model, the Labeling RAam model (LRAAM)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27136774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01641bd7fdbf895e290144d4149c27de9e035e59",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present two unsupervised neural network splitting rules for use with CART-like neural tree algorithms in high-dimensional data space. These splitting rules use an adaptive variance estimate to avoid some possible local minima which arise in unsupervised methods. They explain when the unsupervised splitting rules outperform supervised neural network splitting rules and when the unsupervised splitting rules outperform the standard node impurity splitting rules of CART. Using these unsupervised splitting rules leads to a nonparametric classifier for high-dimensional space that extracts local features in an optimized way.<<ETX>>"
            },
            "slug": "Unsupervised-splitting-rules-for-neural-tree-Perrone-Intrator",
            "title": {
                "fragments": [],
                "text": "Unsupervised splitting rules for neural tree classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Two unsupervised neural network splitting rules for use with CART-like neural tree algorithms in high-dimensional data space use an adaptive variance estimate to avoid some possible local minima which arise in unsuper supervised methods."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "Other related techniques for dealing with structured patterns can be found, for example, in [21, 28, 29, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "The basic LRAAM organizes the representational space in such a way that similar structures get similar reduced representations (see [28] for more details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9868915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb204afe3e9237f50ef8ea3b8dbf751cc096e2a0",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose an extension of the Recursive Auto-Associative Memory (RAAM) by Pollack. This extension, the Labeling RAAM (LRAAM), is able to encode labeled graphs with cycles by representing pointers explicitly. A theoretical analysis of the constraints imposed on the weights by the learning task under the hypothesis of perfect learning and linear output units is presented. Cycles and connuent pointers result to be particularly eeective in imposing constraints on the weights. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent decoding along a cycle. Data encoded in a LRAAM can be ac-cessed by pointer as well as by content. The direct access by content can be achieved by transforming the encoder network of the LRAAM in a Bidirectional Associative Memory (BAM). Diierent access procedures can be deened according to the access key. The access procedures are not wholly reliable, however they seem to have a high likelihood of success. A geometric interpretation of the decoding process is given and the representations developed in the pointer space of a two hidden units LRAAM are presented and discussed. In particular, the pointer space results to be partitioned in a fractal-like fashion. Some eeects on the representations induced by the Hoppeld-like dynamics of the pointer decoding process are discussed and an encoding scheme able to retain the richness of representation devised by the decoding function is outlined. The application of the LRAAM model to the control of the dynamics of recurrent high-order networks is brieey sketched as well."
            },
            "slug": "Labeling-Raam-Sperduti",
            "title": {
                "fragments": [],
                "text": "Labeling Raam"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This extension of the Recursive Auto-Associative Memory (RAAM), the Labeling RAAM, is able to encode labeled graphs with cycles by representing pointers explicitly, and an encoding scheme able to retain the richness of representation devised by the decoding function is outlined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35327752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7411b86dfcff0151c30f095150e029b3bce9fbdf",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper, we propose an extension to the recursive auto-associative memory (RAAM) by Pollack. This extension, the labelling RAAM (LRAAM), can encode labelled graphs with cycles by representing pointers explicitly. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent decoding along a cycle. Theoretical and experimental results show that the performances of the proposed learning scheme depend on the way the graphs are represented in the training set. Critical features for the representation are cycles and confluent pointers. Data encoded in a LRAAM can be accessed by a pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into a particular bidirectional associative memory (BAM). Statistics performed on different instances of LRAAM show..."
            },
            "slug": "Labelling-Recursive-Auto-associative-Memory-Sperduti",
            "title": {
                "fragments": [],
                "text": "Labelling Recursive Auto-associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Theoretical and experimental results show that the performances of the proposed learning scheme depend on the way the graphs are represented in the training set, and the representations developed for the pointers seem to be robust to recurrent decoding along a cycle."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107535271"
                        ],
                        "name": "T. Li",
                        "slug": "T.-Li",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49059191"
                        ],
                        "name": "L. Fang",
                        "slug": "L.-Fang",
                        "structuredName": {
                            "firstName": "Luyuan",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152331171"
                        ],
                        "name": "A. Jennings",
                        "slug": "A.-Jennings",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Jennings",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jennings"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61486709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57b338bd192f91689635413a21e9b5c14ab0866b",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An architecture for an adaptive self-organizing neural tree is proposed. The adaptive neural tree adapts to the changing environment by adding and deleting nodes. It also performs parameter adaptation by constantly adjusting the connection weights. It has the successive approximation property which enables hierarchical classification and fast search implementation. An example is given to illustrate the adaptivity of the neural tree. The statistics of the learning behavior are also given.<<ETX>>"
            },
            "slug": "Structurally-adaptive-self-organizing-neural-trees-Li-Fang",
            "title": {
                "fragments": [],
                "text": "Structurally adaptive self-organizing neural trees"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An architecture for an adaptive self-organizing neural tree that adapts to the changing environment by adding and deleting nodes and performs parameter adaptation by constantly adjusting the connection weights is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15720720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8cf03655d224b0994d0f9d4f5aa80bca07021a",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training."
            },
            "slug": "The-Recurrent-Cascade-Correlation-Architecture-Fahlman",
            "title": {
                "fragments": [],
                "text": "The Recurrent Cascade-Correlation Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Recurrent Cascade-Correlation is a recurrent version of the Cascade- Correlation learning architecture of Fahlman and Lebiere that can learn from examples to map a sequence of inputs into a desired sequence of outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "1 Back-propagation through Structure The task addressed by back-propagation through time networks [23] is to produce particular output sequences in response to speci c input sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60899176,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "249ce7a85b158c16ba108451070c07aa1156e7eb",
            "isKey": false,
            "numCitedBy": 1139,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Very rarely, a book is published which not only advances our knowledge of a particular topic, but fundamentally recasts our methods of investigating and thinking about large tracts of the map of learning. Linguists remember 1957 as the publication year of Noam Chomsky's Syntactic structures-a book whose ostensible subjects were the structure of English grammatical rules and the goals of grammatical description, but which can be seen with hindsight as the first shot in an intellectual revolution which ended by radically changing the texture of day-to-day research activity and discourse throughout almost all of linguistics, and in substantial parts of other cognition-related disciplines. In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was (there have been scattered journal articles in the preceding four or five years). I would go further and suggest that, if the promises of this book can be redeemed, the contrast in linguistics and neighboring disciplines between the 1990's and the 1970's will be significantly greater than the contrast between the 1970's and the 1950's. (I need hardly add, of course, that it is one thing to fire an opening salvo, but another to achieve ultimate predominance.) The new paradigm is called Parallel Distributed Processing by the sixteen writers who contributed to this book, many of whom work either at the University of California, San Diego, or at Carnegie-Mellon University in Pittsburgh. Some other researchers (e.g. Feldman 1985) use the term 'connectionism' for the same concept. These two volumes comprise 26 chapters which, among them, (i) explain the over-all nature and aims of PDP/connectionist models, (ii) define a family of specific variants of the general paradigm, and (iii) exemplify it by describing experiments in which PDP models were used to simulate human performance in various cognitive domains. The experiments, inevitably, treat their respective domains in a simplified, schematic way by comparison with the endless complexity found in any real-life cognitive area; but simplification in this case does not mean trivialization. There are also auxiliary chapters on relevant related topics; thus Chap. 9, by M. I. JORDAN, is a tutorial on linear algebra, a branch of mathematics having special significance for the PDP paradigm. (Each chapter is attributed to a particular author or"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the microstructures of cogni-"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144999249"
                        ],
                        "name": "G. Kane",
                        "slug": "G.-Kane",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kane",
                            "middleNames": [
                                "Stanley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kane"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71267845,
            "fieldsOfStudy": [
                "Computer Science",
                "History"
            ],
            "id": "78f6f0ac3d501cb0073a7d94edde5267044a59ae",
            "isKey": false,
            "numCitedBy": 2758,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Computing: Theory and Practice , by Philip Wasserman, 230 pp, $41.95, with illus, ISBN 0-442-20743-3, New York, NY, Van Nostrand Reinhold, 1989. Neural Networks: A Tutorial , by Michael Chester, 182 pp, $38, with illus, ISBN 0-13-368903-4, Englewood Cliffs, NJ, Prentice Hall, 1993. Neural Networks: Algorithms, Applications, and Programming Techniques , by James Freeman and David Skapura, 401 pp, $50.50, with illus, ISBN 0-201-51376-5, Reading, Mass, Addison-Wesley, 1991. Understanding Neural Networks: Computer Explorations , vol 1: Basic Networks , vol 2: Advanced Networks , 309, 367 pp, by Maureen Caudill and Charles Butler, paper, with illus, spiral-bound, with 1 diskette/vol, $39.95/vol, vol 1: ISBN0-262-53102-X (Macintosh), 0-262-53099-6 (IBM), vol 2: ISBN 0-262-53103-8 (Macintosh), 0-262-53100-3 (IBM), Cambridge, Mass, The MIT Press, 1992. Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons , in which they proved that neural networks, as then conceived, can"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Kane",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol 1: Foundations, vol 2: Psychological and Biological Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons, in which they proved that neural networks, as then conceived, can be proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63871841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9557accd4e4a9dab69fb9a76c7e64c48ee24f5d2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Labeling RAAM model is a neural network able to encode labeled graphs in fixed size representations. In order to speed up the training procedure and for reducing in size the developed compressed representations, we propose a modular Labeling RAAM. In order to develop the modular system, we face two main problems: the mapping problem, i.e., how to map components of the structures into modules; and the membership problem, i.e., discovering which module must be used for decoding a compressed representation. The mapping between components and modules can be decided on the basis of the strongly connected components of the structures. The membership problem is solved by resorting to the BAMs derived from each LRAAM module. Preliminary results on the modular system are encouraging."
            },
            "slug": "Modular-Labeling-RAAM-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Modular Labeling RAAM"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A modular Labeling RAAM system is proposed to speed up the training procedure and for reducing in size the developed compressed representations, by resorting to the BAMs derived from each LRAAM module."
            },
            "venue": {
                "fragments": [],
                "text": "ICANNGA"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111403674"
                        ],
                        "name": "Michael A. Cohen",
                        "slug": "Michael-A.-Cohen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cohen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2215551,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "00e6b6ea28c0217d7c7e90824c17b37528f69104",
            "isKey": false,
            "numCitedBy": 2238,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems that are competitive and possess symmetric interactions admit a global Lyapunov function. However, a global Lyapunov function whose equilibrium set can be effectively analyzed has not yet been discovered. It remains an open question whether the Lyapunov function approach, which requires a study of equilibrium points, or an alternative global approach, such as the Lyapunov functional approach, which sidesteps a direct study of equilibrium points will ultimately handle all of the physically important cases."
            },
            "slug": "Absolute-stability-of-global-pattern-formation-and-Cohen-Grossberg",
            "title": {
                "fragments": [],
                "text": "Absolute stability of global pattern formation and parallel memory storage by competitive neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It remains an open question whether the Lyapunov function approach, which requires a study of equilibrium points, or an alternative global approach, such as the LyAPunov functional approach, will ultimately handle all of the physically important cases."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16762143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f47f99a16f60d649f7d3c1c6a26c6eff68e2e3da",
            "isKey": false,
            "numCitedBy": 1754,
            "numCiting": 178,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-Logic-Programming:-Theory-and-Methods-Muggleton-Raedt",
            "title": {
                "fragments": [],
                "text": "Inductive Logic Programming: Theory and Methods"
            },
            "venue": {
                "fragments": [],
                "text": "J. Log. Program."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5853754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f03db7ef9cf309561eb02eb317b875deb8817c01",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose an extension to the RAAM by Pollack. This extension, the Labeling RAAM (LRAAM), can encode labeled graphs with cycles by representing pointers explicitly. Data encoded in an LRAAM can be accessed by pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units. Different access procedures can be defined depending on the access key. Sufficient conditions on the asymptotical stability of the associated Hopfield network are briefly introduced."
            },
            "slug": "Encoding-Labeled-Graphs-by-Labeling-RAAM-Sperduti",
            "title": {
                "fragments": [],
                "text": "Encoding Labeled Graphs by Labeling RAAM"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The Labeling RAAM (LRAAM), an extension to the RAAM by Pollack, can encode labeled graphs with cycles by representing pointers explicitly by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They are decision trees [ 4 ] where the splitting of the data for each vertex, i.e., the classification of a pattern according to some features, is performed by a perceptron [26] or a more complex neural network [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16307,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136821"
                        ],
                        "name": "D. Rouvray",
                        "slug": "D.-Rouvray",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Rouvray",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rouvray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118946894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a658730fe7dd269186dca9b09cde32fed232c169",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-chemical-graph-theory-Rouvray",
            "title": {
                "fragments": [],
                "text": "Computational chemical graph theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145630387"
                        ],
                        "name": "Ananth Sankar",
                        "slug": "Ananth-Sankar",
                        "structuredName": {
                            "firstName": "Ananth",
                            "lastName": "Sankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ananth Sankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872039"
                        ],
                        "name": "R. Mammone",
                        "slug": "R.-Mammone",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mammone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mammone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117310483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8441e8e1181ad44f7009d1b7b939e56fe1ee0b7c",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-tree-networks-Sankar-Mammone",
            "title": {
                "fragments": [],
                "text": "Neural tree networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145951835"
                        ],
                        "name": "M. Holloway",
                        "slug": "M.-Holloway",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Holloway",
                            "middleNames": [
                                "Katharine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holloway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 93783235,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "150fbd2848798a980ddb66d18b587b1cdd2bcd4d",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reviews-in-Computational-Chemistry,-Volume-10-by-B.-Holloway",
            "title": {
                "fragments": [],
                "text": "Reviews in Computational Chemistry, Volume 10 Edited by Kenny B. Lipkowitz and Donald B. Boyd. VCH Publishers, Inc., New York. 1997. xxiii + 334 pp. 16 \u00d7 24 cm. ISBN 1-56081-957-X. $120.00."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Both supervised [ 3 ], [24]\u2010[26] and unsupervised [16], [18], [19] splitting of the data have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63509811,
            "fieldsOfStudy": [],
            "id": "0a88c17d434e6bc17279951bd3af12a5d2bed13f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A performance comparison of trained multilayer perceptrons and trained classification trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836317"
                        ],
                        "name": "O. Firschein",
                        "slug": "O.-Firschein",
                        "structuredName": {
                            "firstName": "Oscar",
                            "lastName": "Firschein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Firschein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6554647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86dbe878a56e5052a66c036996416a782b4da618",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Syntactic-pattern-recognition-and-applications-Firschein",
            "title": {
                "fragments": [],
                "text": "Syntactic pattern recognition and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Syntactical Pattern Recognition"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Encoding of graphs for neural network processing"
            },
            "venue": {
                "fragments": [],
                "text": "Encoding of graphs for neural network processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A. Sperduti. Labeling RAAM. Connection Science"
            },
            "venue": {
                "fragments": [],
                "text": "A. Sperduti. Labeling RAAM. Connection Science"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "This compressed representation is obtained by using the standard learning algorithm for LRAAM [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fixed length representation of terms in hybrid reasoning  systems"
            },
            "venue": {
                "fragments": [],
                "text": "report i: Classi cation of ground terms. Technical Report TR-19/94, Dipartimento di  Informatica, Universit  a di Pisa"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Task-Dependent Distributed Structure-Representations by Backpropagation Through Structure"
            },
            "venue": {
                "fragments": [],
                "text": "Learning Task-Dependent Distributed Structure-Representations by Backpropagation Through Structure"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fixed length representation of terms in hybrid reasoning systems, report i: Classiication of ground terms"
            },
            "venue": {
                "fragments": [],
                "text": "Fixed length representation of terms in hybrid reasoning systems, report i: Classiication of ground terms"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "1 Case I: DAGs This case has been treated by Goller and K\u007f uchler in [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Task-Dependent Distributed Structure-Representations by Back-  propagation Through Structure. AR-report AR-95-02, Institut f\u007f  ur Informatik, Technische Universit\u007f  at  M\u007f"
            },
            "venue": {
                "fragments": [],
                "text": "unchen,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 188
                            }
                        ],
                        "text": "For example, in molecular biology and chemistry, where chemical compounds are represented as labeled graphs, the encoding process is performed through the de nition of topological indexes [22, 14], which are designed by a very expensive trial and error approach."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational Chemical Graph Theory, page 9. Nova Science Publishers: New York"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational Chemical Graph Theory, page 9"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Chemical Graph Theory, page 9"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Tree Networks, pages 281{302"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Theory and Applications"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "They are decision trees [4] where the splitting of the data for each vertex, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi cation and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Wadsworth  International Group"
            },
            "year": 1984
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Supervised-neural-networks-for-the-classification-Sperduti-Starita/3e33eca03933caaec671e20692e79d1acc9527e1?sort=total-citations"
}