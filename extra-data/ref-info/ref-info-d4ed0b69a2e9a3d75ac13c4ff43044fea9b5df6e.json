{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797063"
                        ],
                        "name": "S. Agarwal",
                        "slug": "S.-Agarwal",
                        "structuredName": {
                            "firstName": "Shivani",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32177941"
                        ],
                        "name": "A. Awan",
                        "slug": "A.-Awan",
                        "structuredName": {
                            "firstName": "Aatif",
                            "lastName": "Awan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Awan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "A handful of large scale datasets exist currently to serve such a purpose, such as Caltech 101 [4], the UIUC car dataset [1], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "A few other object category datasets such as [1] are also used by researchers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8855331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b82d251ed367593366680acebc81fdb070b04a18",
            "isKey": false,
            "numCitedBy": 963,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of detecting objects in still, gray-scale images. Our primary focus is the development of a learning-based approach to the problem that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in the previous work. A secondary focus of this paper is to highlight these issues, and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented."
            },
            "slug": "Learning-to-detect-objects-in-images-via-a-sparse,-Agarwal-Awan",
            "title": {
                "fragments": [],
                "text": "Learning to detect objects in images via a sparse, part-based representation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A learning-based approach to the problem of detecting objects in still, gray-scale images that makes use of a sparse, part-based representation is developed and a critical evaluation of the approach under the proposed standards is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072510260"
                        ],
                        "name": "Brendan M. Collins",
                        "slug": "Brendan-M.-Collins",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Collins",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brendan M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "352 true \u201caccordions\u201d out of 1659 images). 3. Princeton-23 ( Collins et al. 2008 )"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1408150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d14a9033b4ff532ba39c2f7d0aeac8a5548f7",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "As computer vision research considers more object categories and greater variation within object categories, it is clear that larger and more exhaustive datasets are necessary. However, the process of collecting such datasets is laborious and monotonous. We consider the setting in which many images have been automatically collected for a visual category (typically by automatic internet search), and we must separate relevant images from noise. We present a discriminative learning process which employs active, online learning to quickly classify many images with minimal user input. The principle advantage of this work over previous endeavors is its scalability. We demonstrate precision which is often superior to the state-of-the-art, with scalability which exceeds previous work."
            },
            "slug": "Towards-Scalable-Dataset-Construction:-An-Active-Collins-Deng",
            "title": {
                "fragments": [],
                "text": "Towards Scalable Dataset Construction: An Active Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a discriminative learning process which employs active, online learning to quickly classify many images with minimal user input, and demonstrates precision which is often superior to the state-of-the-art, with scalability which exceeds previous work."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7995574"
                        ],
                        "name": "Huamin Feng",
                        "slug": "Huamin-Feng",
                        "structuredName": {
                            "firstName": "Huamin",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huamin Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "proposed a method to refine search engine returns by co-training [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8222554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3228dd21794441938b5cf87880d2ded86f8c1d45",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Huge amount of manual efforts are required to annotate large image/video archives with text annotations. Several recent works attempted to automate this task by employing supervised learning approaches to associate visual information extracted in segmented images with semantic concepts provided by associated text. The main limitation of such approaches, however, is that large labeled training corpus is still needed for effective learning, and semantically meaningful segmentation for images is in general unavailable. This paper explores the use of bootstrapping approach to tackle this problem. The idea is to start from a small set of labeled training examples, and successively annotate a larger set of unlabeled examples. This is done using the cotraining approach, in which two \"statistically independent\" classifiers are used to co-train and co-annotate the unlabeled examples. An active learning approach is used to select the best examples to label at each stage of learning in order to maximize the learning objective. To accomplish this, we break the task of annotating images into the sub-tasks of: (a) segmenting images into meaningful units, (b) extracting appropriate features for the units, and (c) associating these features with text. Because of the uncertainty in sub-tasks (a) and (b), we adopt two independent segmentation methods (task a) and two independent sets of features (task b) to support co-training. We carried out experiments using a mid-sized image collection (comprising about 6,000 images from CorelCD, PhotoCD and Web) and demonstrated that our bootstrapping approach significantly improve the performance of annotation by about 10% in terms of F1 measure as compared to the best results obtained from the traditional supervised learning approach. Moreover, the bootstrapping approach has the key advantage of requiring much fewer labeled examples in training."
            },
            "slug": "A-bootstrapping-approach-to-annotating-large-image-Feng-Chua",
            "title": {
                "fragments": [],
                "text": "A bootstrapping approach to annotating large image collection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The bootstrapping approach is explored and it is demonstrated that it significantly improve the performance of annotation by about 10% in terms of F1 measure as compared to the best results obtained from the traditional supervised learning approach."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "A handful of large scale datasets exist currently to serve such a purpose, such as Caltech 101 [4], the UIUC car dataset [1], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "The most notable ones are [13] and [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Caltech 101 contains 101 object classes each containing between 40 to 400 images [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aedb8df8f953429ec5a6df99fda5c5d71dbee4ff",
            "isKey": false,
            "numCitedBy": 2326,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206769491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a737c107623bcffefa0bac20f1b64677f6a1255a",
            "isKey": false,
            "numCitedBy": 1142,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing only one object per image. We also extend the bag-of-words vocabulary to include 'doublets' which encode spatially local co-occurring regions. It is demonstrated that this extended vocabulary gives a cleaner image segmentation. Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image. These results demonstrate that we can successfully build object class models from an unsupervised analysis of images."
            },
            "slug": "Discovering-objects-and-their-location-in-images-Sivic-Russell",
            "title": {
                "fragments": [],
                "text": "Discovering objects and their location in images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work treats object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics, and develops a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA)."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "5, we also compare our results with LabelMe [16] for each of the 22 categories."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "In Fig.5, we also compare our results with LabelMe [16] for each of the 22 categories."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 85
                            }
                        ],
                        "text": "In all cases, OPTIMOL collected more positive images than the Caltech 101-Human, the LabelMe dataset and the approach in [2], with very few mistakes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "We will show in our experiments that our automatic, online algorithm is capable of collecting object class datasets that are far bigger than Caltech 101 or LabelMe [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Recently, LabelMe has offered an alternative way of collecting datasets of objects by having people upload their images and label them [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 188
                            }
                        ],
                        "text": "\u2022 Our experiments show that our algorithm is capable of both\nlearning highly effective object category models and collecting object category datasets far larger than that of Caltech 101 or LabelMe."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 111
                            }
                        ],
                        "text": "Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to Caltech 101 and LabelMe."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": true,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Due to the vast number of object categorization approaches (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005;  Fergus et al. 2005a, 2005b ; K rempp et al.2002; LeCun et al. 2004; Leibe and Schiele 2004 ;L owe1999), it is out of the scope of our paper to discuss all of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem is intimately related to the problem of learning and modeling generic object classes in computer vision research (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005;  Fergus et al. 2005a, 2005b ; K rempp et al.2002; LeCun et al. 2004; Leibe and Schiele 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8243889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162065fb9de1928f7abd593ee9a1b7d41b5a4310",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a \"parts and structure\" model for object category recognition that can be learnt efficiently and in a semi-supervised manner: the model is learnt from example images containing category instances, without requiring segmentation from background clutter. The model is a sparse representation of the object, and consists of a star topology configuration of parts modeling the output of a variety of feature detectors. The optimal choice of feature types (whose repertoire includes interest points, curves and regions) is made automatically. In recognition, the model may be applied efficiently in an exhaustive manner, bypassing the need for feature detectors, to give the globally optimal match within a query image. The approach is demonstrated on a wide variety of categories, and delivers both successful classification and localization of the object within the image."
            },
            "slug": "A-sparse-object-category-model-for-efficient-and-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "A sparse object category model for efficient learning and exhaustive recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A \"parts and structure\" model for object category recognition that can be learnt efficiently and in a semi-supervised manner is presented, learnt from example images containing category instances, without requiring segmentation from background clutter."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681659"
                        ],
                        "name": "Keiji Yanai",
                        "slug": "Keiji-Yanai",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Yanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keiji Yanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Another method close in spirit to ours is by Yanai and Barnard [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5225665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "300951bb440484da07b03c11a49634cad9bed531",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method for automated large scale gathering of Web images relevant to specified concepts. Our main goal is to build a knowledge base associated with as many concepts as possible for large scale object recognition studies. A second goal is supporting the building of more accurate text-based indexes for Web images. In our method, good quality candidate sets of images for each keyword are gathered as a function of analysis of the surrounding HTML text. The gathered images are then segmented into regions, and a model for the probability distribution of regions for the concept is computed using an iterative algorithm based on the previous work on statistical image annotation. The learned model is then applied to identify which images are visually relevant to the concept implied by the keyword. Implicitly, which regions or the images are relevant is also determined. Our experiments reveal that the new method performs much better than Google Image Search and a simple method based on more standard content based image retrieval methods."
            },
            "slug": "Probabilistic-web-image-gathering-Yanai-Barnard",
            "title": {
                "fragments": [],
                "text": "Probabilistic web image gathering"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A new method for automated large scale gathering of Web images relevant to specified concepts to build a knowledge base associated with as many concepts as possible for large scale object recognition studies and supporting the building of more accurate text-based indexes for Web images."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 16
                            }
                        ],
                        "text": "This can be done either manually or automatically 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "This paper presents a novel automatic dataset collecting and model learning approach that uses object recognition techniques in an incremental method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7005884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c6f0c1917bb0f7e23c4c35b553045fa39663211",
            "isKey": false,
            "numCitedBy": 830,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate tire models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets"
            },
            "slug": "Learning-object-categories-from-Google's-image-Fergus-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Learning object categories from Google's image search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new model, TSI-pLSA, is developed, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner, and can handle the high intra-class variability and large proportion of unrelated images returned by search engines."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "(2)The background class model is learnt by using a published \u2018background\u2019 image dataset [9, 5] \u03b2"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6953475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "isKey": false,
            "numCitedBy": 2365,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "slug": "One-shot-learning-of-object-categories-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "We make this choice, however, based on the recent successes of the \u201cbag of words\u201d models in object and scene recognition [17, 6], particularly the usage of latent topic models for such representation [11, 3, 19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 130
                            }
                        ],
                        "text": "A number of systems based on the bag of words model representation have shown to be effective for object and scene classification [8, 17, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2359343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97c2c77cd76ed176683fac79c115729ad45482bc",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].1 1This work was sponsored in part by the EU Project CogViSys, the University of Oxford, Shell Oil, and the National Geospatial-Intelligence Agency."
            },
            "slug": "Discovering-object-categories-in-image-collections-Sivic-Russell",
            "title": {
                "fragments": [],
                "text": "Discovering object categories in image collections"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Given a set of images containing multiple object categories, this work seeks to discover those categories and their image locations without supervision using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Due to the vast number of object categorization approaches (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b ;K rempp et al.2002; LeCun et al. 2004;  Leibe and Schiele 2004 ; L owe1999), it is out of the scope of our paper to discuss all of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem is intimately related to the problem of learning and modeling generic object classes in computer vision research (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b ;K rempp et al.2002; LeCun et al. 2004;  Leibe and Schiele 2004 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6533591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38101fac622a70b78f13625fc6502000b8756d3a",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for object categorization in real-world scenes. Following a common consensus in the field, we do not assume that a figure- ground segmentation is available prior to recognition. However, in contrast to most standard approaches for object class recognition, our approach automati- cally segments the object as a result of the categorization. This combination of recognition and segmentation into one process is made pos- sible by our use of an Implicit Shape Model, which integrates both into a common probabilistic framework. In addition to the recognition and segmentation result, it also generates a per-pixel confidence measure specifying the area that supports a hypothesis and how much it can be trusted. We use this confidence to derive a nat- ural extension of the approach to handle multiple objects in a scene and resolve ambiguities between overlapping hypotheses with a novel MDL-based criterion. In addition, we present an extensive evaluation of our method on a standard dataset for car detection and compare its performance to existing methods from the literature. Our results show that the proposed method significantly outper- forms previously published methods while needing one order of magnitude less training examples. Finally, we present results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "slug": "Combined-Object-Categorization-and-Segmentation-an-Leibe-Leonardis",
            "title": {
                "fragments": [],
                "text": "Combined Object Categorization and Segmentation With an Implicit Shape Model"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous success of generative model in object recognition ( Fei-Fei et al. 2003;  Sivic et al. 2005) and content based image retrieval (Yanai and Barnard 2005; Fergus et al. 2005b) ensure the potential ability of generative model in our framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For generative models, such as Gaussian mixture models (Weber et al. 2000; Fergus et al. 2003;  Fei-Fei et al. 2003 ), Markov random fields (Pawan Kumar et al. 2005), latent topic model (Sivic et al. 2005; Fei-Fei and Perona 2005; Wang et al. 2006) etc., both the input distribution and the output distribution are modeled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2096065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d044d7d92dd1fb80275d04d035aed71bcd3374e5",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and \"prior\" knowledge is represented as a probability density function on the parameters of these models. The \"posterior\" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a \"prior\" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images."
            },
            "slug": "A-Bayesian-approach-to-unsupervised-one-shot-of-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "A Bayesian approach to unsupervised one-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a method for learning object categories from just a few images, based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories, in a variational Bayesian framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar to  Sudderth et al. (2005a) , Wang et al. (2006), we adapt a nonparametric generative model, Hierarchical Dirichlet process (HDP) (Teh et al. 2006), for our object category model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2411869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64d64af26f90be9ff174479975f7e7f2602f9528",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object-centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images."
            },
            "slug": "Describing-Visual-Scenes-using-Transformed-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Describing Visual Scenes using Transformed Dirichlet Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work develops a hierarchical probabilistic model for the spatial structure of visual scenes based on the transformed Dirichlet process, a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001; Carson et al. 1999 ;L i et al.2000; Chen et al. 2003; Jain and Vailaya 1996; Barnard and Forsyth 2001; Barnard et al. 2003;  Jeon et al. 2003 ) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One major group of research (Barnard and Forsyth 2001; Barnard et al. 2003;  Jeon et al. 2003 ) in CBIR treats images as a collection of blobs or blocks, each corresponding to a word or phrase in the caption (with some considerable variations)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228029e7533e32a025071e31e3f4f08d2bea5f5a",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media relevance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval."
            },
            "slug": "Automatic-image-annotation-and-retrieval-using-Jeon-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Automatic image annotation and retrieval using cross-media relevance models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval by assuming that regions in an image can be described using a small vocabulary of blobs."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One major group of research (Barnard and Forsyth 2001;  Barnard et al. 2003;  Jeon et al. 2003) in CBIR treats images as a collection of blobs or blocks, each corresponding to a word or phrase in the caption (with some considerable variations)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001; Carson et al. 1999 ;L i et al.2000; Chen et al. 2003; Jain and Vailaya 1996; Barnard and Forsyth 2001;  Barnard et al. 2003;  Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 868535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
            },
            "slug": "Matching-Words-and-Pictures-Barnard-Sahin",
            "title": {
                "fragments": [],
                "text": "Matching Words and Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text, is presented, and a number of models for the joint distribution of image regions and words are developed, including several which explicitly learn the correspondence between regions and Words."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7854172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72a2c172cf49edb4a33708e05f53938f4d475432",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues. The work draws together two powerful formulations: pictorial structures (PS) and Markov random fields (MRFs) both of which have efficient algorithms for their solution. The resulting combination, which we call the object category specific MRF, suggests a solution to the problem that has long dogged MRFs namely that they provide a poor prior for specific shapes. In contrast, our model provides a prior that is global across the image plane using the PS. We develop an efficient method, OBJ CUT, to obtain segmentations using this model. Novel aspects of this method include an efficient algorithm for sampling the PS model, and the observation that the expected log likelihood of the model can be increased by a single graph cut. Results are presented on two object categories, cows and horses. We compare our methods to the state of the art in object category specific image segmentation and demonstrate significant improvements."
            },
            "slug": "OBJ-CUT-Kumar-Torr",
            "title": {
                "fragments": [],
                "text": "OBJ CUT"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues and developing an efficient method, OBJ CUT, to obtain segmentations using this model."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264779"
                        ],
                        "name": "T. Kadir",
                        "slug": "T.-Kadir",
                        "structuredName": {
                            "firstName": "Timor",
                            "lastName": "Kadir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kadir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431498"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 825395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "254be2055a84c4d80c4c8eb8e6090b3977cc6fb6",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent.This paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliency; scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated."
            },
            "slug": "Saliency,-Scale-and-Image-Description-Kadir-Brady",
            "title": {
                "fragments": [],
                "text": "Saliency, Scale and Image Description"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar datasets are Caltech-256 ( Griffin et al. 2007 ), PAS-CAL (1), LotusHill (Yao et al. 2007) and Fink and Ullman (2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52118596"
                        ],
                        "name": "S. Krempp",
                        "slug": "S.-Krempp",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Krempp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Krempp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6386555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61b933b8ef5b10ae4f6491a89f89972322534cf0",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Our long-range goal is detecting instances from a large number of object classes in a computationally efficient manner. Detectors involving a hierarchy of tests based on edges have been used elsewhere and shown to be quite fast online. However, significant further gains in efficiency in representation, error rates and computation can be realized if the family of detectors is constructed from common parts. Our parts are flexible, extended edge configurations; they are learned, not pre-designed. In training, object classes are presented sequentially; the objective is then to accommodate new classes by maximally reusing parts. Ideally, the number of distinct parts in the system would grow much more slowly than linearly with the number of classes. Initial experiments on learning to detect several hundred LTEXsymbols are encouraging."
            },
            "slug": "Sequential-Learning-of-Reusable-Parts-for-Object-Krempp-Geman",
            "title": {
                "fragments": [],
                "text": "Sequential Learning of Reusable Parts for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Initial experiments on learning to detect several hundred LTEXsymbols are encouraging, and significant further gains in efficiency in representation, error rates and computation can be realized if the family of detectors is constructed from common parts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "(2)The background class model is learnt by using a published \u2018background\u2019 image dataset [9, 5] \u03b2"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5745749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "isKey": false,
            "numCitedBy": 2487,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "slug": "Object-class-recognition-by-unsupervised-learning-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "Object class recognition by unsupervised scale-invariant learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of systems based on the bag of words model representation have shown to be effective for object and scene recognition (Fergus et al. 2005b; Sivic et al. 2005; Fei-Fei and Perona 2005;  Sudderth et al. 2005b;  Bosch et al. 2006 ;C surka et al.2004; Sivic and Zisserman 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6153430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5331349557fababfac48d47e49b44583e3bd5f6",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects"
            },
            "slug": "Learning-hierarchical-models-of-scenes,-objects,-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical models of scenes, objects, and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available and this hierarchical probabilistic model is extended to scenes containing multiple objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8276602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "711f4fe4d47b4fdd9d81cd4e1740bf42e3796613",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Current object recognition systems aim at achieving two challenging goals: recognizing numerous object classes and learning new object classes from a small number of examples. This paper provides a benchmark for evaluating progress on these fundamental tasks. Several methods have recently proposed to utilize the commonalities between object classes in order to effectively acquire new object classes. Such methods can be termed interclass transfer techniques. However, it is currently difficult to asses which of the proposed methods maximally utilizes the shared structure of related classes. To facilitate the development, as well as the assessment of methods for dealing with multiple related classes, the proposed benchmark provides labeled images of over mammal classes. The images are organized in five levels of variability, and their labels provide information on the objects identity, location and pose. A labeled benchmark containing a large number of related classes is crucial for assessing how well proposed solutions scale up, and which methods better utilize interclass transfer."
            },
            "slug": "From-Aardvark-to-Zorro-:-A-Benchmark-of-Mammal-Fink-Ullman",
            "title": {
                "fragments": [],
                "text": "From Aardvark to Zorro : A Benchmark of Mammal Images"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper provides a benchmark for evaluating progress on recognizing numerous object classes and learning new object classes from a small number of examples, and provides labeled images of over mammal classes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "We make this choice, however, based on the recent successes of the \u201cbag of words\u201d models in object and scene recognition [17, 6], particularly the usage of latent topic models for such representation [11, 3, 19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 130
                            }
                        ],
                        "text": "A number of systems based on the bag of words model representation have shown to be effective for object and scene classification [8, 17, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17027818"
                        ],
                        "name": "C. Rosenberg",
                        "slug": "C.-Rosenberg",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Rosenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rosenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7648360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59dc87ac9fd2c39d7b9f8ab1a2bf43ac53891e96",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The construction of appearance-based object detection systems is time-consuming and difficult because a large number of training examples must be collected and manually labeled in order to capture variations in object appearance. Semi-supervised training is a means for reducing the effort needed to prepare the training set by training the model with a small number of fully labeled examples and an additional set of unlabeled or weakly labeled examples. In this work we present a semi-supervised approach to training object detection systems based on self-training. We implement our approach as a wrapper around the training process of an existing object detector and present empirical results. The key contributions of this empirical study is to demonstrate that a model trained in this manner can achieve results comparable to a model trained in the traditional manner using a much larger set of fully labeled data, and that a training data selection metric that is defined independently of the detector greatly outperforms a selection metric based on the detection confidence generated by the detector."
            },
            "slug": "Semi-Supervised-Self-Training-of-Object-Detection-Rosenberg-Hebert",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Self-Training of Object Detection Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The key contributions of this empirical study are to demonstrate that a model trained in this manner can achieve results comparable to a modeltrained in the traditional manner using a much larger set of fully labeled data, and that a training data selection metric that is defined independently of the detector greatly outperforms a selection metric based on the detection confidence generated by the detector."
            },
            "venue": {
                "fragments": [],
                "text": "2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For generative models, such as Gaussian mixture models ( Weber et al. 2000;  Fergus et al. 2003; Fei-Fei et al. 2003), Markov random fields (Pawan Kumar et al. 2005), latent topic model (Sivic et al. 2005; Fei-Fei and Perona 2005; Wang et al. 2006) etc., both the input distribution and the output distribution are modeled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fergus et al. (2004) extends the constellation model ( Weber et al. 2000;  Fergus et al. 2003) to include heterogeneous parts (e.g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fei-Fei et al. (2004) adopt a generative probabilistic model called constellation model ( Weber et al. 2000;  Fergus et al. 2003) to describe the object categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8970876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf1527807ebb16020b04d4166e7ba8d27652302",
            "isKey": true,
            "numCitedBy": 757,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars."
            },
            "slug": "Unsupervised-Learning-of-Models-for-Recognition-Weber-Welling",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Models for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method to learn object class models from unlabeled and unsegmented cluttered cluttered scenes for the purpose of visual object recognition that achieves very good classification results on human faces and rear views of cars."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem is intimately related to the problem of learning and modeling generic object classes in computer vision research (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b ;K rempp et al.2002;  LeCun et al. 2004;  Leibe and Schiele 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Due to the vast number of object categorization approaches (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b ;K rempp et al.2002;  LeCun et al. 2004;  Leibe and Schiele 2004 ;L owe1999), it is out of the scope of our paper to discuss all of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116664201"
                        ],
                        "name": "Yixin Chen",
                        "slug": "Yixin-Chen",
                        "structuredName": {
                            "firstName": "Yixin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yixin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872595"
                        ],
                        "name": "Robert Krovetz",
                        "slug": "Robert-Krovetz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Krovetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Krovetz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001; Carson et al. 1999 ;L i et al.2000;  Chen et al. 2003;  Jain and Vailaya 1996; Barnard and Forsyth 2001; Barnard et al. 2003; Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Another group of approaches focuses on comparing the query image with exemplar images and retrieving images based on image similarity (Carson et al. 1999;  Chen et al. 2003;  Deng 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4227250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21b777cd55bb76be5205ae582482d2a446085397",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In a typical content-based image retrieval (CBIR) system, query results are a set of images sorted by feature similarities with respect to the query. However, images with high feature similarities to the query may be very different from the query in terms of semantics. This is known as the semantic gap. We introduce a novel image retrieval scheme, CLUster-based rEtrieval of images by unsupervised learning (CLUE), which tackles the semantic gap problem based on a hypothesis: semantically similar images tend to be clustered in some feature space. CLUE attempts to capture semantic concepts by learning the way that images of the same semantics are similar and retrieving image clusters instead of a set of ordered images. Clustering in CLUE is dynamic. In particular, clusters formed depend on which images are retrieved in response to the query. Therefore, the clusters give the algorithm as well as the users semantic relevant clues as to where to navigate. CLUE is a general approach that can be combined with any real-valued symmetric similarity measure (metric or nonmetric). Thus it may be embedded in many current CBIR systems. Experimental results based on a database of about 60, 000 images from COREL demonstrate improved performance."
            },
            "slug": "Content-based-image-retrieval-by-clustering-Chen-Wang",
            "title": {
                "fragments": [],
                "text": "Content-based image retrieval by clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel image retrieval scheme, CLUster-based rEtrieval of images by unsupervised learning (CLUE), which tackles the semantic gap problem based on a hypothesis: semantically similar images tend to be clustered in some feature space."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We introduce our HDP object category model in more detail in Sec.3.2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 65
                            }
                        ],
                        "text": "Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to Caltech 101 and LabelMe."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5626877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d1813749fcb36351fff850f0391968b62fc73b7",
            "isKey": true,
            "numCitedBy": 240,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a large scale general purpose image database with human annotated ground truth. Firstly, an all-in-all labeling framework is proposed to group visual knowledge of three levels: scene level (global geometric description), object level (segmentation, sketch representation, hierarchical decomposition), and low-mid level (2.1D layered representation, object boundary attributes, curve completion, etc.). Much of this data has not appeared in previous databases. In addition, And-Or Graph is used to organize visual elements to facilitate top-down labeling. An annotation tool is developed to realize and integrate all tasks. With this tool, we've been able to create a database consisting of more than 636,748 annotated images and video frames. Lastly, the data is organized into 13 common subsets to serve as benchmarks for diverse evaluation endeavors."
            },
            "slug": "Introduction-to-a-Large-Scale-General-Purpose-Truth-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "Introduction to a Large-Scale General Purpose Ground Truth Database: Methodology, Annotation Tool and Benchmarks"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A large scale general purpose image database with human annotated ground truth consisting of more than 636,748 annotated images and video frames is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMMCVPR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096527"
                        ],
                        "name": "G. Wang",
                        "slug": "G.-Wang",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39724818"
                        ],
                        "name": "Ye Zhang",
                        "slug": "Ye-Zhang",
                        "structuredName": {
                            "firstName": "Ye",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ye Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "According to Eq.3 and Eq.4, the probability of a new customer xji assigned to table t is:\nP (tji = t|xji, t\u2212ji, k) \u221d\n\n\n\na0ptnew for t = tnew\nnjtf(xji|\u03b8kji) for used t (5)\nwhere ptnew is the likelihood for tji = tnew: K \u2211\nk=1\nmk \u2211K\nk=1 mk + \u03b3 f(xji|\u03b8kji) +\n\u03b3 \u2211K\nk=1 mk + \u03b3 f(xji|\u03b8knew)\nf(xji|\u03b8kji) is\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2646984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca021face4bc8dcb10f404b2f7ec03487336bddf",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Bag of words\" models have enjoyed much attention and achieved good performances in recent studies of object categorization. In most of these works, local patches are modeled as basic building blocks of an image, analogous to words in text documents. In most previous works using the \"bag of words\" models (e.g. [4, 20, 7]), the local patches are assumed to be independent with each other. In this paper, we relax the independence assumption and model explicitly the inter-dependency of the local regions. Similarly to previous work , we represent images as a collection of patches, each of which belongs to a latent \"theme\" that is shared across images as well as categories. We learn the theme distributions and patch distributions over the themes in a hierarchical structure [22]. In particular, we introduce a linkage structure over the latent themes to encode the dependencies of the patches. This structure enforces the semantic connections among the patches by facilitating better clustering of the themes. As a result, our models for object categories tend to be more discriminative than the ones obtained under the independent patch assumption. We show highly competitive categorization results on both the Caltech 4 and Caltech 101 object category datasets. By examining the distributions of the latent themes for each object category, we construct an object taxonomy using the 101 object classes from the Caltech 101 datasets."
            },
            "slug": "Using-Dependent-Regions-for-Object-Categorization-a-Wang-Zhang",
            "title": {
                "fragments": [],
                "text": "Using Dependent Regions for Object Categorization in a Generative Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper relaxes the independence assumption and model explicitly the inter-dependency of the local regions of the \"bag of words\" models, and introduces a linkage structure over the latent themes to encode the dependencies of the patches."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14457153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642e328cae81c5adb30069b680cf60ba6b475153",
            "isKey": false,
            "numCitedBy": 6760,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films."
            },
            "slug": "Video-Google:-a-text-retrieval-approach-to-object-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video Google: a text retrieval approach to object matching in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video, represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8348240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e397800e8601631a8e01f210e5665b731fd7ebfc",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate a method for identifying images containing categories of animals. The images we classify depict animals in a wide range of aspects, configurations and appearances. In addition, the images typically portray multiple species that differ in appearance (e.g. ukari\u2019s, vervet monkeys, spider monkeys, rhesus monkeys, etc.). Our method is accurate despite this variation and relies on four simple cues: text, color, shape and texture. Visual cues are evaluated by a voting method that compares local image phenomena with a number of visual exemplars for the category. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required involves identifying which clusters of exemplars refer to which sense of a term (for example, \"monkey\" can refer to an animal or a bandmember). Because our method is applied to web pages with free text, the word cue is extremely noisy. We show unequivocal evidence that visual information improves performance for our task. Our method allows us to produce large, accurate and challenging visual datasets mostly automatically."
            },
            "slug": "Animals-on-the-Web-Berg-Forsyth",
            "title": {
                "fragments": [],
                "text": "Animals on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work demonstrates a method for identifying images containing categories of animals using a clustering method applied to text on web pages and shows unequivocal evidence that visual information improves performance for this task."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423856"
                        ],
                        "name": "Anna Bosch",
                        "slug": "Anna-Bosch",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062941326"
                        ],
                        "name": "X. Mu\u00f1oz",
                        "slug": "X.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Mu\u00f1oz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of systems based on the bag of words model representation have shown to be effective for object and scene recognition (Fergus et al. 2005b; Sivic et al. 2005; Fei-Fei and Perona 2005; Sudderth et al. 2005b;  Bosch et al. 2006 ; C surka et al.2004; Sivic and Zisserman 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1260607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d2aa6511bd0a8de5cb690bf406d90eef902ff1",
            "isKey": false,
            "numCitedBy": 859,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a set of images of scenes containing multiple object categories (e.g. grass, roads, buildings) our objective is to discover these objects in each image in an unsupervised manner, and to use this object distribution to perform scene classification. We achieve this discovery using probabilistic Latent Semantic Analysis (pLSA), a generative model from the statistical text literature, here applied to a bag of visual words representation for each image. The scene classification on the object distribution is carried out by a k-nearest neighbour classifier. \n \nWe investigate the classification performance under changes in the visual vocabulary and number of latent topics learnt, and develop a novel vocabulary using colour SIFT descriptors. Classification performance is compared to the supervised approaches of Vogel & Schiele [19] and Oliva & Torralba [11], and the semi-supervised approach of Fei Fei & Perona [3] using their own datasets and testing protocols. In all cases the combination of (unsupervised) pLSA followed by (supervised) nearest neighbour classification achieves superior results. We show applications of this method to image retrieval with relevance feedback and to scene classification in videos."
            },
            "slug": "Scene-Classification-Via-pLSA-Bosch-Zisserman",
            "title": {
                "fragments": [],
                "text": "Scene Classification Via pLSA"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The classification performance under changes in the visual vocabulary and number of latent topics learnt is investigated, and a novel vocabulary using colour SIFT descriptors is developed using probabilistic Latent Semantic Analysis."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808423"
                        ],
                        "name": "G. Csurka",
                        "slug": "G.-Csurka",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Csurka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csurka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201cbag of words\u201d (Sivic et al. 2005; Fei-Fei and Perona 2005;  Csurka et al. 2004;  Sivic and Zisserman 2003) representation of images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17606900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af",
            "isKey": false,
            "numCitedBy": 5008,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information."
            },
            "slug": "Visual-categorization-with-bags-of-keypoints-Csurka",
            "title": {
                "fragments": [],
                "text": "Visual categorization with bags of keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches and shows that it is simple, computationally efficient and intrinsically invariant."
            },
            "venue": {
                "fragments": [],
                "text": "eccv 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40683457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2aa330150205f30ce23a6fce88c1de6776744574",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nCurrent object recognition systems aim at recognizing numerous object classes under limited supervision conditions. This paper provides a benchmark for evaluating progress on this fundamental task. Several methods have recently proposed to utilize the commonalities between object classes in order to improve generalization accuracy. Such methods can be termed interclass transfer techniques. However, it is currently difficult to asses which of the proposed methods maximally utilizes the shared structure of related classes. In order to facilitate the development, as well as the assessment of methods for dealing with multiple related classes, a new dataset including images of several hundred mammal classes, is provided, together with preliminary results of its use. The images in this dataset are organized into five levels of variability, and their labels include information on the objects\u2019 identity, location and pose. From this dataset, a classification benchmark has been derived, requiring fine distinctions between 72 mammal classes. It is then demonstrated that a recognition method which is highly successful on the Caltech101, attains limited accuracy on the current benchmark (36.5%). Since this method does not utilize the shared structure between classes, the question remains as to whether interclass transfer methods can increase the accuracy to the level of human performance (90%). We suggest that a labeled benchmark of the type provided, containing a large number of related classes is crucial for the development and evaluation of classification methods which make efficient use of interclass transfer.\n"
            },
            "slug": "From-Aardvark-to-Zorro:-A-Benchmark-for-Mammal-Fink-Ullman",
            "title": {
                "fragments": [],
                "text": "From Aardvark to Zorro: A Benchmark for Mammal Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is suggested that a labeled benchmark of the type provided, containing a large number of related classes is crucial for the development and evaluation of classification methods which make efficient use of interclass transfer."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Due to the vast number of object categorization approaches (Fei-Fei et al. 2007;  Felzenszwalb and Huttenlocher 2005;  Fergus et al. 2005a, 2005b ;K rempp et al.2002; LeCun et al. 2004; Leibe and Schiele 2004 ;L owe1999), it is out of the scope of our paper to discuss all of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem is intimately related to the problem of learning and modeling generic object classes in computer vision research (Fei-Fei et al. 2007;  Felzenszwalb and Huttenlocher 2005;  Fergus et al. 2005a, 2005b ;K rempp et al.2002; LeCun et al. 2004; Leibe and Schiele 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277853"
                        ],
                        "name": "A. Vailaya",
                        "slug": "A.-Vailaya",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Vailaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vailaya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001; Carson et al. 1999 ;L i et al.2000; Chen et al. 2003;  Jain and Vailaya 1996;  Barnard and Forsyth 2001; Barnard et al. 2003; Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2825720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "836f91accc7242e85313c24c9be3bfd42c013f3d",
            "isKey": false,
            "numCitedBy": 1060,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Image-retrieval-using-color-and-shape-Jain-Vailaya",
            "title": {
                "fragments": [],
                "text": "Image retrieval using color and shape"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "A 128-dim rotationally invariant SIFT vector is used to represent each region [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5258236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "isKey": false,
            "numCitedBy": 16257,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "slug": "Object-recognition-from-local-scale-invariant-Lowe",
            "title": {
                "fragments": [],
                "text": "Object recognition from local scale-invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One major group of research ( Barnard and Forsyth 2001;  Barnard et al. 2003; Jeon et al. 2003) in CBIR treats images as a collection of blobs or blocks, each corresponding to a word or phrase in the caption (with some considerable variations)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001; Carson et al. 1999 ;L i et al.2000; Chen et al. 2003; Jain and Vailaya 1996;  Barnard and Forsyth 2001;  Barnard et al. 2003; Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13121800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e36d141e2964817c3d926c380793e404a3a3367",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "slug": "Learning-the-semantics-of-words-and-pictures-Barnard-Forsyth",
            "title": {
                "fragments": [],
                "text": "Learning the semantics of words and pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features, and can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683556"
                        ],
                        "name": "G. Wiederhold",
                        "slug": "G.-Wiederhold",
                        "structuredName": {
                            "firstName": "Gio",
                            "lastName": "Wiederhold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wiederhold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6477258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37217f11b576b2b43f245295553f1aca413d59ae",
            "isKey": false,
            "numCitedBy": 442,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Content-based image retrieval using region segmentation has been an active research area. We present IRM (Integrated Region Matching), a novel similarity measure for region-based image similarity comparison. The targeted image retrieval systems represent an image by a set of regions, roughly corresponding to objects, which are characterized by features reflecting color, texture, shape, and location properties. The IRM measure for evaluating overall similarity between images incorporates properties of all the regions in the images by a region-matching scheme. Compared with retrieval based on individual regions, the overall similarity approach reduces the influence of inaccurate segmentation, helps to clarify the semantics of a particular region, and enables a simple querying interface for region-based image retrieval systems. The IRM has been implemented as a part of our experimental SIMPLIcity image retrieval system. The application to a database of about 200,000 general-purpose images shows exceptional robustness to image alterations such as intensity variation, sharpness variation, color distortions, shape distortions, cropping, shifting, and rotation. Compared with several existing systems, our system in general achieves more accurate retrieval at higher speed."
            },
            "slug": "IRM:-integrated-region-matching-for-image-retrieval-Li-Wang",
            "title": {
                "fragments": [],
                "text": "IRM: integrated region matching for image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The IRM measure for evaluating overall similarity between images incorporates properties of all the regions in the images by a region-matching scheme, which achieves more accurate retrieval at higher speed than several existing systems."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969789"
                        ],
                        "name": "C. Carson",
                        "slug": "C.-Carson",
                        "structuredName": {
                            "firstName": "Chad",
                            "lastName": "Carson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Carson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633015"
                        ],
                        "name": "Megan Thomas",
                        "slug": "Megan-Thomas",
                        "structuredName": {
                            "firstName": "Megan",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Megan Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695576"
                        ],
                        "name": "J. Hellerstein",
                        "slug": "J.-Hellerstein",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Hellerstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hellerstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another group of approaches focuses on comparing the query image with exemplar images and retrieving images based on image similarity ( Carson et al. 1999;  Chen et al. 2003; Deng 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) (Zhou and Huang 2002; Deng 2001;  Carson et al. 1999 ; L i et al.2000; Chen et al. 2003; Jain and Vailaya 1996; Barnard and Forsyth 2001; Barnard et al. 2003; Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6398824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d39e930e5723182574b0089e219a4f69890a7ddf",
            "isKey": false,
            "numCitedBy": 954,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (\"blobs\") with associated color and texture descriptors. Queryingi s based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions usinga tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both queryinga nd indexing."
            },
            "slug": "Blobworld:-A-System-for-Region-Based-Image-Indexing-Carson-Thomas",
            "title": {
                "fragments": [],
                "text": "Blobworld: A System for Region-Based Image Indexing and Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work indexes the blob descriptions using a lower-rank approximation to the high-dimensional distance to make large-scale retrieval feasible, and shows encouraging results for both querying and indexing."
            },
            "venue": {
                "fragments": [],
                "text": "VISUAL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634361"
                        ],
                        "name": "Yining Deng",
                        "slug": "Yining-Deng",
                        "structuredName": {
                            "firstName": "Yining",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yining Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50591689"
                        ],
                        "name": "B. S. Manjunath",
                        "slug": "B.-S.-Manjunath",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Manjunath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Manjunath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319358"
                        ],
                        "name": "C. Kenney",
                        "slug": "C.-Kenney",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kenney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kenney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117172289"
                        ],
                        "name": "M. Moore",
                        "slug": "M.-Moore",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Moore",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145822222"
                        ],
                        "name": "H. Shin",
                        "slug": "H.-Shin",
                        "structuredName": {
                            "firstName": "Hyun",
                            "lastName": "Shin",
                            "middleNames": [
                                "Doo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6979297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e27a4e623e5b14f4b88907d79df153daf3716ab",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A compact color descriptor and an efficient indexing method for this descriptor are presented. The target application is similarity retrieval in large image databases using color. Colors in a given region are clustered into a small number of representative colors. The feature descriptor consists of the representative colors and their percentages in the region. A similarity measure similar to the quadratic color histogram distance measure is defined for this descriptor. The representative colors can be indexed in the three-dimensional (3-D) color space thus avoiding the high-dimensional indexing problems associated with the traditional color histogram. For similarity retrieval, each representative color in the query image or region is used independently to find regions containing that color. The matches from all of the query colors are then combined to obtain the final retrievals. An efficient indexing scheme for fast retrieval is presented. Experimental results show that this compact descriptor is effective and compares favorably with the traditional color histogram in terms of overall computational complexity."
            },
            "slug": "An-efficient-color-representation-for-image-Deng-Manjunath",
            "title": {
                "fragments": [],
                "text": "An efficient color representation for image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results show that this compact color descriptor is effective and compares favorably with the traditional color histogram in terms of overall computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685300"
                        ],
                        "name": "J. Hespanha",
                        "slug": "J.-Hespanha",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Hespanha",
                            "middleNames": [
                                "Pedro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hespanha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be86da00efdd8c2a7fdeb2334605796c24b370f0",
            "isKey": false,
            "numCitedBy": 11722,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases."
            },
            "slug": "Eigenfaces-vs.-Fisherfaces:-Recognition-Using-Class-Belhumeur-Hespanha",
            "title": {
                "fragments": [],
                "text": "Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A face recognition algorithm which is insensitive to large variation in lighting direction and facial expression is developed, based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variations in lighting and facial expressions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While for discriminative models, which include boosting (Freund and Schapire 1995; Freund and Schapire 1996), support vector machines ( Boser et al. 1992 ), conditional random field (McCallum et al. 2000) etc., the posterior probabilities are modeled directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[8, 10] use idea of training a good object class model from web images returned by search engines, hence obtaining an object filter to refine these results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14320675,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8fa41eef4c4e60709b0b134bf8d4ee242bff7a74",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the constellation model to include heterogeneous parts which may represent either the appearance or the geometry of a region of the object. The parts and their spatial configuration are learnt simultaneously and automatically, without supervision, from cluttered images."
            },
            "slug": "A-Visual-Category-Filter-for-Google-Images-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "A Visual Category Filter for Google Images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The constellation model is extended to include heterogeneous parts which may represent either the appearance or the geometry of a region of the object and their spatial configuration are learnt simultaneously and automatically, without supervision, from cluttered images."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe how we learn the parameters by Gibbs sampling ( Geman and Geman 1984 ) of the latent"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18709,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40403107"
                        ],
                        "name": "X. Zhou",
                        "slug": "X.-Zhou",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhou",
                            "middleNames": [
                                "Sean"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Image Retrieval from the Web Content-based image retrieval (CBIR) ( Zhou and Huang 2002;  Deng 2001; Carson et al. 1999 ;L i et al.2000; Chen et al. 2003; Jain and Vailaya 1996; Barnard and Forsyth 2001; Barnard et al. 2003; Jeon et al. 2003) has been long an active field of research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18967272,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science",
                "Education"
            ],
            "id": "e9bf035a93b4a873a4ee902af7db79f986937361",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We're interested in using keywords and visual content together in image retrieval. We used a seamless joint querying and relevance feedback scheme based on keywords and lowlevel visual content, incorporating keyword similarities. We developed an algorithm for a learned word similarity matrix and conducted experiments that validated our approach."
            },
            "slug": "Unifying-Keywords-and-Visual-Contents-in-Image-Zhou-Huang",
            "title": {
                "fragments": [],
                "text": "Unifying Keywords and Visual Contents in Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A seamless joint querying and relevance feedback scheme based on keywords and lowlevel visual content, incorporating keyword similarities, is used based on a developed algorithm for a learned word similarity matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Multim."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735369"
                        ],
                        "name": "J. Sethuraman",
                        "slug": "J.-Sethuraman",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Sethuraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sethuraman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122061045,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0dcd37d10175c30d26152a180092c4258e698d9f",
            "isKey": false,
            "numCitedBy": 2433,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The parameter in a Bayesian nonparametric problem is the unknown distribution P of the observation X. A Bayesian uses a prior distribution for P, and after observing X, solves the statistical inference problem by using the posterior distribution of P, which is the conditional distribution of P given X. For Bayesian nonparametrics to be successful one needs a large class of priors for which posterior distributions can be easily calculated. Unless X takes values in a finite space, the unknown distribution P varies in an infinite dimensional space. Thus one has to talk about measures in a complicated space like the space of all probability measures on a large space. This has always required a more careful attention to the attendant measure theoretic problems. A class of priors known as Dirichlet measures have been used for the distribution of a random variable X when it takes values in R sub K."
            },
            "slug": "A-CONSTRUCTIVE-DEFINITION-OF-DIRICHLET-PRIORS-Sethuraman",
            "title": {
                "fragments": [],
                "text": "A CONSTRUCTIVE DEFINITION OF DIRICHLET PRIORS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Neal & Hinton [15] provides a theoretical ground for incrementally learning mixture models via sufficient statistics updates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60688891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932a106c21a1db1e1876459c1521d27fd152caac",
            "isKey": false,
            "numCitedBy": 8461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Looking for competent reading resources? We have pattern recognition and machine learning information science and statistics to read, not only read, but also download them or even check out online. Locate this fantastic book writtern by by now, simply here, yeah just here. Obtain the reports in the kinds of txt, zip, kindle, word, ppt, pdf, as well as rar. Once again, never ever miss to review online and download this book in our site right here. Click the link."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Science-Bishop",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning (Information Science and Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While for discriminative models, which include boosting (Freund and Schapire 1995; Freund and Schapire 1996), support vector machines (Boser et al. 1992), conditional random field ( McCallum et al. 2000 ) etc., the posterior probabilities are modeled directly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 775373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bece46ed303f8eaef2affae2cba4e0aef51fe636",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s."
            },
            "slug": "Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Markov Models for Information Extraction and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new Markovian sequence model is presented that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Compared to parametric latent topic models such as LDA [3] or plSA [11], HDP offers a way to sample an infinite number of latent topics, or clusters, for each object category model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 200
                            }
                        ],
                        "text": "We make this choice, however, based on the recent successes of the \u201cbag of words\u201d models in object and scene recognition [17, 6], particularly the usage of latent topic models for such representation [11, 3, 19]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": false,
            "numCitedBy": 30947,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773821"
                        ],
                        "name": "Matthew J. Beal",
                        "slug": "Matthew-J.-Beal",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Beal",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew J. Beal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Similarly to [18, 20], we adapt the Hierarchical Dirichlet process(HDP) [19] for our object category model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 200
                            }
                        ],
                        "text": "We make this choice, however, based on the recent successes of the \u201cbag of words\u201d models in object and scene recognition [17, 6], particularly the usage of latent topic models for such representation [11, 3, 19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Of the many possibilities, we have chosen to use a variant of the HDP (Hierarchical Dirichlet Process) [19] model based on \u201cthe bag of words\u201d representation of images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "We introduce our HDP object category model in more detail in Sec.3.2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "We model both object and background classes with HDP [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "In order to take full advantage of the non-parametric HDP model, as well as to avoid this \u201cover specialization\u201d, we only use a subset of images to update our model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "We choose the popular Chinese restaurant franchise [19] metaphor to describe this procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Fig.2 shows the graphical model of HDP. \u03b8 corresponds to the distributions of visual words given different latent topics shared among different images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "Given \u03b3, \u03b1 as the concentration parameters and H as a base probability measure, HDP defines a global random probability measure G0 \u223c DP (\u03b3, H)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Compared to parametric latent topic models such as LDA [3] or plSA [11], HDP offers a way to sample an infinite number of latent topics, or clusters, for each object category model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7934949,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b90d922ff07d0eb8d77b8687aba7f55bd3926436",
            "isKey": true,
            "numCitedBy": 3577,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the \u201cChinese restaurant franchise.\u201d We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling."
            },
            "slug": "Hierarchical-Dirichlet-Processes-Teh-Jordan",
            "title": {
                "fragments": [],
                "text": "Hierarchical Dirichlet Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work considers problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups, and considers a hierarchical model, specifically one in which the base measure for the childDirichlet processes is itself distributed according to a Dirichlet process."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240597"
                        ],
                        "name": "David McClosky",
                        "slug": "David-McClosky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McClosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David McClosky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 628455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78a9513e70f596077179101f6cb6eadc51602039",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon."
            },
            "slug": "Effective-Self-Training-for-Parsing-McClosky-Charniak",
            "title": {
                "fragments": [],
                "text": "Effective Self-Training for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work presents a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data and shows that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images are downloaded using words generated by WordNet ( Miller 1995 ) synset as the query input for image search engines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13889,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2792323"
                        ],
                        "name": "T. Ferguson",
                        "slug": "T.-Ferguson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ferguson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ferguson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2013 We first sample \u03c0j , \u03c0j |\u03b1, \u03b2 \u223c DP(\u03b1, \u03b2). DP denotes the Dirichlet Process introduced by Ferguson in 1973 ( Ferguson 1973 ):"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9777424,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b8593122c6b1dd2432125a4eeb1a908c280b1d1d",
            "isKey": false,
            "numCitedBy": 4790,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian approach remained rather unsuccessful in treating nonparametric problems. This is primarily due to the difficulty in finding workable prior distribution on the parameter space , which in nonparametric problems is taken to be a set of probability distributions on a given sample space. Two Desirable Properties of a Prior 1. The support of the prior should be large. 2. Posterior distribution given a sample of observation should be manageable analytically. These properties are antagonistic : One may be obtained at the expense of other."
            },
            "slug": "A-Bayesian-Analysis-of-Some-Nonparametric-Problems-Ferguson",
            "title": {
                "fragments": [],
                "text": "A Bayesian Analysis of Some Nonparametric Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40605780"
                        ],
                        "name": "J. Besemer",
                        "slug": "J.-Besemer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Besemer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besemer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194485"
                        ],
                        "name": "A. Lomsadze",
                        "slug": "A.-Lomsadze",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Lomsadze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lomsadze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715148"
                        ],
                        "name": "M. Borodovsky",
                        "slug": "M.-Borodovsky",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Borodovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Borodovsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In computational biology, self training is employed for gene prediction ( Besemer et al. 2001 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16956175,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d921dadec0e089891652ab6fc2c7f0abbf2e4b70",
            "isKey": false,
            "numCitedBy": 1635,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Improving the accuracy of prediction of gene starts is one of a few remaining open problems in computer prediction of prokaryotic genes. Its difficulty is caused by the absence of relatively strong sequence patterns identifying true translation initiation sites. In the current paper we show that the accuracy of gene start prediction can be improved by combining models of protein-coding and non-coding regions and models of regulatory sites near gene start within an iterative Hidden Markov model based algorithm. The new gene prediction method, called GeneMarkS, utilizes a non-supervised training procedure and can be used for a newly sequenced prokaryotic genome with no prior knowledge of any protein or rRNA genes. The GeneMarkS implementation uses an improved version of the gene finding program GeneMark.hmm, heuristic Markov models of coding and non-coding regions and the Gibbs sampling multiple alignment program. GeneMarkS predicted precisely 83.2% of the translation starts of GenBank annotated Bacillus subtilis genes and 94.4% of translation starts in an experimentally validated set of Escherichia coli genes. We have also observed that GeneMarkS detects prokaryotic genes, in terms of identifying open reading frames containing real genes, with an accuracy matching the level of the best currently used gene detection methods. Accurate translation start prediction, in addition to the refinement of protein sequence N-terminal data, provides the benefit of precise positioning of the sequence region situated upstream to a gene start. Therefore, sequence motifs related to transcription and translation regulatory sites can be revealed and analyzed with higher precision. These motifs were shown to possess a significant variability, the functional and evolutionary connections of which are discussed."
            },
            "slug": "GeneMarkS:-a-self-training-method-for-prediction-of-Besemer-Lomsadze",
            "title": {
                "fragments": [],
                "text": "GeneMarkS: a self-training method for prediction of gene starts in microbial genomes. Implications for finding sequence motifs in regulatory regions."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the accuracy of gene start prediction can be improved by combining models of protein-coding and non-Coding regions and models of regulatory sites near gene start within an iterative Hidden Markov model based algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic acids research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the framework of OPTIMOL, we employ a semisupervised learning approach, specifically self training, to propagate the initial knowledge ( Zhu 2006 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2731141,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a007f46b3303bdb50e705b441c367e595666538c",
            "isKey": false,
            "numCitedBy": 3963,
            "numCiting": 324,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semi-Supervised-Learning-Literature-Survey-Zhu",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Literature Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8147588"
                        ],
                        "name": "N. Nasrabadi",
                        "slug": "N.-Nasrabadi",
                        "structuredName": {
                            "firstName": "Nasser",
                            "lastName": "Nasrabadi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nasrabadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Classification approaches can be grossly divided into generative models, discriminative models and discriminant functions ( Bishop 2006 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63317738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668b1277fbece28c4841eeab1c97e4ebd0079700",
            "isKey": false,
            "numCitedBy": 10185,
            "numCiting": 389,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Bishop-Nasrabadi",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Probability Distributions, linear models for Regression, Linear Models for Classification, Neural Networks, Graphical Models, Mixture Models and EM, Sampling Methods, Continuous Latent Variables, Sequential Data are studied."
            },
            "venue": {
                "fragments": [],
                "text": "J. Electronic Imaging"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62575097,
            "fieldsOfStudy": [],
            "id": "3bcd09a77ea1425175dca70537a058a33475e84f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Harvesting Image Databases from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We repeat this process till a sufficient dataset is collected or we have exhausted all downloaded images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Dirichlet processes. To appear in Journal of the American Statistical Association"
            },
            "venue": {
                "fragments": [],
                "text": "Hierarchical Dirichlet processes. To appear in Journal of the American Statistical Association"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "We use the Kadir and Brady [12] salient point detector to find the informative local regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale, saliency and image description"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision, 45(2):83\u2013105,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Object Categories from Googles Image Search. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Int J Comput Vis"
            },
            "venue": {
                "fragments": [],
                "text": "Int J Comput Vis"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Effective selftraining for parsing Morristown: Association for Computational Lin- guistics"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward scalable dataset construction: An active learning approach"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ECCV"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL object recognition database collection. http://www.pascal-network.org/challenges/VOC/databases.html"
            },
            "venue": {
                "fragments": [],
                "text": "PASCAL Int J Comput Vis"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 3
                            }
                        ],
                        "text": "With this small dataset, we begin the iterative process of model learning and dataset collection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 168
                            }
                        ],
                        "text": "Metaphorically, we describe the jth image as the jth restaurant and the image level mixture component for xji as a table tji, where xji is the ith customer in the jth restaurant."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchy model for learning natural scene categories. Computer Vision and Pattern Recog- nition"
            },
            "venue": {
                "fragments": [],
                "text": "A Bayesian hierarchy model for learning natural scene categories. Computer Vision and Pattern Recog- nition"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 15
                            }
                        ],
                        "text": "\u201cbag of words\u201d (Sivic et al. 2005; Fei-Fei and Perona 2005; Csurka et al. 2004; Sivic and Zisserman 2003) representation of images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 127
                            }
                        ],
                        "text": "A number of systems based on the bag of words model representation have shown to be effective for object and scene recognition (Fergus et al. 2005b; Sivic et al. 2005; Fei-Fei and Perona 2005; Sudderth et al. 2005b; Bosch et al. 2006; Csurka et al. 2004; Sivic and Zisserman 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchy model for learning natural scene categories. Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 126
                            }
                        ],
                        "text": "This problem is intimately related to the problem of learning and modeling generic object classes in computer vision research (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b; Krempp et al. 2002; LeCun et al. 2004; Leibe and Schiele 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 59
                            }
                        ],
                        "text": "Due to the vast number of object categorization approaches (Fei-Fei et al. 2007; Felzenszwalb and Huttenlocher 2005; Fergus et al. 2005a, 2005b; Krempp et al. 2002; LeCun et al. 2004; Leibe and Schiele 2004; Lowe 1999), it is out of the scope of our paper to discuss all of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing and learning object categories"
            },
            "venue": {
                "fragments": [],
                "text": "Short course CVPR. http:// people.csail.mit.edu/torralba/shortCourseRLOC/index.html"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sequential learning with reusable parts for object detection (Technical report)"
            },
            "venue": {
                "fragments": [],
                "text": "Sequential learning with reusable parts for object detection (Technical report)"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing and learning object categories. Short course CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "Recognizing and learning object categories. Short course CVPR"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL object recognition database collection"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.pascal-network.org/challenges/VOC/databases.html."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Object Categories from Googles Image Search . Computer Vision , 2005 . ICCV 2005"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": "It has been used in natural language processing to perform tasks such as parsing strings of words (McClosky et al. 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Effective selftraining for parsing"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics (pp. 152\u2013159)"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 29,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 76,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/OPTIMOL:-Automatic-Online-Picture-Collection-via-Li-Fei-Fei/d4ed0b69a2e9a3d75ac13c4ff43044fea9b5df6e?sort=total-citations"
}