{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36502813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b00e70c64f2cc19cd893067c6e209ea47f37ee6",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer-assisted content-based indexing is a critical enabling technology and currently a bottleneck in productive use of video resources. This paper presents the Video Classification Project, an effort toward automating content-based video indexing and retrieval, at the Institute of Systems Science of the National University of Singapore. We discuss in detail three goals of the project: image processing tools for video parsing, feature extraction and retrieval; a knowledge-based approach to representing video content; and stratified tools which allow greater flexibility in browsing a video resource, either before or after performing specific retrieval operations."
            },
            "slug": "Developing-power-tools-for-video-indexing-and-Zhang-Smoliar",
            "title": {
                "fragments": [],
                "text": "Developing power tools for video indexing and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Video Classification Project is presented, an effort toward automating content-based video indexing and retrieval, at the Institute of Systems Science of the National University of Singapore."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115902487"
                        ],
                        "name": "J. H. Wu",
                        "slug": "J.-H.-Wu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Wu",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62692579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "623719bfd15fc23b5348c0b76cf245eeb480829f",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Browsing is important for multimedia content retrieval, editing, authoring and communications. Yet, we are still lacking browsing tools which are user friendly and content-based, at least for video materials. In this paper, we present a set of video browsing tools which utilize video content information resulting from a parsing process. Video parsing algorithms are briefly discussed and a detail description of both sequential and time-space browsing tools are presented."
            },
            "slug": "Content-based-video-browsing-tools-Zhang-Smoliar",
            "title": {
                "fragments": [],
                "text": "Content-based video browsing tools"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A set of video browsing tools which utilize video content information resulting from a parsing process and a detail description of both sequential and time-space browsing tools are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13087918"
                        ],
                        "name": "F. Arman",
                        "slug": "F.-Arman",
                        "structuredName": {
                            "firstName": "Farshid",
                            "lastName": "Arman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Arman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2998944"
                        ],
                        "name": "R. Depommier",
                        "slug": "R.-Depommier",
                        "structuredName": {
                            "firstName": "Remi",
                            "lastName": "Depommier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Depommier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46891469"
                        ],
                        "name": "A. Hsu",
                        "slug": "A.-Hsu",
                        "structuredName": {
                            "firstName": "Arding",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957701"
                        ],
                        "name": "M. Chiu",
                        "slug": "M.-Chiu",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Chiu",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chiu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1360834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9895be389b31013b477e3bb48a006ad5c73f3a14",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel methodology to represent the contents of a video sequence is presented. The representation is used to allow the user to rapidly view a video sequence in order to find a particular point within the sequence and/or to decide whether the contents of the sequence are relevant to his or her needs. This system, referred to as content-based browsing, forms an abstraction to represent each shot of the sequence by using a representative frame, or an Rframe, and it includes management techniques to allow the user to easily navigate the Rframes. This methodology is superior to the current techniques of fast forward and rewind because rather than using every frame to view and judge the contents, only a few abstractions are used. Therefore, the need to retrieve the video from a storage system and to transmit every frame over the network in its entirety no longer exists, saving time, expenses, and bandwidth."
            },
            "slug": "Content-based-browsing-of-video-sequences-Arman-Depommier",
            "title": {
                "fragments": [],
                "text": "Content-based browsing of video sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The need to retrieve the video from a storage system and to transmit every frame over the network in its entirety no longer exists, saving time, expenses, and bandwidth."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784312"
                        ],
                        "name": "C. Low",
                        "slug": "C.-Low",
                        "structuredName": {
                            "firstName": "Chien",
                            "lastName": "Low",
                            "middleNames": [
                                "Yong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Low"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62216120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b35381320b0dba33c9db315abd7e69a0b41ea84",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing video content is an important first step in the video indexing process. This paper presents algorithms to automate the video parsing task, including video partitioning and video clip classification according to camera operations using compressed video data. We have studied and implemented two algorithms for partitioning video data compressed according to the MPEG standard. The first one is based on discrete cosine transform coefficients of video frames, and the other based on correlation of motion vectors. Algorithms to detect camera operations using motion vectors are presented."
            },
            "slug": "Video-parsing-using-compressed-data-Zhang-Low",
            "title": {
                "fragments": [],
                "text": "Video parsing using compressed data"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Algorithms to automate the video parsing task, including video partitioning and video clip classification according to camera operations using compressed video data, based on correlation of motion vectors are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145814117"
                        ],
                        "name": "D. Swanberg",
                        "slug": "D.-Swanberg",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Swanberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Swanberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144808138"
                        ],
                        "name": "Chiao-Fe Shu",
                        "slug": "Chiao-Fe-Shu",
                        "structuredName": {
                            "firstName": "Chiao-Fe",
                            "lastName": "Shu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chiao-Fe Shu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938732"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3396918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28726944aebed6b3d88935e55c9f228bf4d9bfe5",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual information systems require a new insertion process. Prior to storage within the database, the system must first identify the desired objects (shots and episodes), and then calculate a descriptive representation of these objects. This paper discusses the steps in the insertion process, and some of the tools we have developed to semi-automatically segment the data into domain objects which are meaningful to the user. Image processing routines are necessary to derive features of the video frames. Models are required to represent the desired domain, and similarity measures must compare the models to the derived features."
            },
            "slug": "Knowledge-guided-parsing-in-video-databases-Swanberg-Shu",
            "title": {
                "fragments": [],
                "text": "Knowledge-guided parsing in video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The steps in the insertion process, and some of the tools the authors have developed to semi-automatically segment the data into domain objects which are meaningful to the user are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144562609"
                        ],
                        "name": "Marc Davis",
                        "slug": "Marc-Davis",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5393235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e900ddc95555c1812b275df09434032ea8a4914b",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to enable the search and retrieval of video from large archives, we need a representation of video content. Although some aspects of video can be automatically parsed, a detailed representation requires that video be annotated. We discuss the design criteria for a video annotation language with special attention to the issue of creating a global, reusable video archive. Our prototype system, Media Streams, enables users to create multi-layered, iconic annotations of streams video data. Within Media Streams, the organization and categories of the Director's Workshop allow users to browse and compound over 2200 iconic primitives by means of a cascading hierarchical structure which supports compounding icons across branches of the hierarchy. The problems of creating a representation of action for video are given special attention, as well as describing transitions in video.<<ETX>>"
            },
            "slug": "Media-Streams:-an-iconic-visual-language-for-video-Davis",
            "title": {
                "fragments": [],
                "text": "Media Streams: an iconic visual language for video annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The design criteria for a video annotation language is discussed with special attention to the issue of creating a global, reusable video archive, and the problems ofCreating a representation of action for video are given special attention."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1993 IEEE Symposium on Visual Languages"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765546"
                        ],
                        "name": "Laura Teodosio",
                        "slug": "Laura-Teodosio",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Teodosio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Teodosio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144469516"
                        ],
                        "name": "W. Bender",
                        "slug": "W.-Bender",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Bender",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12715729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a8e4e637c7729a5418498efacee896f53e23bea",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of images called salient stills is demonstrated and a software development platform for their creation is discussed. These images do not represent one discrete moment of time, as do a photograph or single video frame. Rather, one image reflects the aggregate of the temporal changes that occur in a moving image sequence with the salient features preserved. By the application of an affine transformation and non-linear temporal processing, multiple frames of an image sequence, which may include variations in focal-length or field-of-view, are combined to create a single still image. The still image may have multi-resolution patches, a larger field-of-view, or higher overall resolution than any individual frame in the original image sequence. It may also contain selected salient objects from any one of the sequence of video frames. The still can be created automatically or with user intervention. A by-product of the salient still process is a structured representation of moving image data."
            },
            "slug": "Salient-video-stills:-content-and-context-preserved-Teodosio-Bender",
            "title": {
                "fragments": [],
                "text": "Salient video stills: content and context preserved"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new class of images called salient stills is demonstrated and a software development platform for their creation is discussed and a by-product of the salient still process is a structured representation of moving image data."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059302634"
                        ],
                        "name": "H. Chuan",
                        "slug": "H.-Chuan",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Chuan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780232"
                        ],
                        "name": "M. Sakauchi",
                        "slug": "M.-Sakauchi",
                        "structuredName": {
                            "firstName": "Masao",
                            "lastName": "Sakauchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sakauchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35935929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c8c56a41f876a2b860d624dd0e249db38107c46",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "While general object recognition is difficult, it is relatively easy to capture some primitive image properties such as color distribution, prominent regions and their geometrical properties from an image and use these features to narrow down the search space when attempts to retrieving images by contents from an image database are made. The authors are building an image database in which images are indexed by both the numerical index keys generated automatically from the captured primitive image features using a set of rules, and traditional descriptive keywords entered by users when images are loaded. Users can either use the descriptive keywords or provide information regarding these image properties to query and retrieve images from the database. With this approach, the authors turn the difficult problem of image matching into image retrieval by index keys, which can be performed easily and rapidly using current database techniques. Initial experiments on the image database system show promising performance.<<ETX>>"
            },
            "slug": "An-image-database-system-with-content-capturing-and-Gong-Zhang",
            "title": {
                "fragments": [],
                "text": "An image database system with content capturing and fast image indexing abilities"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The authors are building an image database in which images are indexed by both the numerical index keys generated automatically from the captured primitive image features using a set of rules, and traditional descriptive keywords entered by users when images are loaded."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64046874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c051cc68b398327aa6e54b8aadf20a01da27469",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually significant coefficients. We describe three Photobook tools in particular: one that allows search based on gray-level appearance, one that uses 2-D shape, and a third that allows search based on textural properties."
            },
            "slug": "Photobook:-tools-for-content-based-manipulation-of-Pentland-Picard",
            "title": {
                "fragments": [],
                "text": "Photobook: tools for content-based manipulation of image databases"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The Photobook system is described, which is a set of interactive tools for browsing and searching images and image sequences that differ from those used in standard image databases in that they make direct use of the image content rather than relying on annotations."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145934140"
                        ],
                        "name": "Toshikazu Kato",
                        "slug": "Toshikazu-Kato",
                        "structuredName": {
                            "firstName": "Toshikazu",
                            "lastName": "Kato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshikazu Kato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375983"
                        ],
                        "name": "T. Kurita",
                        "slug": "T.-Kurita",
                        "structuredName": {
                            "firstName": "Takio",
                            "lastName": "Kurita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kurita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809629"
                        ],
                        "name": "N. Otsu",
                        "slug": "N.-Otsu",
                        "structuredName": {
                            "firstName": "Nobuyuki",
                            "lastName": "Otsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Otsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055529881"
                        ],
                        "name": "Keiji Hirata",
                        "slug": "Keiji-Hirata",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Hirata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keiji Hirata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60539750,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "fdf2c1986afd802f1163b6c08895eb11ed1e1877",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Gives a basic idea and its fundamental algorithms of the visual interface for image database systems. The QVE (Query by Visual Example) accepts a sketch roughly drawn by a user to retrieve the original image and the similar images. The system evaluates the similarity between the rough sketch, i.e. a visual example, and each of the image data in the database automatically. The QVE interface is implemented and examined on an experimental electronic art gallery called ART MUSEUM. This paper also gives some experimental results and a current evaluation. The algorithms are quite effective for content based image retrieval.<<ETX>>"
            },
            "slug": "A-sketch-retrieval-method-for-full-color-image-by-Kato-Kurita",
            "title": {
                "fragments": [],
                "text": "A sketch retrieval method for full color image database-query by visual example"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The QVE (Query by Visual Example) accepts a sketch roughly drawn by a user to retrieve the original image and the similar images and evaluates the similarity between the rough sketch and each of the image data in the database automatically."
            },
            "venue": {
                "fragments": [],
                "text": "[1992] Proceedings. 11th IAPR International Conference on Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692339"
                        ],
                        "name": "B. Yeo",
                        "slug": "B.-Yeo",
                        "structuredName": {
                            "firstName": "Boon-Lock",
                            "lastName": "Yeo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yeo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108661679"
                        ],
                        "name": "Bede Liu",
                        "slug": "Bede-Liu",
                        "structuredName": {
                            "firstName": "Bede",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bede Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36611702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04142ae104d552672620c88cb9cdbcee33ceb7bc",
            "isKey": false,
            "numCitedBy": 853,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Several rapid scene analysis algorithms for detecting scene changes and flashlight scenes directly on compressed video are proposed. These algorithms operate on the DC sequence which can be readily extracted from video compressed using Motion JPEG or MPEG without full-frame decompression. The DC images occupy only a small fraction of the original data size while retaining most of the essential \"global\" information. Operating on these images offers a significant computation saving. Experimental results show that the proposed algorithms are fast and effective in detecting abrupt scene changes, gradual transitions including fade-ins and fade-outs, flashlight scenes and in deriving intrashot variations."
            },
            "slug": "Rapid-scene-analysis-on-compressed-video-Yeo-Liu",
            "title": {
                "fragments": [],
                "text": "Rapid scene analysis on compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results show that the proposed rapid scene analysis algorithms are fast and effective in detecting abrupt scene changes, gradual transitions including fade-ins and fade-outs, flashlight scenes and in deriving intrashot variations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723155"
                        ],
                        "name": "L. Rowe",
                        "slug": "L.-Rowe",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rowe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2719487"
                        ],
                        "name": "J. Boreczky",
                        "slug": "J.-Boreczky",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Boreczky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boreczky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064865"
                        ],
                        "name": "Charles A. Eads",
                        "slug": "Charles-A.-Eads",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Eads",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles A. Eads"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 8019994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48cc4c8cbb28fcd89d3289c672d3ef80ed8d883d",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Video-on-Demand systems have received a good deal of attention recently. Few studies, however, have addressed the problem of locating a video of interest in a large video database. This paper describes the design and implementation of a metadata database and query interface that attempts to solve this information retrieval problem."
            },
            "slug": "Indexes-for-user-access-to-large-video-databases-Rowe-Boreczky",
            "title": {
                "fragments": [],
                "text": "Indexes for user access to large video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The design and implementation of a metadata database and query interface that attempts to solve the problem of locating a video of interest in a large video database is described."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150222591"
                        ],
                        "name": "M. Mills",
                        "slug": "M.-Mills",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mills",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mills"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110959806"
                        ],
                        "name": "Jonathan Cohen",
                        "slug": "Jonathan-Cohen",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030034"
                        ],
                        "name": "Y. Wong",
                        "slug": "Y.-Wong",
                        "structuredName": {
                            "firstName": "Yin",
                            "lastName": "Wong",
                            "middleNames": [
                                "Yin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 19031139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f351f97c5b7b42406c80f537b80c16a0b766853",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an interface prototype, the Hierarchical Video Magnifier, which allows users to work with a video source at fine-levels of detail while maintaining an awareness of temporal context. The technique allows the user to recursively magnify the temporal resolution of a video source while preserving the levels of magnification in a spatial hierarchy. We discuss how the ability to inspect and manipulate hierarchical views of temporal magnification affords a powerful tool for navigating, analyzing and editing video streams."
            },
            "slug": "A-magnifier-tool-for-video-data-Mills-Cohen",
            "title": {
                "fragments": [],
                "text": "A magnifier tool for video data"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An interface prototype, the Hierarchical Video Magnifier, is described, which allows users to work with a video source at fine-levels of detail while maintaining an awareness of temporal context."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081718"
                        ],
                        "name": "M. Stricker",
                        "slug": "M.-Stricker",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Stricker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stricker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35473334"
                        ],
                        "name": "Markus Orengo",
                        "slug": "Markus-Orengo",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Orengo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Orengo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16156344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c6d4081ea1e1c13afabdc9870e6e27d75facaa0",
            "isKey": false,
            "numCitedBy": 1968,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two new color indexing techniques. The first one is a more robust version of the commonly used color histogram indexing. In the index we store the cumulative color histograms. The L1-, L2-, L(infinity )-distance between two cumulative color histograms can be used to define a similarity measure of these two color distributions. We show that this method produces slightly better results than color histogram methods, but it is significantly more robust with respect to the quantization parameter of the histograms. The second technique is an example of a new approach to color indexing. Instead of storing the complete color distributions, the index contains only their dominant features. We implement this approach by storing the first three moments of each color channel of an image in the index, i.e., for a HSV image we store only 9 floating point numbers per image. The similarity function which is used for the retrieval is a weighted sum of the absolute differences between corresponding moments. Our tests clearly demonstrate that a retrieval based on this technique produces better results and runs faster than the histogram-based methods."
            },
            "slug": "Similarity-of-color-images-Stricker-Orengo",
            "title": {
                "fragments": [],
                "text": "Similarity of color images"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Two new color indexing techniques are described, one of which is a more robust version of the commonly used color histogram indexing and the other which is an example of a new approach tocolor indexing that contains only their dominant features."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11686184,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science",
                "Computer Science"
            ],
            "id": "0b1c14ccf1aad87f215bfa5c6678d975d44ffb3a",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Texture-classification-and-segmentation-using-Mao-Jain",
            "title": {
                "fragments": [],
                "text": "Texture classification and segmentation using multiresolution simultaneous autoregressive models"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18920889,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c21738e116aeabca1e523f612610605718ae00ff",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Investigates a measure of \"dominant perceived orientation\" that has been developed to match the output of a human study involving 40 subjects. The results of this measure are compared with humans analyzing seven \"teaser\" images to test its effectiveness for finding perceptually dominant orientations. The use of low-level orientation is then applied to a \"quick search\" problem important in image database applications. Since both pigeons and humans are able to perform coarse classification of certain kinds of scenes, e.g., city from country, without taking time or brain-power to solve the image understanding problem, the authors conjecture that the collective behavior of low-level textural features such as orientation may be doing most of the work. The authors demonstrate a simple test of global multiscale orientation for quickly searching a database of vacation photos for likely \"city/suburb\" shots. The orientation features achieve agreement with human classification in 91 out of 98 of the scenes."
            },
            "slug": "Texture-orientation-for-sorting-photos-\"at-a-Gorkani-Picard",
            "title": {
                "fragments": [],
                "text": "Texture orientation for sorting photos \"at a glance\""
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors demonstrate a simple test of global multiscale orientation for quickly searching a database of vacation photos for likely \"city/suburb\" shots and find the orientation features achieve agreement with human classification in 91 out of 98 of the scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795183"
                        ],
                        "name": "E. Arkin",
                        "slug": "E.-Arkin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Arkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Arkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543680"
                        ],
                        "name": "L. Chew",
                        "slug": "L.-Chew",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Chew",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939225"
                        ],
                        "name": "K. Kedem",
                        "slug": "K.-Kedem",
                        "structuredName": {
                            "firstName": "Klara",
                            "lastName": "Kedem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kedem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41227416"
                        ],
                        "name": "Joseph B. M. Mitchell",
                        "slug": "Joseph-B.-M.-Mitchell",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "B.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph B. M. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8247618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee262b2490272ee7f094b3d3c16358c895641f46",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Model-based recognition is concerned with comparing a shape A, which is stored as a model for some particular object, with a shape B, which is found to exist in an image. If A and B are close to being the same shape, then a vision system should report a match and return a measure of how good that match is. To be useful this measure should satisfy a number of properties, including: (1) it should be a metric, (2) it should be invariant under translation, rotation, and change-of-scale, (3) it should be reasonably easy to compute, and (4) it should match our intuition (i.e., answers should be similar to those that a person might give). We develop a method for comparing polygons that has these properties. The method works for both convex and nonconvex polygons and runs in time O(mn log mn) where m is the number of vertices in one polygon and n is the number of vertices in the other. Some examples are presented that show the method produces answers that are intuitively reasonable."
            },
            "slug": "An-efficiently-computable-metric-for-comparing-Arkin-Chew",
            "title": {
                "fragments": [],
                "text": "An efficiently computable metric for comparing polygonal shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method for comparing polygons that has these properties and works for both convex and nonconvex polygons and runs in time O(mn log mn) where m is the number of vertices in one polygon and n is the size of the polygons in the other."
            },
            "venue": {
                "fragments": [],
                "text": "SODA '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29187618,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "993b1083455b5c4d631eaf44f230b061994e75c3",
            "isKey": false,
            "numCitedBy": 3379,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters. >"
            },
            "slug": "The-Design-and-Use-of-Steerable-Filters-Freeman-Adelson",
            "title": {
                "fragments": [],
                "text": "The Design and Use of Steerable Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730627"
                        ],
                        "name": "H. Tamura",
                        "slug": "H.-Tamura",
                        "structuredName": {
                            "firstName": "Hideyuki",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072425643"
                        ],
                        "name": "Shunji Mori",
                        "slug": "Shunji-Mori",
                        "structuredName": {
                            "firstName": "Shunji",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shunji Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066168351"
                        ],
                        "name": "Takashi Yamawaki",
                        "slug": "Takashi-Yamawaki",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Yamawaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takashi Yamawaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32197839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "597edc3174dc9f26badd67c4e81d0e8a58f9dbb3",
            "isKey": false,
            "numCitedBy": 2434,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Textural features corresponding to human visual perception are very useful for optimum feature selection and texture analyzer design. We approximated in computational form six basic textural features, namely, coarseness, contrast, directionality, line-likeness, regularity, and roughness. In comparison with psychological measurements for human subjects, the computational measures gave good correspondences in rank correlation of 16 typical texture patterns. Similarity measurements using these features were attempted. The discrepancies between human vision and computerized techniques that we encountered in this study indicate fundamental problems in digital analysis of textures. Some of them could be overcome by analyzing their causes and using more sophisticated techniques."
            },
            "slug": "Textural-Features-Corresponding-to-Visual-Tamura-Mori",
            "title": {
                "fragments": [],
                "text": "Textural Features Corresponding to Visual Perception"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The discrepancies between human vision and computerized techniques that are encountered in this study indicate fundamental problems in digital analysis of textures and could be overcome by analyzing their causes and using more sophisticated techniques."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5248006,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "33ce6c2f2d5128be710fb3ddd8f1117758b9b4a9",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A motion sequence may be represented as a single pattern in x-y-t space; a velocity of motion corresponds to a three-dimensional orientation in this space. Motion sinformation can be extracted by a system that responds to the oriented spatiotemporal energy. We discuss a class of models for human motion mechanisms in which the first stage consists of linear filters that are oriented in space-time and tuned in spatial frequency. The outputs of quadrature pairs of such filters are squared and summed to give a measure of motion energy. These responses are then fed into an opponent stage. Energy models can be built from elements that are consistent with known physiology and psychophysics, and they permit a qualitative understanding of a variety of motion phenomena."
            },
            "slug": "Spatiotemporal-energy-models-for-the-perception-of-Adelson-Bergen",
            "title": {
                "fragments": [],
                "text": "Spatiotemporal energy models for the perception of motion."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A class of models for human motion mechanisms in which the first stage consists of linear filters that are oriented in space-time and tuned in spatial frequency that permit a qualitative understanding of a variety of motion phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49750929"
                        ],
                        "name": "C. Chen",
                        "slug": "C.-Chen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105639"
                        ],
                        "name": "L. Pau",
                        "slug": "L.-Pau",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Pau",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653107"
                        ],
                        "name": "P. S. Wang",
                        "slug": "P.-S.-Wang",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Wang",
                            "middleNames": [
                                "S.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. S. Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58410480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc6955ad31ad5934535052c6ebf4316748a3ded6",
            "isKey": false,
            "numCitedBy": 995,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Both pattern recognition and computer vision have experienced rapid progress in the last twenty-five years. This book provides the latest advances on pattern recognition and computer vision along with their many applications. It features articles written by renowned leaders in the field while topics are presented in readable form to a wide range of readers. The book is divided into five parts: basic methods in pattern recognition, basic methods in computer vision and image processing, recognition applications, life science and human identification, and systems and technology."
            },
            "slug": "Handbook-of-Pattern-Recognition-and-Computer-Vision-Chen-Pau",
            "title": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This book provides the latest advances on pattern recognition and computer vision along with their many applications and features articles written by renowned leaders in the field while topics are presented in readable form to a wide range of readers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1470838762"
                        ],
                        "name": "Mihran T\u00fcceryan",
                        "slug": "Mihran-T\u00fcceryan",
                        "structuredName": {
                            "firstName": "Mihran",
                            "lastName": "T\u00fcceryan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mihran T\u00fcceryan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295483"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5635365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11d1ad1b22610daba6ee797ec213eb29ff36b418",
            "isKey": false,
            "numCitedBy": 2296,
            "numCiting": 175,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reviews and discusses various aspects of texture analysis. The concentration is o the various methods of extracting textural features from images. The geometric, random field, fractal, and signal processing models of texture are presented. The major classes of texture processing pro lems such as segmentation, classification, and shape from texture are discussed. The possible applic tion areas of texture such as automated inspection, document processing, and remote sensing a summarized. A bibliography is provided at the end for further reading."
            },
            "slug": "Texture-Analysis-T\u00fcceryan-Jain",
            "title": {
                "fragments": [],
                "text": "Texture Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The geometric, random field, fractal, and signal processing models of texture are presented and major classes of texture processing such as segmentation, classification, and shape from texture are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92183638"
                        ],
                        "name": "M. Miyahara",
                        "slug": "M.-Miyahara",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Miyahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Miyahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113576126"
                        ],
                        "name": "Yasuhiro Yoshida",
                        "slug": "Yasuhiro-Yoshida",
                        "structuredName": {
                            "firstName": "Yasuhiro",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiro Yoshida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62677082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54cedb498d64f4b399ecba134da320784848c87b",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In studying new-generation color image codings, it is very effective 1) to code signals in the space of inherent tri-attributes of human color perception, and 2) to relate a coding error with perceptual degree of deteriorations. For these purpose, we have adopted the Munsell Renotation System in which color signals of tri-attributes of human color perception (Hue, Value and Chroma) and psychometrical color differences are defined. In the Munsell Renotation System, however, intertransformation between (RGB) data and corresponding color data is very cumbersome. Because the intertransformation depends on a look up table. This article presents a new method of mathematical transformation. The mathematical transformation is obtained by multiple regression analysis of 250 color samples, which are uniformly sampled from whole color ranges that a conventional NTSC color TV camera can present. The new method can transform (RGB) data to the data of the Munsell Renotation System far better than the conventional method given by the CIE(1976)L*a*b*."
            },
            "slug": "Mathematical-Transform-Of-(R,-G,-B)-Color-Data-To-Miyahara-Yoshida",
            "title": {
                "fragments": [],
                "text": "Mathematical Transform Of (R, G, B) Color Data To Munsell (H, V, C) Color Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new method of mathematical transformation that can transform (RGB) data to the data of the Munsell Renotation System far better than the conventional method given by the CIE(1976)L*a*b*."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394256448"
                        ],
                        "name": "Brian C. O'Connor",
                        "slug": "Brian-C.-O'Connor",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "O'Connor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian C. O'Connor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62632160,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "8ce16ffe49c6d81b1c508a375ee8658af014a81b",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Selecting-Key-Frames-of-Moving-Image-Documents:-A-O'Connor",
            "title": {
                "fragments": [],
                "text": "Selecting Key Frames of Moving Image Documents: A Digital Environment for Analysis and Navigation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153294902"
                        ],
                        "name": "Akio Nagasaka",
                        "slug": "Akio-Nagasaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Nagasaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akio Nagasaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865865"
                        ],
                        "name": "Yuzuru Tanaka",
                        "slug": "Yuzuru-Tanaka",
                        "structuredName": {
                            "firstName": "Yuzuru",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzuru Tanaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44976007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "257ff5ae00fb89e0f51b8d5c15ee25db409ccf32",
            "isKey": false,
            "numCitedBy": 838,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Video-Indexing-and-Full-Video-Search-for-Nagasaka-Tanaka",
            "title": {
                "fragments": [],
                "text": "Automatic Video Indexing and Full-Video Search for Object Appearances"
            },
            "venue": {
                "fragments": [],
                "text": "VDB"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Video-parsing,-retrieval-and-browsing:-an-and-Zhang-Low/16daa3e4fd7fb7c3b8fa843ca79ef27a34ebce1f?sort=total-citations"
}