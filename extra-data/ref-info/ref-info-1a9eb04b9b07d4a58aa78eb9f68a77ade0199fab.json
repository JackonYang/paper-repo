{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50170517"
                        ],
                        "name": "M. Blank",
                        "slug": "M.-Blank",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Blank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "A preliminary version of this paper appeared in ICCV \u201905 [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52847824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "320caf8d8ae63331d48ed2d824aa0c13d0af3617",
            "isKey": false,
            "numCitedBy": 1383,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach [14] for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure, and orientation. We show that these features are useful for action recognition, detection, and clustering. The method is fast, does not require video alignment, and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, nonrigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low-quality video."
            },
            "slug": "Actions-as-Space-Time-Shapes-Gorelick-Blank",
            "title": {
                "fragments": [],
                "text": "Actions as Space-Time Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method is fast, does not require video alignment, and is applicable in many scenarios where the background is known, and the robustness of the method is demonstrated to partial occlusions, nonrigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low-quality video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 237
                            }
                        ],
                        "text": "While our space-time volume representation is essentially derived from the same input (concatenation of silhouettes), it is robust to noise at the bounding contour of the extracted silhouettes as opposed to the local surface features in [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "A 2 2 Hessian and its eigenvalues have been used before for describing 3D surface properties [2], [12], [32], [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": ", contour parameterization and frame-to-frame point correspondences) and surface-tosurface point correspondence between actions as described in [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "A similar approach was recently presented in [32], where human actions were presented as 3D spatio-temporal surfaces and analyzed using differential geometric surface properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18724425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "487c37dce9b93d48f753ab2ec3fc997edb5639ce",
            "isKey": true,
            "numCitedBy": 490,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose to model an action based on both the shape and the motion of the performing object. When the object performs an action in 3D, the points on the outer boundary of the object are projected as 2D (x, y) contour in the image plane. A sequence of such 2D contours with respect to time generates a spatiotemporal volume (STV) in (x, y, t), which can be treated as 3D object in the (x, y, t) space. We analyze STV by using the differential geometric surface properties to identify action descriptors capturing both spatial and temporal properties. A set of action descriptors is called an action sketch. The first step in our approach is to generate STV by solving the point correspondence problem between consecutive frames. The correspondences are determined using a two-step graph theoretical approach. After the STV is generated, actions descriptors are computed by analyzing the differential geometric properties of STV. Finally, using these descriptors, we perform action recognition, which is also formulated as graph theoretical problem. Several experimental results are presented to demonstrate our approach."
            },
            "slug": "Actions-sketch:-a-novel-action-representation-Yilmaz-Shah",
            "title": {
                "fragments": [],
                "text": "Actions sketch: a novel action representation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper proposes to model an action based on both the shape and the motion of the performing object, and generates STV by solving the point correspondence problem between consecutive frames using a two-step graph theoretical approach."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719780"
                        ],
                        "name": "Yan Ke",
                        "slug": "Yan-Ke",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Ke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Ke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Some of the recent successful work done in the area of action recognition [10], [33], [17], [25], [16] have shown that it is useful to analyze actions by looking at a video sequence as a space-time volume (of intensities, gradients, optical flow, or other local features)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27f2c4969e424242200eb1901143aeda7d0a4030",
            "isKey": false,
            "numCitedBy": 632,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the use of volumetric features as an alternative to popular local descriptor approaches for event detection in video sequences. Motivated by the recent success of similar ideas in object detection on static images, we generalize the notion of 2D box features to 3D spatio-temporal volumetric features. This general framework enables us to do real-time video analysis. We construct a realtime event detector for each action of interest by learning a cascade of filters based on volumetric features that efficiently scans video sequences in space and time. This event detector recognizes actions that are traditionally problematic for interest point methods - such as smooth motions where insufficient space-time interest points are available. Our experiments demonstrate that the technique accurately detects actions on real-world sequences and is robust to changes in viewpoint, scale and action speed. We also adapt our technique to the related task of human action classification and confirm that it achieves performance comparable to a current interest point based human activity recognizer on a standard database of human activities."
            },
            "slug": "Efficient-visual-event-detection-using-volumetric-Ke-Sukthankar",
            "title": {
                "fragments": [],
                "text": "Efficient visual event detection using volumetric features"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper constructs a realtime event detector for each action of interest by learning a cascade of filters based on volumetric features that efficiently scans video sequences in space and time and confirms that it achieves performance comparable to a current interest point based human activity recognizer on a standard database of human activities."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 228
                            }
                        ],
                        "text": "This is done by fitting a smooth trajectory (2nd order polynomial) to the centers of mass collected from the entire sequence and then by aligning this trajectory to a reference point (similarly to figurecentric stabilization in [11])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Many of them involve computation of optical flow [3], [11], whose estimation is difficult due to, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Some of the recent successful work done in the area of action recognition [10], [33], [17], [25], [16] have shown that it is useful to analyze actions by looking at a video sequence as a space-time volume (of intensities, gradients, optical flow, or other local features)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2619278,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f90d79809325d2b78e35a79ecb372407f81b3993",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds."
            },
            "slug": "Space-time-interest-points-Laptev-Lindeberg",
            "title": {
                "fragments": [],
                "text": "Space-time interest points"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work builds on the idea of the Harris and Forstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time to detect spatio-temporal events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153120475"
                        ],
                        "name": "S. Carlsson",
                        "slug": "S.-Carlsson",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Carlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50626295"
                        ],
                        "name": "J. Sullivan",
                        "slug": "J.-Sullivan",
                        "structuredName": {
                            "firstName": "Josephine",
                            "lastName": "Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "Moreover, we demonstrate the robustness of our method to partial occlusions, nonrigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low-quality video."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14178806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01d6e7994655604d5f1b81463b60339f7c23028f",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action is, in general, characterized by a sequence of specific body postures. An action is often recognizable from a single view of an action specific posture. In this paper we demonstrate that specific actions can be recognized in long video sequence by matching shape information extracted from individual frames to stored prototypes representing key frames of the action. The matching algorithm is tolerant to substantial deformation between image and prototype and recognizes qualitatively similar image shapes produced by the body postures. The algorithm is applied to the recognition of specific tennis strokes in up to 2 min long video sequences. Using key-frames within the sequence all (1015) forehand strokes are found with no false positives. The backhand strokes are detected with somewhat lower performance. We conclude that this type of approach to action recognition has large potential for application to automatic video analysis and editing."
            },
            "slug": "Action-Recognition-by-Shape-Matching-to-Key-Frames-Carlsson-Sullivan",
            "title": {
                "fragments": [],
                "text": "Action Recognition by Shape Matching to Key Frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that specific actions can be recognized in long video sequence by matching shape information extracted from individual frames to stored prototypes representing key frames of the action, which has large potential for application to automatic video analysis and editing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "We chose to demonstrate our method on the ballet movie example used in [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "These results are comparable to the results reported in [25]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Some of the recent successful work done in the area of action recognition [10], [33], [17], [25], [16] have shown that it is useful to analyze actions by looking at a video sequence as a space-time volume (of intensities, gradients, optical flow, or other local features)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6891864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4811a11937448ea5d40a0af36c974e08ab12c08c",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a behavior-based similarity measure which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field. This is done directly from the intensity information, without explicitly computing the underlying motions. Such a measure allows us to detect similarity between video segments of differently dressed people performing the same type of activity. It requires no foreground/background segmentation, no prior learning of activities, and no motion estimation or tracking. Using this behavior-based similarity measure, we extend the notion of 2-dimensional image correlation into the 3-dimensional space-time volume, thus allowing to correlate dynamic behaviors and actions. Small space-time video segments (small video clips) are \"correlated\" against entire video sequences in all three dimensions (x,y, and t). Peak correlation values correspond to video locations with similar dynamic behaviors. Our approach can detect very complex behaviors in video sequences (e.g., ballet movements, pool dives, running water), even when multiple complex activities occur simultaneously within the field-of-view of the camera."
            },
            "slug": "Space-time-behavior-based-correlation-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Space-time behavior based correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A behavior-based similarity measure is introduced which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field, thus allowing to correlate dynamic behaviors and actions."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917469"
                        ],
                        "name": "Shanon X. Ju",
                        "slug": "Shanon-X.-Ju",
                        "structuredName": {
                            "firstName": "Shanon",
                            "lastName": "Ju",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shanon X. Ju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Space-Time Saliency. Human action can often be described as a moving torso and a collection of parts undergoing articulated motion [7], [ 15 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [ 15 ], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5170789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e3b20fb94803d71910043059f402554aa5137b2",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the work of Black and Yacoob (1995) on the tracking and recognition of human facial expressions using parametrized models of optical flow to deal with the articulated motion of human limbs. We define a \"card-board person model\" in which a person's limbs are represented by a set of connected planar patches. The parametrized image motion of these patches in constrained to enforce articulated motion and is solved for directly using a robust estimation technique. The recovered motion parameters provide a rich and concise description of the activity that can be used for recognition. We propose a method for performing view-based recognition of human activities from the optical flow parameters that extends previous methods to cope with the cyclical nature of human motion. We illustrate the method with examples of tracking human legs of long image sequences."
            },
            "slug": "Cardboard-people:-a-parameterized-model-of-image-Ju-Black",
            "title": {
                "fragments": [],
                "text": "Cardboard people: a parameterized model of articulated image motion"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A method for performing view-based recognition of human activities from the optical flow parameters that extends previous methods to cope with the cyclical nature of human motion is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724754"
                        ],
                        "name": "C. Dyer",
                        "slug": "C.-Dyer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dyer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": ", [21], [24], [13]) and are thus limited to cyclic actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 295961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0f9b4dc5bc8cdeb6dd65073ee253e92ad8c3790",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general framework for image-based analysis of 3D repeating motions that addresses two limitations in the state of the art. First, the assumption that a motion be perfectly even from one cycle to the next is relaxed. Real repeating motions tend not to be perfectly even, i.e., the length of a cycle varies through time because of physically important changes in the scene. A generalization of period is defined for repeating motions that makes this temporal variation explicit. This representation, called the period trace, is compact and purely temporal, describing the evolution of an object or scene without reference to spatial quantities such as position or velocity. Second, the requirement that the observer be stationary is removed. Observer motion complicates image analysis because an object that undergoes a 3D repeating motion will generally not produce a repeating sequence of images. Using principles of affine invariance, we derive necessary and sufficient conditions for an image sequence to be the projection of a 3D repeating motion, accounting for changes in viewpoint and other camera parameters. Unlike previous work in visual invariance, however, our approach is applicable to objects and scenes whose motion is highly non-rigid. Experiments on real image sequences demonstrate how the approach may be used to detect several types of purely temporal motion features, relating to motion trends and irregularities. Applications to athletic and medical motion analysis are discussed."
            },
            "slug": "View-Invariant-Analysis-of-Cyclic-Motion-Seitz-Dyer",
            "title": {
                "fragments": [],
                "text": "View-Invariant Analysis of Cyclic Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents a general framework for image-based analysis of 3D repeating motions that addresses two limitations in the state of the art, and derives necessary and sufficient conditions for an image sequence to be the projection of a3D repeating motion, accounting for changes in viewpoint and other camera parameters."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652428"
                        ],
                        "name": "R. Polana",
                        "slug": "R.-Polana",
                        "structuredName": {
                            "firstName": "Ramprasad",
                            "lastName": "Polana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Polana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31614700"
                        ],
                        "name": "R. Nelson",
                        "slug": "R.-Nelson",
                        "structuredName": {
                            "firstName": "Randal",
                            "lastName": "Nelson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [21], [24], [13]) and are thus limited to cyclic actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14822037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64740c6256fa082bd4e9f75a647b2de8a6745892",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The recognition of nonrigid motion, particularly that arising from human movement (and by extension from the locomotory activity of animals) has typically made use of high-level parametric models representing the various body parts (legs, arms, trunk, head etc.) and their connections to each other. Such model-based recognition has been successful in some cases; however, the methods are often difficult to apply to real-world scenes, and are severely limited in their generalizability. The first problem arises from the difficulty of acquiring and tracking the requisite model parts, usually specific joints such as knees, elbows or ankles. This generally requires some prior high-level understanding and segmentation of the scene, or initialization by a human operator. The second problem, with generalization, is due to the fact that the human model is not much good for dogs or birds, and for each new type of motion, a new model must be hand-crafted. In this paper, we show that the recognition of human or animal locomotion, and, in fact, any repetitive activity can be done using low-level, non-parametric representations. Such an approach has the advantage that the same underlying representation is used for all examples, and no individual tailoring of models or prior scene understanding is required. We show in particular, that repetitive motion is such a strong cue, that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatio-temporal template of motion features. We have implemented a real-time system that can recognize and classify repetitive motion activities in normal gray-scale image sequences. Results on a number of real-world sequences are described."
            },
            "slug": "Detection-and-Recognition-of-Periodic,-Nonrigid-Polana-Nelson",
            "title": {
                "fragments": [],
                "text": "Detection and Recognition of Periodic, Nonrigid Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown, that repetitive motion is such a strong cue, that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatio-temporal template of motion features."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Many of them involve computation of optical flow [3], [11], whose estimation is difficult due to, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1716488,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "0092153b941790b81136177b1af2c08575ad5959",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A spatio-temporal representation for complex optical flow events is developed that generalizes traditional parameterized motion models (e.g. affine). These generative spatio-temporal models may be non-linear or stochastic and are event-specific in that they characterize a particular type of object motion (e.g. sitting or walking). Within a Bayesian framework we seek the appropriate model, phase, rate, spatial position, and scale to account for the image variation. The posterior distribution over this parameter space conditioned on image measurements is typically non-Gaussian. The distribution is represented using factored sampling and is predicted and updated over time using the condensation algorithm. The resulting framework automatically detects, localizes, and recognizes motion events."
            },
            "slug": "Explaining-optical-flow-events-with-parameterized-Black",
            "title": {
                "fragments": [],
                "text": "Explaining optical flow events with parameterized spatio-temporal models"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A spatio-temporal representation for complex optical flow events is developed that generalizes traditional parameterized motion models and automatically detects, localizes, and recognizes motion events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2006961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "886431a362bfdbcc6dd518f844eb374950b9de86",
            "isKey": false,
            "numCitedBy": 2877,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A view-based approach to the representation and recognition of human movement is presented. The basis of the representation is a temporal template-a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: The first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence. We then develop a recognition method matching temporal templates against stored instances of views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on standard platforms."
            },
            "slug": "The-Recognition-of-Human-Movement-Using-Temporal-Bobick-Davis",
            "title": {
                "fragments": [],
                "text": "The Recognition of Human Movement Using Temporal Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A view-based approach to the representation and recognition of human movement is presented, and a recognition method matching temporal templates against stored instances of views of known actions is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Human action can often be described as a moving torso and a collection of parts undergoing articulated motion [7], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Others, [31], [7] employ feature tracking and face difficulties in cases of self-occlusions, change of appearance, and problems of reinitialization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14497446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dbb7a71d67fbc1b8afa16aec6b34e788a74050c",
            "isKey": false,
            "numCitedBy": 766,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a probabilistic decomposition of human dynamics at multiple abstractions, and shows how to propagate hypotheses across space, time, and abstraction levels. Recognition in this framework is the succession of very general low level grouping mechanisms to increased specific and learned model based grouping techniques at higher levels. Hard decision thresholds are delayed and resolved by higher level statistical models and temporal context. Low-level primitives are areas of coherent motion found by EM clustering, mid-level categories are simple movements represented by dynamical systems, and high-level complex gestures are represented by Hidden Markov Models as successive phases of ample movements. We show how such a representation can be learned from training data, and apply It to the example of human gait recognition."
            },
            "slug": "Learning-and-recognizing-human-dynamics-in-video-Bregler",
            "title": {
                "fragments": [],
                "text": "Learning and recognizing human dynamics in video sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A probabilistic decomposition of human dynamics at multiple abstractions is described, and how to propagate hypotheses across space, time, and abstraction levels is shown."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398327241"
                        ],
                        "name": "Lihi Zelnik-Manor",
                        "slug": "Lihi-Zelnik-Manor",
                        "structuredName": {
                            "firstName": "Lihi",
                            "lastName": "Zelnik-Manor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihi Zelnik-Manor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The authors thank Lihi Zelnik-Manor for helpful discussions and her assistance in adjusting the method in [33] for comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Some of the recent successful work done in the area of action recognition [10], [33], [17], [25], [16] have shown that it is useful to analyze actions by looking at a video sequence as a space-time volume (of intensities, gradients, optical flow, or other local features)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Moreover, only absolute values of the gradient are taken in [33]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "(b) Action confusion in classification experiment using the method in [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "For comparison with our method, we applied the method of [33]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "6b shows that most of the errors of the method of [33] occur"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 86361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cef1a7aab17a1e4c7e7abdc027c7706287d6edad",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic events can be regarded as long-term temporal objects, which are characterized by spatio-temporal features at multiple temporal scales. Based on this, we design a simple statistical distance measure between video sequences (possibly of different lengths) based on their behavioral content. This measure is non-parametric and can thus handle a wide range of dynamic events. We use this measure for isolating and clustering events within long continuous video sequences. This is done without prior knowledge of the types of events, their models, or their temporal extent. An outcome of such a clustering process is a temporal segmentation of long video sequences into event-consistent sub-sequences, and their grouping into event-consistent clusters. Our event representation and associated distance measure can also be used for event-based indexing into long video sequences, even when only one short example-clip is available. However, when multiple example-clips of the same event are available (either as a result of the clustering process, or given manually), these can be used to refine the event representation, the associated distance measure, and accordingly the quality of the detection and clustering process."
            },
            "slug": "Event-based-analysis-of-video-Zelnik-Manor-Irani",
            "title": {
                "fragments": [],
                "text": "Event-based analysis of video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple statistical distance measure between video sequences based on their behavioral content that can handle a wide range of dynamic events and be used for isolating and clustering events within long continuous video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46992776"
                        ],
                        "name": "T. Fan",
                        "slug": "T.-Fan",
                        "structuredName": {
                            "firstName": "Ting-Jun",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463966"
                        ],
                        "name": "G. Medioni",
                        "slug": "G.-Medioni",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Medioni",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Medioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "A 2 2 Hessian and its eigenvalues have been used before for describing 3D surface properties [2], [12], [32], [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46591769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db394dc715f820d7c624116ffd12fb2f1fc51966",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is developed to extract important curves, corresponding to physical boundaries of objects, from a range image. It is shown how to infer, from these curves, a segmentation of the scene into surface patches, and how to use these descriptions to establish correspondences between two scenes. In a first step, labeled curves corresponding to jump boundaries, creases, and limbs of objects are grouped into boundaries of regions. Each region is therefore described by its boundaries and by a polynomial approximation, which allows each path individually and also the complete scene to be reconstructed. In a second step, objects (or partial objects) are inferred from surface patches, and then two range images are at this partial object level. Graphs of objects are matched using a best-first search under three types of constraints: unary constraints between corresponding nodes, binary constraints between corresponding linked pairs of nodes, and constraints imposed by the computed geometric transformation. Substantial partial occlusion is allowed. The generality and robustness of this approach is illustrated by several examples.<<ETX>>"
            },
            "slug": "Matching-3-D-objects-using-surface-descriptions-Fan-Medioni",
            "title": {
                "fragments": [],
                "text": "Matching 3-D objects using surface descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A method is developed to extract important curves, corresponding to physical boundaries of objects, from a range image, and it is shown how to infer, from these curves, a segmentation of the scene into surface patches, and how to use these descriptions to establish correspondences between two scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1988 IEEE International Conference on Robotics and Automation"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Inspired byearlierworks [ 22 ],[18]intheareaof perceptual grouping, and 3D shape reconstruction, we distinguish between the following three types of local space-time structures:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121993192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc67ba8025995068c9808be1d9aa50c14a420cb",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an approach to function-based object recognition that reasons about the functionality of an object's intuitive parts. We extend the popular \"recognition by parts\" shape recognition framework to support \"recognition by functional parts,\" by combining a set of functional primitives and their relations with a set of abstract volumetric shape primitives and their relations. Previous approaches have relied on more global object features, often ignoring the problem of object segmentation and thereby restricting themselves to range images of unoccluded scenes. We show how these shape primitives and relations can be easily recovered from superquadric ellipsoids which, in turn, can be recovered from either range or intensity images of occluded scenes, Furthermore, the proposed framework supports both unexpected (bottom-up) object recognition and expected (top-down) object recognition, We demonstrate the approach on a simple domain by recognizing a restricted class of hand-tools from 2-D images."
            },
            "slug": "Recognition-by-Functional-Parts-Rivlin-Dickinson",
            "title": {
                "fragments": [],
                "text": "Recognition by Functional Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how these shape primitives and relations can be easily recovered from superquadric ellipsoids which, in turn, can be recovered from either range or intensity images of occluded scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991672"
                        ],
                        "name": "M. Galun",
                        "slug": "M.-Galun",
                        "structuredName": {
                            "firstName": "Meirav",
                            "lastName": "Galun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Galun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952010"
                        ],
                        "name": "E. Sharon",
                        "slug": "E.-Sharon",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Sharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144880062"
                        ],
                        "name": "A. Brandt",
                        "slug": "A.-Brandt",
                        "structuredName": {
                            "firstName": "Achi",
                            "lastName": "Brandt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Brandt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "The isosurfaces of the solution U represent smoother versions of the Dirichlet bounding surface and are perpendicular to the Neumann bounding surfaces (first and last frames) [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [14] presented an alternative approach based on a solution to a Poisson equation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "The eigenvectors and eigenvalues of H then reveal the local orientation and aspect ratio of the shape [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "This measure can be computed [14] by solving a Poisson equation of the form: U\u00f0x; y; t\u00de 1\u20444 1, with \u00f0x; y; t\u00de 2 S, where the Laplacian of U is defined as U 1\u20444 Uxx \u00fe Uyy \u00fe Utt, subject to the Dirichlet boundary conditionsU\u00f0x; y; t\u00de 1\u20444 0 at the bounding surface @S."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "The solution to the Poisson equation can be used to extract a wide variety of useful local shape properties [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "Its eigenvectors correspond to the local principal directions and its eigenvalues are related to the local curvature in the direction of the corresponding eigenvectors and therefore inversely proportional to the length [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Below, we generalize the approach in [14] from 2D shapes in images to to deal with volumetric space-time shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "In this paper, we generalize a method developed for the analysis of 2D shapes [14] to deal with volumetric space-time shapes induced by human actions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "We adopt a recent approach [14] for analyzing 2D shapes and"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 110745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7df998fefc9ceaa085f329ecb29bdf71e4794765",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Silhouettes contain rich information about the shape of objects that can be used for recognition and classification. We present a novel approach that allows us to reliably compute many useful properties of a silhouette. Our approach assigns for every internal point of the silhouette a value reflecting the mean time required for a random walk beginning at the point to hit the boundaries. This function can be computed by solving Poisson's equation, with the silhouette contours providing boundary conditions. We show how this function can be used to reliably extract various shape properties including part structure and rough skeleton, local orientation and aspect ratio of different parts, and convex and concave sections of the boundaries. In addition to this we discuss properties of the solution and show how to efficiently compute this solution using multi-grid algorithms. We demonstrate the utility of the extracted properties by using them for shape classification."
            },
            "slug": "Shape-representation-and-classification-using-the-Gorelick-Galun",
            "title": {
                "fragments": [],
                "text": "Shape representation and classification using the Poisson equation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work presents a novel approach that allows us to reliably compute many useful properties of a silhouette that assigns for every internal point of the silhouette a value reflecting the mean time required for a random walk beginning at the point to hit the boundaries."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050737"
                        ],
                        "name": "P. Besl",
                        "slug": "P.-Besl",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Besl",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Besl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938732"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "A 2 2 Hessian and its eigenvalues have been used before for describing 3D surface properties [2], [12], [32], [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15124901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57a2991f56fa4972ff8fe2a8236001ff5d0e47af",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Invariant-surface-characteristics-for-3D-object-in-Besl-Jain",
            "title": {
                "fragments": [],
                "text": "Invariant surface characteristics for 3D object recognition in range images"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1904861"
                        ],
                        "name": "Ulrich Weidenbacher",
                        "slug": "Ulrich-Weidenbacher",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Weidenbacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrich Weidenbacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331203"
                        ],
                        "name": "Pierre Bayerl",
                        "slug": "Pierre-Bayerl",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Bayerl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Bayerl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718494"
                        ],
                        "name": "H. Neumann",
                        "slug": "H.-Neumann",
                        "structuredName": {
                            "firstName": "Heiko",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436224"
                        ],
                        "name": "R. Fleming",
                        "slug": "R.-Fleming",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Fleming",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fleming"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "A 2 2 Hessian and its eigenvalues have been used before for describing 3D surface properties [2], [12], [32], [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1492355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a454b86679a4c24f2991caf864610925fb5961a7",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Many materials including water, plastic, and metal have specular surface characteristics. Specular reflections have commonly been considered a nuisance for the recovery of object shape. However, the way that reflections are distorted across the surface depends crucially on 3D curvature, suggesting that they could, in fact, be a useful source of information. Indeed, observers can have a vivid impression of, 3D shape when an object is perfectly mirrored (i.e., the image contains nothing but specular reflections). This leads to the question what are the underlying mechanisms of our visual system to extract this 3D shape information from a perfectly mirrored object. In this paper we propose a biologically motivated recurrent model for the extraction of visual features relevant for the perception of 3D shape information from images of mirrored objects. We qualitatively and quantitatively analyze the results of computational model simulations and show that bidirectional recurrent information processing leads to better results than pure feedforward processing. Furthermore, we utilize the model output to create a rough nonphotorealistic sketch representation of a mirrored object, which emphasizes image features that are mandatory for 3D shape perception (e.g., occluding contour and regions of high curvature). Moreover, this sketch illustrates that the model generates a representation of object features independent of the surrounding scene reflected in the mirrored object."
            },
            "slug": "Sketching-shiny-surfaces:-3D-shape-extraction-and-Weidenbacher-Bayerl",
            "title": {
                "fragments": [],
                "text": "Sketching shiny surfaces: 3D shape extraction and depiction of specular surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A biologically motivated recurrent model for the extraction of visual features relevant for the perception of 3D shape information from images of mirrored objects is proposed and it is shown that bidirectional recurrent information processing leads to better results than pure feedforward processing."
            },
            "venue": {
                "fragments": [],
                "text": "TAP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463966"
                        ],
                        "name": "G. Medioni",
                        "slug": "G.-Medioni",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Medioni",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Medioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088295"
                        ],
                        "name": "Chi-Keung Tang",
                        "slug": "Chi-Keung-Tang",
                        "structuredName": {
                            "firstName": "Chi-Keung",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Keung Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109461722"
                        ],
                        "name": "Mi-Suen Lee",
                        "slug": "Mi-Suen-Lee",
                        "structuredName": {
                            "firstName": "Mi-Suen",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mi-Suen Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Inspired by earlier works [22], [18] in the area of perceptual grouping, and 3D shape reconstruction, we distinguish between the following three types of local space-time structures:"
                    },
                    "intents": []
                }
            ],
            "corpusId": 17054282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b89e367616701394c24900e3b7290c9c847beae",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified computational framework which properly implements the smoothness constraint to generate descriptions in terms of surfaces, regions, curves, and labelled junctions, from sparse, noisy, binary data in 2-D or 3-D. Each input site can be a point, a point with an associated tangent direction, a point with an associated normal direction, or any combination of the above. The methodology is grounded on two elements: tensor calculus for representation, and linear voting for communication: each input site communicates its information (a tensor) to its neighborhood through a predefined (tensor) field, and therefore casts a (tensor) vote. Each site collects all the votes cast at its location and encodes them into a new tensor. A local, parallel marching process then simultaneously detects features. The proposed approach is very different from traditional variational approaches, as it is non-iterative. Furthermore, the only free parameter is the size of the neighborhood, related to the scale. We have developed several algorithms based on the proposed methodology to address a number of early vision problems, including perceptual grouping in 2-D and 3-D, shape from stereo, and motion grouping and segmentation, and the results are very encouraging."
            },
            "slug": "Tensor-Voting-:-Theory-and-Applications-Medioni-Tang",
            "title": {
                "fragments": [],
                "text": "Tensor Voting : Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Several algorithms are developed based on the proposed methodology to address a number of early vision problems, including perceptual grouping in 2-D and 3-D, shape from stereo, and motion grouping and segmentation, and the results are very encouraging."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50529086"
                        ],
                        "name": "Roman Goldenberg",
                        "slug": "Roman-Goldenberg",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Goldenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Goldenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923265"
                        ],
                        "name": "R. Kimmel",
                        "slug": "R.-Kimmel",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kimmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kimmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695425"
                        ],
                        "name": "M. Rudzsky",
                        "slug": "M.-Rudzsky",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rudzsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rudzsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [13]) lack information about the motion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": ", [21], [24], [13]) and are thus limited to cyclic actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2915489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c5253f79d021b5b88b3f2cf8175d03b108c6487",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Behavior-classification-by-eigendecomposition-of-Goldenberg-Kimmel",
            "title": {
                "fragments": [],
                "text": "Behavior classification by eigendecomposition of periodic motions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Others, [ 31 ], [7] employ feature tracking and face difficulties in cases of self-occlusions, change of appearance, and problems of reinitialization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [ 31 ], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 438781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef02336545db21d6d994c637f31887cd2de6d1bc",
            "isKey": false,
            "numCitedBy": 463,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework for modeling and recognition of temporal activities is proposed. The modeling of sets of exemplar activities is achieved by parameterizing their representation in the form of principal components. Recognition of spatio-temporal variants of modeled activities is achieved by parameterizing the search in the space of admissible transformations that the activities can undergo. Experiments on recognition of articulated and deformable object motion from image motion parameters are presented."
            },
            "slug": "Parameterized-modeling-and-recognition-of-Yacoob-Black",
            "title": {
                "fragments": [],
                "text": "Parameterized Modeling and Recognition of Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments on recognition of articulated and deformable object motion from image motion parameters are presented, and a framework for modeling and recognition of temporal activities is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2746710"
                        ],
                        "name": "J. Tangelder",
                        "slug": "J.-Tangelder",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Tangelder",
                            "middleNames": [
                                "W.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tangelder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739867"
                        ],
                        "name": "R. Veltkamp",
                        "slug": "R.-Veltkamp",
                        "structuredName": {
                            "firstName": "Remco",
                            "lastName": "Veltkamp",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Veltkamp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Methods for 3D shape analysis and matching have been recently used in computer graphics (see survey in [28])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 987239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c05fc6a0dc1ea59f38a5f5130d17c342b79e54f8",
            "isKey": false,
            "numCitedBy": 927,
            "numCiting": 174,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in techniques for modeling, digitizing and visualizing 3D shapes has led to an explosion in the number of available 3D models on the Internet and in domain-specific databases. This has led to the development of 3D shape retrieval systems that, given a query object, retrieve similar 3D objects. For visualization, 3D shapes are often represented as a surface, in particular polygonal meshes, for example in VRML format. Often these models contain holes, intersecting polygons, are not manifold, and do not enclose a volume unambiguously. On the contrary, 3D volume models, such as solid models produced by CAD systems, or voxels models, enclose a volume properly. This paper surveys the literature on methods for content based 3D retrieval, taking into account the applicability to surface models as well as to volume models. The methods are evaluated with respect to several requirements of content based 3D shape retrieval, such as: (1) shape representation requirements, (2) properties of dissimilarity measures, (3) efficiency, (4) discrimination abilities, (5) ability to perform partial matching, (6) robustness, and (7) necessity of pose normalization. Finally, the advantages and limitations of the several approaches in content based 3D shape retrieval are discussed."
            },
            "slug": "A-survey-of-content-based-3D-shape-retrieval-Tangelder-Veltkamp",
            "title": {
                "fragments": [],
                "text": "A survey of content based 3D shape retrieval methods"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper surveys the literature on methods for content based 3D retrieval, taking into account the applicability to surface models as well as to volume models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Shape Modeling Applications, 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153120475"
                        ],
                        "name": "S. Carlsson",
                        "slug": "S.-Carlsson",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Carlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 31050079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dcd7c0c8d6dbeaef6d29ad83dbefa84529dc41ee",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general method for finding pointwise correspondence between 2-D shapes based on the concept of order structure and using geometric hashing. The problem of finding correspondence and the problem of establishing shape equivalence can be considered as one and the same problem. Given shape equivalence, we can in general and pointwise correspondence and the existence of a unambiguous correspondence mapping can be used as a rule for deciding shape equivalence. As a measure of shape equivalence we will use the concept of order structure which in principle can be defined for arbitrary geometric configurations such as points lines and curves. The order structure equivalence of subsets of points and tangent directions of a shape is will be used to establish pointwise correspondence. The finding of correspondence between different views of the same object and different instances of the same object category can be used as a foundation for establishment and recognition of visual categories."
            },
            "slug": "Order-Structure,-Correspondence,-and-Shape-Based-Carlsson",
            "title": {
                "fragments": [],
                "text": "Order Structure, Correspondence, and Shape Based Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A general method for finding pointwise correspondence between 2-D shapes based on the concept of order structure and using geometric hashing is proposed, which can be defined for arbitrary geometric configurations such as points lines and curves."
            },
            "venue": {
                "fragments": [],
                "text": "Shape, Contour and Grouping in Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894358"
                        ],
                        "name": "Olivier Chomat",
                        "slug": "Olivier-Chomat",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chomat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Chomat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some of the recent successful work done in the area of action recognition [ 10 ], [33], [17], [25], [16] have shown that it is useful to analyze actions by looking at a video sequence as a space-time volume (of intensities, gradients, optical flow, or other local features)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [ 10 ], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6343494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afa916190157b379ab5198b386e2afd6fe59e68c",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new technique for the perception of activities using a statistical description of spatio-temporal properties. With this approach, the probability of an activity in a spatio-temporal image sequence is computed by applying a Bayes rule to the joint statistics of the responses of motion energy receptive fields. A set of motion energy receptive fields is designed in order to sample the power spectrum of a moving texture. Their structure relates to the spatio-temporal energy models of Adelson and Bergen where measures of local visual motion information are extracted comparing the outputs of triad of Gabor energy filters. Then the probability density function required for the Bayes rule is estimated for each class of activity by computing multi-dimensional histograms from the outputs from the set of receptive fields. The perception of activities is achieved according to the Bayes rule. The result at a given time is the map of the conditional probabilities that each pixel belongs to an activity of the training set. The approach is validated with experiments in the perception of activities of walking persons in a visual surveillance scenario. Results are robust to changes in illumination conditions, to occlusions and to changes in texture."
            },
            "slug": "A-probabilistic-sensor-for-the-perception-of-Chomat-Crowley",
            "title": {
                "fragments": [],
                "text": "A probabilistic sensor for the perception of activities"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This approach, the probability of an activity in a spatio-temporal image sequence is computed by applying a Bayes rule to the joint statistics of the responses of motion energy receptive fields, which results in a map of the conditional probabilities that each pixel belongs to an activity of the training set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894358"
                        ],
                        "name": "Olivier Chomat",
                        "slug": "Olivier-Chomat",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chomat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Chomat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110132354"
                        ],
                        "name": "J\u00e9r\u00f4me Martin",
                        "slug": "J\u00e9r\u00f4me-Martin",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00e9r\u00f4me Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6167708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53cd2b8ea7296220e6981041bacf27a7c0285a5f",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new technique for the perception and recognition of activities using statistical descriptions of their spatio-temporal properties. A set of motion energy receptive fields is designed in order to sample the power spectrum of a moving texture. Their structure relates to the spatio-temporal energy models of Adelson and Bergen where measures of local visual motion information are extracted by comparing the outputs of a triad of Gabor energy filters. Then the probability density function required for Bayes rule is estimated for each class of activity by computing multi-dimensional histograms from the outputs from the set of receptive fields. The perception of activities is achieved according to Bayes rule. The result at each instant of time is the map of the conditional probabilities that each pixel belongs to each one of the activities of the training set. Since activities are perceived over a short integration time, a temporal analysis of outputs is done using Hidden Markov Models. \n \nThe approach is validated with experiments in the perception and recognition of activities of people walking in visual surveillance scenari. The presented work is in progress and preliminary results are encouraging, since recognition is robust to variations in illumination conditions, to partial occlusions and to changes in texture. It is shown that it constitute a powerful early vision tool for human behaviors analysis for smart-environnements."
            },
            "slug": "A-Probabilistic-Sensor-for-the-Perception-and-of-Chomat-Martin",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Sensor for the Perception and Recognition of Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Preliminary results are encouraging, since recognition is robust to variations in illumination conditions, to partial occlusions and to changes in texture, and it is shown that it constitute a powerful early vision tool for human behaviors analysis for smart-environnements."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003778"
                        ],
                        "name": "Kaleem Siddiqi",
                        "slug": "Kaleem-Siddiqi",
                        "structuredName": {
                            "firstName": "Kaleem",
                            "lastName": "Siddiqi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaleem Siddiqi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740078"
                        ],
                        "name": "A. Shokoufandeh",
                        "slug": "A.-Shokoufandeh",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Shokoufandeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shokoufandeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The Medial Axis Transform opened the way to the advent of skeleton-based representations such as [26], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9212861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "426c8c85829de02b4a39d96a3960f429ec40582d",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "We have been developing a theory for the generic representation of 2-D shape, where structural descriptions are derived from the shocks (singularities) of a curve evolution process, acting on bounding contours. We now apply the theory to the problem of shape matching. The shocks are organized into a directed, acyclic shock graph, and complexity is managed by attending to the most significant (central) shape components first. The space of all such graphs is highly structured and can be characterized by the rules of a shock graph grammar. The grammar permits a reduction of a shock graph to a unique rooted shock tree. We introduce a novel tree matching algorithm which finds the best set of corresponding nodes between two shock trees in polynomial time. Using a diverse database of shapes, we demonstrate our system's performance under articulation, occlusion, and moderate changes in viewpoint."
            },
            "slug": "Shock-Graphs-and-Shape-Matching-Siddiqi-Shokoufandeh",
            "title": {
                "fragments": [],
                "text": "Shock Graphs and Shape Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel tree matching algorithm is introduced which finds the best set of corresponding nodes between two shock trees in polynomial time and is demonstrated under articulation, occlusion, and moderate changes in viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874997"
                        ],
                        "name": "S. Niyogi",
                        "slug": "S.-Niyogi",
                        "structuredName": {
                            "firstName": "Sourabh",
                            "lastName": "Niyogi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18566850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57854a0e8309af7ad6f5d9612e20e2ba1a171a96",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel algorithm for gait analysis. A person walking frontoparallel to the image plane generates a characteristic \"braided\" pattern in a spatiotemporal (XYT) volume. Our algorithm detects this pattern, and fits it with a set of spatiotemporal snakes. The snakes can be used to find the bounding contours of the walker. The contours vary over time in a manner characteristic of each walker. Individual gaits can be recognized by applying standard pattern recognition techniques to the contour signals.<<ETX>>"
            },
            "slug": "Analyzing-and-recognizing-walking-figures-in-XYT-Niyogi-Adelson",
            "title": {
                "fragments": [],
                "text": "Analyzing and recognizing walking figures in XYT"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A novel algorithm for gait analysis that fits a characteristic \"braided\" pattern in a spatiotemporal volume, and fits it with a set of spatiotsemporal snakes that can be used to find the bounding contours of the walker."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32682854"
                        ],
                        "name": "T. Sebastian",
                        "slug": "T.-Sebastian",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Sebastian",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sebastian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36772245"
                        ],
                        "name": "P. Klein",
                        "slug": "P.-Klein",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Klein",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715265"
                        ],
                        "name": "B. Kimia",
                        "slug": "B.-Kimia",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Kimia",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kimia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The Medial Axis Transform opened the way to the advent of skeleton-based representations such as [26], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18019615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dde0e0ba609de345be0e7e67c2245152bb5ac40",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines issues arising in applying a previously developed edit-distance shock graph matching technique to indexing into large shape databases. This approach compares the shock graph topology and attributes to produce a similarity metric, and results in 100% recognition rate in querying a database of approximately 200 shapes. However, indexing into a significantly larger database is faced with both the lack of a suitable database, and more significantly with the expense related to computing the metric. We have thus (i) gathered shapes from a variety of sources to create a database of over 1000 shapes from forty categories as a stage towards developing an approach for indexing into a much larger database; (ii) developed a coarse-scale approximate similarly measure which relies on the shock graph topology and a very coarse sampling of link attributes. We show that this is a good first-order approximation of the similarly metric and is two orders of magnitude more efficient to compute. An interesting outcome of using this efficient but approximate similarity measure is that the approximation naturally demands a notion of categories to give high precision; (iii) developed an exemplar-based indexing scheme which discards a large number of non-matching shapes solely based on distance to exemplars, coarse scale representatives of each category. The use of a coarse-scale matching measure in conjunction with a coarse-scale sampling of the database leads to a significant reduction in the computational effort without discarding correct matches, thus paving the way for indexing into databases of tens of thousands of shapes."
            },
            "slug": "Shock-Based-Indexing-into-Large-Shape-Databases-Sebastian-Klein",
            "title": {
                "fragments": [],
                "text": "Shock-Based Indexing into Large Shape Databases"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper developed a coarse-scale approximate similarly measure which is a good first-order approximation of the similarly metric and is two orders of magnitude more efficient to compute, and developed an exemplar-based indexing scheme which discards a large number of non-matching shapes solely based on distance to exemplars, coarse scale representatives of each category."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "algorithm [19] to 90 unlabelled action sequences."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18764978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c02dfd94b11933093c797c362e2f8f6a3b9b8012",
            "isKey": false,
            "numCitedBy": 8412,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite many empirical successes of spectral clustering methods\u2014 algorithms that cluster points using eigenvectors of matrices derived from the data\u2014there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems."
            },
            "slug": "On-Spectral-Clustering:-Analysis-and-an-algorithm-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Spectral Clustering: Analysis and an algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A simple spectral clustering algorithm that can be implemented using a few lines of Matlab is presented, and tools from matrix perturbation theory are used to analyze the algorithm, and give conditions under which it can be expected to do well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97306284"
                        ],
                        "name": "K. Fu",
                        "slug": "K.-Fu",
                        "structuredName": {
                            "firstName": "King-Sun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Below we generalize the approach in [14] from 2D shapes in images to to deal with volumetric space-time shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "This measure can be computed [14] by solving a Poisson equation of the form: \u2206U(x, y, t) = \u22121, with (x, y, t) \u2208 S, where the Laplacian of U is defined as\u2206U = Uxx + Uyy + Utt, subject to the Dirichlet boundary conditions U(x, y, t) = 0 at the bounding surface\u2202S."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "We adopt a recent approach [14] for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [14] presented an alternative approach based on a solution to a Poisson equation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "The eigenvectors and eigenvalues of H then reveal the local orientation and aspect ratio of the shape [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "The solution to the Poisson equation can be used to extract a wide variety of useful local shape properties [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "Its eigenvectors correspond to the local principal directions and its eigenvalues are related to the local curvature in the direction of the corresponding eigenvectors and therefore inversely proportional to the length [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "The isosurfaces of the solutionU represent smoother versions of the Dirichlet bounding surface and are perpendicular to the Neumann bounding surfaces (first and last frames) [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "In this paper we generalize a method developed for the analysis of 2D shapes [14] to deal with volumetric space-time shapes induced by human actions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60977545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb66ae5f36bc84243979c522d8e3f93539cb6a9f",
            "isKey": true,
            "numCitedBy": 3689,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "IEEE-Transactions-on-Pattern-Analysis-and-Machine-Fu",
            "title": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Index Terms\u2014Action representation, action recognition, space-time analysis, shape analysis, poisson equation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "We first show how the Poisson equation can be used to characterize space-time points by identifying space-time saliency of moving parts and locally judging the orientation and rough aspect ratios of the space-time shape."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Then, we describe how these local properties can be integrated into a compact vector of global features to represent an action."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Event-Based Analysis of Video Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Event-Based Analysis of Video Computer Vision and Pattern Recognition"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "In this paper we generalize a method developed for analysis of 2D shapes [9], to deal with volumetric space-time shapes induced by human actions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Below we generalize the approach in [9] to deal with volumetric space-time shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Below we generalize the analysis in [9] to characterize actions as space-time shapes using measures that estimate locally the second order moments of a shape near any given point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "The solution to the Poisson equation can be used to extract a wide variety of useful local shape properties [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "We adopt a recent approach [9] for analyzing 2D shapes and generalize it to deal with volumetric spacetime action shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "This measure can be computed [9] by solving a Poisson equation of the form:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape representation and recognition using the poisson equation"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR, 2:61\u201367,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(9) Degree of \u201cPlateness\u201d Degree of \u201cStickness\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(9)) of up to order 7 in space and in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(9)), giving rise to a 280 feature vector representation per space-time cube."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape representation and recognition using the poisson equation.CVPR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shock graphs and shape matching. IEEE Int. Conf. on Computer Vision (ICCV)"
            },
            "venue": {
                "fragments": [],
                "text": "Shock graphs and shape matching. IEEE Int. Conf. on Computer Vision (ICCV)"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Human action can often be described as a moving torso and a collection of parts undergoing articulated motion [7], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Moreover, in cases of motion discontinuities, motion aliasing, and low-quality video, working with silhouettes may be advantageous over many existing methods ([3], [7], [10], [11], [15], [16], [17], [21], [20], [24], [25], [31], [33]) that compute optical flow, local space-time gradients, or other intensity-based features."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cardboard People: A Parametrized Model of Aticulated Image Motion,"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Second Int\u2019l Conf. Automatic Face and Gesture Recognition,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition by functional parts.CVPR, pages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "In order to cope with the artificial boundary at the first and last frames of the video, we impose the Neumann boundary conditions requiring Ut = 0 at those frames [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "We used a simple \u201cw-cycle\u201d of a geometric multigrid solver which is linear in the number of space-time points [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multigrid"
            },
            "venue": {
                "fragments": [],
                "text": "Academic Press,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "One of the well-known shape descriptors is the Medial Axis Distance Transform [5], where each internal pixel of a silhouette is assigned a value reflecting its minimum distance to the boundary contour."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Transformation for Extracting New Descriptors of Shape,\u201d Models for the Perception of Speech and Visual Form"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Symp.,"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape representation and recognition using the poisson equation"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parametrized modeling and recognition of activities.CVIU"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "DH(s, ak) = median i (min j \u2016ci \u2212 cj\u2016) (12)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tensor voting: Theory and applications.Proceedings of RFIA, Paris, France"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 14,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Actions-as-space-time-shapes-Blank-Gorelick/1a9eb04b9b07d4a58aa78eb9f68a77ade0199fab?sort=total-citations"
}