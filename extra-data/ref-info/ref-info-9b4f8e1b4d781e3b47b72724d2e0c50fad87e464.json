{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 107
                            }
                        ],
                        "text": "Approximations therefore must be made (see, e.g., Cheeseman and Stutz 1995; Chickering and Heckerman 1997; Friedman 1998), the major schemes being Markov chain Monte Carlo meth ods and Laplace approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 38
                            }
                        ],
                        "text": "Approximations therefore must be made (see, e.g., Cheeseman and Stutz 1995; Chickering and Heckerman 1997; Friedman 1998), the major schemes being Markov chain Monte Carlo methods and Laplace approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 447055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e0bb90262160c26d8c9ec216716d57122c8672",
            "isKey": false,
            "numCitedBy": 688,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data--that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "slug": "The-Bayesian-Structural-EM-Algorithm-Friedman",
            "title": {
                "fragments": [],
                "text": "The Bayesian Structural EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper extends Structural EM to deal directly with Bayesian model selection and proves the convergence of the resulting algorithm and shows how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "The Bayesian framework (Mackay 1992a, 1992b; Cooper and Herskovits 1992; Heckerman et al. 1995) provides, in principle, a solution to the first two prob lems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "earn the structure of the graph, since more complicated graphs assign a higher likelihood to the data. Third, it is computationally tractable only for a small class of models. The Bayesian framework (Mackay 1992a, 1992b; Cooper and Herskovits 1992; Heckerman et al. 1995) provides, in principle, a solution to the first two prob lems. In this framework one considers an ensemble of models, characterized by a p"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": false,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "The Bayesian framework (Mackay 1992a, 1992b; Cooper and Herskovits 1992; Heckerman et al. 1995) provides, in principle, a solution to the first two prob lems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 236
                            }
                        ],
                        "text": "We point out that in the large sample limit, the covariance of A vanishes and its mean becomes A = Cyx(Cxx) (1), a form appearing in the ordinary EM algorithms for factor analysis (Rubin and Thayer 1982) and independent factor analysis (Attias 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "Since the computational complexity of the algorithm increases exponentially with the number of sources, the large m case is treated in (Attias 1999a) by a structured variational approximation (Ghahramani and Jordan 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 746481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2307fd6058ab4f7554a0b1f188507150ddb5b9a2",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the independent factor analysis (IFA) method for recovering independent hidden sources from their observed mixtures. IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. For this purpose we present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation. Each source in our model is described by a mixture of gaussians; thus, all the probabilistic calculations can be performed analytically. In the second step, the sources are reconstructed from the observed data by an optimal nonlinear estimator. A variational approximation of this algorithm is derived for cases with a large number of sources, where the exact algorithm becomes intractable. Our IFA algorithm reduces to the one for ordinary FA when the sources become gaussian, and to an EM algorithm for PCA in the zero-noise limit. We derive an additional EM algorithm specifically for noiseless IFA. This algorithm is shown to be superior to ICA since it can learn arbitrary source densities from the data. Beyond blind separation, IFA can be used for modeling multidimensional data by a highly constrained mixture of gaussians and as a tool for nonlinear signal encoding."
            },
            "slug": "Independent-Factor-Analysis-Attias",
            "title": {
                "fragments": [],
                "text": "Independent Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An expectation-maximization (EM) algorithm is presented, which performs unsupervised learning of an associated probabilistic model of the mixing situation and is shown to be superior to ICA since it can learn arbitrary source densities from the data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31658843"
                        ],
                        "name": "C. Schreiner",
                        "slug": "C.-Schreiner",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schreiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 140
                            }
                        ],
                        "text": "We point out that realistic situations include many additional complications, such as multipath propagation and reverberant conditions (see (Attias and Schreiner 1998) for a treatment of the zero-noise convolutive blind separation problem), as well as non-stationarity; these issues are beyond the scope of the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 131
                            }
                        ],
                        "text": "We shall use the model pi(xi) = cosh (2)(xi=2)=2, which has been shown to be accurate for the purpose of separating speech sources (Bell and Sejnowski 1995; Attias and Schreiner 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 207
                            }
                        ],
                        "text": "4 Blind Source Separation\n4.1 Definitions\nIn the blind source separation (BSS) problem, a.k.a. independent component analysis (ICA) (Jutten and Herault 1991; Bell and Sejnowski 1995; Cardoso and Lahed 1996; Attias and Schreiner 1998), one is pre sented with multivariable time series data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Most existing ICA algorithms address the simplified case where the noise vanishes and the mixing matrix is square invertible, so the number of sensors equals the number of sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Attias\nWe point out that realistic situations include many ad ditional complications, such as multipath propagation and reverberant conditions (see (Attias and Schreiner 1998) for a treatment of the zero-noise convolutive blind separation problem), as well as non-stationarity; these issues are\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 37
                            }
                        ],
                        "text": "independent component analysis (ICA) (Jutten and Herault 1991; Bell and Sejnowski 1995; Cardoso and Lahed 1996; Attias and Schreiner 1998), one is presented with multivariable time series data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 126
                            }
                        ],
                        "text": "= cosh-2(x; /2)/2, which has been shown to be accurate for the purpose of separating speech sources (Bell and Sejnowski 1995; Attias and Schreiner 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15395847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1b2ec1a2637b8ba811311e1ca7f756b14a480d2",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a novel family of unsupervised learning algorithms for blind separation of mixed and convolved sources. Our approach is based on formulating the separation problem as a learning task of a spatiotemporal generative model, whose parameters are adapted iteratively to minimize suitable error functions, thus ensuring stability of the algorithms. The resulting learning rules achieve separation by exploiting high-order spatiotemporal statistics of the mixture data. Different rules are obtained by learning generative models in the frequency and time domains, whereas a hybrid frequency-time model leads to the best performance. These algorithms generalize independent component analysis to the case of convolutive mixtures and exhibit superior performance on instantaneous mixtures. An extension of the relative-gradient concept to the spatiotemporal case leads to fast and efficient learning rules with equivariant properties. Our approach can incorporate information about the mixing situation when available, resulting in a semiblind separation method. The spatiotemporal redundancy reduction performed by our algorithms is shown to be equivalent to information-rate maximization through a simple network. We illustrate the performance of these algorithms by successfully separating instantaneous and convolutive mixtures of speech and noise signals."
            },
            "slug": "Blind-Source-Separation-and-Deconvolution:-The-Attias-Schreiner",
            "title": {
                "fragments": [],
                "text": "Blind Source Separation and Deconvolution: The Dynamic Component Analysis Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel family of unsupervised learning algorithms for blind separation of mixed and convolved sources is derived, based on formulating the separation problem as a learning task of a spatiotemporal generative model, whose parameters are adapted iteratively to minimize suitable error functions, thus ensuring stability of the algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50130827"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 167
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 165
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in principle, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 197459132,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6b64f5e4b2813bbe0a8be53fadc9aafc89055648",
            "isKey": false,
            "numCitedBy": 1005,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We regret to report that there is an error in equation (12) on p. 740, concerning the birth\u00b1 death move for empty components. The correct expression is A \u0088 p\u0085k\u0087 1\u0086 p\u0085k\u0086 1 B\u0085k , \u0086 w y1 j* \u00851y wj*\u0086 yk\u0085k\u0087 1\u0086 dk\u00871 \u0085k0 \u0087 1\u0086bk 1 g1,k\u0085wj*\u0086 \u00851y wj*\u0086: In the paper as printed, the power of 1y wj* in the \u00aenal factor, the Jacobian term, was given as k instead of ky 1. The source of the error was neglect of the condition j wj \u0088 1 in computing the partial derivatives of wj with respect to wj. Note that expression (11) for the split\u00b1combine move acceptance ratio is correct as printed. Having made the correction, we repeated the calculations leading to all the numerical results reported in the paper. As might be expected, the e\u0080ects of the error are noticeable but small. The maximum changes to any of the posterior probabilities p\u0085kjy\u0086 presented in Tables 1 and 2 and Fig. 6 are 0.015, 0.011 and 0.020 respectively; in each case the maximum discrepancy occurs near the mode of the distributions, and so has little impact. The error in Fig. 2 is within plotting accuracy. In none of the \u00aegures is the visual impression altered, and none of our qualitative conclusions are a\u0080ected. We are grateful to Tobias Ryden of Lund University for discovering this error, and we apologize for any confusion that it has caused."
            },
            "slug": "Corrigendum:-On-Bayesian-analysis-of-mixtures-with-Richardson-Green",
            "title": {
                "fragments": [],
                "text": "Corrigendum: On Bayesian analysis of mixtures with an unknown number of components"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2426066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9a1048d0f5356f2e08b182dc1a125423d827072",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Overcomplete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component analysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcomplete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency."
            },
            "slug": "Learning-Nonlinear-Overcomplete-Representations-for-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Nonlinear Overcomplete Representations for Efficient Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The utility of overcomplete representations on natural speech is demonstrated and it is shown that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103186642"
                        ],
                        "name": "ModelCarl Edward RasmussenDepartment",
                        "slug": "ModelCarl-Edward-RasmussenDepartment",
                        "structuredName": {
                            "firstName": "ModelCarl",
                            "lastName": "RasmussenDepartment",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ModelCarl Edward RasmussenDepartment"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 194
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 165
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in principle, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15355118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0eb04159ea9492e9db1404d3800cca39b0f0adc",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In a Bayesian mixture model, there is no need a priori to restrict the number of components to be nite. Innnite mixture models sidestep the problem of nding the \\correct\" number of components, and may be handled using a nite amount of computation. In this paper it is demonstrated how inference may be done in innnite mixture models using a Markov Chain whose implementation relies entirely on Gibbs sampling. An example is given of application to multivariate density estimation."
            },
            "slug": "The-Countably-In-nite-Bayesian-Gaussian-Mixture-RasmussenDepartment",
            "title": {
                "fragments": [],
                "text": "The Countably In nite Bayesian Gaussian Mixture Density"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In this paper it is demonstrated how inference may be done in innnite mixture models using a Markov Chain whose implementation relies entirely on Gibbs sampling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69495445"
                        ],
                        "name": "Dorothy T. Thayer",
                        "slug": "Dorothy-T.-Thayer",
                        "structuredName": {
                            "firstName": "Dorothy",
                            "lastName": "Thayer",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dorothy T. Thayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 178
                            }
                        ],
                        "text": "We point out that in the large sample limit, the covariance of A vanishes and its mean becomes A= Cyx(Cxx)-1, a form appearing in the ordinary EM algorithms for factor analysis (Rubin and Thayer 1982) and independent factor analysis (Attias 1999a)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 123437256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e9222ee44916c976c80f11303002e850de0c63e",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq \u00d7q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation."
            },
            "slug": "EM-algorithms-for-ML-factor-analysis-Rubin-Thayer",
            "title": {
                "fragments": [],
                "text": "EM algorithms for ML factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 101
                            }
                        ],
                        "text": "= cosh-2(x; /2)/2, which has been shown to be accurate for the purpose of separating speech sources (Bell and Sejnowski 1995; Attias and Schreiner 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 158
                            }
                        ],
                        "text": "4 Blind Source Separation\n4.1 Definitions\nIn the blind source separation (BSS) problem, a.k.a. independent component analysis (ICA) (Jutten and Herault 1991; Bell and Sejnowski 1995; Cardoso and Lahed 1996; Attias and Schreiner 1998), one is pre sented with multivariable time series data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": true,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 243
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 50
                            }
                        ],
                        "text": "Approximations therefore must be made (see, e.g., Cheeseman and Stutz 1995; Chickering and Heckerman 1997; Friedman 1998), the major schemes being Markov chain Monte Carlo meth ods and Laplace approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6176762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42f75b297aed474599c8e598dd211a1999804138",
            "isKey": false,
            "numCitedBy": 1298,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe AutoClass an approach to unsupervised classi cation based upon the classical mixture model supplemented by a Bayesian method for determining the optimal classes We include a moderately detailed exposition of the mathematics behind the AutoClass system We emphasize that no current unsupervised classi cation system can produce maximally useful results when operated alone It is the interaction between domain experts and the machine searching over the model space that generates new knowledge Both bring unique information and abilities to the database analysis task and each enhances the others e ectiveness We illustrate this point with several applications of AutoClass to complex real world databases and describe the resulting successes and failures"
            },
            "slug": "Bayesian-Classification-(AutoClass):-Theory-and-Cheeseman-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification (AutoClass): Theory and Results"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is emphasized that no current unsupervised classi cation system can produce maximally useful results when operated alone and that it is the interaction between domain experts and the machine searching over the model space that generates new knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Knowledge Discovery and Data Mining"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7424318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79433b5feacd9e8feeafa629dae5a85f362fef",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-Networks-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The utility of a mean field theory for sigmoid belief networks based on ideas from statistical mechanics is demonstrated on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 133
                            }
                        ],
                        "text": "4 Blind Source Separation\n4.1 Definitions\nIn the blind source separation (BSS) problem, a.k.a. independent component analysis (ICA) (Jutten and Herault 1991; Bell and Sejnowski 1995; Cardoso and Lahed 1996; Attias and Schreiner 1998), one is pre sented with multivariable time series data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Most existing ICA algorithms address the simplified case where the noise vanishes and the mixing matrix is square invertible, so the number of sensors equals the number of sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 37
                            }
                        ],
                        "text": "independent component analysis (ICA) (Jutten and Herault 1991; Bell and Sejnowski 1995; Cardoso and Lahed 1996; Attias and Schreiner 1998), one is presented with multivariable time series data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": true,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "in the large sample limit, this algorithm reduces to ordinary EM (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 213
                            }
                        ],
                        "text": "In the E-step the hidden variable posterior is computed using the old SS; in the M-step the new SS are computed, updating the parameter posterior. in the large sample limit, this algorithm reduces to ordinary EM (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 136
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor responding to using flat prior p( e) and exact (rather than variational)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In addition, the BIC/MDL model selection criteria are obtained from VB in the large sample limit."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor responding to using flat prior p( e) and exact (rather than variational) posterior q(H)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), corresponding to using at prior p( ) and exact (rather than variational) posterior q(H)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B, 49, 223-239 and 253-265."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 136
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor responding to using flat prior p( e) and exact (rather than variational)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor\u00ad responding to using flat prior p( e) and exact (rather than variational) posterior q(H)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In addition, the BIC/MDL model selection criteria are obtained from VB in the large sample limit."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor responding to using flat prior p( e) and exact (rather than variational) posterior q(H)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity (with dis\u00ad cussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B, 4 9, 223-239 and 253-265."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 83
                            }
                        ],
                        "text": "It will be exciting to ap ply the VB framework to complex Bayesian networks (e.g., Attias 1999b), including dynamic models, and demonstrate its performance on real-world tasks such as speech recognition and scene analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Attias (1999a) solved this problem by mod eling each source density by a 1-dim mixture of Gaus sians, which allows the above integral to be calculated analytically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 234
                            }
                        ],
                        "text": "We point out that in the large sample limit, the covariance of A vanishes and its mean becomes A= Cyx(Cxx)-1, a form appearing in the ordinary EM algorithms for factor analysis (Rubin and Thayer 1982) and independent factor analysis (Attias 1999a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "Since the computational complexity of the algorithm increases exponentially with the number of sources, the large m case is treated in (Attias 1999a) by a structured varia tional approximation (Ghallramani and Jordan 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical !FA belief networks. Statistics and Artificial Intelligence 7 (Heckerman"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 83
                            }
                        ],
                        "text": "It will be exciting to ap ply the VB framework to complex Bayesian networks (e.g., Attias 1999b), including dynamic models, and demonstrate its performance on real-world tasks such as speech recognition and scene analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Attias (1999a) solved this problem by mod eling each source density by a 1-dim mixture of Gaus sians, which allows the above integral to be calculated analytically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 234
                            }
                        ],
                        "text": "We point out that in the large sample limit, the covariance of A vanishes and its mean becomes A= Cyx(Cxx)-1, a form appearing in the ordinary EM algorithms for factor analysis (Rubin and Thayer 1982) and independent factor analysis (Attias 1999a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "Since the computational complexity of the algorithm increases exponentially with the number of sources, the large m case is treated in (Attias 1999a) by a structured varia tional approximation (Ghallramani and Jordan 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19109214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fa6ec8776596977d1e901df99870f71e64f3e85",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hierarchical-IFA-Belief-Networks-Attias",
            "title": {
                "fragments": [],
                "text": "Hierarchical IFA Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "V0 reduces in this limit to a term that is linear in the number of the ML model param eters, plus a simple regularirer -logp(eo). Finally, we point out that the Bayesian information criterion (BIC) (Schwartz 1978) and the minimum description length criterion (MDL) (Rissanen 1987) both emerge as a special case of our large sample expression (7), cor responding to using flat prior p( e) and exact (rather than v"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": false,
            "numCitedBy": 39572,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factorial hid\u00ad den Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 208
                            }
                        ],
                        "text": "\u2026non-square mixing and non zero noise is harder, since one has to compute p(y I A, m) = J dx p(y I x, A, m)p (x I m), where the m dim integration is non-trivial due to the non-Gaussian nature of the sources: Lewicki and Sejnowski (1998) integrated over the sources using the Laplace approx imation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning non \u00ad linear overcomplete representations for efficient cod"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "The Bayesian framework (Mackay 1992a, 1992b; Cooper and Herskovits 1992; Heckerman et al. 1995) provides, in principle, a solution to the first two prob lems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A practical Bayesian frame\u00ad work for backpropagation networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computa\u00ad tion 4, 448-4"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Bayesian analysis of mixtures with an unknown number of com\u00ad ponents"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 17
                            }
                        ],
                        "text": "Whereas this approach usually pro duces overfitting and suboptimal generaliza tion performance, carrying out the Bayesian program of computing the full posterior dis tributions over the parameters remains a dif ficult problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 198
                            }
                        ],
                        "text": "\u2026together variational ideas from intractable hidden variables models (Saul, Jaakkola and Jordan 1996; Ghahramani and Jor dan 1997) and from Bayesian inference (Waterhouse, Mackay and Robinson 1996; Jaakkola & Jordan 1997; Mackay 1998), which, in turn, draw on the work of Neal and Hinton (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Logistic Regression : A Variational Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Artificial Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for mixture of experts. Advances in Neural Information Processing Systems 8 (Touretzky, D.S. et al, Eds)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ensemble learning for hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Cavendish Labora\u00ad tory, Cambridge University. Neal, R.M. & Hinton, G.E. (1998). A view of the EM algorithm that justifies incremental, sparse, and"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 167
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Bayesian analysis of mixtures with an unknown number of com \u00ad ponents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 208
                            }
                        ],
                        "text": "\u2026non-square mixing and non zero noise is harder, since one has to compute p(y I A, m) = J dx p(y I x, A, m)p (x I m), where the m dim integration is non-trivial due to the non-Gaussian nature of the sources: Lewicki and Sejnowski (1998) integrated over the sources using the Laplace approx imation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning non - linear overcomplete representations for e\u00c6cient cod"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 208
                            }
                        ],
                        "text": "\u2026non-square mixing and non zero noise is harder, since one has to compute p(y I A, m) = J dx p(y I x, A, m)p (x I m), where the m dim integration is non-trivial due to the non-Gaussian nature of the sources: Lewicki and Sejnowski (1998) integrated over the sources using the Laplace approx imation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning non\u00ad linear overcomplete representations for efficient cod\u00ad ing"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Sys\u00ad tems 10"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for mixture of experts"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 8 ("
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factorial hid \u00ad den Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical ! FA belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Artificial Intelligence"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 172
                            }
                        ],
                        "text": "\u2026together variational ideas from intractable hidden variables models (Saul, Jaakkola and Jordan 1996; Ghahramani and Jor dan 1997) and from Bayesian inference (Waterhouse, Mackay and Robinson 1996; Jaakkola & Jordan 1997; Mackay 1998), which, in turn, draw on the work of Neal and Hinton (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for mixture of experts"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "The variational EM algorithm (Saul et al. 1996) was introduced for cases where the computation of the exact posterior p(H j Y ) is intractable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mean eld theory of sigmoid belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Arti cial Intelligence Research"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 194
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 167
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin\u00ad ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The countably infinite Bayesian Gaussian mixture density model"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Tech. U. of Denmark. Richardson, S. & Green, P.J. (1997). On Bayesian analysis of mixtures with an unknown number of com\u00ad"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 198
                            }
                        ],
                        "text": "\u2026together variational ideas from intractable hidden variables models (Saul, Jaakkola and Jordan 1996; Ghahramani and Jor dan 1997) and from Bayesian inference (Waterhouse, Mackay and Robinson 1996; Jaakkola & Jordan 1997; Mackay 1998), which, in turn, draw on the work of Neal and Hinton (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Logistic Regression: A Variational Approach. Statistics and Arti cial Intelligence 6 (Smyth, P"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classi\u00ad fication (Autoclass): Theory and results Advances in Knowledge Discovery and Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian classi\u00ad fication (Autoclass): Theory and results Advances in Knowledge Discovery and Data Mining"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "in the large sample limit, this algorithm reduces to ordinary EM (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 213
                            }
                        ],
                        "text": "In the E-step the hidden variable posterior is computed using the old SS; in the M-step the new SS are computed, updating the parameter posterior. in the large sample limit, this algorithm reduces to ordinary EM (Dempster et al. 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxi\u00ad mum likelihood from incomplete data via the EM al\u00ad gorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 243
                            }
                        ],
                        "text": "While the Bayesian approach provides the solution in prin ciple, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (Richardson and Green 1997; Rasmussen 1999) and approximation methods (e.g., Cheeseman and Stutz 1995) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 50
                            }
                        ],
                        "text": "Approximations therefore must be made (see, e.g., Cheeseman and Stutz 1995; Chickering and Heckerman 1997; Friedman 1998), the major schemes being Markov chain Monte Carlo meth ods and Laplace approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classi\u00ad fication (Autoclass): Theory and results. Advances in Knowledge Discovery and Data Mining, 153-180"
            },
            "venue": {
                "fragments": [],
                "text": "(Fayyad, U. et a!.,"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Inferring-Parameters-and-Structure-of-Latent-Models-Attias/9b4f8e1b4d781e3b47b72724d2e0c50fad87e464?sort=total-citations"
}