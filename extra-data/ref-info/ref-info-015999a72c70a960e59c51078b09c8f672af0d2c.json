{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Some of the results in this paper were presented at NIPS`96 [5]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 52
                            }
                        ],
                        "text": "Some of the results in this paper were presented at NIPS`96 [5].1.1 Outline of the paperThe next section gives estimates of the misclassi cation probability in terms of the proportionof \\distinctly correct\" examples and the fat-shattering dimension."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14197727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4c16b4bfb2794f17f4816b621f4b97e70b7abdf",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. More specifically, consider an l-layer feed-forward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A. The misclassification probability converges to an error estimate (that is closely related to squared error on the training set) at rate O((cA)l(l+1)/2 \u221a(log n)/m) ignoring log factors, where m is the number of training patterns, n is the input dimension, and c is a constant. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training."
            },
            "slug": "For-Valid-Generalization-the-Size-of-the-Weights-is-Bartlett",
            "title": {
                "fragments": [],
                "text": "For Valid Generalization the Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "First, neural networks often perform successfully with training sets that are considerably smaller than the number of network parameters (see, for example, [29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6558191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2893d64bb2ad5fb4cf69ee63f8de3abb7906481c",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important aspects of any machine learning paradigm is how i t scales according to problem size and complexity. Using a task with known optimal train ing error, and a pre-specified maximum number of training updates, we investigate the convergence of th backpropagation algorithm with respect to a) the complexity of the required function approximatio n, b) the size of the network in relation to the size required for an optimal solution, and c) the degree o f noise in the training data. In general, for a) the solution found is worse when the function to be app roximated is more complex, for b) oversized networks can result in lower training and generalization error in certain cases, and for c) the use of committee or ensemble techniques can be more beneficial as the level o f noise in the training data is increased. For the experiments we performed, we do not obtain the optimal solution in any case. We further support the observation that larger networks can produce bett er training and generalization error using a face recognition example where a network with many more par ameters than training points generalizes better than smaller networks."
            },
            "slug": "What-Size-Neural-Network-Gives-Optimal-Convergence-Lawrence-Giles",
            "title": {
                "fragments": [],
                "text": "What Size Neural Network Gives Optimal Generalization? Convergence Properties of Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The convergence of the backpropagation algorithm with respect to a) the complexity of the required function approximatio n, b) the size of the network in relation to the size required for an optimal solution, and c) the degree of noise in the training data is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12154028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7020c2455b74deda5af696248cc41c53e32c00e2",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms."
            },
            "slug": "Automatic-Capacity-Tuning-of-Very-Large-Classifiers-Guyon-Boser",
            "title": {
                "fragments": [],
                "text": "Automatic Capacity Tuning of Very Large VC-Dimension Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Results from statistical learning theory (for example, [8], [10], [19], and [40]) give sample size bounds"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "Bounds for are known in these cases (see [8], [16], and [25], respectively)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Bounds on the pseudodimension of various neural network classes have been established (see, for example, [8], [19], [25], and [34]), but these are all at least linear in the number of parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "We will make use of the following result on approximation in Hilbert spaces, which has been attributed to Maurey (see [4] and [24])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15383918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04113e8974341f97258800126d05fd8df2751b7e",
            "isKey": false,
            "numCitedBy": 2593,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. >"
            },
            "slug": "Universal-approximation-bounds-for-superpositions-a-Barron",
            "title": {
                "fragments": [],
                "text": "Universal approximation bounds for superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings and the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 361066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4184c4e81d5ee51d34dba168e76e94eab771e421",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that the class of two-layer neural networks with bounded fan-in is efficiently learnable in a realistic extension to the probably approximately correct (PAC) learning model. In this model, a joint probability distribution is assumed to exist on the observations and the learner is required to approximate the neural network which minimizes the expected quadratic error. As special cases, the model allows learning real-valued functions with bounded noise, learning probabilistic concepts, and learning the best approximation to a target function that cannot be well approximated by the neural network. The networks we consider have real-valued inputs and outputs, an unlimited number of threshold hidden units with bounded fan-in, and a bound on the sum of the absolute values of the output weights. The number of computation steps of the learning algorithm is bounded by a polynomial in 1//spl epsiv/, 1//spl delta/, n and B where /spl epsiv/ is the desired accuracy, /spl delta/ is the probability that the algorithm fails, n is the input dimension, and B is the bound on both the absolute value of the target (which may be a random variable) and the sum of the absolute values of the output weights. In obtaining the result, we also extended some results on iterative approximation of functions in the closure of the convex hull of a function class and on the sample complexity of agnostic learning with the quadratic loss function."
            },
            "slug": "Efficient-agnostic-learning-of-neural-networks-with-Lee-Bartlett",
            "title": {
                "fragments": [],
                "text": "Efficient agnostic learning of neural networks with bounded fan-in"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is shown that the class of two-layer neural networks with bounded fan-in is efficiently learnable in a realistic extension to the probably approximately correct (PAC) learning model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Related techniques have recently been used [36] to explain the generalization performance of boosting algorithms [14], [15], which use composite hypotheses that are convex combinations of hypotheses produced by weak learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13124,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6459896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5c04f0b1962c3ab1075091c2a5798b6a48a3401",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feedforward-Nets-for-Interpolation-and-Sontag",
            "title": {
                "fragments": [],
                "text": "Feedforward Nets for Interpolation and Classification"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11902833,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a6415df16c642be588aa1ab051a0a108bab7a482",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new general-purpose algorithm for learning classes of 0, 1-valued functions in a generalization of the prediction model and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi, and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each?>0, we establish weaker sufficient and stronger necessary conditions for a class of 0, 1-valued functions to be agnostically learnable to within?and to be an?-uniform Glivenko?Cantelli class."
            },
            "slug": "Prediction,-Learning,-Uniform-Convergence,-and-Bartlett-Long",
            "title": {
                "fragments": [],
                "text": "Prediction, Learning, Uniform Convergence, and Scale-Sensitive Dimensions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi, and Haussler is proved."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491389"
                        ],
                        "name": "L. Gurvits",
                        "slug": "L.-Gurvits",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Gurvits",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gurvits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710207"
                        ],
                        "name": "P. Koiran",
                        "slug": "P.-Koiran",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Koiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koiran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "[17] L. Gurvits and P. Koiran."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Gurvits and Koiran [17] have obtained an upperbound on the fat-shattering dimension for two-layer networks with bounded output weightsand hidden units chosen from a class of binary-valued functions with nite VC-dimension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Proposition 16 [17]: Let be the class of functions defined on for ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": ") Gurvits and Koiran [17] have obtained an upper bound on the fat-shattering dimension for two-layer networks with bounded output weights and hidden units chosen from a class of binary-valued functions with finite VC dimension."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2100523,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "37bb1325c36258486fad8d1c380de83a3323f00e",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fairly general method for constructing classes of functions of finite scale-sensitive dimension (the scale-sensitive dimension is a generalization of the Vapnik-Chervonenkis dimension to real-valued functions). The construction is as follows: start from a class F of functions of finite VC dimension, take the convex hull coF of F, and then take the closure \\({\\cal L}\\) of coF in an appropriate sense. As an example, we study in more detail the case where F is the class of threshold functions. It is shown that \\({\\cal L}\\) includes two important classes of functions: \n \n \nneural networks with one hidden layer and bounded output weights; \n \n \nthe so-called \u0393 class of Barron, which was shown to satisfy a number of interesting approximation and closure properties."
            },
            "slug": "Approximation-and-Learning-of-Convex-Superpositions-Gurvits-Koiran",
            "title": {
                "fragments": [],
                "text": "Approximation and Learning of Convex Superpositions"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A fairly general method for constructing classes of functions of finite scale-sensitive dimension, which includes the so-called \u0393 class of Barron, which was shown to satisfy a number of interesting approximation and closure properties."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "We will make use of the following result on approximation in Hilbert spaces, which has been attributed to Maurey (see [4] and [24])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122240265,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e7f56734291de81e99976d092b58e4e4a2b6f60",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A general convergence criterion for certain iterative sequences in Hilbert space is presented. For an important subclass of these sequences, estimates of the rate of convergence are given. Under very mild assumptions these results establish an 0(1/ F4n) nonsampling convergence rate for projection pursuit regression and neural network training; where n represents the number of ridge functions, neurons or coefficients in a greedy basis expansion."
            },
            "slug": "A-Simple-Lemma-on-Greedy-Approximation-in-Hilbert-Jones",
            "title": {
                "fragments": [],
                "text": "A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729473"
                        ],
                        "name": "M. Karpinski",
                        "slug": "M.-Karpinski",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Karpinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karpinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144293634"
                        ],
                        "name": "A. Macintyre",
                        "slug": "A.-Macintyre",
                        "structuredName": {
                            "firstName": "Angus",
                            "lastName": "Macintyre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Macintyre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1012445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39ebcd1e604ae888f8d1b1fb2f0955fbe5f13df4",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method for proving explicit upper bounds on the VC Dimension of general functional basis networks, and prove as an application, for the first time, the VC Dimension of analog neural networks with the sigmoid activation function o(y) = 1/1 + e-y to be bounded by a quadratic polynomial in the number of programmable parameters."
            },
            "slug": "Polynomial-bounds-for-VC-dimension-of-sigmoidal-Karpinski-Macintyre",
            "title": {
                "fragments": [],
                "text": "Polynomial bounds for VC dimension of sigmoidal neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new method is introduced for proving explicit upper bounds on the VC Dimension of general functional basis networks, and theVC Dimension of analog neural networks with the sigmoid activation function o(y) = 1/1 + e-y to be bounded by a quadratic polynomial in the number of programmable parameters."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107580601"
                        ],
                        "name": "D. H. Lee",
                        "slug": "D.-H.-Lee",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Lee",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. H. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716704"
                        ],
                        "name": "D. Neuhoff",
                        "slug": "D.-Neuhoff",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Neuhoff",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Neuhoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9040780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad779f1ed86dc540fa795a18ad84c5a32214315f",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-rate (or asymptotic) quantization theory has found formulas for the average squared length (more generally, the qth moment of the length) of the error produced by various scalar and vector quantizers with many quantization points. In contrast, this paper finds an asymptotic formula for the probability density of the length of the error and, in certain special cases, for the probability density of the multidimensional error vector, itself. The latter can be used to analyze the distortion of two-stage vector quantization. The former permits one to learn about the point density and cell shapes of a quantizer from a histogram of quantization error lengths. Histograms of the error lengths in simulations agree well with the derived formulas. Also presented are a number of properties of the error density, including the relationship between the error density, the point density, and the cell shapes, the fact that its qth moment equals Bennett's integral (a formula for the average distortion of a scalar or vector quantizer), and the fact that for stationary sources, the marginals of the multidimensional error density of an optimal vector quantizer with large dimension are approximately i.i.d. Gaussian."
            },
            "slug": "Asymptotic-distribution-of-the-errors-in-scalar-and-Lee-Neuhoff",
            "title": {
                "fragments": [],
                "text": "Asymptotic distribution of the errors in scalar and vector quantizers"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "High-rate (or asymptotic) quantization theory has found formulas for the average squared length and the probability density of the length of the error and, in certain special cases, for the probabilitydensity of the multidimensional error vector, itself, which can be used to analyze the distortion of two-stage vector quantization."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Theorem 5 [1]: Consider a class of functions that map from to with ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] is useful to get bounds on these covering numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8347198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e07c3df9d8d53c3be8cb9e982da98a4471322d90",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>"
            },
            "slug": "Scale-sensitive-dimensions,-uniform-convergence,-Alon-Ben-David",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive dimensions, uniform convergence, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire, and shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068856605"
                        ],
                        "name": "M. Horv\u00e1th",
                        "slug": "M.-Horv\u00e1th",
                        "structuredName": {
                            "firstName": "M\u00e1rta",
                            "lastName": "Horv\u00e1th",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horv\u00e1th"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117974551,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2f97b007902ee9c67ca7ddd7c1d76444a9cc44b3",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical binary classification problem is investigated when it is known in advance that the posterior probability function (or regression function) belongs to some class of functions. We introduce and analyze a method which effectively exploits this knowledge. The method is based on minimizing the empirical risk over a carefully selected ``skeleton'' of the class of regression functions. The skeleton is a covering of the class based on a data--dependent metric, especially fitted for classification. A new scale--sensitive dimension is introduced which is more useful for the studied classification problem than other, previously defined, dimension measures. This fact is demonstrated by performance bounds for the skeleton estimate in terms of the new dimension."
            },
            "slug": "A-Data-Dependent-Skeleton-Estimate-and-a-Dimension-Horv\u00e1th-Lugosi",
            "title": {
                "fragments": [],
                "text": "A Data-Dependent Skeleton Estimate and a Scale-Sensitive Dimension for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new scale--sensitive dimension is introduced which is more useful for the studied classification problem than other, previously defined, dimension measures and is demonstrated by performance bounds for the skeleton estimate in terms of the new dimension."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 43822355,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8e89889d355b03928a55b3550705e02129c54886",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the PAC learnability of classes of {0, ..., n}-valued functions (n 1 several generalizations of the VC-dimension, each yielding a distinct characterization of learnability, have been proposed by a number of researchers. In this paper we present a general scheme for extending the VC-dimension to the case n > 1. Our scheme defines a wide variety of notions of dimension in which all these variants of the VC-dimension, previously introduced in the context of learning, appear as special cases. Our main result is a simple condition characterizing the set of notions of dimension whose finiteness is necessary and sufficient for learning. This provides a variety of new tools for determining the learnability of a class of multi-valued functions. Our characterization is also shown to hold in the \"robust\" variant of PAC model and for any \"reasonable\" loss function."
            },
            "slug": "Characterizations-of-Learnability-for-Classes-of-Ben-David-Cesa-Bianchi",
            "title": {
                "fragments": [],
                "text": "Characterizations of Learnability for Classes of {0, ..., n}-Valued Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A general scheme for extending the VC-dimension to the case n > 1 is presented, which defines a wide variety of notions of dimension in which all these variants of theVC-dimension, previously introduced in the context of learning, appear as special cases."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1925579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b83396caf4762c906530c9219a9e4dd0658232b0",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-general-lower-bound-on-the-number-of-examples-for-Ehrenfeucht-Haussler",
            "title": {
                "fragments": [],
                "text": "A general lower bound on the number of examples needed for learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "This is qualitatively similar to popular heuristics (such as \\weight decay\" and \\early stopping\" | see, for example, [21]) that attempt to nd neural network functions that have small error and small weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The result gives theoretical support for the use of \\weight decay\" and \\early stopping\" (see, for example, [21]), two heuristic techniques that encourage gradient descent algorithms to produce networks with small weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": false,
            "numCitedBy": 6517,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "The proof uses techniques that go back to Pollard [35] and Vapnik and Chervonenkis [41], but using an cover as in [2], rather than the covers used by Pollard."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The proof of this theorem is very similar to the proof in [2], which closely followed the proofs of Vapnik"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "a similar but slightly weaker result follows trivially from the main result in [2]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 10454061,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07e5be1e9339af65685b08a84a9d0fc9153507af",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study a statistical property of classes of real-valued functions that we call approximation from interpolated examples. We derive a characterization of function classes that have this property, in terms of their \u2018fat-shattering function\u2019, a notion that has proved useful in computational learning theory. The property is central to a problem of learning real-valued functions from random examples in which we require satisfactory performance from every algorithm that returns a function which approximately interpolates the training examples."
            },
            "slug": "Function-Learning-from-Interpolation-Anthony-Bartlett",
            "title": {
                "fragments": [],
                "text": "Function Learning From Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper requires that, for most training samples, with high probability the absolute difference between the values of the learner's hypothesis and the target function on a random point is small."
            },
            "venue": {
                "fragments": [],
                "text": "Comb. Probab. Comput."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065748442"
                        ],
                        "name": "P. Goldberg",
                        "slug": "P.-Goldberg",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875953"
                        ],
                        "name": "M. Jerrum",
                        "slug": "M.-Jerrum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Jerrum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jerrum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Bounds for are known in these cases (see [8], [16], and [25], respectively)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11293090,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "94d43b9ba85a57523c9b553dd5ca645aa5716d03",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The Vapnik-Chervonenkis (V-C) dimension is an important combinatorial tool in the analysis of learning problems in the PAC framework. For polynomial learnability, we seek upper bounds on the V-C dimension that are polynomial in the syntactic complexity of concepts. Such upper bounds are automatic for discrete concept classes, but hitherto little has been known about what general conditions guarantee polynomial bounds on V-C dimension for classes in which concepts and examples are represented by tuples of real numbers. In this paper, we show that for two general kinds of concept class the V-C dimension is polynomially bounded in the number of real numbers used to define a problem instance. One is classes where the criterion for membership of an instance in a concept can be expressed as a formula (in the first-order theory of the reals) with fixed quantification depth and exponentially-bounded length, whose atomic predicates are polynomial inequalities of exponentially-bounded degree, The other is classes where containment of an instance in a concept is testable in polynomial time, assuming we may compute standard arithmetic operations on reals exactly in constant time.Our results show that in the continuous case, as in the discrete, the real barrier to efficient learning in the Occam sense is complexity-theoretic and not information-theoretic. We present examples to show how these results apply to concept classes defined by geometrical figures and neural nets, and derive polynomial bounds on the V-C dimension for these classes."
            },
            "slug": "Bounding-the-Vapnik-Chervonenkis-Dimension-of-by-Goldberg-Jerrum",
            "title": {
                "fragments": [],
                "text": "Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show that for two general kinds of concept class the V-C dimension is polynomially bounded in the number of real numbers used to define a problem instance, and that in the continuous case, as in the discrete, the real barrier to efficient learning in the Occam sense is complexity- theoretic and not information-theoretic."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Results from statistical learning theory (for example, [8], [10], [19], and [40]) give sample size bounds"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The standard PAC learning results (see [10] and [40]) show that, if the learning algorithm and error estimates are constrained to make use of the sample only through the function that maps from hypotheses to the proportion of training examples that they misclassify, there is no distribution-independent error bound any better than"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100859010"
                        ],
                        "name": "V. Tikhomirov",
                        "slug": "V.-Tikhomirov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Tikhomirov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tikhomirov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "It is well known (see, for example, [27]) that every -separated set in a totally bounded metric space has cardinality no more than ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121117171,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b21b75140076a263f25d017fe24dd13f5a6a131e",
            "isKey": false,
            "numCitedBy": 756,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The article is mainly devoted to the systematic exposition of results that were published in the years 1954\u20131958 by K. I. Babenko [1], A. G. Vitushkin [2,3], V. D. Yerokhin [4], A. N. Kolmogorov [5,6] and V. M. Tikhomirov [7]. It is natural that when these materials were systematically rewritten, several new theorems were proved and certain examples were computed in more detail. This review also incorporates results not published previously which go beyond the framework of such a systematization, and belong to V. I. Arnold (\u00a76) and V. M. Tikhomirov (\u00a7\u00a74,7 and \u00a78)."
            },
            "slug": "Entropy-and-\"-capacity-of-sets-in-func-tional-Kolmogorov-Tikhomirov",
            "title": {
                "fragments": [],
                "text": "Entropy and \"-capacity of sets in func-tional spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70416525"
                        ],
                        "name": "W. Hoeffding",
                        "slug": "W.-Hoeffding",
                        "structuredName": {
                            "firstName": "Wassily",
                            "lastName": "Hoeffding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hoeffding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121341745,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c66db9b93d75c0ff52e7f84605b8389345307006",
            "isKey": false,
            "numCitedBy": 8037,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for Pr {S \u2013 ES \u2265 nt} depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population."
            },
            "slug": "Probability-inequalities-for-sum-of-bounded-random-Hoeffding",
            "title": {
                "fragments": [],
                "text": "Probability Inequalities for sums of Bounded Random Variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079246689"
                        ],
                        "name": "Axthonv G. Oettinger",
                        "slug": "Axthonv-G.-Oettinger",
                        "structuredName": {
                            "firstName": "Axthonv",
                            "lastName": "Oettinger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axthonv G. Oettinger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207762321,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04a5241e96d0e7ac63eb44ed8cfbcdcda9df1583",
            "isKey": false,
            "numCitedBy": 1598,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since at most T of these equations can come from (7), at least (m T) of them come from (8\u2019). Thus, for the distribution (PI\u2019, . . . , p,\u2018), at most T of the probabilities are nonzero. The proof of our contention now follows by letting plo, . . . , 10~0 in the above argument be a distribution for which channel capacity is attained. It should be remarked that this proof provides (via the Simplex Method of linear programming) a means for reducing any transmitter with more than r symbols, to another, equally good or better, with at most T symbols. For a discussion of the possibility of reducing the number of receiver symbols, see Feinstein.5"
            },
            "slug": "IEEE-TRANSACTIONS-ON-INFORMATION-THEORY-Oettinger",
            "title": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON INFORMATION THEORY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143689789"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "and Chervonenkis [41] and Pollard [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The proof uses techniques that go back to Pollard [35] and Vapnik and Chervonenkis [41], but using an cover as in [2], rather than the covers used by Pollard."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122104450,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01a1d065a5292be740e75029622a3ab5e71e3150",
            "isKey": false,
            "numCitedBy": 1852,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I Functional on Stochastic Processes.- 1. Stochastic Processes as Random Functions.- Notes.- Problems.- II Uniform Convergence of Empirical Measures.- 1. Uniformity and Consistency.- 2. Direct Approximation.- 3. The Combinatorial Method.- 4. Classes of Sets with Polynomial Discrimination.- 5. Classes of Functions.- 6. Rates of Convergence.- Notes.- Problems.- III Convergence in Distribution in Euclidean Spaces.- 1. The Definition.- 2. The Continuous Mapping Theorem.- 3. Expectations of Smooth Functions.- 4. The Central Limit Theorem.- 5. Characteristic Functions.- 6. Quantile Transformations and Almost Sure Representations.- Notes.- Problems.- IV Convergence in Distribution in Metric Spaces.- 1. Measurability.- 2. The Continuous Mapping Theorem.- 3. Representation by Almost Surely Convergent Sequences.- 4. Coupling.- 5. Weakly Convergent Subsequences.- Notes.- Problems.- V The Uniform Metric on Spaces of Cadlag Functions.- 1. Approximation of Stochastic Processes.- 2. Empirical Processes.- 3. Existence of Brownian Bridge and Brownian Motion.- 4. Processes with Independent Increments.- 5. Infinite Time Scales.- 6. Functional of Brownian Motion and Brownian Bridge.- Notes.- Problems.- VI The Skorohod Metric on D(0, ?).- 1. Properties of the Metric.- 2. Convergence in Distribution.- Notes.- Problems.- VII Central Limit Theorems.- 1. Stochastic Equicontinuity.- 2. Chaining.- 3. Gaussian Processes.- 4. Random Covering Numbers.- 5. Empirical Central Limit Theorems.- 6. Restricted Chaining.- Notes.- Problems.- VIII Martingales.- 1. A Central Limit Theorem for Martingale-Difference Arrays.- 2. Continuous Time Martingales.- 3. Estimation from Censored Data.- Notes.- Problems.- Appendix A Stochastic-Order Symbols.- Appendix B Exponential Inequalities.- Notes.- Problems.- Appendix C Measurability.- Notes.- Problems.- References.- Author Index."
            },
            "slug": "Convergence-of-stochastic-processes-Pollard",
            "title": {
                "fragments": [],
                "text": "Convergence of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4362653"
                        ],
                        "name": "N. Fisher",
                        "slug": "N.-Fisher",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Fisher",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731053"
                        ],
                        "name": "P. Sen",
                        "slug": "P.-Sen",
                        "structuredName": {
                            "firstName": "Pranab",
                            "lastName": "Sen",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Hoeffding\u2019s inequality [22] implies that this is no more than ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123205318,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "30ffd4a8e479d04b1dea5749eac4a466dccde64b",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "If S is a random variable with finite rnean and variance, the Bienayme-Chebyshev inequality states that for x > 0, \n \n$$\\Pr \\left[ {\\left| {S - ES} \\right| \\geqslant x{{{(\\operatorname{var} S)}}^{{1/2}}}} \\right] \\leqslant {{x}^{{ - 2}}}$$ \n \n(1) \n \nIf S is the surn of n independent, identically distributed random variables, then, by the central limit theorem*, as n \u2192 \u221e, the probability on the left approaehes 2\u0424( - x), where \u0424(x) is the standard normal distribution function. For x large, \u0424( - x) behaves as const. x -1 exp( - x2/2)."
            },
            "slug": "Probability-Inequalities-for-Sums-of-Bounded-Random-Fisher-Sen",
            "title": {
                "fragments": [],
                "text": "Probability Inequalities for Sums of Bounded Random Variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071092667"
                        ],
                        "name": "M\u00e1rta Pint\u00e9r",
                        "slug": "M\u00e1rta-Pint\u00e9r",
                        "structuredName": {
                            "firstName": "M\u00e1rta",
                            "lastName": "Pint\u00e9r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rta Pint\u00e9r"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Horv\u00e1th and Lugosi [23], [33] have also obtained bounds on misclassification probability in terms of properties of regression functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3027744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10e59b273d8fc2866df6d4655e4bf65d1d05666e",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and analyze a method for learning based on minimizing the empirical risk over a carefully selected \u201cskeleton\u201d of a class of regression functions (concepts). The skeleton is a covering of the class based on a datadependent metric, especially fitted for classification. The method is shown to outperform empirical risk minimization and other non-datadependent skeleton-based estimates."
            },
            "slug": "A-data-dependent-skeleton-estimate-for-learning-Lugosi-Pint\u00e9r",
            "title": {
                "fragments": [],
                "text": "A data-dependent skeleton estimate for learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A method for learning based on minimizing the empirical risk over a carefully selected \u201cskeleton\u201d of a class of regression functions (concepts) is introduced and shown to outperform empirical risk minimization and other non-datadependent skeleton-based estimates."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [37] and [38] we also gave a more abstract"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "The problem was further investigated in [37]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5280896,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "67f9e3de2fb39f051ef23b8fbed6d72de7b02900",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces a framework for studying structural risk minimisation. The model views structural risk minimisation in a PAC context. It then considers the more general case when the hierarchy of classes is chosen in response to the data. This theoretically explains the impressive performance of the maximal margin hyperplane algorithm of Vapnik. It may also provide a general technique for exploitingserendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "slug": "A-framework-for-structural-risk-minimisation-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "A framework for structural risk minimisation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper introduces a framework for studying structural risk minimisation in a PAC context and considers the more general case when the hierarchy of classes is chosen in response to the data."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34923849"
                        ],
                        "name": "G. Lorentz",
                        "slug": "G.-Lorentz",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lorentz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lorentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The dimension has been used in approximation theory [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121058713,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9744f03355bccb69a0563d1f47d59206e5f9e9ec",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Possibility of Approximation: 1. Basic notions 2. Linear operators 3. Approximation theorems 4. The theorem of Stone 5. Notes Polynomials of Best Approximation: 1. Existence of polynomials of best approximation 2. Characterization of polynomials of best approximation 3. Applications of convexity 4. Chebyshev systems 5. Uniqueness of polynomials of best approximation 6. Chebyshev's theorem 7. Chebyshev polynomials 8. Approximation of some complex functions 9. Notes Properties of Polynomials and Moduli of Continuity: 1. Interpolation 2. Inequalities of Bernstein 3. The inequality of Markov 4. Growth of polynomials in the complex plane 5. Moduli of continuity 6. Moduli of smoothness 7. Classes of functions 8. Notes The Degree of Approximation by Trigonometric Polynomials: 1. Generalities 2. The theorem of Jackson 3. The degree of approximation of differentiable functions 4. Inverse theorems 5. Differentiable functions 6. Notes The Degree of Approximation by Algebraic Polynomials: 1. Preliminaries 2. The approximation theorems 3. Inequalities for the derivatives of polynomials 4. Inverse theorems 5. Approximation of analytic functions 6. Notes Approximation by Rational Functions. Functions of Several Variables: 1. Degree of rational approximation 2. Inverse theorems 3. Periodic functions of several variables 4. Approximation by algebraic polynomials 5. Notes Approximation by Linear Polynomial Operators: 1. Sums of de la Vallee-Poussin. Positive operators 2. The principle of uniform boundedness 3. Operators that preserve trigonometric polynomials 4. Trigonometric saturation classes 5. The saturation class of the Bernstein polynomials 6. Notes Approximation of Classes of Functions: 1. Introduction 2. Approximation in the space 3. The degree of approximation of the classes 4. Distance matrices 5. Approximation of the classes 6. Arbitrary moduli of continuity Approximation by operators 7. Analytic functions 8. Notes Widths: 1. Definitions and basic properties 2. Sets of continuous and differentiable functions 3. Widths of balls 4. Applications of theorem 2 5. Differential operators 6. Widths of the sets 7. Notes Entropy: 1. Entropy and capacity 2. Sets of continuous and differentiable functions 3. Entropy of classes of analytic functions 4. More general sets of analytic functions 5. Relations between entropy and widths 6. Notes Representation of Functions of Several Variables by Functions of One Variable: 1. The Theorem of Kolmogorov 2. The fundamental lemma 3. The completion of the proof 4. Functions not representable by superpositions 5. Notes Bibliography Index."
            },
            "slug": "Approximation-of-Functions-Lorentz",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697413"
                        ],
                        "name": "S. Kulkarni",
                        "slug": "S.-Kulkarni",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145122944"
                        ],
                        "name": "S. Posner",
                        "slug": "S.-Posner",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Posner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Posner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 187
                            }
                        ],
                        "text": "The idea of the proof of Theorem 17 is to rst derive a general upper bound on an `1 covering number of the class H, and then apply the following result (which is implicit in the proof of [6], Theorem 2) to give a bound on the fat-shattering dimension."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Lemma 19 ([6]) If F is a class of [0; 1]-valued functions de ned on a set X with fatF (4 ) d, then log2N1(F; ; d) d=32."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12122693,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0e6ce43aceab1c52056f8c41d9d6b01634620c51",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We find tight upper and lower bounds on the growth rate for the covering numbers of functions of bounded variation in the /spl Lscr//sub 1/ metric in terms of all the relevant constants. We also find upper and lower bounds on covering numbers for general function classes over the family of /spl Lscr//sub 1/(dP) metrics in terms of a scale-sensitive combinatorial dimension of the function class."
            },
            "slug": "Covering-numbers-for-real-valued-function-classes-Bartlett-Kulkarni",
            "title": {
                "fragments": [],
                "text": "Covering numbers for real-valued function classes"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Tight upper and lower bounds on the growth rate for the covering numbers of functions of bounded variation in the /spl Lscr//sub 1/ metric in terms of all the relevant constants are found."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46442363"
                        ],
                        "name": "K. Lang",
                        "slug": "K.-Lang",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Lang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "For example, the \u201ctwo spirals\u201d problem was a popular test of the generalization performance of neural network learning algorithms [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59803371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de8f292d456f8541b53587f46f30ea22a9b8ad52",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Alexis P. Wieland recently proposed a useful benchmark task for neural networks: distinguishing between two intertwined spirals. Although this task is easy to visualize, it is hard for a network to learn due to its extreme nonlinearity. In this report we exhibit a networkarchitecture that facilitates the learning of the spiral task, and then compare the leaming speed of several variants of the back-propagation algorithm."
            },
            "slug": "Learning-to-tell-two-spirals-apart-Lang",
            "title": {
                "fragments": [],
                "text": "Learning to tell two spirals apart"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A networkarchitecture is exhibited that facilitates the learning of the spiral task, and the leaming speed of several variants of the back-propagation algorithm is compared."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "The following result can be proved using the techniques from the proof of Lemma 4, together with the proof of the corresponding result in [40] (or the simpler proof in [3])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10570532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306c40863d0c9d57b238b980bb288b008425b475",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Result-of-Vapnik-with-Applications-Anthony-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "A Result of Vapnik with Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 27
                            }
                        ],
                        "text": "Learnability and theVapnik-Chervonenkis dimension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 20
                            }
                        ],
                        "text": "Bounding the Vapnik-Chervonenkis dimensionof concept classes parametrized by real numbers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "Results from statistical learning theory (for exam-ple, [40, 10, 8, 19]) give sample size bounds that are linear in the Vapnik-Chervonenkisdimension of the class of functions used by the learning system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 7
                            }
                        ],
                        "text": "Vapnik-Chervonenkis dimension of neural nets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "The proof uses techniques that go back to Pollard [35] and Vapnik and Chervonenkis [41], but using an cover as in [2], rather than the covers used by Pollard."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "and Chervonenkis [41] and Pollard [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 43
                            }
                        ],
                        "text": ":The proof uses techniques that go back to Vapnik and Chervonenkis [41] and Pollard [35],but using an `1 cover as in [2], rather than the `1 covers used by Pollard."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": "[41] V. N. Vapnik and A. Y. Chervonenkis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 101
                            }
                        ],
                        "text": "The proof of this theorem is very similar4\nto the proof in [2], which closely followed the proofs of Vapnik and Chervonenkis [41] andPollard [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Standard techniques (see, for example, [41]) show that the probability above is no more than"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": true,
            "numCitedBy": 3709,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Lemma 23 [20]: Let be a class of functions that take values in with finite."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41342869,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "415e448f1b5fdb95b496772fa02d78190d3660e9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Generalization-of-Sauer's-Lemma-Haussler-Long",
            "title": {
                "fragments": [],
                "text": "A Generalization of Sauer's Lemma"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Theory, Ser. A"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 25
                            }
                        ],
                        "text": "[14] Y. Freund and R. E. Schapire."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 25
                            }
                        ],
                        "text": "[15] Y. Freund and R. E. Schapire."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "[36] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 176
                            }
                        ],
                        "text": "AcknowledgementsThanks to Andrew Barron, Jonathan Baxter, Mostefa Golea, Mike Jordan, AdamKowalczyk,Wee Sun Lee, Phil Long, Gabor Lugosi, Llew Mason, John Shawe-Taylor, Robert Schapire,and Robert Slaviero for helpful discussions and comments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "De ne the fat-shattering dimension of H as the function fatH( ) = maxfm : H -shatters some x 2 Xmg : The fat-shattering dimension was introduced by Kearns and Schapire [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "De nethe fat-shattering dimension of H as the functionfatH( ) = maxfm : H -shatters some x 2 Xmg :The fat-shattering dimension was introduced by Kearns and Schapire [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 28
                            }
                        ],
                        "text": "[26] M. J. Kearns and R. E. Schapire."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient distribution-free learning of probabilistic con-  cepts"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 31st Symposium on the Foundations of Computer"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "to the proof in [2], which closely followed the proofs of Vapnik and Chervonenkis [41] and Pollard [35]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 27
                            }
                        ],
                        "text": "Learnability and theVapnik-Chervonenkis dimension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 20
                            }
                        ],
                        "text": "Bounding the Vapnik-Chervonenkis dimensionof concept classes parametrized by real numbers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "Results from statistical learning theory (for exam-ple, [40, 10, 8, 19]) give sample size bounds that are linear in the Vapnik-Chervonenkisdimension of the class of functions used by the learning system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 7
                            }
                        ],
                        "text": "Vapnik-Chervonenkis dimension of neural nets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "Then with probability at least 1 , every h in H has erP (h) < \u00ear z (h) +vuut 2 m ln 2N1( (H); =2; 2m) !: The proof uses techniques that go back to Vapnik and Chervonenkis [41] and Pollard [35], but using an `1 cover as in [2], rather than the `1 covers used by Pollard."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 43
                            }
                        ],
                        "text": ":The proof uses techniques that go back to Vapnik and Chervonenkis [41] and Pollard [35],but using an `1 cover as in [2], rather than the `1 covers used by Pollard."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": "[41] V. N. Vapnik and A. Y. Chervonenkis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 101
                            }
                        ],
                        "text": "The proof of this theorem is very similar4\nto the proof in [2], which closely followed the proofs of Vapnik and Chervonenkis [41] andPollard [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Standard techniques (see, for example, [41]) show that the probability above is no more than 2Pr 9h 2 H; 1 m jfi : j (h(~ xi)) ~ yij gj 1 m jfi : (h(xi)) 6= yigj+ =2 (1) (where the probability is over the double sample (z; ~ z)), provided 2m 2 ln 4, and we shall see later that our choice of always satis es this inequality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative fre-  quencies of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Theory of Probability and its Applications,"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 39
                            }
                        ],
                        "text": "The standard PAC learning results (see [40, 10]) show that, if the learning algorithm and error estimates are constrained to make use of the sample only through the function erz : H ! [0; 1] that maps from hypotheses to the proportion of training examples that they misclassify, there is no distribution independent error bound any better than O(VCdim(H)=m)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "The following result can be proved using the techniques from the proof of Lemma 4, together with the proof of the corresponding result in [40] (or the simpler proof in [3])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 55
                            }
                        ],
                        "text": "Results from statistical learning theory (for example, [40, 10, 8, 19]) give sample size bounds that are linear in the Vapnik-Chervonenkis dimension of the class of functions used by the learning system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "We use the notation fatV, as in [7], since this is a scale-sensitive version of a dimension introduced by Vapnik in [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "The idea of using the magnitudes of the values of h(xi) to give a more precise estimate of the generalization performance was rst proposed in [40], and was further developed in [18] and [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Rather than giving bounds on the generalization error, the results in [40] were restricted to bounds on the misclassi cation probability for a xed test sample, presented in advance."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data. Springer-Verlag,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and the size of the weights: an experimen-  tal study"
            },
            "venue": {
                "fragments": [],
                "text": "To appear in Proceedings of the Eighth Australian Conference on Neural  Networks,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Bounds for dimP (F ) are known in these cases (see [8], [16], and [25] respectively)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 105
                            }
                        ],
                        "text": "Bounds on the pseudodimension of various neural network classes have been established (see, for example, [8, 19, 34, 25]), but these are all at least linear in the number of parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quadratic VC-dimension bounds for sigmoidal net-  works"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 27th Annual Symposium on the Theory of Computing,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Characterizations of learnability for classes of f0; 1 1 1 ; ng-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kolmogorov and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "It is easy to see thatM1( (H); =2; 2m) M1(F; =2; 2m), and it is well known thatN1( (H); =2; 2m) M1( (H); =2; 2m) and M1(F; =2; 2m) N1(F; =4; 2m) (see [27]), hence N1( (H); =2; 2m) N1(F; =4; 2m): Applying Theorem 5 with n = 2m and b = 17 gives log2N1( (H); =2; 2m) < 1 + d log2(34em=d) log2(578m); provided that m d log2(34em=d) + 1, which can be assumed since the result is trivial otherwise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "It is well known (see, for example, [27]) that every =L-separated set in a totally bounded metric space (X; ) has cardinality no more than N (X; =(2L); )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tihomirov. -entropy and -capacity of sets in function  spaces"
            },
            "venue": {
                "fragments": [],
                "text": "American Mathematics Society Translations"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation of functions. Holt, Rinehart and Winston"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "It is also possible to show (by quantizing and then applying the pigeonhole principle|the proof is identical to that of Theorem 5 in [9]) that fat (H)( =16) c1fatV (H)(c2 ) 12"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Characterizations of  learnability for classes"
            },
            "venue": {
                "fragments": [],
                "text": "ng-valued functions. Journal of Computer and System  Sciences,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Bounds for are known in these cases (see [8], [16], and [25], respectively)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Bounds on the pseudodimension of various neural network classes have been established (see, for example, [8], [19], [25], and [34]), but these are all at least linear in the number of parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quadratic VC-dimension bounds for sigmoidal networks"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 27th Annu. Symp. on the Theory of Computing, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "A preliminary investigation in [32] for an artificial pattern classification problem reveals that the relationship between misclassification probability and the parameter magnitudes is qualitatively very similar to the estimates given here."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and the size of the weights: An experimental study"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 8th Australian Conf. on Neural Networks  , M. Dale, A. Kowalczyk, R. Slaviero, and J Szymanski, Eds., Telstra Res. Labs., 1997, pp. 60\u201364."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The dimension has been used in approximation theory [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lorentz,Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Holt, Rinehart and Winston,"
            },
            "year": 1966
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 13,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 55,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Sample-Complexity-of-Pattern-Classification-The-Bartlett/015999a72c70a960e59c51078b09c8f672af0d2c?sort=total-citations"
}