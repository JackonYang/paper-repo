{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7803,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6213072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f29d9348ce38198ad270d440ba6a5e9f3fc4afb6",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a new method for building a natural language understanding (NLU) system, in which the system's rules are learnt automatically from training data. The method has been applied to design of a speech understanding (SU) system. Designers of such systems rely increasingly on robust matchers to perform the task of extracting meaning from one or several word sequence hypotheses generated by a speech recognizer. We describe a new data structure, the semantic classification tree (SCT), that learns semantic rules from training data and can be a building block for robust matchers for NLU tasks. By reducing the need for handcoding and debugging a large number of rules, this approach facilitates rapid construction of an NLU system. In the case of an SU system, the rules learned by an SCT are highly resistant to errors by the speaker or by the speech recognizer because they depend on a small number of words in each utterance. Our work shows that semantic rules can be learned automatically from training data, yielding successful NLU for a realistic application. >"
            },
            "slug": "The-Application-of-Semantic-Classification-Trees-to-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "The Application of Semantic Classification Trees to Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new data structure is described, the semantic classification tree (SCT), that learns semantic rules from training data and can be a building block for robust matchers for NLU tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804633"
                        ],
                        "name": "G. Hendrix",
                        "slug": "G.-Hendrix",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Hendrix",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hendrix"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767123"
                        ],
                        "name": "E. Sacerdoti",
                        "slug": "E.-Sacerdoti",
                        "structuredName": {
                            "firstName": "Earl",
                            "lastName": "Sacerdoti",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sacerdoti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795468"
                        ],
                        "name": "Daniel Sagalowicz",
                        "slug": "Daniel-Sagalowicz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sagalowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Sagalowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763912"
                        ],
                        "name": "J. Slocum",
                        "slug": "J.-Slocum",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Slocum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slocum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15391397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f0aa57820d5f1700461b317faabad9b98d0f70d",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed."
            },
            "slug": "Developing-a-natural-language-interface-to-complex-Hendrix-Sacerdoti",
            "title": {
                "fragments": [],
                "text": "Developing a natural language interface to complex data"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components."
            },
            "venue": {
                "fragments": [],
                "text": "TODS"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 263135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7c0e47f8b768258b7d536c21b218e6c46ab8791",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "slug": "Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Database Queries Using Inductive Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart, and provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255957"
                        ],
                        "name": "Cynthia A. Thompson",
                        "slug": "Cynthia-A.-Thompson",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Thompson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cynthia A. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8502079,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "744285041424022aaf3e38ba9bd662fb8134e64c",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of words paired with meaning representations. WOLFIE is part of an integrated system that learns to parse novel sentences into semantic representations, such as logical database queries. Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages. The lexicons learned by WOLFIE are compared to those acquired by a similar system developed by Siskind (1996)."
            },
            "slug": "Automatic-Construction-of-Semantic-Lexicons-for-Thompson-Mooney",
            "title": {
                "fragments": [],
                "text": "Automatic Construction of Semantic Lexicons for Learning Natural Language Interfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9079926"
                        ],
                        "name": "W. Woods",
                        "slug": "W.-Woods",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Woods",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Woods"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18366823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09550accec47459a61fe1710a0a32c2ec22449bd",
            "isKey": false,
            "numCitedBy": 1450,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described."
            },
            "slug": "Transition-network-grammars-for-natural-language-Woods",
            "title": {
                "fragments": [],
                "text": "Transition network grammars for natural language analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The use of augmented transition network grammars for the analysis of natural language sentences is described, and structure-building actions associated with the arcs of the grammar network allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110271578"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145482266"
                        ],
                        "name": "D. Stallard",
                        "slug": "D.-Stallard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stallard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stallard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189985"
                        ],
                        "name": "R. Bobrow",
                        "slug": "R.-Bobrow",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bobrow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bobrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10983275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "slug": "A-Fully-Statistical-Approach-to-Natural-Language-Miller-Stallard",
            "title": {
                "fragments": [],
                "text": "A Fully Statistical Approach to Natural Language Interfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a natural language interface system which is based entirely on trained statistical models, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6159430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e340a9b752f9b358cda4dc7a1c6e3c6280867158",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Invention-of-First-Order-Predicates-by-Muggleton-Buntine",
            "title": {
                "fragments": [],
                "text": "Machine Invention of First Order Predicates by Inverting Resolution"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 89
                            }
                        ],
                        "text": "A clause needs no further re nement when it meets the following criterion (as in Ripper (Cohen, 1995)):\np n p+ n > (6)\nwhere p is the number of positive examples covered by the clause, n is the number of negative examples covered and 1 1 is a parameter."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6492502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6665e03447f989c9bdb3432d93e89b516b9d18a7",
            "isKey": false,
            "numCitedBy": 4149,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-Effective-Rule-Induction-Cohen",
            "title": {
                "fragments": [],
                "text": "Fast Effective Rule Induction"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": "One could use a variety of linear combination methods to estimate the weights k (e.g. Bayesian combination (Buntine, 1990))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60943338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f44fbc2fb3df4425654ae429c6cd1e175c3a522d",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contributions of this thesis are a Bayesian theory of learning classi cation rules, the uni cation and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learning class probability trees and bounding error when learning logical rules. The thesis is motivated by considering some current research issues in machine learning such as bias, over tting and search, and considering the requirements placed on a learning system when it is used for knowledge acquisition. Basic Bayesian decision theory relevant to the problem of learning classi cation rules is reviewed, then a Bayesian framework for such learning is presented. The framework has three components: the hypothesis space, the learning protocol, and criteria for successful learning. Several learning protocols are analysed in detail: queries, logical, noisy, uncertain and positive-only examples. The analysis is done by interpreting a protocol as a likelihood function and by performing conditional likelihood analysis. The aim of this framework and theoretical treatment is to consider how three important questions should be addressed: what actions a learner should take, whether the learner's initial subjective knowledge is appropriate, and how con dent the learner can be in results obtained. The resultant learning framework is compared and uni ed with the corresponding learning frameworks of Gold, Valiant, minimum encoding approaches such as MDL and MML, and classical methods from statistics and pattern recognition. Finally, two extensive case studies are reported. The rst case study works through conjunctive and complete hypothesis spaces to illustrate problems such as the inherent worst-case and sampleindependent analysis of the Valiantmodel and how this can be overcome in the Bayesian framework. Experiments reported indicate considerable improvements can be achieved. The second case study presents a Bayesian approach to learning decision trees based on the theory presented for the uncertain examples protocol. Experiments reported indicate the approach compares favourably with existing decision tree methods. The Bayesian method presented should readily transfer to the learning of disjunctive rules or other more exible representations, and to more exible search strategies. The Bayesian method should also provide much stronger feedback for an interactive style of learning."
            },
            "slug": "A-theory-of-learning-classification-rules-Buntine",
            "title": {
                "fragments": [],
                "text": "A theory of learning classification rules"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian theory of learning classi cation rules, the comparison and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learningclass probability trees and bounding error when learning logical rules are reported."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834139"
                        ],
                        "name": "Joshua B. Konvisser",
                        "slug": "Joshua-B.-Konvisser",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Konvisser",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua B. Konvisser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5406859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5bd46825dc9dae3b4ecaa063ad62a772e99dc8f",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-Top-down-and-Bottom-up-Techniques-in-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Combining Top-down and Bottom-up Techniques in Inductive Logic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730104"
                        ],
                        "name": "N. Lavrac",
                        "slug": "N.-Lavrac",
                        "structuredName": {
                            "firstName": "Nada",
                            "lastName": "Lavrac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lavrac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693549"
                        ],
                        "name": "S. D\u017eeroski",
                        "slug": "S.-D\u017eeroski",
                        "structuredName": {
                            "firstName": "Sa\u0161o",
                            "lastName": "D\u017eeroski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D\u017eeroski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "We maintain this advantage by using ILP to learn a committee of hypotheses, and basing probability estimates on a weighted vote of them (Ali and Pazzani, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 267
                            }
                        ],
                        "text": "3.1 The Basic Tabulate Algorithm\nMost ILP methods use a set-covering method to learn one clause (rule) at a time and construct clauses using either a strictly top-down (general to speci c) or bottom-up (speci c to general) search through the space of possible rules (Lavrac and Dzeroski, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Advantages of combining both ILP approaches were explored in Chillin (Zelle and Mooney, 1994), an ILP method which motivated the design of Tabulate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "The advantage of ILP is that it can perform induction over the logical description of the complete parse state without the need to pre-engineer a xed set of features (which vary greatly from one domain to another) that are relevant to making decisions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "3 The Tabulate ILP Method\nThis section discusses the ILP method used to build a committee of logical control hypotheses for each action."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 326
                            }
                        ],
                        "text": "The only exceptions of which we are aware are a statistical approach to map-\nping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approach using relational learning (a.k.a. inductive logic programming, ILP) to learn a logic-based semantic parser described in (Zelle and Mooney, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 35
                            }
                        ],
                        "text": "The initial Chill system used ILP (Lavrac and Dzeroski, 1994) to learn Prolog control rules and employed deterministic parsing, using the learned rules to decide the appropriate parse action for each state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "A new ILP learning system was also introduced which learns multiple hypotheses."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36237350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58095bae1d836943bdaa52b76fa8d17cf77d06b3",
            "isKey": true,
            "numCitedBy": 931,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Empirical inductive logic programming: introduction empirical ILP systems - an overview LINUS - using attribute-value learners in an ILP framework experiments in learning relations with LINUS ILP as search for program clauses. Part 2 Learning relations from imperfect data: handling imperfect data in ILP using heuristics to handle noise in ILP mFOIL - extending noise-handling in FOIL experiments in learning relations from noisy examples. Part 3 Applications of inductive logic programming: learning rules for early diagnosis of rheumatic diseases finite element mesh design an overview of selected ILP applications."
            },
            "slug": "Inductive-logic-programming-techniques-and-Lavrac-D\u017eeroski",
            "title": {
                "fragments": [],
                "text": "Inductive logic programming - techniques and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Applications of inductive logic programming: learning rules for early diagnosis of rheumatic diseases finite element mesh design an overview of selected ILP applications."
            },
            "venue": {
                "fragments": [],
                "text": "Ellis Horwood series in artificial intelligence"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476128"
                        ],
                        "name": "B. Cestnik",
                        "slug": "B.-Cestnik",
                        "structuredName": {
                            "firstName": "Bojan",
                            "lastName": "Cestnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cestnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 42
                            }
                        ],
                        "text": "We measure accuracy using the m-estimate (Cestnik, 1990), a smoothed measure of accuracy on the training data which in the case of a two-class problem is de ned as:\naccuracy(H) = s+m p+\nn+m (1)\nwhere s is the number of positive examples covered by the hypothesis H, n is the total number of examples\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20779819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad58594194155af8b48eaf3ab9525a1fafd24e58",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-Probabilities:-A-Crucial-Task-in-Machine-Cestnik",
            "title": {
                "fragments": [],
                "text": "Estimating Probabilities: A Crucial Task in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The application of semantic classi cation trees to natural language understanding"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5):449{ 460."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Advantages of combining both ILP approaches were explored in Chillin (Zelle and Mooney, 1994), an ILP method which motivated the design of Tabulate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining top-down and bottom-up methods in induc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 86
                            }
                        ],
                        "text": "Clauses are learned using both top-down specialization using a method similar to Foil (Quinlan, 1990) and bottom-up generalization using Least General Generalizations (LGG's)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning logical de nitions from relations"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning, 5(3):239{ 266."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mauning and H. Sch/itze. 1999. Foundations of Statistical Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 88
                            }
                        ],
                        "text": "A clause needs no further re nement when it meets the following criterion (as in Ripper (Cohen, 1995)):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "A clause needs no further re nement when it meets the following criterion (as in Ripper (Cohen, 1995)):\np n p+ n > (6)\nwhere p is the number of positive examples covered by the clause, n is the number of negative examples covered and 1 1 is a parameter."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast e ective rule induction"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth International Conference on Machine Learning, pages 115{123."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining top-down and bottom-up methods in induetive logic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eleventh International Conference on Machine Learning"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of learning classi cation rules"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, University of Technology, Sydney, Australia."
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Automated-Construction-of-Database-Interfaces:-and-Tang-Mooney/151411a7b32e40dca8a51503135c6e9e3cdbc70c?sort=total-citations"
}