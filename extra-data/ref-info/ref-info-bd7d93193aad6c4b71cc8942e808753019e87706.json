{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116927"
                        ],
                        "name": "John Blitzer",
                        "slug": "John-Blitzer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Blitzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Blitzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "This compares favourably with complexity of ex\u00adact inference in the \nlatent variable model proposed in (Blitzer et al., 2005a), which is exponential in the num\u00adber of hidden \nvariables."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "Recently, a stochastic model with hidden variables has been proposed for language modelling (Blitzer \net al., 2005a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Blitzer et al. (2005a) have proposed a model with directed interac\u00adtions for this task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "Morin, F., &#38; Bengio, Y. (2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 191
                            }
                        ],
                        "text": "Feed-forward \nneural networks that operate on real-valued vector rep\u00adresentations of words have been both the most \npopu\u00adlar and most successful models of this type (Bengio et al., 2003; Morin &#38; Bengio, 2005; Emami \net al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 164
                            }
                        ],
                        "text": "Since all discrete values are equally similar \n(or dissimilar) as\u00adsigning similar probabilities to similar inputs, which is typically done for continuous \ninputs, does not work Appearing in Proceedings of the 24 th International Confer\u00adence on Machine Learning, \nCorvallis, OR, 2007."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 11
                            }
                        ],
                        "text": "References Bengio, Y., Ducharme, R., Vincent, P., &#38; Jauvin, C. (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 171
                            }
                        ],
                        "text": "A number of techniques have been proposed to address the main drawback of these models \n their long training times (Bengio &#38; Sen\u00b4ecal, 2003; Schwenk &#38; Gauvain, 2005; Morin &#38; Bengio, \n2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "Hierarchi\u00adcal alternatives to feed-forward networks, which are faster to train and use, have been \nconsidered (Morin &#38; Bengio, 2005; Blitzer et al., 2005b), but they do not perform quite as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Bengio, Y., &#38; \nSen\u00b4ecal, J.-S. (2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9260035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c27f79a8c5db9347a85b024e68bb21a26d7eefd",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models estimate the probability of a word occurring in a given context. The most common language models rely on a discrete enumeration of predictive contexts (e.g., n-grams) and consequently fail to capture and exploit statistical regularities across these contexts. In this paper, we show how to learn hierarchical, distributed representations of word contexts that maximize the predictive value of a statistical language model. The representations are initialized by unsupervised algorithms for linear and nonlinear dimensionality reduction [14], then fed as input into a hierarchical mixture of experts, where each expert is a multinomial distribution over predicted words [12]. While the distributed representations in our model are inspired by the neural probabilistic language model of Bengio et al. [2, 3], our particular architecture enables us to work with significantly larger vocabularies and training corpora. For example, on a large-scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentences, we demonstrate consistent improvement over class-based bigram models [10, 13]. We also discuss extensions of our approach to longer multiword contexts."
            },
            "slug": "Hierarchical-Distributed-Representations-for-Blitzer-Weinberger",
            "title": {
                "fragments": [],
                "text": "Hierarchical Distributed Representations for Statistical Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how to learn hierarchical, distributed representations of word contexts that maximize the predictive value of a statistical language model, and demonstrates consistent improvement over class-based bigram models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": "This model is similar to the energy-based model proposed in (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 161
                            }
                        ],
                        "text": "Feed-forward neural networks that operate on real-valued vector representations of words have been both the most popular and most successful models of this type (Bengio et al., 2003; Morin & Bengio, 2005; Emami et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "E(wn; w1:n-1)). wn This model is similar to the energy-based model pro\u00adposed \nin (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "In our second experiment, we used the same training, \nvalidation, and test sets as in (Bengio et al., 2003), which means that our results are directly compara\u00adble \nto theirs4 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "This \nmight explain why the n-gram scores we obtained are slightly di.erent from the scores reported in (Bengio \net al., 2003). model."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "A more detailed description of preprocessing can be found in (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 78
                            }
                        ],
                        "text": "Unlike other language models, with the exception of the energy based model in (Bengio et al., 2003), our models use distributed representations not only for context words, but for the word being predicted as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 85
                            }
                        ],
                        "text": "In our second experiment, we used the same training, validation, and test sets as in (Bengio et al., 2003), which means that our results are directly comparable to theirs(4)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "Feed-forward \nneural networks that operate on real-valued vector rep\u00adresentations of words have been both the most \npopu\u00adlar and most successful models of this type (Bengio et al., 2003; Morin &#38; Bengio, 2005; Emami \net al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 3
                            }
                        ],
                        "text": "In (Bengio et al., 2003), a neural language model with a context size of 5 is trained but its individual score on the test set is not reported."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "In (Bengio et al., 2003), a neural lan\u00adguage model with a context size of 5 is trained but \nits individual score on the test set is not reported."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 79
                            }
                        ],
                        "text": "Unlike other language models, with the exception \nof the energy based model in (Bengio et al., 2003), our models use distributed representations not only \nfor context words, but for the word being predicted as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "Moreover, in some applications, such as speech recognition, we are interested in ratios of probabilities of words from a short list and as a result we do not have to compute the normalizing constant at all (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 163
                            }
                        ],
                        "text": "This type of parameterization has been used in feed-forward neural networks for modelling symbolic relations (Hinton, 1986) and for statistical language modelling (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 164
                            }
                        ],
                        "text": "This type of parameterization has been used in feed-forward \nneural networks for mod\u00adelling symbolic relations (Hinton, 1986) and for statis\u00adtical language modelling \n(Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 104
                            }
                        ],
                        "text": "This might explain why the n-gram scores we obtained are slightly different from the scores reported in (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "A more detailed description of preprocessing can be found in \n(Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 207
                            }
                        ],
                        "text": "Moreover, in some applications, such as speech \nrecognition, we are inter\u00adested in ratios of probabilities of words from a short list and as a result \nwe do not have to compute the normalizing constant at all (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": true,
            "numCitedBy": 6011,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057272295"
                        ],
                        "name": "Frederic Morin",
                        "slug": "Frederic-Morin",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Morin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederic Morin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 138
                            }
                        ],
                        "text": "A number of techniques have been proposed to address the main drawback of these models \n their long training times (Bengio &#38; Sen\u00b4ecal, 2003; Schwenk &#38; Gauvain, 2005; Morin &#38; Bengio, \n2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 14
                            }
                        ],
                        "text": "Bengio, Y., &#38; \nSen\u00b4ecal, J.-S. (2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1326925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c19fbefdeead6a4154a22a9c8551a18b1530033a",
            "isKey": false,
            "numCitedBy": 942,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy."
            },
            "slug": "Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio",
            "title": {
                "fragments": [],
                "text": "Hierarchical Probabilistic Neural Network Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition, constrained by the prior knowledge extracted from the WordNet semantic hierarchy is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797272"
                        ],
                        "name": "Ahmad Emami",
                        "slug": "Ahmad-Emami",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Emami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Emami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 161
                            }
                        ],
                        "text": "Feed-forward neural networks that operate on real-valued vector representations of words have been both the most popular and most successful models of this type (Bengio et al., 2003; Morin & Bengio, 2005; Emami et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 80
                            }
                        ],
                        "text": "E(wn; w1:n-1)). wn This model is similar to the energy-based model pro\u00adposed \nin (Bengio et al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 86
                            }
                        ],
                        "text": "In our second experiment, we used the same training, \nvalidation, and test sets as in (Bengio et al., 2003), which means that our results are directly compara\u00adble \nto theirs4 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 104
                            }
                        ],
                        "text": "This \nmight explain why the n-gram scores we obtained are slightly di.erent from the scores reported in (Bengio \net al., 2003). model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 33
                            }
                        ],
                        "text": "Acknowledgements \nWe thank Yoshua Bengio for providing us with the APNews dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Morin, F., &#38; Bengio, Y. (2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 205
                            }
                        ],
                        "text": "Feed-forward \nneural networks that operate on real-valued vector rep\u00adresentations of words have been both the most \npopu\u00adlar and most successful models of this type (Bengio et al., 2003; Morin &#38; Bengio, 2005; Emami \net al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "In (Bengio et al., 2003), a neural lan\u00adguage model with a context size of 5 is trained but \nits individual score on the test set is not reported."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 79
                            }
                        ],
                        "text": "Unlike other language models, with the exception \nof the energy based model in (Bengio et al., 2003), our models use distributed representations not only \nfor context words, but for the word being predicted as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 11
                            }
                        ],
                        "text": "References Bengio, Y., Ducharme, R., Vincent, P., &#38; Jauvin, C. (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "This type of parameterization has been used in feed-forward \nneural networks for mod\u00adelling symbolic relations (Hinton, 1986) and for statis\u00adtical language modelling \n(Bengio et al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 114
                            }
                        ],
                        "text": "A number of techniques have been proposed to address the main drawback of these models \n their long training times (Bengio &#38; Sen\u00b4ecal, 2003; Schwenk &#38; Gauvain, 2005; Morin &#38; Bengio, \n2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 117
                            }
                        ],
                        "text": "Hierarchi\u00adcal alternatives to feed-forward networks, which are faster to train and use, have been \nconsidered (Morin &#38; Bengio, 2005; Blitzer et al., 2005b), but they do not perform quite as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 62
                            }
                        ],
                        "text": "A more detailed description of preprocessing can be found in \n(Bengio et al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Bengio, Y., &#38; \nSen\u00b4ecal, J.-S. (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 207
                            }
                        ],
                        "text": "Moreover, in some applications, such as speech \nrecognition, we are inter\u00adested in ratios of probabilities of words from a short list and as a result \nwe do not have to compute the normalizing constant at all (Bengio et al., 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6529491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d6036af971c1f11ab712cc41487376a94e63673",
            "isKey": true,
            "numCitedBy": 34,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the performance of the Structured Language Model when one of its components is modeled by a connectionist model. Using a connectionist model and a distributed representation of the items in the history makes the component able to use much longer contexts than possible with currently used interpolated or backoff models, both because of the inherent capability of the connectionist model to fight the data sparseness problem, and because of the only sub-linear growth in the model size when increasing the context length. Experiments show significant improvement in perplexity and moderate reduction in word error rate over the baseline SLM results on the UPENN treebank and Wall Street Journal (WSJ) corpora respectively. The results also show that the probability distribution obtained by our model is much less correlated to regular N-grams than the baseline SLM model."
            },
            "slug": "Using-a-connectionist-model-in-a-syntactical-based-Emami-Xu",
            "title": {
                "fragments": [],
                "text": "Using a connectionist model in a syntactical based language model"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work investigates the performance of the Structured Language Model when one of its components is modeled by a connectionist model, and shows that the probability distribution obtained by the model is much less correlated to regular N-grams than the baseline SLM model."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "Density estimation for discrete distributions is inherently di.cult because there \nis no simple way to do smoothing based on input similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12469208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b395470a57c48d174c4216ea21a7a58bc046917",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time."
            },
            "slug": "Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Training Neural Network Language Models on Very Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "New algorithms to train a neural network language model on very large text corpora are presented, making possible the use of the approach in domains where several hundreds of millions words of texts are available."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216805232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611e948f2cf0b6e519ac04ce689d48a43a78f9ce",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods."
            },
            "slug": "An-Empirical-Study-of-Smoothing-Techniques-for-Chen-Goodman",
            "title": {
                "fragments": [],
                "text": "An Empirical Study of Smoothing Techniques for Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An extensive empirical comparison of several smoothing techniques in the domain of language modeling is presented, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991)."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2837110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c74e230a5a6fd5e2db6ace765ce38afe65f96214",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box."
            },
            "slug": "Learning-Multilevel-Distributed-Representations-for-Sutskever-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Multilevel Distributed Representations for High-Dimensional Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems are described, and their performance is demonstrated using synthetic video sequences of two balls bouncing in a box."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116927"
                        ],
                        "name": "John Blitzer",
                        "slug": "John-Blitzer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Blitzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Blitzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "This compares favourably with complexity of ex\u00adact inference in the \nlatent variable model proposed in (Blitzer et al., 2005a), which is exponential in the num\u00adber of hidden \nvariables."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Blitzer et al. (2005a)  have proposed a model with directed interactions for this task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "Recently, a stochastic model with hidden variables has been proposed for language modelling (Blitzer \net al., 2005a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Blitzer et al. (2005a) have proposed a model with directed interac\u00adtions for this task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "Morin, F., &#38; Bengio, Y. (2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 191
                            }
                        ],
                        "text": "Feed-forward \nneural networks that operate on real-valued vector rep\u00adresentations of words have been both the most \npopu\u00adlar and most successful models of this type (Bengio et al., 2003; Morin &#38; Bengio, 2005; Emami \net al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This compares favourably with complexity of exact inference in the latent variable model proposed in ( Blitzer et al., 2005a ), which is exponential in the number of hidden variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, a stochastic model with hidden variables has been proposed for language modelling ( Blitzer et al., 2005a )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 11
                            }
                        ],
                        "text": "References Bengio, Y., Ducharme, R., Vincent, P., &#38; Jauvin, C. (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 171
                            }
                        ],
                        "text": "A number of techniques have been proposed to address the main drawback of these models \n their long training times (Bengio &#38; Sen\u00b4ecal, 2003; Schwenk &#38; Gauvain, 2005; Morin &#38; Bengio, \n2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "Hierarchi\u00adcal alternatives to feed-forward networks, which are faster to train and use, have been \nconsidered (Morin &#38; Bengio, 2005; Blitzer et al., 2005b), but they do not perform quite as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Bengio, Y., &#38; \nSen\u00b4ecal, J.-S. (2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11966979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4823ecc86603ae5b8238c3101c2089c91a5e1da1",
            "isKey": true,
            "numCitedBy": 24,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Low-dimensional representations for lexical co-occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks. This work presents a distributed latent variable model for inducing these low-dimensional representations. The model takes inspiration from both connectionist language models [1, 16] and latent variable models [13, 9]. We give results which show that the new model significantly improves both bigram and trigram models."
            },
            "slug": "Distributed-Latent-Variable-Models-of-Lexical-Blitzer-Globerson",
            "title": {
                "fragments": [],
                "text": "Distributed Latent Variable Models of Lexical Co-occurrences"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work presents a distributed latent variable model for inducing low-dimensional representations for lexical co-occurrence data which improves both bigram and trigram models."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 112
                            }
                        ],
                        "text": "Training and testing of the n-gram models was performed using programs from the SRI Language Modelling \ntoolkit (Stolcke, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 111
                            }
                        ],
                        "text": "Training and testing of the n-gram models was performed using programs from the SRI Language Modelling toolkit (Stolcke, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1988103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399da68d3b97218b6c80262df7963baa89dcc71b",
            "isKey": false,
            "numCitedBy": 4997,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "slug": "SRILM-an-extensible-language-modeling-toolkit-Stolcke",
            "title": {
                "fragments": [],
                "text": "SRILM - an extensible language modeling toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The functionality of the SRILM toolkit is summarized and its design and implementation is discussed, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 270
                            }
                        ],
                        "text": "\u2026we have used only a single layer of hid\u00adden units, but now that we have shown how to use factored \nmatrices to take care of the very large num\u00adber of free parameters, it would be easy to make use of the \ngreedy, multilayer learning algorithm for RBMs that was described in (Hinton et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13410,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "While maximum likelihood learning in RBMs is intractable, RBMs can be trained efficiently using contrastive divergence learning (Hinton, 2002) and the learning rule is unaffected when binary units are replaced by multinomial ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 185
                            }
                        ],
                        "text": "One attractive property of RBMs is that the probability of a configuration of visible units can be computed up to a multiplicative constant in time linear in the number of hidden units (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 293
                            }
                        ],
                        "text": "While the resulting configuration (vn, h) (called a \u201cconfabulation\u201d or \u201creconstruction\u201d) is not a sample from P (vn, h|w1:n\u22121), it has been shown empirically that learning still works well when confabulations are used instead of samples from P (vn, h|w1:n\u22121) in the learning rules given above (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 154
                            }
                        ],
                        "text": "This can \nbe viewed as us\u00ading mean-.eld updates for visible units and stochastic updates for the hidden units, \nwhich is common prac\u00adtice when training RBMs (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 256
                            }
                        ],
                        "text": "In \nthis paper we have used only a single layer of hid\u00adden units, but now that we have shown how to use factored \nmatrices to take care of the very large num\u00adber of free parameters, it would be easy to make use of the \ngreedy, multilayer learning algorithm for RBMs that was described in (Hinton et al., 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Since the log-bilinear models performed well and, com\u00adpared to the FRBMs, were fast to \ntrain, we used only log-bilinear models in the second experiment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 281
                            }
                        ],
                        "text": "\u2026resulting con.guration (vn, h) (called a confabulation or re\u00adconstruction ) is not a sample \nfrom P (vn, h|w1:n-1), it has been shown empirically that learning still works well when confabulations \nare used instead of samples from P (vn, h|w1:n-1) in the learning rules given above (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "See (Sutskever &#38; Hinton, 2007) \nfor a more detailed description of learning in temporal RBMs. 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 154
                            }
                        ],
                        "text": "This can be viewed as using mean-field updates for visible units and stochastic updates for the hidden units, which is common practice when training RBMs (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "While maximum likelihood learning in RBMs is in\u00adtractable, RBMs can be trained e.ciently \nusing con\u00adtrastive divergence learning (Hinton, 2002) and the learning rule is una.ected when binary \nunits are re\u00adplaced by multinomial ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 204
                            }
                        ],
                        "text": "Making Predictions One attractive property of RBMs is that \nthe probabil\u00adity of a con.guration of visible units can be computed up to a multiplicative constant in \ntime linear in the number of hidden units (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem is usually reduced to learning the conditional distribution of the next word given a xed number of preceding words, a task at which n-gram models have been very successful ( Chen & Goodman, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221583858,
            "fieldsOfStudy": [],
            "id": "769dbbe88801b57a9b44f89c5516264f16cbed60",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "This type of parameterization has been used in feed-forward neural networks for modelling symbolic relations (Hinton, 1986) and for statistical language modelling (Bengio et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "This type of parameterization has been used in feed-forward \nneural networks for mod\u00adelling symbolic relations (Hinton, 1986) and for statis\u00adtical language modelling \n(Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 902,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079911577"
                        ],
                        "name": "Blockin Blockin",
                        "slug": "Blockin-Blockin",
                        "structuredName": {
                            "firstName": "Blockin",
                            "lastName": "Blockin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Blockin Blockin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2606135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b388f0151ab37adb3d57738b8f52a3f943f86c8",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quick-Training-of-Probabilistic-Neural-Nets-by-Blockin",
            "title": {
                "fragments": [],
                "text": "Quick Training of Probabilistic Neural Nets by Importance Sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 117
                            }
                        ],
                        "text": "We propose a sim\u00adple extension to the \nfactored RBM language model to achieve that goal (at least in theory) following Sutskever and Hinton \n(2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 5
                            }
                        ],
                        "text": "See (Sutskever &#38; Hinton, 2007) \nfor a more detailed description of learning in temporal RBMs. 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning multilevel distributed representations for highdimensional sequences. AISTATS\u201907"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 7,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Three-new-graphical-models-for-statistical-language-Mnih-Hinton/bd7d93193aad6c4b71cc8942e808753019e87706?sort=total-citations"
}