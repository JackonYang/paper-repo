{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700187"
                        ],
                        "name": "Anders S\u00f8gaard",
                        "slug": "Anders-S\u00f8gaard",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "S\u00f8gaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders S\u00f8gaard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15690273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24aed4104a50769a41772afccaac715110323e53",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semi-supervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004)."
            },
            "slug": "Simple-Semi-Supervised-Training-of-Part-Of-Speech-S\u00f8gaard",
            "title": {
                "fragments": [],
                "text": "Simple Semi-Supervised Training of Part-Of-Speech Taggers"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177175"
                        ],
                        "name": "Drahom\u00edra johanka Spoustov\u00e1",
                        "slug": "Drahom\u00edra-johanka-Spoustov\u00e1",
                        "structuredName": {
                            "firstName": "Drahom\u00edra",
                            "lastName": "Spoustov\u00e1",
                            "middleNames": [
                                "johanka"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Drahom\u00edra johanka Spoustov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002335"
                        ],
                        "name": "Jan Hajic",
                        "slug": "Jan-Hajic",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Hajic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Hajic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152949642"
                        ],
                        "name": "J. Raab",
                        "slug": "J.-Raab",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Raab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Raab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376410"
                        ],
                        "name": "Miroslav Spousta",
                        "slug": "Miroslav-Spousta",
                        "structuredName": {
                            "firstName": "Miroslav",
                            "lastName": "Spousta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miroslav Spousta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 54
                            }
                        ],
                        "text": "22\u201324 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17293592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68e3a23cc4ca2f7fb97672d939935499df31fb53",
            "isKey": true,
            "numCitedBy": 99,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by (Collins, 2002). Experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with a relatively modest (in the order of 108 tokens) unsupervised (plain) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %)."
            },
            "slug": "Semi-Supervised-Training-for-the-Averaged-POS-Spoustov\u00e1-Hajic",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Training for the Averaged Perceptron POS Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by Collins, 2002, showing significant improvement of the POS classification task on typologically different languages."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31565315"
                        ],
                        "name": "Chris Biemann",
                        "slug": "Chris-Biemann",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Biemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Biemann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "0\u201318, andx2i is a prediction onwi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "Our approach is also significantly better than S\u00f8gaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos.\nacc ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "0\u201318, and x2i is a prediction onwi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8977153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb953c729dbb8dbcaa8c89b4c6428f15f336c540",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers."
            },
            "slug": "Unsupervised-Part-of-Speech-Tagging-Employing-Graph-Biemann",
            "title": {
                "fragments": [],
                "text": "Unsupervised Part-of-Speech Tagging Employing Efficient Graph Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods and a Viterbi POS tagger is trained, which is refined by a morphological component."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40655383"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "Our approach is also significantly better than S\u00f8gaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos.\nacc ("
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "Our approach is also significantly better than S\u00f8gaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 406684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d685ff8532ecb972c9382f86dea53ee7528264",
            "isKey": false,
            "numCitedBy": 908,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance."
            },
            "slug": "Tri-training:-exploiting-unlabeled-data-using-three-Zhou-Li",
            "title": {
                "fragments": [],
                "text": "Tri-training: exploiting unlabeled data using three classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561559"
                        ],
                        "name": "P. Berck",
                        "slug": "P.-Berck",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Berck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Berck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47567800"
                        ],
                        "name": "S. Gillis",
                        "slug": "S.-Gillis",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gillis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gillis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6082156a2270b6567ebdc85f6570ffacc3d903c2",
            "isKey": false,
            "numCitedBy": 424,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. 1 I n t r o d u c t i o n Part of Speech (POS) tagging is a process in which syntactic categories are assigned to words. It can be seen as a mapping from sentences to strings of tags. Automatic tagging is useful for a number of applications: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In"
            },
            "slug": "MBT:-A-Memory-Based-Part-of-Speech-Tagger-Generator-Daelemans-Zavrel",
            "title": {
                "fragments": [],
                "text": "MBT: A Memory-Based Part of Speech Tagger-Generator"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A large-scale application of the memory-based approach to part of speech tagging is shown to be feasible, obtaining a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763166"
                        ],
                        "name": "F. Angiulli",
                        "slug": "F.-Angiulli",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Angiulli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Angiulli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "Several strategies have been proposed to make nearest neighbor classification more efficient (Angiulli, 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16882811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ed20cf9b6895253d48501e7b1696efda177898b",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule. The algorithm, called FCNN rule, has some desirable properties. Indeed, it is order independent, and has subquadratic worst case time complexity, while it requires few iterations to converge, and it is likely to select points very close to the decision boundary. We compare the FCNN rule with state of the art competence preservation algorithms on large multidimensional training sets, showing that it outperforms existing methods in terms of learning speed and learning scaling behavior, and in terms of size of the model, while it guarantees a comparable prediction accuracy."
            },
            "slug": "Fast-condensed-nearest-neighbor-rule-Angiulli",
            "title": {
                "fragments": [],
                "text": "Fast condensed nearest neighbor rule"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work presents a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule, and compares it with state of the art competence preservation algorithms on large multidimensional training sets, showing that it outperforms existing methods in terms of learning speed and learning scaling behavior."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081481900"
                        ],
                        "name": "ci UniversityTR",
                        "slug": "ci-UniversityTR",
                        "structuredName": {
                            "firstName": "ci",
                            "lastName": "UniversityTR",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ci UniversityTR"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 267
                            }
                        ],
                        "text": "\u2026the NN rule is stable, and cannot be improved by\nT = {\u3008x1, y1\u3009, . . . , \u3008xn, yn\u3009}, C = \u2205 for \u3008xi, yi\u3009 \u2208 T do\nif C(xi) 6= yi then C = C \u222a {\u3008xi, yi\u3009}\nend if end for return C\nFigure 2: WEAKENED CONDENSED NEAREST NEIGHBOR.\ntechniques such as bagging (Breiman, 1996), CNN is unstable (Alpaydin, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "techniques such as bagging (Breiman, 1996), CNN is unstable (Alpaydin, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 61
                            }
                        ],
                        "text": "We have compared the algorithm to condensed nearest neighbor (Hart, 1968; Alpaydin, 1997) and showed"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 74
                            }
                        ],
                        "text": "We have compared the algorithm to condensed nearest neighbor (Hart, 1968; Alpaydin, 1997) and showed that the algorithm leads to more condensed models, and that it performs significantly better than condensed nearest neighbor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 108
                            }
                        ],
                        "text": "Note that we have simplified the CNN algorithm a bit compared to Hart (1968), as suggested, for example, in Alpaydin (1997), iterating only once over data rather than waiting for convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8021462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d1d9b25ee761a58fed29cbaccb439190261ae80",
            "isKey": true,
            "numCitedBy": 71,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Lazy learning methods like the k-nearest neighbor classiier require storing the whole training set and may be too costly when this set is large. The condensed nearest neighbor classiier incrementally stores a subset of the sample, thus decreasing storage and computation requirements. We propose to train multiple such subsets and take a vote over them, thus combining predictions from a set of concept descriptions. We investigate two voting schemes: simple voting where voters have equal weight and weighted voting where weights depend on classiiers' conndences in their predictions. We consider ways to form such subsets for improved performance: When the training set is small, voting improves performance considerably. If the training set is not small, then voters converge to similar solutions and we do not gain anything by voting. To alleviate this, when the training set is of intermediate size, we use bootstrapping to generate smaller training sets over which we train the voters. When the training set is large, we partition it into smaller, mutually exclusive subsets and then train the voters. Simulation results on six datasets are reported with good results. We give a review of methods for combining multiple learners. The idea of taking a vote over multiple learners can be applied with any type of learning scheme."
            },
            "slug": "Voting-over-Multiple-Condensed-Nearest-Neighbors-UniversityTR",
            "title": {
                "fragments": [],
                "text": "Voting over Multiple Condensed Nearest Neighbors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes to train multiple such subsets and take a vote over them, thus combining predictions from a set of concept descriptions, and investigates two voting schemes: simple voting where voters have equal weight and weighted voting where weights depend on classiiers' conndences in their predictions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804668"
                        ],
                        "name": "Sandra K\u00fcbler",
                        "slug": "Sandra-K\u00fcbler",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "K\u00fcbler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandra K\u00fcbler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691035"
                        ],
                        "name": "D. Zhekova",
                        "slug": "D.-Zhekova",
                        "structuredName": {
                            "firstName": "Desislava",
                            "lastName": "Zhekova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zhekova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16945062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0223555357b14762ec0fe12ecbabc38cbd713d9",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we discuss the importance of the quality against the quantity of automatically extracted examples for word sense disambiguation (WSD). We first show that we can build a competitive WSD system with a memory-based classifier and a feature set reduced to easily and eciently computable features. We then show that adding automatically annotated examples improves the performance of this system when the examples are carefully selected based on their quality."
            },
            "slug": "Semi-Supervised-Learning-for-Word-Sense-Quality-vs.-K\u00fcbler-Zhekova",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning for Word Sense Disambiguation: Quality vs. Quantity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows that a competitive WSD system with a memory-based classifier and a feature set reduced to easily and eciently computable features can be built, and shows that adding automatically annotated examples improves the performance of this system."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 234
                            }
                        ],
                        "text": "Many different semi-supervised algorithms have been applied to natural language processing tasks, but the simplest algorithm, namely self-training, is the one that has attracted most attention, together with expectation maximization (Abney, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17857830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d21c796c6af518da905752a64d54b541384eba12",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The rapid advancement in the theoretical understanding of statistical and machine learning methods for semisupervised learning has made it difficult for nonspecialists to keep up to date in the field. Providing a broad, accessible treatment of the theory as well as linguistic applications, Semisupervised Learning for Computational Linguistics offers self-contained coverage of semisupervised methods that includes background material on supervised and unsupervised learning. The book presents a brief history of semisupervised learning and its place in the spectrum of learning methods before moving on to discuss well-known natural language processing methods, such as self-training and co-training. It then centers on machine learning techniques, including the boundary-oriented methods of perceptrons, boosting, support vector machines (SVMs), and the null-category noise model. In addition, the book covers clustering, the expectation-maximization (EM) algorithm, related generative methods, and agreement methods. It concludes with the graph-based method of label propagation as well as a detailed discussion of spectral methods. Taking an intuitive approach to the material, this lucid book facilitates the application of semisupervised learning methods to natural language processing and provides the framework and motivation for a more systematic study of machine learning."
            },
            "slug": "Semisupervised-Learning-for-Computational-Abney",
            "title": {
                "fragments": [],
                "text": "Semisupervised Learning for Computational Linguistics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Taking an intuitive approach to the material, this lucid book facilitates the application of semisupervised learning methods to natural language processing and provides the framework and motivation for a more systematic study of machine learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16907511"
                        ],
                        "name": "C. Hilborn",
                        "slug": "C.-Hilborn",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Hilborn",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hilborn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645792"
                        ],
                        "name": "D. Lainiotis",
                        "slug": "D.-Lainiotis",
                        "structuredName": {
                            "firstName": "Demetrios",
                            "lastName": "Lainiotis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lainiotis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 61
                            }
                        ],
                        "text": "We have compared the algorithm to condensed nearest neighbor (Hart, 1968; Alpaydin, 1997) and showed"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9664198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c3771fd6829630cf450af853df728ecd8da4ab2",
            "isKey": false,
            "numCitedBy": 985,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Since, by (8) pertaining to the nearest neighbor decision rule (NN rule). We briefly review the NN rule and then describe the CNN rule. The NN rule['l-[ \" I assigns an unclassified sample to the same class as the nearest of n stored, correctly classified samples. In other words, given a collection of n reference points, each classified by some external source, a new point is assigned to the same class as its nearest neighbor. The most interesting t)heoretical property of the NN rule is that under very mild regularity assumptions on the underlying statistics, for any metric, and for a variety of loss functions , the large-sample risk incurred is less than twice the Bayes risk. (The Bayes decision rule achieves minimum risk but ,requires complete knowledge of the underlying statistics.) From a practical point of view, however, the NN rule is not a prime candidate for many applications because of the storage requirements it imposes. The CNN rule is suggested as a rule which retains the basic approach of the NN rule without imposing such stringent storage requirements. Before describing the CNN rule we first define the notion of a consistent subset of a sample set. This is a subset which, when used as a stored reference set for the NN rule, correctly classifies all of the remaining points in the sample set. A minimal consistent subset is a consistent subset with a minimum number of elements. Every set has a consistent subset, since every set is trivially a consistent subset of itself. Obviously, every finite set has a minimal consistent subset, although the minimum size is not, in general, achieved uniquely. The CNN rule uses the following algorithm to determine a consistent subset of the original sample set. In general, however, the algorithm will not find a minimal consistent subset. We assume that the original sample set is arranged in some order; then we set up bins called STORE and GRABHAG and proceed as follows. 1) The first sample is placed in STORE. 2) The second sample is classified by the NN rule, using as a reference set the current contents of STORE. (Since STORE has only one point, the classification is trivial at this stage.) If the second sample is classified correctly it is placed in GRABBAG; otherwise it is placed in STORE. 3) Proceeding inductively, the ith sample is classified by the current contents of \u2026"
            },
            "slug": "The-Condensed-Nearest-Neighbor-Rule-Hilborn-Lainiotis",
            "title": {
                "fragments": [],
                "text": "The Condensed Nearest Neighbor Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The CNN rule is suggested as a rule which retains the basic approach of the NN rule without imposing such stringent storage requirements, and the notion of a consistent subset of a sample set is defined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144042991"
                        ],
                        "name": "Jun Suzuki",
                        "slug": "Jun-Suzuki",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789234"
                        ],
                        "name": "Hideki Isozaki",
                        "slug": "Hideki-Isozaki",
                        "structuredName": {
                            "firstName": "Hideki",
                            "lastName": "Isozaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideki Isozaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701734"
                        ],
                        "name": "X. Carreras",
                        "slug": "X.-Carreras",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Carreras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Carreras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 19
                            }
                        ],
                        "text": "22\u201324 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 160543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b47f0211778a2b84202bd5e0ac6003dd1909274d",
            "isKey": true,
            "numCitedBy": 100,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Tree-bank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech."
            },
            "slug": "An-Empirical-Study-of-Semi-supervised-Structured-Suzuki-Isozaki",
            "title": {
                "fragments": [],
                "text": "An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The effectiveness of the proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Tree-bank for Czech are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 107
                            }
                        ],
                        "text": "Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36924412"
                        ],
                        "name": "J. Gim\u00e9nez",
                        "slug": "J.-Gim\u00e9nez",
                        "structuredName": {
                            "firstName": "Jes\u00fas",
                            "lastName": "Gim\u00e9nez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049328"
                        ],
                        "name": "Llu\u00eds M\u00e0rquez i Villodre",
                        "slug": "Llu\u00eds-M\u00e0rquez-i-Villodre",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Villodre",
                            "middleNames": [
                                "M\u00e0rquez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds M\u00e0rquez i Villodre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 196
                            }
                        ],
                        "text": "lowing S\u00f8gaard (2010): Each word in the data wi is associated with a feature vector xi = \u3008x1i , x 2 i \u3009 wherex1i is the prediction onwi of a supervised partof-speech tagger, in our case SVMTool 1 (Gimenez and Marquez, 2004) trained on Sect."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10242516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b56b3008918d07165d765285808a11edd971091",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the SVMTool, a simple, flexible, effective and efficient part\u2013of\u2013speech tagger based on Support Vector Machines. The SVMTool offers a fairly good balance among these properties which make it really practical for current NLP applications. It is very easy to use and easily configurable so as to perfectly fit the needs of a number of different applications. Results are also very competitive, achieving an accuracy of 97.16% for English on the Wall Street Journal corpus. It has been also successfully applied to Spanish exhibiting a similar performance. A first release of the SVMTool Perl prototype is now freely available for public use. A most efficient C++ version is coming very soon."
            },
            "slug": "SVMTool:-A-general-POS-Tagger-Generator-Based-on-Gim\u00e9nez-Villodre",
            "title": {
                "fragments": [],
                "text": "SVMTool: A general POS Tagger Generator Based on Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The SVMTool offers a fairly good balance among these properties which make it really practical for current NLP applications, and it is very easy to use and easily configurable so as to perfectly fit the needs of a number of different applications."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5246200,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0efb841403aa6252b39ae6975c1cc5410554ef7b",
            "isKey": false,
            "numCitedBy": 10769,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R of such a rule must be at least as great as the Bayes probability of error R^{\\ast} --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M -category case that R^{\\ast} \\leq R \\leq R^{\\ast}(2 --MR^{\\ast}/(M-1)) , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor."
            },
            "slug": "Nearest-neighbor-pattern-classification-Cover-Hart",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor pattern classification"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points, so it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 152
                            }
                        ],
                        "text": "Finding a subset of the labeled data points may lead to faster and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1458119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f906eb7b38a1ff22538eb32110313d7bca41868",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose E is a set of labeled points (examples) in some metric space. A subset C of E is said to be a consistent subset ofE if it has the property that for any example e\u2208E, the label of the closest..."
            },
            "slug": "Nearest-neighbor-problems-Wilfong",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor problems"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A subset C of E is said to be a consistent subset of E if it has the property that for any example e\u2208E, the label of the closest example, is e\u2206E."
            },
            "venue": {
                "fragments": [],
                "text": "SCG '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145977875"
                        ],
                        "name": "Antal van den Bosch",
                        "slug": "Antal-van-den-Bosch",
                        "structuredName": {
                            "firstName": "Antal",
                            "lastName": "van den Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antal van den Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026elements and removing elements by class prediction strength, were argued not to be useful for most problems in natural language processing in Daelemans et al. (1999), but our experiments showed that CNN often perform about as well as NN, and our semi-supervised CNN algorithm leads to\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11455311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "111b870e874ded07f4af222b4754e36202c70d8e",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms."
            },
            "slug": "Forgetting-Exceptions-is-Harmful-in-Language-Daelemans-Bosch",
            "title": {
                "fragments": [],
                "text": "Forgetting Exceptions is Harmful in Language Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "It is shown that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy, and that decision-tree learning often performs worse than memory-based learning."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068269859"
                        ],
                        "name": "G. Gates",
                        "slug": "G.-Gates",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gates",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gates"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "CNN was first generalized tok-NN in Gates (1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59807665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a64897e672e3f112a012e41ef40d905b599d1e1",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Fig. 3 shows PG,.,,,,, (m) for various numbers of observations N and various sizes of memory m. The Chernoff bound on P,*(a) was used for N 2 32. Quite naturally one does better with more memory. The P~,sym(m) curve for any given value of m follows the P,*(co) line for low values of N, diverges from it for larger values of N, and approaches a nonzero limit P,*(m) as N + co. This behavior is easily explained. Any given machine can \u201cremember\u201d all of the observations for low values of N. Here infinite memory offers no advantages. For larger values of N, a finite-state machine necessarily loses some information and thus does not do so well as one with infinite memory. As N -+ co, Pz sym(m) approaches Pm*(m), the infinite-time lower bound on the probability of error, since from [I] we know that for N = co the optimal machine is symmetric."
            },
            "slug": "The-Reduced-Nearest-Neighbor-Rule-Gates",
            "title": {
                "fragments": [],
                "text": "The Reduced Nearest Neighbor Rule"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 234
                            }
                        ],
                        "text": "\u2026the NN rule is stable, and cannot be improved by\nT = {\u3008x1, y1\u3009, . . . , \u3008xn, yn\u3009}, C = \u2205 for \u3008xi, yi\u3009 \u2208 T do\nif C(xi) 6= yi then C = C \u222a {\u3008xi, yi\u3009}\nend if end for return C\nFigure 2: WEAKENED CONDENSED NEAREST NEIGHBOR.\ntechniques such as bagging (Breiman, 1996), CNN is unstable (Alpaydin, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59829005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24439a2a35b834aa710373d62da4c6f86a180125",
            "isKey": false,
            "numCitedBy": 607,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar."
            },
            "slug": "An-Efficient-Algorithm-for-Projective-Dependency-Nivre",
            "title": {
                "fragments": [],
                "text": "An Efficient Algorithm for Projective Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper presents a deterministic parsing algorithm for projective dependency grammar that has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 88518535,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "66c0a10e25203eb6befaca947d8189e6646e98df",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistics is a uniquely difficult field to convey to the uninitiated. It sits astride the abstract and the concrete, the theoretical and the applied. It has a mathematical flavor and yet it is not simply a branch of mathematics. Its core problems blend into those of the disciplines that probe into the nature of intelligence and thought, in particular philosophy, psychology and artificial intelligence. Debates over foundational issues have waxed and waned, but the field has not yet arrived at a single foundational perspective. Given these complexities it might seem surprising that human beings could have definite opinions about core issues in statistics, and surprising that working in such a field could be pleasurable. And yet there was Leo Breiman, who had his definite opinions about statistics and who took great pleasure in waking up every morning to see what more he could do to bring the field along. To the extent that most statisticians have a vision about the final conclusive form the field might take, I suspect that this vision is a mathematical one\u2014a set of core definitions, axioms and theorems. Moreover, I think that many statisticians will expect for these mathematical ideas to involve a set of optimality principles, such that it will be possible for a user of statistics circa 2500 AD to dial in the description of a problem and out will pop the optimal procedure. I think that Leo had come to a different vision. In thinking about Leo I think about the box of tools in my basement. It contains hammers, screwdrivers, pliers, nails, screws and rivets. Of the infinite number of possible physical forms that objects for manipulating the physical world could have taken, these are the ones that have come to us from our ancestors in the applied field of \u201cmanagement of uncertainty in physical structures.\u201d They arose via little bits of human genius and they have stood the test of time. My vision of Leo\u2019s vision is, of course, an inference, and to support my inference I will exhibit some of the (anecdotal) data on which it is based."
            },
            "slug": "Leo-Breiman-Jordan",
            "title": {
                "fragments": [],
                "text": "Leo Breiman"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learn-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 37
                            }
                        ],
                        "text": "The nearest neighbor (NN) algorithm (Cover and Hart, 1967) is conceptually simple, yet very powerful."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 266
                            }
                        ],
                        "text": "It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distribu-\ntions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 265
                            }
                        ],
                        "text": "It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 32
                            }
                        ],
                        "text": "The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor pattern classification.IEEE Transactions on Information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 72
                            }
                        ],
                        "text": ", 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K\u00fcbler and Zhekova, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semisupervised learning for word-sense disambiguation: quality vs"
            },
            "venue": {
                "fragments": [],
                "text": "quantity. InRANLP."
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Semi-supervised-condensed-nearest-neighbor-for-S\u00f8gaard/2a6626dadb6b624011c39270cc7678ac98e162a3?sort=total-citations"
}