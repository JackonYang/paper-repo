{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39737792"
                        ],
                        "name": "Zili Yi",
                        "slug": "Zili-Yi",
                        "structuredName": {
                            "firstName": "Zili",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zili Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39497427"
                        ],
                        "name": "Hao Zhang",
                        "slug": "Hao-Zhang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145604260"
                        ],
                        "name": "P. Tan",
                        "slug": "P.-Tan",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34077629"
                        ],
                        "name": "Minglun Gong",
                        "slug": "Minglun-Gong",
                        "structuredName": {
                            "firstName": "Minglun",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minglun Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "DualGAN and CycleGAN perform much better, especially in small and middle scale objects, such as sidewalk, building, car and bicycle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 80
                            }
                        ],
                        "text": "However, CycleGAN always mistakenly generates the tree areas as the wall, while DualGAN is still not clear enough."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Without the need of labeled or paired images as supervision, DualGAN [31] proposes a closed cyclic structure, allowing transferring images between two domains mutually."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "DualGAN and CycleGAN have similar performance since they follow the similar idea in essence, which achieve better performance than DistanceGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 131
                            }
                        ],
                        "text": "StarGAN and ComboGAN are extensions of CycleGAN, which\n1https://github.com/sagiebenaim/DistanceGAN 2https://github.com/duxingren14/DualGAN 3https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix 4https://github.com/yunjey/StarGAN 5https://github.com/AAnoosheh/ComboGAN\nhave slightly better performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "The input of DualGAN is composed of an image and some artificial noises, which enables it to generate different images from one input."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 84
                            }
                        ],
                        "text": "StarGAN outperforms other baselines, which has around 10% improvement compared with DualGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In comparison, DualGAN and CycleGAN have clearer outlines and richer textures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "DistanceGAN and DualGAN have similar performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "[31] Z. Yi, H. Zhang, P. Tan, and M. Gong, \u201cDualGAN: Unsupervised dual learning for image-to-image translation,\u201d in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "DualGAN [31] shares the similar idea of dual learning with CycleGAN [12] to constrain the unsupervised crossdomain translation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1082740,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "ef3c1f6c177e37f1d0d2a61702b60c766971700b",
            "isKey": true,
            "numCitedBy": 1366,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently [7, 8, 21, 12, 4, 18]. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation [23], we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data."
            },
            "slug": "DualGAN:-Unsupervised-Dual-Learning-for-Translation-Yi-Zhang",
            "title": {
                "fragments": [],
                "text": "DualGAN: Unsupervised Dual Learning for Image-to-Image Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel dual-GAN mechanism is developed, which enables image translators to be trained from two sets of unlabeled images from two domains, and can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793900"
                        ],
                        "name": "Ming-Yu Liu",
                        "slug": "Ming-Yu-Liu",
                        "structuredName": {
                            "firstName": "Ming-Yu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Yu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690538"
                        ],
                        "name": "J. Kautz",
                        "slug": "J.-Kautz",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Kautz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kautz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "This framework is extended by combining VAE [34] and GAN in [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [11], [28]\u2013[32]) have been proposed to tackle the problem of image-to-image translation by adopting unsupervised strategies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3783306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b69badabc3fddc9710faa44c530473397303b0b9",
            "isKey": false,
            "numCitedBy": 1801,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL ."
            },
            "slug": "Unsupervised-Image-to-Image-Translation-Networks-Liu-Breuel",
            "title": {
                "fragments": [],
                "text": "Unsupervised Image-to-Image Translation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work makes a shared-latent space assumption and proposes an unsupervised image-to-image translation framework based on Coupled GANs that achieves state-of-the-art performance on benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30187096"
                        ],
                        "name": "Yunjey Choi",
                        "slug": "Yunjey-Choi",
                        "structuredName": {
                            "firstName": "Yunjey",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunjey Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8486223"
                        ],
                        "name": "Min-Je Choi",
                        "slug": "Min-Je-Choi",
                        "structuredName": {
                            "firstName": "Min-Je",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Je Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110103236"
                        ],
                        "name": "M. Kim",
                        "slug": "M.-Kim",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Kim",
                            "middleNames": [
                                "Su"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577039"
                        ],
                        "name": "Jung-Woo Ha",
                        "slug": "Jung-Woo-Ha",
                        "structuredName": {
                            "firstName": "Jung-Woo",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Woo Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787729"
                        ],
                        "name": "Sunghun Kim",
                        "slug": "Sunghun-Kim",
                        "structuredName": {
                            "firstName": "Sunghun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunghun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795455"
                        ],
                        "name": "J. Choo",
                        "slug": "J.-Choo",
                        "structuredName": {
                            "firstName": "Jaegul",
                            "lastName": "Choo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Choo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "StarGAN uses only one single model for translation between these two domains, which is extremely suitable for this task, since the two given domains have consistent colors and semantics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "[36] Y. Choi et al., \u201cStarGAN: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "StarGAN and ComboGAN are extensions of CycleGAN, which\n1https://github.com/sagiebenaim/DistanceGAN 2https://github.com/duxingren14/DualGAN 3https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix 4https://github.com/yunjey/StarGAN 5https://github.com/AAnoosheh/ComboGAN\nhave slightly better performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "StarGAN extremely lacks details and textures, making the outputs not so realistic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 119
                            }
                        ],
                        "text": "The proposed BranchGAN also uses only one encoder for feature extraction, which has a slightly better performance than StarGAN in this task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "The training time for most of the approaches is around 17 hours for Cityscapes and 6 hours for CelebA, since StarGAN and ComboGAN adopt the similar network structure as CycleGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "StarGAN and ComboGAN have good performance in middle and large scale objects such as road and sidewalk, and keep stable in small scale objects such as car and person."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "StarGAN outperforms other baselines, which has around 10% improvement compared with DualGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "StarGAN [36] is proposed to solve the problem of training across multiple domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "Especially for female\u2192male task, the generated eyes and eyebrows are much more masculinized than StarGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Other extensions based on CycleGAN have been proposed, such as ComboGAN [35] and StarGAN [36], which can both transfer images between multiple domains without supervision, using less or even only one unified model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "Only subjective evaluation is performed for StarGAN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9417016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
            "isKey": true,
            "numCitedBy": 2120,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks."
            },
            "slug": "StarGAN:-Unified-Generative-Adversarial-Networks-Choi-Choi",
            "title": {
                "fragments": [],
                "text": "StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network, which leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2732737"
                        ],
                        "name": "Konstantinos Bousmalis",
                        "slug": "Konstantinos-Bousmalis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Bousmalis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantinos Bousmalis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2286640"
                        ],
                        "name": "N. Silberman",
                        "slug": "N.-Silberman",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Silberman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Silberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35363891"
                        ],
                        "name": "David Dohan",
                        "slug": "David-Dohan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Dohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Dohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707347"
                        ],
                        "name": "Dilip Krishnan",
                        "slug": "Dilip-Krishnan",
                        "structuredName": {
                            "firstName": "Dilip",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dilip Krishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Different strategies are proposed to enforce the output to be similar to the input in certain predefined aspects, such as class label [28], image pixel [29] and image feature [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": ", [11], [28]\u2013[32]) have been proposed to tackle the problem of image-to-image translation by adopting unsupervised strategies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206595056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "220ac48a22547a455d05f416e1fd22bbd0b0788d",
            "isKey": false,
            "numCitedBy": 1070,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training."
            },
            "slug": "Unsupervised-Pixel-Level-Domain-Adaptation-with-Bousmalis-Silberman",
            "title": {
                "fragments": [],
                "text": "Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain, and outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094770"
                        ],
                        "name": "Phillip Isola",
                        "slug": "Phillip-Isola",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Isola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Isola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436356"
                        ],
                        "name": "Jun-Yan Zhu",
                        "slug": "Jun-Yan-Zhu",
                        "structuredName": {
                            "firstName": "Jun-Yan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-Yan Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822702"
                        ],
                        "name": "Tinghui Zhou",
                        "slug": "Tinghui-Zhou",
                        "structuredName": {
                            "firstName": "Tinghui",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tinghui Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 184
                            }
                        ],
                        "text": "In addition, Pix2Pix has explored that L2 distance always produces blurry results for the image generation task, so better performance will be achieved using L1 distance instead of L2 [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "construct a set of image pairs belonging to inconsistent domains [10], which are then fed into the models to learn the appearance transform relationship of these two domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Pix2Pix [10] and SRGAN [13]) are dominated by the supervised training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Similar to CycleGAN [12] and Pix2Pix [10], FCN-score [40] is adopted as the performance metric for the task of semantic labels\u2192 photo gen-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "To ensure the fairness, the settings of FCN training are kept the same for all tested models [10], [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Therefore, a supervised image-translation model, named Pix2Pix [10] is proposed, which is trained with paired images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Combining a U-Net [10] structure with a single Encoder-Decoder and L1 self-regularization loss, Pix2Pix [10] ensures the consistency between the inputs and outputs, which achieves good results in paired image-to-image translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Pix2Pix [10] is trained on paired data, which is a supervised method for cross-domain image generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Traditional supervised learning methods, like Pix2Pix [10], can find the mapping relationship easier, because semantic labels and photos have one-to-one correspondence and the paired images will be passed to the network at the same time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6200260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8acbe90d5b852dadea7810345451a99608ee54c7",
            "isKey": true,
            "numCitedBy": 11193,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either."
            },
            "slug": "Image-to-Image-Translation-with-Conditional-Isola-Zhu",
            "title": {
                "fragments": [],
                "text": "Image-to-Image Translation with Conditional Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Conditional adversarial networks are investigated as a general-purpose solution to image-to-image translation problems and it is demonstrated that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188620"
                        ],
                        "name": "Yaniv Taigman",
                        "slug": "Yaniv-Taigman",
                        "structuredName": {
                            "firstName": "Yaniv",
                            "lastName": "Taigman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaniv Taigman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33964593"
                        ],
                        "name": "A. Polyak",
                        "slug": "A.-Polyak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Polyak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Polyak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Different strategies are proposed to enforce the output to be similar to the input in certain predefined aspects, such as class label [28], image pixel [29] and image feature [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10756563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04bd2907111855b9fde9413bb25b9788a4c03f26",
            "isKey": false,
            "numCitedBy": 735,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity."
            },
            "slug": "Unsupervised-Cross-Domain-Image-Generation-Taigman-Polyak",
            "title": {
                "fragments": [],
                "text": "Unsupervised Cross-Domain Image Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The Domain Transfer Network (DTN) is presented, which employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304525"
                        ],
                        "name": "Alexandre Alahi",
                        "slug": "Alexandre-Alahi",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Alahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Alahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 980236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fa3720371e78d04973ce9752781bc337480b68f",
            "isKey": false,
            "numCitedBy": 5988,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."
            },
            "slug": "Perceptual-Losses-for-Real-Time-Style-Transfer-and-Johnson-Alahi",
            "title": {
                "fragments": [],
                "text": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work considers image transformation problems, and proposes the use of perceptual loss functions for training feed-forward networks for image transformation tasks, and shows results on image style transfer, where aFeed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9480437"
                        ],
                        "name": "Asha Anoosheh",
                        "slug": "Asha-Anoosheh",
                        "structuredName": {
                            "firstName": "Asha",
                            "lastName": "Anoosheh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asha Anoosheh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2794259"
                        ],
                        "name": "E. Agustsson",
                        "slug": "E.-Agustsson",
                        "structuredName": {
                            "firstName": "Eirikur",
                            "lastName": "Agustsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Agustsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732855"
                        ],
                        "name": "R. Timofte",
                        "slug": "R.-Timofte",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Timofte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Timofte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 12
                            }
                        ],
                        "text": "StarGAN and ComboGAN are extensions of CycleGAN, which\n1https://github.com/sagiebenaim/DistanceGAN 2https://github.com/duxingren14/DualGAN 3https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix 4https://github.com/yunjey/StarGAN 5https://github.com/AAnoosheh/ComboGAN\nhave slightly better performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 121
                            }
                        ],
                        "text": "The training time for most of the approaches is around 17 hours for Cityscapes and 6 hours for CelebA, since StarGAN and ComboGAN adopt the similar network structure as CycleGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 61
                            }
                        ],
                        "text": "[35] A. Anoosheh, E. Agustsson, R. Timofte, and L. Van Gool, \u201cComboGAN: Unrestrained scalability for image domain translation,\u201d in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "It has around 15% and 20% improvement compared to ComboGAN and CycleGAN, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "As the follow-up of CycleGAN, ComboGAN has no sufficient details as CycleGAN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 12
                            }
                        ],
                        "text": "StarGAN and ComboGAN have good performance in middle and large scale objects such as road and sidewalk, and keep stable in small scale objects such as car and person."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "ComboGAN [35] is a multi-component image translation model, which conducts the training across multiple do-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "ComboGAN [35] is a multi-component image translation model, which conducts the training across multiple domains by dividing each model to two parts: one transfers the source domain to a common representation and the other converts the common representation to the target domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Other extensions based on CycleGAN have been proposed, such as ComboGAN [35] and StarGAN [36], which can both transfer images between multiple domains without supervision, using less or even only one unified model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 126
                            }
                        ],
                        "text": "The performance comparison with the state-of-the-art approaches for gender transformation is listed in Table VI. CycleGAN and ComboGAN share the similar idea, which have\nthe worst performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6441226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4aaf11031211846681aaa0ad9ef189afbc767fc",
            "isKey": true,
            "numCitedBy": 144,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The past year alone has seen unprecedented leaps in the area of learning-based image translation, namely Cycle-GAN, by Zhu et al. But experiments so far have been tailored to merely two domains at a time, and scaling them to more would require an quadratic number of models to be trained. And with two-domain models taking days to train on current hardware, the number of domains quickly becomes limited by the time and resources required to process them. In this paper, we propose a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains. We demonstrate its capabilities on a dataset of paintings by 14 different artists and on images of the four different seasons in the Alps. Note that 14 data groups would need (14 choose 2) = 91 different CycleGAN models: a total of 182 generator/discriminator pairs; whereas our model requires only 14 generator/discriminator pairs."
            },
            "slug": "ComboGAN:-Unrestrained-Scalability-for-Image-Domain-Anoosheh-Agustsson",
            "title": {
                "fragments": [],
                "text": "ComboGAN: Unrestrained Scalability for Image Domain Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains and demonstrates its capabilities on a dataset of paintings by 14 different artists."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793900"
                        ],
                        "name": "Ming-Yu Liu",
                        "slug": "Ming-Yu-Liu",
                        "structuredName": {
                            "firstName": "Ming-Yu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Yu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577513"
                        ],
                        "name": "Oncel Tuzel",
                        "slug": "Oncel-Tuzel",
                        "structuredName": {
                            "firstName": "Oncel",
                            "lastName": "Tuzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oncel Tuzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "CoGAN [33] uses two GAN networks with weight sharing to generate cross-domain samples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10627900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "372bc106c61e7eb004835e85bbfee997409f176a",
            "isKey": false,
            "numCitedBy": 1223,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation."
            },
            "slug": "Coupled-Generative-Adversarial-Networks-Liu-Tuzel",
            "title": {
                "fragments": [],
                "text": "Coupled Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work proposes coupled generative adversarial network (CoGAN), which can learn a joint distribution without any tuple of corresponding images, and applies it to several joint distribution learning tasks, and demonstrates its applications to domain adaptation and image transformation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779917"
                        ],
                        "name": "C. Ledig",
                        "slug": "C.-Ledig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Ledig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ledig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073063"
                        ],
                        "name": "Lucas Theis",
                        "slug": "Lucas-Theis",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Theis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucas Theis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108066"
                        ],
                        "name": "Ferenc Husz\u00e1r",
                        "slug": "Ferenc-Husz\u00e1r",
                        "structuredName": {
                            "firstName": "Ferenc",
                            "lastName": "Husz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferenc Husz\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145372820"
                        ],
                        "name": "Jose Caballero",
                        "slug": "Jose-Caballero",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Caballero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jose Caballero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49931957"
                        ],
                        "name": "Andrew P. Aitken",
                        "slug": "Andrew-P.-Aitken",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Aitken",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew P. Aitken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41203992"
                        ],
                        "name": "Alykhan Tejani",
                        "slug": "Alykhan-Tejani",
                        "structuredName": {
                            "firstName": "Alykhan",
                            "lastName": "Tejani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alykhan Tejani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853456"
                        ],
                        "name": "J. Totz",
                        "slug": "J.-Totz",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Totz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Totz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34627233"
                        ],
                        "name": "Zehan Wang",
                        "slug": "Zehan-Wang",
                        "structuredName": {
                            "firstName": "Zehan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zehan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46810836"
                        ],
                        "name": "Wenzhe Shi",
                        "slug": "Wenzhe-Shi",
                        "structuredName": {
                            "firstName": "Wenzhe",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenzhe Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "It overcomes the drawbacks of traditional supervised models (such as Pix2Pix and SRGAN) that can only perform one-directional domain transformation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Pix2Pix [10] and SRGAN [13]) are dominated by the supervised training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "ric, which has been used to evaluate the performance of gender transformation by [13], [43], [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 76
                            }
                        ],
                        "text": "The majority of existing image translation solutions (e.g. Pix2Pix [10] and SRGAN [13]) are dominated by the supervised training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "2) SSIM: SSIM (Structural Similarity index) [41] is a metric to measure the structural similarity of two images, which is widely used in quality evaluation of the generated images [13], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 211227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
            "isKey": true,
            "numCitedBy": 6170,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method."
            },
            "slug": "Photo-Realistic-Single-Image-Super-Resolution-Using-Ledig-Theis",
            "title": {
                "fragments": [],
                "text": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "SRGAN, a generative adversarial network (GAN) for image super-resolution (SR), is presented, to its knowledge, the first framework capable of inferring photo-realistic natural images for 4x upscaling factors and a perceptual loss function which consists of an adversarial loss and a content loss."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1490900960"
                        ],
                        "name": "A. Shrivastava",
                        "slug": "A.-Shrivastava",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945962"
                        ],
                        "name": "Tomas Pfister",
                        "slug": "Tomas-Pfister",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577513"
                        ],
                        "name": "Oncel Tuzel",
                        "slug": "Oncel-Tuzel",
                        "structuredName": {
                            "firstName": "Oncel",
                            "lastName": "Tuzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oncel Tuzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49158771"
                        ],
                        "name": "J. Susskind",
                        "slug": "J.-Susskind",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Susskind",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Susskind"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108465550"
                        ],
                        "name": "Wenda Wang",
                        "slug": "Wenda-Wang",
                        "structuredName": {
                            "firstName": "Wenda",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenda Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51138986"
                        ],
                        "name": "Russ Webb",
                        "slug": "Russ-Webb",
                        "structuredName": {
                            "firstName": "Russ",
                            "lastName": "Webb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Russ Webb"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "Different strategies are proposed to enforce the output to be similar to the input in certain predefined aspects, such as class label [28], image pixel [29] and image feature [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "These images instead of the latest generated images are used to update our discriminators [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8229065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68cb9fce1e6af2740377494350b650533c9a29e1",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data."
            },
            "slug": "Learning-from-Simulated-and-Unsupervised-Images-Shrivastava-Pfister",
            "title": {
                "fragments": [],
                "text": "Learning from Simulated and Unsupervised Images through Adversarial Training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work develops a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors, and makes several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887364"
                        ],
                        "name": "Tim Salimans",
                        "slug": "Tim-Salimans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Salimans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Salimans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563432"
                        ],
                        "name": "Wojciech Zaremba",
                        "slug": "Wojciech-Zaremba",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Zaremba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Zaremba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34415167"
                        ],
                        "name": "Vicki Cheung",
                        "slug": "Vicki-Cheung",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Cheung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicki Cheung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41192764"
                        ],
                        "name": "Xi Chen",
                        "slug": "Xi-Chen",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "evaluate the quality of the generated images [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1687220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "isKey": false,
            "numCitedBy": 5549,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
            },
            "slug": "Improved-Techniques-for-Training-GANs-Salimans-Goodfellow",
            "title": {
                "fragments": [],
                "text": "Improved Techniques for Training GANs"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work focuses on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic, and presents ImageNet samples with unprecedented resolution and shows that the methods enable the model to learn recognizable features of ImageNet classes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891828"
                        ],
                        "name": "Leon A. Gatys",
                        "slug": "Leon-A.-Gatys",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Gatys",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leon A. Gatys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746183"
                        ],
                        "name": "Alexander S. Ecker",
                        "slug": "Alexander-S.-Ecker",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Ecker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander S. Ecker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731199"
                        ],
                        "name": "M. Bethge",
                        "slug": "M.-Bethge",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Bethge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bethge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "IMAGE translation is the way to transfer the input images from one domain to another domain, such as color images to sketches, photos to paintings, and so on [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "tends to modify certain visual properties, such as color [7], texture [8], style [1], or their combinations [1], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206593710,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7568d13a82f7afa4be79f09c295940e48ec6db89",
            "isKey": true,
            "numCitedBy": 3122,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation."
            },
            "slug": "Image-Style-Transfer-Using-Convolutional-Neural-Gatys-Ecker",
            "title": {
                "fragments": [],
                "text": "Image Style Transfer Using Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Neural Algorithm of Artistic Style is introduced that can separate and recombine the image content and style of natural images and provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Generative Adversarial Networks (GAN) [17], [18] are a kind of particular generative models, which utilize adversarial learning strategies to control the weight updating of the generator and discriminator, until a dynamic equilibrium is constructed to achieve an optimal solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15876696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ba23d9b46027e47b4483243871760e315213ffe",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images."
            },
            "slug": "Energy-based-Generative-Adversarial-Network-Zhao-Mathieu",
            "title": {
                "fragments": [],
                "text": "Energy-based Generative Adversarial Network"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Generative Adversarial Networks (GAN) [17], [18] are a kind of particular generative models, which utilize adversarial learning strategies to control the weight updating of the generator and discriminator, until a dynamic equilibrium is constructed to achieve an optimal solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 91185868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5717143b3a98ebcbf72e18fb49464d56b0f2ffca",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Energy-based-Generative-Adversarial-Networks-Zhao-Mathieu",
            "title": {
                "fragments": [],
                "text": "Energy-based Generative Adversarial Networks"
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152998391"
                        ],
                        "name": "Yijun Li",
                        "slug": "Yijun-Li",
                        "structuredName": {
                            "firstName": "Yijun",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yijun Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391885"
                        ],
                        "name": "Sifei Liu",
                        "slug": "Sifei-Liu",
                        "structuredName": {
                            "firstName": "Sifei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sifei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768964"
                        ],
                        "name": "Jimei Yang",
                        "slug": "Jimei-Yang",
                        "structuredName": {
                            "firstName": "Jimei",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimei Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "ric, which has been used to evaluate the performance of gender transformation by [13], [43], [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9459250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0351087a2b85f70e60fc79dfa4110b4985cc00a",
            "isKey": false,
            "numCitedBy": 450,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results."
            },
            "slug": "Generative-Face-Completion-Li-Liu",
            "title": {
                "fragments": [],
                "text": "Generative Face Completion"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper demonstrates qualitatively and quantitatively that the proposed effective face completion algorithm is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117940996"
                        ],
                        "name": "Ziwei Liu",
                        "slug": "Ziwei-Liu",
                        "structuredName": {
                            "firstName": "Ziwei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziwei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47571885"
                        ],
                        "name": "Ping Luo",
                        "slug": "Ping-Luo",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 51
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which outperforms the state-of-the-art unsupervised methods and achieves the competitive performance as the supervised solution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "2) Dataset: CelebFaces Attributes Dataset (CelebA) [14] is a popular face attribute dataset with over 200K celebrity images,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 459456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "isKey": false,
            "numCitedBy": 4549,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts."
            },
            "slug": "Deep-Learning-Face-Attributes-in-the-Wild-Liu-Luo",
            "title": {
                "fragments": [],
                "text": "Deep Learning Face Attributes in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel deep learning framework for attribute prediction in the wild that cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096458"
                        ],
                        "name": "Luke Metz",
                        "slug": "Luke-Metz",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Metz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Metz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11758569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8388f1be26329fa45e5807e968a641ce170ea078",
            "isKey": false,
            "numCitedBy": 9853,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
            },
            "slug": "Unsupervised-Representation-Learning-with-Deep-Radford-Metz",
            "title": {
                "fragments": [],
                "text": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrates that they are a strong candidate for unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48570713"
                        ],
                        "name": "L. Zhang",
                        "slug": "L.-Zhang",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798982"
                        ],
                        "name": "Bingpeng Ma",
                        "slug": "Bingpeng-Ma",
                        "structuredName": {
                            "firstName": "Bingpeng",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bingpeng Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143164613"
                        ],
                        "name": "Guorong Li",
                        "slug": "Guorong-Li",
                        "structuredName": {
                            "firstName": "Guorong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guorong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876831"
                        ],
                        "name": "Q. Tian",
                        "slug": "Q.-Tian",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Tian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 150
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23526678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605f5d08199b3202dd180590aef1a1648c0e309e",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated by the fact that unlabeled data can be easily collected and help to exploit the correlations among different modalities, this paper proposes a novel method named generalized semi-supervised structured subspace learning (GSS-SL) for the task of cross-modal retrieval. First, to predict more relevant class labels for unlabeled data, we propose a label graph constraint that ensures the intrinsic geometric structures of different feature spaces consistent with that of label space. Second, considering that class labels directly reveal the semantic information of multimedia data, GSS-SL takes the label space as a linkage to model the correlations among different modalities. Concretely, the label graph constraint, label-linked loss function, and regularization are integrated into a joint minimization formulation to learn a discriminative common subspace. Finally, an efficient optimization algorithm is designed to alternately optimize multiple linear transformations for different modalities and update the class indicator matrices for unlabeled data. Furthermore, an arbitrary number of modalities can be solved in the proposed framework. Extensive experiments on three standard benchmark datasets demonstrate that GSS-SL outperforms previous methods on exploiting the correlations among different modalities."
            },
            "slug": "Generalized-Semi-supervised-and-Structured-Subspace-Zhang-Ma",
            "title": {
                "fragments": [],
                "text": "Generalized Semi-supervised and Structured Subspace Learning for Cross-Modal Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel method named generalized semi-supervised structured subspace learning (GSS-SL) is proposed for the task of cross-modal retrieval of unlabeled data and outperforms previous methods on exploiting the correlations among different modalities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469209"
                        ],
                        "name": "Lantao Yu",
                        "slug": "Lantao-Yu",
                        "structuredName": {
                            "firstName": "Lantao",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lantao Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108309275"
                        ],
                        "name": "Weinan Zhang",
                        "slug": "Weinan-Zhang",
                        "structuredName": {
                            "firstName": "Weinan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weinan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39055225"
                        ],
                        "name": "Jun Wang",
                        "slug": "Jun-Wang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811427"
                        ],
                        "name": "Yong Yu",
                        "slug": "Yong-Yu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Sequence GAN (SeqGAN) [22] is used to generate sequences by integrating capability of sequence modeling of LSTM networks [23] into GAN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3439214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2966ecd82505ecd55ead0e6a327a304c8f9868e3",
            "isKey": false,
            "numCitedBy": 1620,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.\n \n"
            },
            "slug": "SeqGAN:-Sequence-Generative-Adversarial-Nets-with-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47133874"
                        ],
                        "name": "Orest Kupyn",
                        "slug": "Orest-Kupyn",
                        "structuredName": {
                            "firstName": "Orest",
                            "lastName": "Kupyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Orest Kupyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30025233"
                        ],
                        "name": "Volodymyr Budzan",
                        "slug": "Volodymyr-Budzan",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Budzan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Budzan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30162343"
                        ],
                        "name": "Mykola Mykhailych",
                        "slug": "Mykola-Mykhailych",
                        "structuredName": {
                            "firstName": "Mykola",
                            "lastName": "Mykhailych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mykola Mykhailych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40369725"
                        ],
                        "name": "Dmytro Mishkin",
                        "slug": "Dmytro-Mishkin",
                        "structuredName": {
                            "firstName": "Dmytro",
                            "lastName": "Mishkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmytro Mishkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "2) SSIM: SSIM (Structural Similarity index) [41] is a metric to measure the structural similarity of two images, which is widely used in quality evaluation of the generated images [13], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4552226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9184da852d02f1a15bc465d9cceae613b0a03e51",
            "isKey": false,
            "numCitedBy": 796,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss. DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem - object detection on (de-)blurred images. The method is 5 times faster than the closest competitor - Deep-Deblur [25]. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation. The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN"
            },
            "slug": "DeblurGAN:-Blind-Motion-Deblurring-Using-Networks-Kupyn-Budzan",
            "title": {
                "fragments": [],
                "text": "DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance and is 5 times faster than the closest competitor - Deep-Deblur."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41192764"
                        ],
                        "name": "Xi Chen",
                        "slug": "Xi-Chen",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144581158"
                        ],
                        "name": "Yan Duan",
                        "slug": "Yan-Duan",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127100"
                        ],
                        "name": "Rein Houthooft",
                        "slug": "Rein-Houthooft",
                        "structuredName": {
                            "firstName": "Rein",
                            "lastName": "Houthooft",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rein Houthooft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47971768"
                        ],
                        "name": "J. Schulman",
                        "slug": "J.-Schulman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Schulman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schulman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "images, such as writing style manipulation [5], fabric color transfer [7], image retargeting [8], semantic classification [26] and semantic segmentation [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5002792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35da0a2001eea88486a5de677ab97868c93d0824",
            "isKey": false,
            "numCitedBy": 3072,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
            },
            "slug": "InfoGAN:-Interpretable-Representation-Learning-by-Chen-Duan",
            "title": {
                "fragments": [],
                "text": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34443348"
                        ],
                        "name": "Xudong Mao",
                        "slug": "Xudong-Mao",
                        "structuredName": {
                            "firstName": "Xudong",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xudong Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117895101"
                        ],
                        "name": "Qing Li",
                        "slug": "Qing-Li",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3607957"
                        ],
                        "name": "Haoran Xie",
                        "slug": "Haoran-Xie",
                        "structuredName": {
                            "firstName": "Haoran",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoran Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144031692"
                        ],
                        "name": "Raymond Y. K. Lau",
                        "slug": "Raymond-Y.-K.-Lau",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Lau",
                            "middleNames": [
                                "Y.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond Y. K. Lau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118453660"
                        ],
                        "name": "Zhen Wang",
                        "slug": "Zhen-Wang",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32309056"
                        ],
                        "name": "Stephen Paul Smolley",
                        "slug": "Stephen-Paul-Smolley",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smolley",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Paul Smolley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "First, we replace the negative log likelihood objective with least-square loss [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206771128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74ff6d48f9c62e937023106629d27ef2d2ddf8bc",
            "isKey": false,
            "numCitedBy": 2801,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs."
            },
            "slug": "Least-Squares-Generative-Adversarial-Networks-Mao-Li",
            "title": {
                "fragments": [],
                "text": "Least Squares Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper proposes the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator, and shows that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "4) Performance Comparison With the State-of-the-Art Approaches: To verify the performance, FCN-scores are adopted to compare the proposed method with the baselines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "FCN-score metric consists of three parts, Pixel Accuracy (ACC_P), Classification Accuracy (ACC_C) and Intersectionover-Union (IOU), which are defined as follows: Pixel Accuracy (ACC_P) [40]: ACC_P is the ratio of the\nnumber of correctly classified pixels to the total number of pixels, which is defined as follows:\nACC_P = \u2211 i pii\u2211 i pi\n(9)\nwhere pi = \u2211\nj pij is the total number of pixels of class i, and pij represents the number of pixels belonging to class i are falsely predicted as class j. Classification Accuracy (ACC_C) [40]: ACC_C calculates the average proportion of correctly classified pixels in all class, which is defined as:\nACC_C = 1\nNc \u2211 i pii pi\n(10)\nwhere Nc is the total number of classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The detailed performance of IOU per class and the examples of generated images for different loss functions are listed in Table V and illustrated in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "If the difference of FCN-score between the generated image and the ground truth is small, it indicates that the model has a good performance, i.e., image translation is accurate and realistic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "In this paper, we set 8-times of downsampling to construct the FCN model, which performs the best among all settings [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "1) FCN-score: There are no widely accepted methods to evaluate the quality of the generated images [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "over-Union (IOU), which are defined as follows: Pixel Accuracy (ACC_P) [40]: ACC_P is the ratio of the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Intersection-over-Union (IOU) [40]: IOU computes the average ratio of the intersections and unions of the sets of ground truth and the predicted segmentation on each class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Classification Accuracy (ACC_C) [40]: ACC_C calcu-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Similar to CycleGAN [12] and Pix2Pix [10], FCN-score [40] is adopted as the performance metric for the task of semantic labels\u2192 photo gen-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "We select 1,000 male and 1,000 female frontal faces as our\nTABLE V IOU PER CLASS ON FCN USING DIFFERENT LOSS FUNCTIONS\nFig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Similar to CycleGAN [12] and Pix2Pix [10], FCN-score [40] is adopted as the performance metric for the task of semantic labels\u2192 photo generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Since the pre-trained model of FCN on Cityscapes is not available, we have to train an FCN from scratch and utilize it to measure the performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "Since the resolution of the generated images is only 128 \u00d7 128, some small objects such as traffic light, traffic sign, rider and motorcycle are failed to be detected by FCN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "The input images of FCN are N-times down-sampling after convolution and pooling operations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "To ensure the fairness, the settings of FCN training are kept the same for all tested models [10], [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "IOU = 1\nNc\n\u2211 i pii\npi + \u2211 j pji \u2212 pii (11)\n2) SSIM: SSIM (Structural Similarity index) [41] is a metric to measure the structural similarity of two images, which is widely used in quality evaluation of the generated images [13], [42]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "FCN classifies the image content at the pixel level and predicts a label map, which is compared with the input ground truth label."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "The input images of FCN models are resized to 128 \u00d7 128, which is the same size as the generated images of the proposed method and baselines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "The labeled maps from the generated photos are compared with the ground truth using standard semantic segmentation metrics [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "FCN-score metric can effectively evaluate the accuracy of image generation with Fully Convolutional Networks (FCN) [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56507745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9201bf6f8222c2335913002e13fbac640fc0f4ec",
            "isKey": true,
            "numCitedBy": 9461,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image."
            },
            "slug": "Fully-convolutional-networks-for-semantic-Long-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Fully convolutional networks for semantic segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38690089"
                        ],
                        "name": "Weiming Dong",
                        "slug": "Weiming-Dong",
                        "structuredName": {
                            "firstName": "Weiming",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiming Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2866997"
                        ],
                        "name": "Fuzhang Wu",
                        "slug": "Fuzhang-Wu",
                        "structuredName": {
                            "firstName": "Fuzhang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuzhang Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40311083"
                        ],
                        "name": "Yan Kong",
                        "slug": "Yan-Kong",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Kong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143700357"
                        ],
                        "name": "Xing Mei",
                        "slug": "Xing-Mei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Mei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Mei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39945992"
                        ],
                        "name": "Tong-Yee Lee",
                        "slug": "Tong-Yee-Lee",
                        "structuredName": {
                            "firstName": "Tong-Yee",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong-Yee Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21458018"
                        ],
                        "name": "Xiaopeng Zhang",
                        "slug": "Xiaopeng-Zhang",
                        "structuredName": {
                            "firstName": "Xiaopeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaopeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "tends to modify certain visual properties, such as color [7], texture [8], style [1], or their combinations [1], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "images, such as writing style manipulation [5], fabric color transfer [7], image retargeting [8], semantic classification [26] and semantic segmentation [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19034664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a04605fc98c4b6f0695b2ddc2daab8719b1c8e7",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on exampled-based texture synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural & non-textural regions with different strategies since they have different natures. We propose to retarget the textural regions by example-based synthesis and non-textural regions by fast multi-operator. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image retargeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method."
            },
            "slug": "Image-Retargeting-by-Texture-Aware-Synthesis-Dong-Wu",
            "title": {
                "fragments": [],
                "text": "Image Retargeting by Texture-Aware Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An automatic and fast texture detection method that can detect multiple disjoint textural regions and adjust the saliency of the image according to the features of thetextural regions to achieve practical retargeting applications for general images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562966"
                        ],
                        "name": "Philipp Kr\u00e4henb\u00fchl",
                        "slug": "Philipp-Kr\u00e4henb\u00fchl",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Kr\u00e4henb\u00fchl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Kr\u00e4henb\u00fchl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 84591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1db6e3078597386ac4222ba6c3f4f61b61f53539",
            "isKey": false,
            "numCitedBy": 1359,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
            },
            "slug": "Adversarial-Feature-Learning-Donahue-Kr\u00e4henb\u00fchl",
            "title": {
                "fragments": [],
                "text": "Adversarial Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Bidirectional Generative Adversarial Networks are proposed as a means of learning the inverse mapping of GANs, and it is demonstrated that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367683"
                        ],
                        "name": "H. Pirsiavash",
                        "slug": "H.-Pirsiavash",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Pirsiavash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pirsiavash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9933254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
            "isKey": false,
            "numCitedBy": 1109,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation."
            },
            "slug": "Generating-Videos-with-Scene-Dynamics-Vondrick-Pirsiavash",
            "title": {
                "fragments": [],
                "text": "Generating Videos with Scene Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background is proposed that can generate tiny videos up to a second at full frame rate better than simple baselines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114791719"
                        ],
                        "name": "Zhi-Chao Song",
                        "slug": "Zhi-Chao-Song",
                        "structuredName": {
                            "firstName": "Zhi-Chao",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Chao Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50152173"
                        ],
                        "name": "Shi-guang Liu",
                        "slug": "Shi-guang-Liu",
                        "structuredName": {
                            "firstName": "Shi-guang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi-guang Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "ture features, a new image appearance translation method is introduced to obtain sufficient appearance translation results [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "tends to modify certain visual properties, such as color [7], texture [8], style [1], or their combinations [1], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21223421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5777bf9a31e2ec7ebb5f591d542b0560652f859c",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional color transfer methods can achieve satisfactory results for transferring the color style from a reference image to a source image, provided that the source image shares the similar color mood with the reference image. However, color transfer solutions are always sensitive to color category, which cannot generate natural results when the contents of the reference image and the source image are different, e.g., a lush tree in the reference image and a bare tree in the source image. In this situation, it is insufficient only through color transfer to transfer the appearance from the reference image to the source image only through color transfer, since other information such as texture should also be considered. To obtain sufficient appearance transfer results, we propose a new image appearance transfer method combining both color and texture features. Given a source image and a reference image, our method starts with feature detection and matching between the source image and the reference image. Then, we design a new method for expanding feature point sets to get texture transfer mark (TTM) and color transfer mark (CTM). TTM and CTM will guide texture transfer and color transfer, respectively. We demonstrate our appearance transfer algorithm between quantities of images and compare with results of existing methods. Experiment results show that given only a single reference image, our approach can produce more sufficient appearance transfer results than the state-of-the-art algorithms."
            },
            "slug": "Sufficient-Image-Appearance-Transfer-Combining-and-Song-Liu",
            "title": {
                "fragments": [],
                "text": "Sufficient Image Appearance Transfer Combining Color and Texture"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a new image appearance transfer method combining both color and texture features and shows that given only a single reference image, this approach can produce more sufficient appearance transfer results than the state-of-the-art algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2509132"
                        ],
                        "name": "Taeksoo Kim",
                        "slug": "Taeksoo-Kim",
                        "structuredName": {
                            "firstName": "Taeksoo",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taeksoo Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9959922"
                        ],
                        "name": "Moonsu Cha",
                        "slug": "Moonsu-Cha",
                        "structuredName": {
                            "firstName": "Moonsu",
                            "lastName": "Cha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moonsu Cha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118020246"
                        ],
                        "name": "Hyunsoo Kim",
                        "slug": "Hyunsoo-Kim",
                        "structuredName": {
                            "firstName": "Hyunsoo",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyunsoo Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119170990"
                        ],
                        "name": "Jung Kwon Lee",
                        "slug": "Jung-Kwon-Lee",
                        "structuredName": {
                            "firstName": "Jung",
                            "lastName": "Lee",
                            "middleNames": [
                                "Kwon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung Kwon Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3968500"
                        ],
                        "name": "Jiwon Kim",
                        "slug": "Jiwon-Kim",
                        "structuredName": {
                            "firstName": "Jiwon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiwon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "It takes only 0.2 hours to train DiscoGAN, which is extremely efficient."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "3) Performance Comparison: Although CycleGAN [12] and DiscoGAN [3] can perform gender transformation, they did not evaluate the performance in their works."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "Different from other baselines where the batch size is set to 1, the original batch size of DiscoGAN is set to 200, which greatly decreases the number of iterations for each epoch."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "It has been widely used by [3], [12], [32] for gender transformation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8239952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7778b2b6ee67df2a0a06fe82e79acfdf714c7399",
            "isKey": true,
            "numCitedBy": 1399,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations when given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity."
            },
            "slug": "Learning-to-Discover-Cross-Domain-Relations-with-Kim-Cha",
            "title": {
                "fragments": [],
                "text": "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN) and successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] are novelly proposed to solve this problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12803511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
            "isKey": false,
            "numCitedBy": 6052,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels."
            },
            "slug": "Conditional-Generative-Adversarial-Nets-Mirza-Osindero",
            "title": {
                "fragments": [],
                "text": "Conditional Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The conditional version of generative adversarial nets is introduced, which can be constructed by simply feeding the data, y, to the generator and discriminator, and it is shown that this model can generate MNIST digits conditioned on class labels."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "In order to obtain the feature maps, three Residual Block (RB) layers [37] are applied to the final 2D convolutional block."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841796"
                        ],
                        "name": "Marius Cordts",
                        "slug": "Marius-Cordts",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Cordts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Cordts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187309"
                        ],
                        "name": "Mohamed Omran",
                        "slug": "Mohamed-Omran",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Omran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Omran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39940699"
                        ],
                        "name": "Sebastian Ramos",
                        "slug": "Sebastian-Ramos",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ramos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ramos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393153"
                        ],
                        "name": "Timo Rehfeld",
                        "slug": "Timo-Rehfeld",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Rehfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo Rehfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798000"
                        ],
                        "name": "Rodrigo Benenson",
                        "slug": "Rodrigo-Benenson",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Benenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Benenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582788"
                        ],
                        "name": "Uwe Franke",
                        "slug": "Uwe-Franke",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 67
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which outperforms the state-of-the-art unsupervised methods and achieves the competitive performance as the supervised solution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 12
                            }
                        ],
                        "text": "2) Dataset: CelebFaces Attributes Dataset (CelebA) [14] is a popular face attribute dataset with over 200K celebrity images, which have posture changes and cluttered background."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "2) Dataset: Cityscapes Dataset [15] is a large-scale image"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "2) Dataset: Cityscapes Dataset [15] is a large-scale image dataset with high quality pixel-level annotation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 502946,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
            "isKey": true,
            "numCitedBy": 6029,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."
            },
            "slug": "The-Cityscapes-Dataset-for-Semantic-Urban-Scene-Cordts-Omran",
            "title": {
                "fragments": [],
                "text": "The Cityscapes Dataset for Semantic Urban Scene Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling, and exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766628"
                        ],
                        "name": "W. Chu",
                        "slug": "W.-Chu",
                        "structuredName": {
                            "firstName": "Wei-Ta",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115562828"
                        ],
                        "name": "Yi-Ling Wu",
                        "slug": "Yi-Ling-Wu",
                        "structuredName": {
                            "firstName": "Yi-Ling",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Ling Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "2) Encoding Loss: It means that the original image from the source domain and the generated image in the target domain should maintain similar semantic information [26], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "images, such as writing style manipulation [5], fabric color transfer [7], image retargeting [8], semantic classification [26] and semantic segmentation [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52022296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8bfe7e44bf151c3675e552501e19b5402bc2b1b",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a comprehensive study of deep correlation features on image style classification. Inspired by that, correlation between feature maps can effectively describe image texture, and we design various correlations and transform them into style vectors, and investigate classification performance brought by different variants. In addition to intralayer correlation, interlayer correlation is proposed as well, and its effectiveness is verified. After showing the effectiveness of deep correlation features, we further propose a learning framework to automatically learn correlations between feature maps. Through extensive experiments on image style classification and artist classification, we demonstrate that the proposed learnt deep correlation features outperform several variants of convolutional neural network features by a large margin, and achieve the state-of-the-art performance."
            },
            "slug": "Image-Style-Classification-Based-on-Learnt-Deep-Chu-Wu",
            "title": {
                "fragments": [],
                "text": "Image Style Classification Based on Learnt Deep Correlation Features"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper demonstrates that the proposed learnt deep correlation features outperform several variants of convolutional neural network features by a large margin, and achieve the state-of-the-art performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19310335"
                        ],
                        "name": "Sagie Benaim",
                        "slug": "Sagie-Benaim",
                        "structuredName": {
                            "firstName": "Sagie",
                            "lastName": "Benaim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sagie Benaim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "DistanceGAN [32] proposes a constrain to predict if the unsupervised cross-domain mapping will be successful, enhancing the performance of unsupervised image transmission."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "DistanceGAN [32] proposes a new distance constraint which outperforms existing circularity-based constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "It has been widely used by [3], [12], [32] for gender transformation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": ", [11], [28]\u2013[32]) have been proposed to tackle the problem of image-to-image translation by adopting unsupervised strategies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1629634,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "fd4537b92ab9fa7c653e9e5b9c4f815914a498c0",
            "isKey": true,
            "numCitedBy": 178,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In unsupervised domain mapping, the learner is given two unmatched datasets $A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample in $A$ to the analog sample in $B$. Recent approaches have shown that when learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$, convincing mappings are obtained. In this work, we present a method of learning $G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at this https URL ."
            },
            "slug": "One-Sided-Unsupervised-Domain-Mapping-Benaim-Wolf",
            "title": {
                "fragments": [],
                "text": "One-Sided Unsupervised Domain Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a method of learning G_{AB} without learning G_BA, a mapping that maintains the distance between a pair of samples, and presents experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41210105"
                        ],
                        "name": "Zhou Wang",
                        "slug": "Zhou-Wang",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747569"
                        ],
                        "name": "A. Bovik",
                        "slug": "A.-Bovik",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Bovik",
                            "middleNames": [
                                "Conrad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bovik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387140"
                        ],
                        "name": "H. Sheikh",
                        "slug": "H.-Sheikh",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Sheikh",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sheikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "2) SSIM: SSIM (Structural Similarity index) [41] is a metric to measure the structural similarity of two images, which is widely used in quality evaluation of the generated images [13], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207761262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eae2e0fa72e898c289365c0af16daf57a7a6cf40",
            "isKey": false,
            "numCitedBy": 30878,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/."
            },
            "slug": "Image-quality-assessment:-from-error-visibility-to-Wang-Bovik",
            "title": {
                "fragments": [],
                "text": "Image quality assessment: from error visibility to structural similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A structural similarity index is developed and its promise is demonstrated through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877311"
                        ],
                        "name": "Mart\u00edn Arjovsky",
                        "slug": "Mart\u00edn-Arjovsky",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Arjovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Arjovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "WGAN uses wasserstein distance [19] and removes the sigmod activation function at the last layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Wasserstein GAN (WGAN) [19] and condition GAN (cGAN)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Wasserstein GAN (WGAN) [19] and condition GAN (cGAN) [20] are novelly proposed to solve this problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2057420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
            "isKey": true,
            "numCitedBy": 3929,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions."
            },
            "slug": "Wasserstein-Generative-Adversarial-Networks-Arjovsky-Chintala",
            "title": {
                "fragments": [],
                "text": "Wasserstein Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This work introduces a new algorithm named WGAN, an alternative to traditional GAN training that can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325946"
                        ],
                        "name": "Yu Han",
                        "slug": "Yu-Han",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153075701"
                        ],
                        "name": "Chen Xu",
                        "slug": "Chen-Xu",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152347"
                        ],
                        "name": "G. Baciu",
                        "slug": "G.-Baciu",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Baciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145625086"
                        ],
                        "name": "Min Li",
                        "slug": "Min-Li",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30664640"
                        ],
                        "name": "Md. Robiul Islam",
                        "slug": "Md.-Robiul-Islam",
                        "structuredName": {
                            "firstName": "Md.",
                            "lastName": "Islam",
                            "middleNames": [
                                "Robiul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Md. Robiul Islam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "It can also be applied to video effect generation [1], [2], cross-domain retrieval [3], [4], semantic feature learning [5], [6] and style transfer [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "tends to modify certain visual properties, such as color [7], texture [8], style [1], or their combinations [1], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Image-to-image translation aims to change images in one domain to another by modifying their properties, such as colors, textures or styles [1], [3], [4], [7]\u2013[9], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "images, such as writing style manipulation [5], fabric color transfer [7], image retargeting [8], semantic classification [26] and semantic segmentation [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30748925,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "73fe6c0223d3cae49567a20cdd4797b5b14603e5",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A color design process for fabric images can resort to a solution of a color transfer problem based on given color themes. Usually, the color transfer process contains an image segmentation phase and an image construction phase. In this paper, a novel color transfer method for fabric images is proposed. Compared with classical color transfer methods, the new method has the following three main innovations. First, the new method, in its image segmentation phase, follows an assumption that a fabric image can be decomposed into cartoon and texture components, which means the new color transfer method, in its image segmentation, phase incorporates an image decomposition process. The advantage of the innovation is that the cartoon component is more suitable than the original image to be used to partition the fabric image. Second, the new color transfer method can generate more vivid color transfer results since the above texture component is used to describe yarn texture details in the image construction phase. Third, the total generalized variation (TGV) regularizer is used to further improve the performance of image decomposition. Here, the TGV regularizer is good at estimating the weak lightness variation of the cartoon component with the CIELab color scheme. In addition, by using the augmented Lagrange multiplier method, we derive an efficient algorithm to search for the solutions to the proposed color transfer problem. Numerical results demonstrate that the proposed color transfer method can generate better results for fabric images."
            },
            "slug": "Cartoon-and-Texture-Decomposition-Based-Color-for-Han-Xu",
            "title": {
                "fragments": [],
                "text": "Cartoon and Texture Decomposition-Based Color Transfer for Fabric Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Numerical results demonstrate that the proposed color transfer method can generate better results for fabric image results by using the augmented Lagrange multiplier method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118259696"
                        ],
                        "name": "You Lei",
                        "slug": "You-Lei",
                        "structuredName": {
                            "firstName": "You",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "You Lei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114108477"
                        ],
                        "name": "W. Yuan",
                        "slug": "W.-Yuan",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7643245"
                        ],
                        "name": "Hongpeng Wang",
                        "slug": "Hongpeng-Wang",
                        "structuredName": {
                            "firstName": "Hongpeng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongpeng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2269293"
                        ],
                        "name": "Wenhu You",
                        "slug": "Wenhu-You",
                        "structuredName": {
                            "firstName": "Wenhu",
                            "lastName": "You",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhu You"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087907122"
                        ],
                        "name": "Wu Bo",
                        "slug": "Wu-Bo",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wu Bo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "2) Encoding Loss: It means that the original image from the source domain and the generated image in the target domain should maintain similar semantic information [26], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "images, such as writing style manipulation [5], fabric color transfer [7], image retargeting [8], semantic classification [26] and semantic segmentation [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1402370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a982ca3fdf740874db2bb4cd04041a014b5d374",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A good skin detector that is capable of capturing skin tones under different conditions is important for human\u2013machine interaction applications. In a general situation, skin detectors, such as skin probability maps or Gaussian mixture models, achieve acceptable skin segmentation results. However, the false positive rate increases significantly when the skin tones are in shadow or when skin-like background objects are under similar illumination. In this paper, we propose a novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks. To overcome the problems encountered in skin segmentation that are caused by different ethnicities and varying illumination conditions, the stacked autoencoders are utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space. Unlike traditional machine learning methods, instead of predicting each pixel individually, our algorithm utilizes blocks to learn the representations and detect the skin areas. The algorithm exploits the learning ability of deep neural networks to learn high-level representations of skin tones. Experiments on test images show that the proposed algorithm achieves acceptable results on several publicly available data sets. To reduce the difficulty of detecting skin pixels in these data sets, the ground truths of these data sets are commonly focused on foreground skin area detection. Our skin detector is also able to detect background areas, as shown in our experiments."
            },
            "slug": "A-Skin-Segmentation-Algorithm-Based-on-Stacked-Lei-Yuan",
            "title": {
                "fragments": [],
                "text": "A Skin Segmentation Algorithm Based on Stacked Autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "This framework is extended by combining VAE [34] and GAN in [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 216078090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "isKey": false,
            "numCitedBy": 16774,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            },
            "slug": "Auto-Encoding-Variational-Bayes-Kingma-Welling",
            "title": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Sequence GAN (SeqGAN) [22] is used to generate sequences by integrating capability of sequence modeling of LSTM networks [23] into GAN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51691,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Compared to traditional image generating models, such as Autoencoder [24], GANs can generate images with richer details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "SEDD integrates the merits of the autoencoder (AE) [24] and GAN [17] structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "In our model, we have one encoder and two decoders, which can be treated as two \u201cautoencoders\u201d [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14641,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2686593"
                        ],
                        "name": "Radim Tylecek",
                        "slug": "Radim-Tylecek",
                        "structuredName": {
                            "firstName": "Radim",
                            "lastName": "Tylecek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radim Tylecek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144953681"
                        ],
                        "name": "R. S\u00e1ra",
                        "slug": "R.-S\u00e1ra",
                        "structuredName": {
                            "firstName": "Radim",
                            "lastName": "S\u00e1ra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S\u00e1ra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "2) Dataset: CMP Facades Dataset [16] is a small-scale dataset of facade images, including 606 images of facades from various places with 12 architectural classes annotated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 86
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which outperforms the state-of-the-art unsupervised methods and achieves the competitive performance as the supervised solution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Experiments conducted on three benchmark datasets, CityScape [14], CelebFace [15] and Facades [16], demonstrate the effectiveness of the proposed BranchGAN, which"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "2) Dataset: CMP Facades Dataset [16] is a small-scale dataset of facade images, including 606 images of facades from"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "Note that the capability of capturing semantic information is not conspicuous on CelebA and Facades datasets."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6060524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0881bc06bef13f1cd645c99dfa66d1e65b295b2e",
            "isKey": true,
            "numCitedBy": 250,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for semantic parsing of images with regular structure. The structured objects are modeled in a densely connected CRF. The paper describes how to embody specific spatial relations in a representation called Spatial Pattern Templates (SPT), which allows us to capture regularity constraints of alignment and equal spacing in pairwise and ternary potentials."
            },
            "slug": "Spatial-Pattern-Templates-for-Recognition-of-with-Tylecek-S\u00e1ra",
            "title": {
                "fragments": [],
                "text": "Spatial Pattern Templates for Recognition of Objects with Regular Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper describes how to embody specific spatial relations in a representation called Spatial Pattern Templates (SPT), which allows us to capture regularity constraints of alignment and equal spacing in pairwise and ternary potentials."
            },
            "venue": {
                "fragments": [],
                "text": "GCPR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436356"
                        ],
                        "name": "Jun-Yan Zhu",
                        "slug": "Jun-Yan-Zhu",
                        "structuredName": {
                            "firstName": "Jun-Yan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-Yan Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150426657"
                        ],
                        "name": "\u62d3\u6d77 \u6749\u5c71",
                        "slug": "\u62d3\u6d77-\u6749\u5c71",
                        "structuredName": {
                            "firstName": "\u62d3\u6d77",
                            "lastName": "\u6749\u5c71",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u62d3\u6d77 \u6749\u5c71"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094770"
                        ],
                        "name": "Phillip Isola",
                        "slug": "Phillip-Isola",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Isola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Isola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Unlike the aforementioned approaches, CycleGAN [12] is an unsupervised method without the need of paired label and does not rely on a specific task, which provides a universal solution for cross-domain image translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "cycle consistency loss by calculating the distance between the original image and the generated image [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "CycleGAN [12] is the state-of-the-art model for unsuper-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Adapted from CycleGAN [12], two techniques are applied to"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Similar to CycleGAN [12] and Pix2Pix [10], FCN-score [40] is adopted as the performance metric for the task of semantic labels\u2192 photo gen-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "To ensure the fairness, the settings of FCN training are kept the same for all tested models [10], [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "L1 norm has been proved useful in keeping parameters sparse, which has been adopted by many GANs, such as CycleGAN [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Recently, unsupervised methods, such as CycleGAN [12], have been proposed to alleviate the problem of limited training data, in which the images from two domains do not have to be aligned in pairs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Unfortunately, unsupervised methods such as CycleGAN [12], can only obtain the label information and do not know the information of its corresponding real photo, which poses a great challenge to find this corresponding mapping."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "DualGAN [31] shares the similar idea of dual learning with CycleGAN [12] to constrain the unsupervised crossdomain translation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195944196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8a5f27e7805f8de84ea008d59452ff864271696",
            "isKey": true,
            "numCitedBy": 7005,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u201cUnpaired-Image-to-Image-Translation-using-Zhu-\u62d3\u6d77",
            "title": {
                "fragments": [],
                "text": "\u201cUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\u201d\u306e\u5b66\u7fd2\u5831\u544a"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Generative Adversarial Networks (GAN) [17], [18] are a kind of particular generative models, which utilize adversarial learning strategies to control the weight updating of the generator and discriminator, until a dynamic equilibrium is constructed to achieve an optimal solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "SEDD integrates the merits of the autoencoder (AE) [24] and GAN [17] structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "The idea is induced from traditional GAN [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generative adversarial networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Adv. Neural Inf. Process. Syst., 2014, vol. 3, pp. 2672\u20132680."
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised cross - domain image generation DualGAN : Unsupervised dual learning for image - to - image translation One - sided unsupervised domain mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IEEE Conf . Comp . Vis . Pattern Recognit ."
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 35,
            "methodology": 28
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/BranchGAN:-Unsupervised-Mutual-Image-to-Image-With-Zhou-Jiang/3560ebaec26312ee0ad36b24416d97ac04168c36?sort=total-citations"
}