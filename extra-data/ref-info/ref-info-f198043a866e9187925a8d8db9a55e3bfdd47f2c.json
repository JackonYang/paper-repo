{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 132
                            }
                        ],
                        "text": "In recent work we have used pairs of LDA modules to model relationships between images and their corresponding descriptive captions (Blei and Jordan, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207561477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473f4b7f8ae2b03dda2593f54b316ff7d55db26b",
            "isKey": false,
            "numCitedBy": 1214,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval."
            },
            "slug": "Modeling-annotated-data-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Modeling annotated data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Three hierarchical probabilistic mixture models which aim to describe annotated data with multiple types, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12129013,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "91e62d27c08db29cf011a0326a61509e574cf772",
            "isKey": false,
            "numCitedBy": 1719,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis\u2014Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors."
            },
            "slug": "Markov-Chain-Sampling-Methods-for-Dirichlet-Process-Neal",
            "title": {
                "fragments": [],
                "text": "Markov Chain Sampling Methods for Dirichlet Process Mixture Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 117
                            }
                        ],
                        "text": "It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1768942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45b4dde8e0945912a39666f2715cdf10a4445b1c",
            "isKey": false,
            "numCitedBy": 537,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets."
            },
            "slug": "Expectation-Propogation-for-the-Generative-Aspect-Minka-Lafferty",
            "title": {
                "fragments": [],
                "text": "Expectation-Propogation for the Generative Aspect Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model, and develops an alternative approach that leads to higher accuracy at comparable cost."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Best choices of \u21b5 and K have strong correlation, as is shown in [5], which chooses hyperparameters as \u21b5 = 50/K, = 0."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15671300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e99f196cf21e0781ef1e119d14e6db45cd71bf3b",
            "isKey": false,
            "numCitedBy": 5573,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. & Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying \u201chot topics\u201d by examining temporal dynamics and tagging abstracts to illustrate semantic content."
            },
            "slug": "Finding-scientific-topics-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Finding scientific topics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A generative model for documents is described, introduced by Blei, Ng, and Jordan, and a Markov chain Monte Carlo algorithm is presented for inference in this model, which is used to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145503401"
                        ],
                        "name": "P. Raghavan",
                        "slug": "P.-Raghavan",
                        "structuredName": {
                            "firstName": "Prabhakar",
                            "lastName": "Raghavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Raghavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145955092"
                        ],
                        "name": "H. Tamaki",
                        "slug": "H.-Tamaki",
                        "structuredName": {
                            "firstName": "Hisao",
                            "lastName": "Tamaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tamaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 253
                            }
                        ],
                        "text": "To substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is useful to develop a generative probabilistic model of text corpora and to study the ability of LSI to recover aspects of the generative model from data (Papadimitriou et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1479546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "546fdb984bd63214dac8f552ef8d8ae6fa7c7d1a",
            "isKey": false,
            "numCitedBy": 1050,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic indexing LSI is an information retrieval technique based on the spectral analysis of the term document matrix whose empirical success had heretofore been without rigorous prediction and explanation We prove that under certain conditions LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance We also propose the technique of random projection as a way of speeding up LSI We complement our theorems with encouraging experimental results We also argue that our results may be viewed in a more general framework as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative ltering"
            },
            "slug": "Latent-semantic-indexing:-a-probabilistic-analysis-Papadimitriou-Raghavan",
            "title": {
                "fragments": [],
                "text": "Latent semantic indexing: a probabilistic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that under certain conditions LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance."
            },
            "venue": {
                "fragments": [],
                "text": "PODS '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50130827"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3032815,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac59a2ec67391bc7dcdd1618a7e9dd95f47b7511",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the relationships between Dirichlet process (DP) based models and allocation models for a variable number of components, based on exchangeable distributions. It is shown that the DP partition distribution is a limiting case of a Dirichlet\u2013multinomial allocation model. Comparisons of posterior performance of DP and allocation models are made in the Bayesian paradigm and illustrated in the context of univariate mixture models. It is shown in particular that the unbalancedness of the allocation distribution, present in the prior DP model, persists a posteriori. Exploiting the model connections, a new MCMC sampler for general DP based models is introduced, which uses split/merge moves in a reversible jump framework. Performance of this new sampler relative to that of some traditional samplers for DP processes is then explored."
            },
            "slug": "Modelling-Heterogeneity-With-and-Without-the-Green-Richardson",
            "title": {
                "fragments": [],
                "text": "Modelling Heterogeneity With and Without the Dirichlet Process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "If we augment the unigram model with a discrete random topic variablez (Figure 3b), we obtain a mixture of unigramsmodel (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": false,
            "numCitedBy": 3071,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 201
                            }
                        ],
                        "text": "In the mixture of unigrams model, overfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the supervised setting, where this model is known as the naive Bayes model (Rennie, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11810036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c515ba762cd5f9a211b501f14550c209d11d4cb3",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification. Thesis Supervisor: Tommi Jaakkola Title: Assistant Professor of Electrical Engineering and Computer Science"
            },
            "slug": "Improving-multi-class-text-classification-with-Rennie",
            "title": {
                "fragments": [],
                "text": "Improving multi-class text classification with Naive Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A theorem is given which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes and a statistics-based framework for text feature selection is developed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735369"
                        ],
                        "name": "J. Sethuraman",
                        "slug": "J.-Sethuraman",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Sethuraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sethuraman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122061045,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0dcd37d10175c30d26152a180092c4258e698d9f",
            "isKey": false,
            "numCitedBy": 2432,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The parameter in a Bayesian nonparametric problem is the unknown distribution P of the observation X. A Bayesian uses a prior distribution for P, and after observing X, solves the statistical inference problem by using the posterior distribution of P, which is the conditional distribution of P given X. For Bayesian nonparametrics to be successful one needs a large class of priors for which posterior distributions can be easily calculated. Unless X takes values in a finite space, the unknown distribution P varies in an infinite dimensional space. Thus one has to talk about measures in a complicated space like the space of all probability measures on a large space. This has always required a more careful attention to the attendant measure theoretic problems. A class of priors known as Dirichlet measures have been used for the distribution of a random variable X when it takes values in R sub K."
            },
            "slug": "A-CONSTRUCTIVE-DEFINITION-OF-DIRICHLET-PRIORS-Sethuraman",
            "title": {
                "fragments": [],
                "text": "A CONSTRUCTIVE DEFINITION OF DIRICHLET PRIORS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50742419"
                        ],
                        "name": "David A. Cohn",
                        "slug": "David-A.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7133953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ba18b2f35515f7f3ad3bc38100730c5808a52af",
            "isKey": false,
            "numCitedBy": 517,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis."
            },
            "slug": "The-Missing-Link-A-Probabilistic-Model-of-Document-Cohn-Hofmann",
            "title": {
                "fragments": [],
                "text": "The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives is described, based on a Probabilistic factor decomposition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "Although the posterior distribution is intractable for exact inference, a wide variety of approximate inference algorithms can be considered for LDA, including Laplace approximation, variational approximation, and Markov chain Monte Carlo (Jordan, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60578841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c9529259e180dea589447d9b7414a998286e1c2",
            "isKey": false,
            "numCitedBy": 1466,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams."
            },
            "slug": "Learning-in-Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents an introduction to inference for Bayesian networks and a view of the EM algorithm that justifies incremental, sparse and other variants, as well as an information-theoretic analysis of hard and soft assignment methods for clustering."
            },
            "venue": {
                "fragments": [],
                "text": "NATO ASI Series"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840562"
                        ],
                        "name": "R. Kass",
                        "slug": "R.-Kass",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kass",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4173210"
                        ],
                        "name": "D. Steffey",
                        "slug": "D.-Steffey",
                        "structuredName": {
                            "firstName": "Duane",
                            "lastName": "Steffey",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Steffey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 234
                            }
                        ],
                        "text": "Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to ashierarchical models(Gelman et al., 1995), or more precisely asconditionally independent hierarchical models(Kass and Steffey, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1995), or more precisely as conditionally independent hierarchical models (Kass and Steffey, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124300843,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "974bf001626115b669aa92f6851d02accd8b9f00",
            "isKey": false,
            "numCitedBy": 445,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider two-stage models of the kind used in parametric empirical Bayes (PEB) methodology, calling them conditionally independent hierarchical models. We suppose that there are k \u201cunits,\u201d which may be experimental subjects, cities, study centers, etcetera. At the first stage, the observation vectors Yi for units i = 1, \u2026, k are independently distributed with densities p(yi | \u03b8i ), or more generally, p(yi | \u03b8i, \u03bb). At the second stage, the unit-specific parameter vectors \u03b8i are iid with densities p(\u03b8i | \u03bb). The PEB approach proceeds by regarding the second-stage distribution as a prior and noting that, if \u03bb were known, inference about \u03b8 could be based on its posterior. Since \u03bb is not known, the simplest PEB methods estimate the parameter \u03bb by maximum likelihood or some variant, and then treat \u03bb as if it were known to be equal to this estimate. Although this procedure is sometimes satisfactory, a well-known defect is that it neglects the uncertainty due to the estimation of \u03bb. In this article w..."
            },
            "slug": "Approximate-Bayesian-Inference-in-Conditionally-Kass-Steffey",
            "title": {
                "fragments": [],
                "text": "Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To address these shortcomings, IR researchers have proposed several other dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "To address these shortcomings, IR researchers have proposed several other dimensionality reduction techniques, most notablylatent semantic indexing (LSI)(Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7010,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 152
                            }
                        ],
                        "text": "The basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 10
                            }
                        ],
                        "text": "Following Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 224
                            }
                        ],
                        "text": "\u2026representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question wereindependentandidentically distributed, conditioned on that parameter."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2073260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6120cc252bc74239012f11b8b075cb7cb16bee26",
            "isKey": false,
            "numCitedBy": 2941,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case."
            },
            "slug": "An-Introduction-to-Variational-Methods-for-Models-Jordan-Ghahramani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Variational Methods for Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields), and describes a general framework for generating variational transformations based on convex duality."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 574041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656859af2ed88cfa23f2bd063c1816a8fc04c47e",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes the use of maximum entropy techniques for text classification. Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform. Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform. The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm. In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document. In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse. Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification."
            },
            "slug": "Using-Maximum-Entropy-for-Text-Classification-Nigam-Lafferty",
            "title": {
                "fragments": [],
                "text": "Using Maximum Entropy for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper uses maximum entropy techniques for text classification by estimating the conditional distribution of the class variable given the document by comparing accuracy to naive Bayes and showing that maximum entropy is sometimes significantly better, but also sometimes worse."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729548"
                        ],
                        "name": "Alexandrin Popescul",
                        "slug": "Alexandrin-Popescul",
                        "structuredName": {
                            "firstName": "Alexandrin",
                            "lastName": "Popescul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandrin Popescul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766638"
                        ],
                        "name": "D. Pennock",
                        "slug": "D.-Pennock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pennock",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pennock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7473074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2f8842663375d47ab8d57e726ee28fc3d88bf5d",
            "isKey": false,
            "numCitedBy": 521,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Recommender systems leverage product and community information to target products to consumers. Researchers have developed collaborative recommenders, content-based recommenders, and a few hybrid systems. We propose a unified probabilistic framework for merging collaborative and content-based recommendations. We extend Hofmarm's (1999) aspect model to incorporate three-way co-occurrence data among users, items, and item content. The relative influence of collaboration data versus content data is not imposed as an exogenous parameter, but rather emerges naturally from the given data sources. However, global probabilistic models coupled with standard EM learning algorithms tend to drastically overfit in the sparsedata situations typical of recommendation applications. We show that secondary content information can often be used to overcome sparsity. Experiments on data from the Researchlndex library of Computer Science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors (k-NN). Global probabilistic models also allow more general inferences than local methods like k-NN."
            },
            "slug": "Probabilistic-Models-for-Unified-Collaborative-and-Popescul-Ungar",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that secondary content information can often be used to overcome sparsity and appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors (k-NN)."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 25532997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee1443888bec216f31b5aadfb1d5e2a28377045",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare the three basic algorithms for model-based clustering on high-dimensional discrete-variable datasets. All three algorithms use the same underlying model: a naive-Bayes model with a hidden root node, also known as a multinomial-mixture model. In the first part of the paper, we perform an experimental comparison between three batch algorithms that learn the parameters of this model: the Expectation\u2013Maximization (EM) algorithm, a \u201cwinner take all\u201d version of the EM algorithm reminiscent of the K-means algorithm, and model-based agglomerative clustering. We find that the EM algorithm significantly outperforms the other methods, and proceed to investigate the effect of various initialization methods on the final solution produced by the EM algorithm. The initializations that we consider are (1) parameters sampled from an uninformative prior, (2) random perturbations of the marginal distribution of the data, and (3) the output of agglomerative clustering. Although the methods are substantially different, they lead to learned models that are similar in quality."
            },
            "slug": "An-Experimental-Comparison-of-Model-Based-Methods-Meil\u0103-Heckerman",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Model-Based Clustering Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper performs an experimental comparison between three batch algorithms for model-based clustering on high-dimensional discrete-variable datasets, and finds that the Expectation\u2013Maximization (EM) algorithm significantly outperforms the other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48455749"
                        ],
                        "name": "J. Dickey",
                        "slug": "J.-Dickey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Dickey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dickey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160100227"
                        ],
                        "name": "Jhy-Ming Jiang",
                        "slug": "Jhy-Ming-Jiang",
                        "structuredName": {
                            "firstName": "Jhy-Ming",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jhy-Ming Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259959"
                        ],
                        "name": "J. Kadane",
                        "slug": "J.-Kadane",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Kadane",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kadane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "It has been used in a Bayesian context for censored discrete data to represent the posterior on\u03b8 which, in that setting, is a random parameter (Dickey et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122893122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10c71e54b22a0cc2218f9fd5eacf40bb9457efa0",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Bayesian methods are given for finite-category sampling when some of the observations suffer missing category distinctions. Dickey's (1983) generalization of the Dirichlet family of prior distributions is found to be closed under such censored sampling. The posterior moments and predictive probabilities are proportional to ratios of B. C. Carlson's multiple hypergeometric functions. Closed-form expressions are developed for the case of nested reported sets, when Bayesian estimates can be computed easily from relative frequencies. Effective computational methods are also given in the general case. An example involving surveys of death-penalty attitudes is used throughout to illustrate the theory. A simple special case of categorical missing data is a two-way contingency table with cross-classified count data xij (i = 1, \u2026, r; j = 1, \u2026, c), together with supplementary trials counted only in the margin distinguishing the rows, yi (i = 1, \u2026, r). There could also be further supplementary trials report..."
            },
            "slug": "Bayesian-Methods-for-Censored-Categorical-Data-Dickey-Jiang",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Censored Categorical Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741460"
                        ],
                        "name": "H. Ishwaran",
                        "slug": "H.-Ishwaran",
                        "structuredName": {
                            "firstName": "Hemant",
                            "lastName": "Ishwaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishwaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40439165"
                        ],
                        "name": "Lancelot F. James",
                        "slug": "Lancelot-F.-James",
                        "structuredName": {
                            "firstName": "Lancelot",
                            "lastName": "James",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lancelot F. James"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12391796,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "45db16309fbf7e9fc8907047e7e1a9933c4e1b85",
            "isKey": false,
            "numCitedBy": 1534,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson\u2013Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a P\u00f3lya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known P\u00f3lya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the P\u00f3lya urn approach and should be simpler for nonexperts to use."
            },
            "slug": "Gibbs-Sampling-Methods-for-Stick-Breaking-Priors-Ishwaran-James",
            "title": {
                "fragments": [],
                "text": "Gibbs Sampling Methods for Stick-Breaking Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors are presented and the blocked Gibbs sampler, based on an entirely different approach that works by directly sampling values from the posterior of the random measure."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48455749"
                        ],
                        "name": "J. Dickey",
                        "slug": "J.-Dickey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Dickey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dickey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "a function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 212
                            }
                        ],
                        "text": "(3) in terms of the model parameters:\np(w |\u03b1,\u03b2) = \u0393(\u2211i \u03b1i) \u220fi \u0393(\u03b1i) \u222b ( k \u220f i=1 \u03b8\u03b1i\u22121i )( N \u220f n=1 k \u2211 i=1 V \u220f j=1 (\u03b8i\u03b2i j )w j n ) d\u03b8,\na function which is intractable due to the coupling between\u03b8 and\u03b2 in the summation over latent topics (Dickey, 1983)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122118902,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1f0f6f848170b7bcf6f42d06989331ae32ca2fb3",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article reviews and interprets recent mathematics of special functions, with emphasis on integral representations of multiple hypergeometric functions. B.C. Carlson's centrally important parameterized functions R and \u211b, initially defined as Dirichlet averages, are expressed as probability-generating functions of mixed multinomial distributions. Various nested families generalizing the Dirichlet distributions are developed for Bayesian inference in multinomial sampling and contingency tables. In the case of many-way tables, this motivates a new generalization of the function \u211b. These distributions are also useful for the modeling of populations of personal probabilities evolving under the process of inference from statistical data. A remarkable new integral identity is adapted from Carlson to represent the moments of quadratic forms under multivariate normal and, more generally, elliptically contoured distributions. This permits the computation of such moments by simple quadrature."
            },
            "slug": "Multiple-Hypergeometric-Functions:-Probabilistic-Dickey",
            "title": {
                "fragments": [],
                "text": "Multiple Hypergeometric Functions: Probabilistic Interpretations and Statistical Uses"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 64
                            }
                        ],
                        "text": "This is essentially an approximation to the scheme described in Heckerman and Meila (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 668084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dddd774bc73acf5ed3d360e291b3265bf069b8b9",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine methods for clustering in high dimensions. In the first part of the paper, we perform an experimental comparison between three batch clustering algorithms: the Expectation-Maximization (EM) algorithm, a \"winner take all\" version of the EM algorithm reminiscent of the K-means algorithm, and model-based hierarchical agglomerative clustering. We learn naive-Bayes models with a hidden root node, using high-dimensional discrete-variable data sets (both real and synthetic). We find that the EM algorithm significantly outperforms the other methods, and proceed to investigate the effect of various initialization schemes on the final solution produced by the EM algorithm. The initializations that we consider are (1) parameters sampled from an uninformative prior, (2) random perturbations of the marginal distribution of the data, and (3) the output of hierarchical agglomerative clustering. Although the methods are substantially different, they lead to learned models that are strikingly similar in quality."
            },
            "slug": "An-Experimental-Comparison-of-Several-Clustering-Meil\u0103-Heckerman",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Several Clustering and Initialization Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper performs an experimental comparison between three batch clustering algorithms: the Expectation-Maximization (EM) algorithm, a \"winner take all\" version of the EM algorithm reminiscent of the K-means algorithm, and model-based hierarchical agglomerative clustering."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 130
                            }
                        ],
                        "text": "We consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, \u03b8, and z (Attias, 2000):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "We consider a variational approach to Bayesian inference that places a separable distribution on the random variables\u03b2, \u03b8, andz (Attias, 2000):\nq(\u03b21:k,z1:M,\u03b81:M |\u03bb,\u03c6,\u03b3) = k\n\u220f i=1\nDir(\u03b2i |\u03bbi) M\n\u220f d=1 qd(\u03b8d,zd |\u03c6d,\u03b3d),\nwhereqd(\u03b8,z|\u03c6,\u03b3) is the variational distribution defined for LDA in Eq."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14399513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c8ee0bc40d8158bba48df860622649a12cabd49",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation."
            },
            "slug": "A-Variational-Baysian-Framework-for-Graphical-Attias",
            "title": {
                "fragments": [],
                "text": "A Variational Baysian Framework for Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models that approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733999"
                        ],
                        "name": "L. Wasserman",
                        "slug": "L.-Wasserman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Wasserman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wasserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60826655,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b3f8348133c1d2f76f1dc1272f748a0b28874d80",
            "isKey": false,
            "numCitedBy": 1373,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "WINNER OF THE 2005 DEGROOT PRIZE! This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level."
            },
            "slug": "All-of-Statistics:-A-Concise-Course-in-Statistical-Wasserman",
            "title": {
                "fragments": [],
                "text": "All of Statistics: A Concise Course in Statistical Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book covers a much wider range of topics than a typical introductory text on mathematical statistics, and includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145779280"
                        ],
                        "name": "J. Carlin",
                        "slug": "J.-Carlin",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38818938"
                        ],
                        "name": "H. Stern",
                        "slug": "H.-Stern",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Stern",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Stern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39775017"
                        ],
                        "name": "D. Dunson",
                        "slug": "D.-Dunson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Dunson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dunson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104170"
                        ],
                        "name": "Aki Vehtari",
                        "slug": "Aki-Vehtari",
                        "structuredName": {
                            "firstName": "Aki",
                            "lastName": "Vehtari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aki Vehtari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to ashierarchical models(Gelman et al., 1995), or more precisely asconditionally independent hierarchical models(Kass and Steffey, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as hierarchical models (Gelman et al., 1995), or more precisely as conditionally independent hierarchical models (Kass and Steffey, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62610127,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8d76672d52622d9c45014d630717ce911d1292ba",
            "isKey": false,
            "numCitedBy": 11106,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter."
            },
            "slug": "Bayesian-Data-Analysis-Gelman-Carlin",
            "title": {
                "fragments": [],
                "text": "Bayesian Data Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Detailed notes on Bayesian Computation Basics of Markov Chain Simulation, Regression Models, and Asymptotic Theorems are provided."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31543015"
                        ],
                        "name": "Thomas J. Jiang",
                        "slug": "Thomas-J.-Jiang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Jiang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas J. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259959"
                        ],
                        "name": "J. Kadane",
                        "slug": "J.-Kadane",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Kadane",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kadane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48455749"
                        ],
                        "name": "J. Dickey",
                        "slug": "J.-Dickey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Dickey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dickey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "De Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question wereindependentandidentically distributed, conditioned on that\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120298812,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3d03b20d932128101f5ed212b867ce22c2d21529",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Carlson's multiple hypergeometric functions arise in Bayesian inference, including methods for multinomial data with missing category distinctions and for local smoothing of histograms. To use these methods one needs to calculate Carlson functions and their ratios. We discuss properties of the functions and explore computational methods for them, including closed form methods, expansion methods, Laplace approximations, and Monte Carlo methods. Examples are given to illustrate and compare methods."
            },
            "slug": "Computation-of-Carlson's-Multiple-Hypergeometric-R-Jiang-Kadane",
            "title": {
                "fragments": [],
                "text": "Computation of Carlson's Multiple Hypergeometric Function R for Bayesian Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2792323"
                        ],
                        "name": "T. Ferguson",
                        "slug": "T.-Ferguson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ferguson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ferguson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9777424,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b8593122c6b1dd2432125a4eeb1a908c280b1d1d",
            "isKey": false,
            "numCitedBy": 4788,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian approach remained rather unsuccessful in treating nonparametric problems. This is primarily due to the difficulty in finding workable prior distribution on the parameter space , which in nonparametric problems is taken to be a set of probability distributions on a given sample space. Two Desirable Properties of a Prior 1. The support of the prior should be large. 2. Posterior distribution given a sample of observation should be manageable analytically. These properties are antagonistic : One may be obtained at the expense of other."
            },
            "slug": "A-Bayesian-Analysis-of-Some-Nonparametric-Problems-Ferguson",
            "title": {
                "fragments": [],
                "text": "A Bayesian Analysis of Some Nonparametric Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31727329"
                        ],
                        "name": "C. Morris",
                        "slug": "C.-Morris",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Morris",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Morris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such models are also often referred to as parametric empirical Bayes models, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 206
                            }
                        ],
                        "text": "Such models are also often referred to asparametric empirical Bayes models, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53601432,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2446ff36818d62c8851e39bef95da9c02235388a",
            "isKey": false,
            "numCitedBy": 1376,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article reviews the state of multiparameter shrinkage estimators with emphasis on the empirical Bayes viewpoint, particularly in the case of parametric prior distributions. Some successful applications of major importance are considered. Recent results concerning estimates of error and confidence intervals are described and illustrated with data."
            },
            "slug": "Parametric-Empirical-Bayes-Inference:-Theory-and-Morris",
            "title": {
                "fragments": [],
                "text": "Parametric Empirical Bayes Inference: Theory and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 200
                            }
                        ],
                        "text": "The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12495425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "231f6de83cfa4d641da1681e97a11b689a48e3aa",
            "isKey": false,
            "numCitedBy": 2251,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method."
            },
            "slug": "Statistical-methods-for-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Statistical methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2291789"
                        ],
                        "name": "S. Ghosal",
                        "slug": "S.-Ghosal",
                        "structuredName": {
                            "firstName": "Subhashis",
                            "lastName": "Ghosal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghosal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144991580"
                        ],
                        "name": "J. Ghosh",
                        "slug": "J.-Ghosh",
                        "structuredName": {
                            "firstName": "Jayanta",
                            "lastName": "Ghosh",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095271626"
                        ],
                        "name": "Tapas Samanta",
                        "slug": "Tapas-Samanta",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Samanta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tapas Samanta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122955416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4f39acc546e051acd676ce94717f1cfb00ab3fb",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A general (asymptotic) theory of estimation was developed by Ibragimov and Has'minskii under certain conditions on the normalized likelihood ratios. In an earlier work, the present authors studied the limiting behaviour of the posterior distributions under the general setup of Ibragimov and Has'minskii. In particular, they obtained a necessary condition for the convergence of a suitably centered (and normalized) posterior to a constant limit in terms of the limiting likelihood ratio process. In this paper, it is shown that this condition is also sufficient to imply the posterior convergence. Some related results are also presented."
            },
            "slug": "On-convergence-of-posterior-distributions-Ghosal-Ghosh",
            "title": {
                "fragments": [],
                "text": "On convergence of posterior distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 80
                            }
                        ],
                        "text": "Treating individual words as features yields a rich but very large feature set (Joachims, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "Using the SVMLight software package (Joachims, 1999), we compared an SVM trained on all the word features with those trained on features induced by a 50-topic LDA model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5546,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934481"
                        ],
                        "name": "K. Doksum",
                        "slug": "K.-Doksum",
                        "structuredName": {
                            "firstName": "Kjell",
                            "lastName": "Doksum",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doksum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120450474,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "92d92fd3de1af6a46e8cd9ca841d5433e659179f",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "(NOTE: Each chapter concludes with Problems and Complements, Notes, and References.) 1. Statistical Models, Goals, and Performance Criteria. Data, Models, Parameters, and Statistics. Bayesian Models. The Decision Theoretic Framework. Prediction. Sufficiency. Exponential Families. 2. Methods of Estimation. Basic Heuristics of Estimation. Minimum Contrast Estimates and Estimating Equations. Maximum Likelihood in Multiparameter Exponential Families. Algorithmic Issues. 3. Measures of Performance. Introduction. Bayes Procedures. Minimax Procedures. Unbiased Estimation and Risk Inequalities. Nondecision Theoretic Criteria. 4. Testing and Confidence Regions. Introduction. Choosing a Test Statistic: The Neyman-Pearson Lemma. Uniformly Most Powerful Tests and Monotone Likelihood Ratio Models. Confidence Bounds, Intervals and Regions. The Duality between Confidence Regions and Tests. Uniformly Most Accurate Confidence Bounds. Frequentist and Bayesian Formulations. Prediction Intervals. Likelihood Ratio Procedures. 5. Asymptotic Approximations. Introduction: The Meaning and Uses of Asymptotics. Consistency. First- and Higher-Order Asymptotics: The Delta Method with Applications. Asymptotic Theory in One Dimension. Asymptotic Behavior and Optimality of the Posterior Distribution. 6. Inference in the Multiparameter Case. Inference for Gaussian Linear Models. Asymptotic Estimation Theory in p Dimensions. Large Sample Tests and Confidence Regions. Large Sample Methods for Discrete Data. Generalized Linear Models. Robustness Properties and Semiparametric Models. Appendix A: A Review of Basic Probability Theory. The Basic Model. Elementary Properties of Probability Models. Discrete Probability Models. Conditional Probability and Independence. Compound Experiments. Bernoulli and Multinomial Trials, Sampling with and without Replacement. Probabilities on Euclidean Space. Random Variables and Vectors: Transformations. Independence of Random Variables and Vectors. The Expectation of a Random Variable. Moments. Moment and Cumulant Generating Functions. Some Classical Discrete and Continuous Distributions. Modes of Convergence of Random Variables and Limit Theorems. Further Limit Theorems and Inequalities. Poisson Process. Appendix B: Additional Topics in Probability and Analysis. Conditioning by a Random Variable or Vector. Distribution Theory for Transformations of Random Vectors. Distribution Theory for Samples from a Normal Population. The Bivariate Normal Distribution. Moments of Random Vectors and Matrices. The Multivariate Normal Distribution. Convergence for Random Vectors: Op and Op Notation. Multivariate Calculus. Convexity and Inequalities. Topics in Matrix Theory and Elementary Hilbert Space Theory. Appendix C: Tables. The Standard Normal Distribution. Auxiliary Table of the Standard Normal Distribution. t Distribution Critical Values. X 2 Distribution Critical Values. F Distribution Critical Values. Index."
            },
            "slug": "Mathematical-Statistics:-Basic-Ideas-and-Selected-Bickel-Doksum",
            "title": {
                "fragments": [],
                "text": "Mathematical Statistics: Basic Ideas and Selected Topics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50528060"
                        ],
                        "name": "J. Pritchard",
                        "slug": "J.-Pritchard",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Pritchard",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pritchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145761702"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Stephens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144427783"
                        ],
                        "name": "P. Donnelly",
                        "slug": "P.-Donnelly",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Donnelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Donnelly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52871542,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fa02f9123abacd5ba13d41e937d99c077da8d3f6",
            "isKey": false,
            "numCitedBy": 28403,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html."
            },
            "slug": "Inference-of-population-structure-using-multilocus-Pritchard-Stephens",
            "title": {
                "fragments": [],
                "text": "Inference of population structure using multilocus genotype data."
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations that can be applied to most of the commonly used genetic markers, provided that they are not closely linked."
            },
            "venue": {
                "fragments": [],
                "text": "Genetics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46701966,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
            "isKey": false,
            "numCitedBy": 12383,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research. Chapter 12 concludes the book with some commentary about the scienti\u008e c contributions of MTS. The Taguchi method for design of experiment has generated considerable controversy in the statistical community over the past few decades. The MTS/MTGS method seems to lead another source of discussions on the methodology it advocates (Montgomery 2003). As pointed out by Woodall et al. (2003), the MTS/MTGS methods are considered ad hoc in the sense that they have not been developed using any underlying statistical theory. Because the \u201cnormal\u201d and \u201cabnormal\u201d groups form the basis of the theory, some sampling restrictions are fundamental to the applications. First, it is essential that the \u201cnormal\u201d sample be uniform, unbiased, and/or complete so that a reliable measurement scale is obtained. Second, the selection of \u201cabnormal\u201d samples is crucial to the success of dimensionality reduction when OAs are used. For example, if each abnormal item is really unique in the medical example, then it is unclear how the statistical distance MD can be guaranteed to give a consistent diagnosis measure of severity on a continuous scale when the larger-the-better type S/N ratio is used. Multivariate diagnosis is not new to Technometrics readers and is now becoming increasingly more popular in statistical analysis and data mining for knowledge discovery. As a promising alternative that assumes no underlying data model, The Mahalanobis\u2013Taguchi Strategy does not provide suf\u008e cient evidence of gains achieved by using the proposed method over existing tools. Readers may be very interested in a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods. Overall, although the idea of MTS/MTGS is intriguing, this book would be more valuable had it been written in a rigorous fashion as a technical reference. There is some lack of precision even in several mathematical notations. Perhaps a follow-up with additional theoretical justi\u008e cation and careful case studies would answer some of the lingering questions."
            },
            "slug": "The-Elements-of-Statistical-Learning-Ziegel",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research, and a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 80
                            }
                        ],
                        "text": "Treating individual words as features yields a rich but very large feature set (Joachims, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "Using the SVMLight software package (Joachims, 1999), we compared an SVM trained on all the word features with those trained on features induced by a 50-topic LDA model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 29
                            }
                        ],
                        "text": "In the populartf-idf scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12606,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145953676"
                        ],
                        "name": "G. Ronning",
                        "slug": "G.-Ronning",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Ronning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ronning"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 85
                            }
                        ],
                        "text": "This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120134200,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "881f487b91e57bebe9f3f2ff0c23319a04fda6d8",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Global concavity of the likelihood function is proved by means of an inequality involving the trigamma function. The computation of maximum likelihood estimates is discussed."
            },
            "slug": "Maximum-likelihood-estimation-of-dirichlet-Ronning",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation of dirichlet distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067836"
                        ],
                        "name": "Martijn A. R. Leisink",
                        "slug": "Martijn-A.-R.-Leisink",
                        "structuredName": {
                            "firstName": "Martijn",
                            "lastName": "Leisink",
                            "middleNames": [
                                "A.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martijn A. R. Leisink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792269"
                        ],
                        "name": "H. Kappen",
                        "slug": "H.-Kappen",
                        "structuredName": {
                            "firstName": "Hilbert",
                            "lastName": "Kappen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kappen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds."
                    },
                    "intents": []
                }
            ],
            "corpusId": 117688573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39442090cff15082ec5254f7c67244ad14e61a60",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we show the rough outline of a computer algorithm to generate lower bounds on the exponential function of (in principle) arbitrary precision. We implemented this to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds of any order. It turns out that the extra variational parameters can be optimized analytically. We show that bounds upto nineth order are still reasonably calculable in practical situations. The generated terms can also be used as extra correction terms (beyond TAP)in mean field expansions."
            },
            "slug": "Computer-generated-higher-order-expansions-Leisink-Kappen",
            "title": {
                "fragments": [],
                "text": "Computer generated higher order expansions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This article implemented a computer algorithm to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds of any order, and it turns out that the extra variational parameters can be optimized analytically."
            },
            "venue": {
                "fragments": [],
                "text": "UAI 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067836"
                        ],
                        "name": "Martijn A. R. Leisink",
                        "slug": "Martijn-A.-R.-Leisink",
                        "structuredName": {
                            "firstName": "Martijn",
                            "lastName": "Leisink",
                            "middleNames": [
                                "A.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martijn A. R. Leisink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792269"
                        ],
                        "name": "H. Kappen",
                        "slug": "H.-Kappen",
                        "structuredName": {
                            "firstName": "Hilbert",
                            "lastName": "Kappen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kappen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5562173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "044ea8333e05f7e75f5393c08f42238bcd72ecdc",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we show the rough outline of a computer algorithm to generate lower bounds on the exponential function of (in principle) arbitrary precision. We implemented this to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds any order. It turns out that the extra variational parameters can be optimized analytically. We show that bounds upto nineth order are still reasonably calculable in practical situations. The gen\u00ad erated terms can also be used as extra cor\u00ad rection terms (beyond TAP) in mean field ex\u00ad pansions."
            },
            "slug": "General-Lower-Bounds-based-on-Computer-Generated-Leisink-Kappen",
            "title": {
                "fragments": [],
                "text": "General Lower Bounds based on Computer Generated Higher Order Expansions"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This article implemented a computer algorithm to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds any order, and it turns out that the extra variational parameters can be optimized analytically."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748956291"
                        ],
                        "name": "M. Abramowitz",
                        "slug": "M.-Abramowitz",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Abramowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abramowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66422945"
                        ],
                        "name": "I. Stegun",
                        "slug": "I.-Stegun",
                        "structuredName": {
                            "firstName": "Irene",
                            "lastName": "Stegun",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Stegun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145871495"
                        ],
                        "name": "David M. Miller",
                        "slug": "David-M.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 235
                            }
                        ],
                        "text": "(7)\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\nEq[log(\u03b8i) |\u03b3] = \u03a8(\u03b3i)\u2212\u03a8 ( \u2211kj=1 \u03b3 j ) , (8)\nwhere\u03a8 is the first derivative of the log\u0393 function which is computable via Taylor approximations (Abramowitz and Stegun, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121782574,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "92d5f6f2d13484c688ca4c08c1279229ba266089",
            "isKey": false,
            "numCitedBy": 1605,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A handbook of mathematical functions that is designed to provide scientific investigations with a comprehensive and self-contained summary of the mathematical functions that arise in physical and engineering problems."
            },
            "slug": "Handbook-of-Mathematical-Functions-With-Formulas,-Abramowitz-Stegun",
            "title": {
                "fragments": [],
                "text": "Handbook of Mathematical Functions With Formulas, Graphs and Mathematical Tables (National Bureau of Standards Applied Mathematics Series No. 55)"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The handbook of mathematical functions with formulas graphs and mathematical tables national bureau of standards applied mathematics series 55 that the authors provide for you will be ultimate to give preference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52024479"
                        ],
                        "name": "B. M. Hill",
                        "slug": "B.-M.-Hill",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Hill",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M. Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46184313"
                        ],
                        "name": "B. D. Finetti",
                        "slug": "B.-D.-Finetti",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Finetti",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Finetti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125007256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f5dc0b76ef9c4eff1cc430c24c4aa41482d3c0f4",
            "isKey": false,
            "numCitedBy": 914,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Probability-Hill-Finetti",
            "title": {
                "fragments": [],
                "text": "Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 235
                            }
                        ],
                        "text": "(7)\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\nEq[log(\u03b8i) |\u03b3] = \u03a8(\u03b3i)\u2212\u03a8 ( \u2211kj=1 \u03b3 j ) , (8)\nwhere\u03a8 is the first derivative of the log\u0393 function which is computable via Taylor approximations (Abramowitz and Stegun, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124717216,
            "fieldsOfStudy": [],
            "id": "16e9c99e1e0290473d8d1b70f03ec0d4cf197d33",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071389378"
                        ],
                        "name": "David J. Aldous",
                        "slug": "David-J.-Aldous",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Aldous",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Aldous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085908183"
                        ],
                        "name": "I. Ibragimov",
                        "slug": "I.-Ibragimov",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Ibragimov",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ibragimov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93983240"
                        ],
                        "name": "J. Jacod",
                        "slug": "J.-Jacod",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Jacod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jacod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405623937"
                        ],
                        "name": "\u00c9cole d'\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour",
                        "slug": "\u00c9cole-d'\u00e9t\u00e9-de-probabilit\u00e9s-de-Saint-Flour",
                        "structuredName": {
                            "firstName": "\u00c9cole",
                            "lastName": "Saint-Flour",
                            "middleNames": [
                                "d'\u00e9t\u00e9",
                                "de",
                                "probabilit\u00e9s",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9cole d'\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46504003"
                        ],
                        "name": "P. Hennequin",
                        "slug": "P.-Hennequin",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Hennequin",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hennequin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119344500,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e21aebdef3c5549b49687c794bace5ad60ba30a3",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u00c9cole-d'\u00e9t\u00e9-de-probabilit\u00e9s-de-Saint-Flour-XIII-Aldous-Ibragimov",
            "title": {
                "fragments": [],
                "text": "\u00c9cole d'\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour XIII - 1983"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071389381"
                        ],
                        "name": "D. Aldous",
                        "slug": "D.-Aldous",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Aldous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Aldous"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "In the language of probability theory, this is an assumption ofexchangeabilityfor the words in a document (Aldous, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118018098,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e858eefcccbcb0c6980a0a1c963ac905400139a1",
            "isKey": false,
            "numCitedBy": 1508,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exchangeability-and-related-topics-Aldous",
            "title": {
                "fragments": [],
                "text": "Exchangeability and related topics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123729051"
                        ],
                        "name": "L. M. M.-T.",
                        "slug": "L.-M.-M.-T.",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "M.-T.",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. M.-T."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "A classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4036480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f1f4386524be3ed96caaf05f661aacb94db1e566",
            "isKey": false,
            "numCitedBy": 5286,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Probability-M.-T.",
            "title": {
                "fragments": [],
                "text": "Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1929
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 67
                            }
                        ],
                        "text": "Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30624137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939a03ff92e92e295002ce01d70ecf50981986c7",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overview-of-the-First-Text-REtrieval-Conference-Harman",
            "title": {
                "fragments": [],
                "text": "Overview of the First Text REtrieval Conference (TREC-1)"
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "Although the posterior distribution is intractable for exact inference, a wide variety of approximate inference algorithms can be considered for LDA, including Laplace approximation, variational approximation, and Markov chain Monte Carlo (Jordan, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long Nguyen (Univ of Michigan) Clustering, mixture models & BNP VIASM, Hanoi 2012 76 / 86 Variations of the same theme Graphical model for dynamic topic modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Example An example article from Science corpusUniv of Michigan) Clustering, mixture models & BNP VIASM"
            },
            "year": 1880
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 126
                            }
                        ],
                        "text": "Significant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report UCB//CSD-02-1202,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to probablistic graphical models. Unpublished text book"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to probablistic graphical models. Unpublished text book"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to probablistic graphical models. Unpublished text book"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to probablistic graphical models. Unpublished text book"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence of posterior distributions.Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 527,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multinomial allocation model (DMA)of [2], which is a finite alternative to the Dirichlet process. The DMA places a mixture of Dirichlet priors on p(wl z ) and sets O i = 00 for all i"
            },
            "venue": {
                "fragments": [],
                "text": "3These remarks also distinguish our model from the Bayesian Dirichlet"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dirichlet processes, Chinese restaurant processes and all that"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2005 Tutorial,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 152
                            }
                        ],
                        "text": "The basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 176
                            }
                        ],
                        "text": "2 Variational inference The basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 10
                            }
                        ],
                        "text": "Following Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to variational methods for graphical models.Machine Learning, 37:183\u2013233"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Appendix A. Inference and parameter estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Appendix A. Inference and parameter estimation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolution of two topics from the dynamic model fitted to the Science archive (1880\u20132002) Long Nguyen (Univ of Michigan) Clustering, mixture models & BNP VIASM"
            },
            "venue": {
                "fragments": [],
                "text": "Hanoi 2012 78 / 86 Incomplete References For Part 1"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "Although the posterior distribution is intractable for exact inference, a wide variety of approximate inference algorithms can be considered for LDA, including Laplace approximation, variational approximation, and Markov chain Monte Carlo (Jordan, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters * and ) for simplicity, we have: log p(w |#,$"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet allocation. Journal of Machine Learning Research"
            },
            "venue": {
                "fragments": [],
                "text": "Latent Dirichlet allocation. Journal of Machine Learning Research"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recent progress on de Finetti's notions of exchangeability"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian statistics"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet allocation,\u201d in Proc"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 110
                            }
                        ],
                        "text": "Significant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "In our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Caenorrhabditis genetic center bibliography"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "All of Statistics: a concise course in statistical inference,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Ghosh & R. Ramamoorthi. Bayesian nonparametrics"
            },
            "venue": {
                "fragments": [],
                "text": "J. Ghosh & R. Ramamoorthi. Bayesian nonparametrics"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abramowitz and I. Stegun, editors. Handbook of Mathematical Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Abramowitz and I. Stegun, editors. Handbook of Mathematical Functions"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using maximum entrop"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exchangeability and related topics. InIn\u00b4In\u00c9cole d'\u00b4 et\u00e9 de probabilit\u00e9s de Saint-Flour"
            },
            "venue": {
                "fragments": [],
                "text": "Exchangeability and related topics. InIn\u00b4In\u00c9cole d'\u00b4 et\u00e9 de probabilit\u00e9s de Saint-Flour"
            },
            "year": 1983
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 22
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 69,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Latent-Dirichlet-Allocation-Blei-Ng/f198043a866e9187925a8d8db9a55e3bfdd47f2c?sort=total-citations"
}