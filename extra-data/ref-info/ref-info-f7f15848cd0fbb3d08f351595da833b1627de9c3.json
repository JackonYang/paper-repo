{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 4
                            }
                        ],
                        "text": "See (Wiberg, 1996; Frey, 1998; McEliece et al., 1998) for further discussion of the sum\u2013product algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 172
                            }
                        ],
                        "text": "For further information about the use of the sum\u2013product algorithm in turbo codes, and the rarely-used but highly recommended stopping criteria for halting their decoding, Frey (1998) is essential reading."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "The definition of Er(R) is given in Gallager (1968), p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62488180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions."
            },
            "slug": "Graphical-Models-for-Machine-Learning-and-Digital-Frey",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Machine Learning and Digital Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions and how this affects research directions is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 56
                            }
                        ],
                        "text": "Gaussian processes and support vector learning machines (Scholkopf et al., 1995; Vapnik, 1995) have a lot in common."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 44
                            }
                        ],
                        "text": "My favourite reading on this topic includes (Jaynes, 1983; Gull, 1988; Loredo, 1990; Berger, 1985; Jaynes, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7326,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 53
                            }
                        ],
                        "text": "The stochastic Hopfield network or Boltzmann machine (Hinton and Sejnowski, 1986) has the following activity rule:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 47
                            }
                        ],
                        "text": "This is the important innovation introduced by Hinton and Sejnowski (1986). The idea is that the high-order correlations among the visible variables are described by including extra hidden variables and sticking to a model that has only second-order interactions between its variables; the hidden variables induce higher-order correlations between the visible variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064612704"
                        ],
                        "name": "David Saad",
                        "slug": "David-Saad",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Saad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 8
                            }
                        ],
                        "text": "(2001), Yedidia et al. (2000c), Yedidia et al. (2000a), Yedidia et al. (2002), Wainwright et al. (2002), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 8
                            }
                        ],
                        "text": "(2001), Yedidia et al. (2000c), Yedidia et al. (2000a), Yedidia et al. (2002), Wainwright et al. (2002), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996). Interesting message-passing algorithms that have different capabilities from the sum\u2013product algorithm include expectation propagation (Minka, 2001) and survey propagation (Braunstein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 8
                            }
                        ],
                        "text": "(2001), Yedidia et al. (2000c), Yedidia et al. (2000a), Yedidia et al. (2002), Wainwright et al. (2002), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125183395,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "22f464167e25d0e38cf38b035fce6ba9fee7a643",
            "isKey": true,
            "numCitedBy": 46,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Inference, Some Models from Statistical Physics, The Gibbs Free Energy, Mean Field Theory: The Variational Approach, Correcting Mean Field Theory, The Bethe Approximation, Belief Propagation, Kikuchi Approximations and Generalized Belief Propagation, Acknowledgments, References"
            },
            "slug": "An-Idiosyncratic-Journey-Beyond-Mean-Field-Theory-Opper-Saad",
            "title": {
                "fragments": [],
                "text": "An Idiosyncratic Journey Beyond Mean Field Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38089959"
                        ],
                        "name": "Mark N. Gibbs",
                        "slug": "Mark-N.-Gibbs",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gibbs",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark N. Gibbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 63
                            }
                        ],
                        "text": "Further reading on such methods can be found in the references (Jaakkola and Jordan, 2000a; Jaakkola and Jordan, 2000b; Jaakkola and Jordan, 1996; Gibbs and MacKay, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14456885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d429b63e0b8e329c565766289b4189c9398174",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a promising nonlinear regression tool, but it is not straightforward to solve classification problems with them. In this paper the variational methods of Jaakkola and Jordan are applied to Gaussian processes to produce an efficient Bayesian binary classifier."
            },
            "slug": "Variational-Gaussian-process-classifiers-Gibbs-Mackay",
            "title": {
                "fragments": [],
                "text": "Variational Gaussian process classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The variational methods of Jaakkola and Jordan are applied to Gaussian processes to produce an efficient Bayesian binary classifier."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720068"
                        ],
                        "name": "E. Soljanin",
                        "slug": "E.-Soljanin",
                        "structuredName": {
                            "firstName": "Emina",
                            "lastName": "Soljanin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Soljanin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549634"
                        ],
                        "name": "E. Offer",
                        "slug": "E.-Offer",
                        "structuredName": {
                            "firstName": "Elke",
                            "lastName": "Offer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Offer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19603737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c49e252154f8ef370d28a330d4f1ba5a71b435d3",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LDPC-codes:-a-group-algebra-formulation-Soljanin-Offer",
            "title": {
                "fragments": [],
                "text": "LDPC codes: a group algebra formulation"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Discret. Math."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16302605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d275cf94e620bf5b3776bba8a88acccdcfcd9a19",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The attempt to find a single \"optimal\" weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "slug": "Bayesian-Learning-via-Stochastic-Dynamics-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning via Stochastic Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Bayesian methods avoid overfitting and poor generalization by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data, by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152251438"
                        ],
                        "name": "J. M. Smith",
                        "slug": "J.-M.-Smith",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Smith",
                            "middleNames": [
                                "Maynard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "Is it reasonable to model fitness, to first order, as a sum of independent terms? Maynard Smith (1968) argues that it is: the more good genes you have, the higher you come in the pecking order, for example."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 864709,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c0d3b376d354695daa3e7a29f5a5243fe2775139",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A recent estimate of the maximum rate of evolution by natural selection may be too low, based as it is on a maxim that seems to be erroneous."
            },
            "slug": "\u201cHaldane's-Dilemma\u201d-and-the-Rate-of-Evolution-Smith",
            "title": {
                "fragments": [],
                "text": "\u201cHaldane's Dilemma\u201d and the Rate of Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A recent estimate of the maximum rate of evolution by natural selection may be too low, based as it is on a maxim that seems to be erroneous."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "These defects are rectified by arithmetic codes, which were invented by Elias, by Rissanen and by Pasco, and subsequently made practical by Witten et al. (1987). In an arithmetic code, the probabilistic modelling is clearly separated from the encoding operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3343393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd23c9168418324e81881365f297fb6a1caa3a07",
            "isKey": false,
            "numCitedBy": 3142,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "slug": "Arithmetic-coding-for-data-compression-Witten-Neal",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding for data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method, which gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123401616,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e5f63913677bda24058e0761ad9dcf346c0d78fc",
            "isKey": false,
            "numCitedBy": 981,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction 1. Likelihood analysis for spatial Gaussian processes 2. Edge correction for spatial point processes 3. Parameter estimation for Gibbsian point processes 4. Modelling spatial images 5. Summarizing binary images."
            },
            "slug": "Statistical-inference-for-spatial-processes-Ripley",
            "title": {
                "fragments": [],
                "text": "Statistical inference for spatial processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 32
                            }
                        ],
                        "text": "Within the geostatistics field, Matheron (1963) proposed a framework for regression using optimal linear estimators which he called \u2018kriging\u2019 after D.G. Krige, a South African mining engineer. This framework is identical to the Gaussian process approach to regression. Kriging has been developed considerably in the last thirty years (see Cressie (1993) for a review) including several Bayesian treatments (Omre, 1987; Kitanidis, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 32
                            }
                        ],
                        "text": "Within the geostatistics field, Matheron (1963) proposed a framework for regression using optimal linear estimators which he called \u2018kriging\u2019 after D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2333969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "599e2a3bb312d4e7ac9ddf77ce50f3333b29a762",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis."
            },
            "slug": "Choice-of-Basis-for-Laplace-Approximation-Mackay",
            "title": {
                "fragments": [],
                "text": "Choice of Basis for Laplace Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143732395"
                        ],
                        "name": "C. Holmes",
                        "slug": "C.-Holmes",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Holmes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789565"
                        ],
                        "name": "B. Mallick",
                        "slug": "B.-Mallick",
                        "structuredName": {
                            "firstName": "Bani",
                            "lastName": "Mallick",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mallick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14553207,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7b30fdffa96316bd4cd710c40f689e9a07246a44",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we demonstrate how to generate independent and identically distributed samples from the model space of the Bayes linear model with orthogonal predictors. We use the method of coupled Markov chains from the past as introduced by Propp and Wilson (1996). This procedure alleviates any concerns over convergence and sample mixing. We present a number of examples including a perfect simulation of Bayesian wavelet selection in a 1024 dimensional model space, a knot selection problem for spline smoothers and, a standard linear regression variable selection problem."
            },
            "slug": "Perfect-Simulation-for-orthogonal-model-mixing-Holmes-Mallick",
            "title": {
                "fragments": [],
                "text": "Perfect Simulation for orthogonal model mixing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A number of examples are presented including a perfect simulation of Bayesian wavelet selection in a 1024 dimensional model space, a knot selection problem for spline smoothers and, a standard linear regression variable selection problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 45
                            }
                        ],
                        "text": "This chapter\u2019s material is originally due to Polya (1954) and Cover (1965) and the exposition that follows is Yaser Abu-Mostafa\u2019s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60835229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cb516690edbb1875dc3a5d4adc380cf5901f23e",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian probability theory provides a unifying framework for data modeling. In this framework, the overall aims are to find models that are well matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers, and weight decay constants) also then can be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This provides powerful and practical methods for controlling, comparing, and using adaptive network models. This chapter describes numerical techniques based on Gaussian approximations for implementation of these methods."
            },
            "slug": "Bayesian-Methods-for-Backpropagation-Networks-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This chapter describes numerical techniques based on Gaussian approximations for implementation of powerful and practical methods for controlling, comparing, and using adaptive network models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19920501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c8293e7054230cc6cc6e3172f761d89d267f7a7",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning algorithms have been used both on feed-forward deterministic networks and on feed-back statistical networks to capture input-output relations and do pattern classification. These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers. In simple but nontrivial networks the two learning rules are closely related. Under some circumstances the learning problem for the statistical networks can be solved without Monte Carlo procedures. The usual arbitrary learning goals of feed-forward networks can be given useful probabilistic meaning."
            },
            "slug": "Learning-algorithms-and-probability-distributions-Hopfield",
            "title": {
                "fragments": [],
                "text": "Learning algorithms and probability distributions in feed-forward and feed-back networks."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers, in simple but nontrivial networks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46561670"
                        ],
                        "name": "B. Mandelbrot",
                        "slug": "B.-Mandelbrot",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Mandelbrot",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mandelbrot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Mandelbrot\u2019s (1982) modification of Zipf\u2019s law introduces a third parameter v, asserting that the probabilities are given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 57
                            }
                        ],
                        "text": ") In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over nonlinear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 728,
                                "start": 57
                            }
                        ],
                        "text": ") In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over nonlinear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes. The hyperparameters of the neural network model determine the characteristic lengthscales of the Gaussian process. Neal\u2019s observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes. Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process. In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997b), Barber and Williams (1997) and Gibbs and MacKay (2000), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43776837,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "d9c33f310c8af5dbdbc79d1609d3e1bc45180847",
            "isKey": true,
            "numCitedBy": 17996,
            "numCiting": 357,
            "paperAbstract": {
                "fragments": [],
                "text": "\"...a blend of erudition (fascinating and sometimes obscure historical minutiae abound), popularization (mathematical rigor is relegated to appendices) and exposition (the reader need have little knowledge of the fields involved) ...and the illustrations include many superb examples of computer graphics that are works of art in their own right.\" Nature"
            },
            "slug": "Fractal-Geometry-of-Nature-Mandelbrot",
            "title": {
                "fragments": [],
                "text": "Fractal Geometry of Nature"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book is a blend of erudition, popularization, and exposition, and the illustrations include many superb examples of computer graphics that are works of art in their own right."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 51
                            }
                        ],
                        "text": "In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997b), Barber and Williams (1997) and Gibbs and MacKay (2000), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 132
                            }
                        ],
                        "text": "This observation motivated the idea of replacing supervised neural networks by Gaussian processes, a research direction explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparision of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 132
                            }
                        ],
                        "text": "This observation motivated the idea of replacing supervised neural networks by Gaussian processes, a research direction explored by Williams and Rasmussen (1996) and Neal (1997b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 340,
                                "start": 220
                            }
                        ],
                        "text": "From left to right: Irregular low\u2013density parity\u2013 check code over GF (8), blocklength 48000 bits (Davey, 1999); JPL turbo code (JPL, 1996) blocklength 65536; Regular LDPCC over GF (16), blocklength 24448 bits (Davey and MacKay, 1998); Irregular binary low\u2013density parity\u2013check code, blocklength 16000 bits (Davey, 1999); Luby et al. (1998) irregular binary low\u2013density parity\u2013check code, blocklength 64000 bits; JPL code for Galileo (in 1992, this was the best known code of rate 1/4); Regular binary low\u2013density parity\u2013check code: blocklength 40000 bits (MacKay, 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 79
                            }
                        ],
                        "text": "Or we can perform the integration over \u0398 numerically using Monte Carlo methods (Williams and Rasmussen, 1996; Neal, 1997b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2877073,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
            "isKey": true,
            "numCitedBy": 1160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results."
            },
            "slug": "Gaussian-Processes-for-Regression-Williams-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 9
                            }
                        ],
                        "text": "(2000c); Welling and Teh (2001); Yuille (2001); Yedidia et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 93
                            }
                        ],
                        "text": "The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparision of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996). Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 93
                            }
                        ],
                        "text": "The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 93
                            }
                        ],
                        "text": "The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparision of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3175219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "973cd47a48c79209aac17b24594636361941a051",
            "isKey": true,
            "numCitedBy": 111,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel inference algorithm for arbitrary, binary, undirected graphs. Unlike loopy belief propagation, which iterates fixed point equations, we directly descend on the Bethe free energy. The algorithm consists of two phases, first we update the pairwise probabilities, given the marginal probabilities at each unit, using an analytic expression. Next, we update the marginal probabilities, by following the negative gradient of the Bethe free energy. Both steps are guaranteed to decrease the Bethe free energy, and since it is lower bounded, the algorithm is guaranteed to converge to a local minimum. We also show that the Bethe free energy is equal to the TAP free energy up to second order in the weights. In experiments we confirm that when belief propagation converges it usually finds identical solutions as our belief optimization method. The stable nature of belief optimization makes it ideally suited for learning graphical models from data."
            },
            "slug": "Belief-Optimization-for-Binary-Networks:-A-Stable-Welling-Teh",
            "title": {
                "fragments": [],
                "text": "Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel inference algorithm for arbitrary, binary, undirected graphs that directly descend on the Bethe free energy, which is ideally suited for learning graphical models from data."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14332165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks."
            },
            "slug": "Probable-networks-and-plausible-predictions-a-of-Mackay",
            "title": {
                "fragments": [],
                "text": "Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735471"
                        ],
                        "name": "S. Litsyn",
                        "slug": "S.-Litsyn",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Litsyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Litsyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144554219"
                        ],
                        "name": "V. Shevelev",
                        "slug": "V.-Shevelev",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Shevelev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Shevelev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 92
                            }
                        ],
                        "text": "This calculation can also be carried out for the ensemble of low-density parity-check codes (Gallager, 1963; MacKay, 1999b; Litsyn and Shevelev, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6215918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b98bb6ed9db5e7f613908d3c3a0b803d2188f8f",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive expressions for the average distance distributions in several ensembles of regular low-density parity-check codes (LDPC). Among these ensembles are the standard one defined by matrices having given column and row sums, ensembles defined by matrices with given column sums or given row sums, and an ensemble defined by bipartite graphs."
            },
            "slug": "On-ensembles-of-low-density-parity-check-codes:-Litsyn-Shevelev",
            "title": {
                "fragments": [],
                "text": "On ensembles of low-density parity-check codes: Asymptotic distance distributions"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 96
                            }
                        ],
                        "text": "In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997b), Barber and Williams (1997) and Gibbs and MacKay (2000), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 0
                            }
                        ],
                        "text": "Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations (Chapter 27). Neal (1997b) has implemented a Monte Carlo approach to implementing a Gaussian process classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 0
                            }
                        ],
                        "text": "Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations (Chapter 27). Neal (1997b) has implemented a Monte Carlo approach to implementing a Gaussian process classifier. Gibbs and MacKay (2000) have implemented another cheap and cheerful approach based on the methods of Jaakkola and Jordan (section 33."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations (Chapter 27)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2931477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5804ac03602286c830a89713e49353956d56b2a",
            "isKey": true,
            "numCitedBy": 89,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The full Bayesian method for applying neural networks to a prediction problem is to set up the prior/hyperprior structure for the net and then perform the necessary integrals. However, these integrals are not tractable analytically, and Markov Chain Monte Carlo (MCMC) methods are slow, especially if the parameter space is high-dimensional. Using Gaussian processes we can approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods. We have applied this idea to classification problems, obtaining excellent results on the real-world problems investigated so far."
            },
            "slug": "Gaussian-Processes-for-Bayesian-Classification-via-Barber-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Gaussian processes are used to approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35632583"
                        ],
                        "name": "G. P\u00f3lya",
                        "slug": "G.-P\u00f3lya",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "P\u00f3lya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. P\u00f3lya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117891694,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "355dd143dd9d756d533666e120a5db6d86dcde81",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A guide to the practical art of plausible reasoning, this book has relevance in every field of intellectual activity. Professor Polya, a world-famous mathematician from Stanford University, uses mathematics to show how hunches and guesses play an important part in even the most rigorously deductive science. He explains how solutions to problems can be guessed at; good guessing is often more important than rigorous deduction in finding correct solutions. Vol. I, on \"Induction and Analogy in Mathematics,\" covers a wide variety of mathematical problems, revealing the trains of thought that lead to solutions, pointing out false bypaths, discussing techniques of searching for proofs. Problems and examples challenge curiosity, judgment, and power of invention."
            },
            "slug": "Induction-and-Analogy-in-Mathematics-P\u00f3lya",
            "title": {
                "fragments": [],
                "text": "Induction and Analogy in Mathematics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Professor Polya uses mathematics to show how hunches and guesses play an important part in even the most rigorously deductive science, explaining how solutions to problems can be guessed at; good guessing is often more important than rigorous deduction in finding correct solutions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1001953,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "5079ea296646fbbb0c1ceb8bbadf86c698c842ef",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the \"right\" number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling."
            },
            "slug": "The-Infinite-Gaussian-Mixture-Model-Rasmussen",
            "title": {
                "fragments": [],
                "text": "The Infinite Gaussian Mixture Model"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents an infinite Gaussian mixture model which neatly sidesteps the difficult problem of finding the \"right\" number of mixture components and uses an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8981636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e597460557d44de07ec570738cd2b42cdcc2580",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is O(n) and the cost to compute confidence bounds is O(nm), where n \u226a m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems."
            },
            "slug": "Sparse-Greedy-Gaussian-Process-Regression-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m, and shows applications to large scale problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076280104"
                        ],
                        "name": "J. W. Miskin",
                        "slug": "J.-W.-Miskin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Miskin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Miskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 79
                            }
                        ],
                        "text": "A variational free energy minimization approach to ICA-like models is given in (Miskin, 2001; Miskin and MacKay, 2000; Miskin and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117794182,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "824f112c4a1c7b1d8955bc61441da43d6c75998f",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter, ensemble learning is applied to the problem of blind source separation and deconvolution of images. It is assumed that the observed images were constructed by mixing a set of images (consisting of independent, identically distributed pixels), convolving the mixtures with unknown blurring filters and then adding Gaussian noise."
            },
            "slug": "Ensemble-Learning-for-Blind-Image-Separation-and-Miskin-Mackay",
            "title": {
                "fragments": [],
                "text": "Ensemble Learning for Blind Image Separation and Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "In this chapter, ensemble learning is applied to the problem of blind source separation and deconvolution of images by convolving mixtures with unknown blurring filters and then adding Gaussian noise."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18112763"
                        ],
                        "name": "F. Reif",
                        "slug": "F.-Reif",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Reif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Reif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715631"
                        ],
                        "name": "S. Rice",
                        "slug": "S.-Rice",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Rice",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 35
                            }
                        ],
                        "text": "One approach based on the ideas of Skilling (1993) makes approximations to C\u22121t and TraceC\u22121 using iterative methods with cost O(N 2) (Gibbs and MacKay, 1996; Gibbs, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118835488,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "e3e2901a80d7f663fc4ff7cf89456cf79d040e6e",
            "isKey": false,
            "numCitedBy": 3398,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is designed for the junior-senior thermodynamics course given in all departments as a standard part of the curriculum. The book is devoted to a discussion of some of the basic physical concepts and methods useful in the description of situations involving systems which consist of very many particulars. It attempts, in particular, to introduce the reader to the disciplines of thermodynamics, statistical mechanics, and kinetic theory from a unified and modern point of view. The presentation emphasizes the essential unity of the subject matter and develops physical insight by stressing the microscopic content of the theory."
            },
            "slug": "Fundamentals-of-Statistical-and-Thermal-Physics-Reif-Rice",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Statistical and Thermal Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720405"
                        ],
                        "name": "J. Ziv",
                        "slug": "J.-Ziv",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50154247"
                        ],
                        "name": "A. Lempel",
                        "slug": "A.-Lempel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Lempel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lempel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 55
                            }
                        ],
                        "text": "gzip is based on a version of Lempel\u2013Ziv called \u2018LZ77\u2019 (Ziv and Lempel, 1977)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9267632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c9f2036e673d8bc9713eed851d12c6c9fe53cb",
            "isKey": false,
            "numCitedBy": 5369,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source."
            },
            "slug": "A-universal-algorithm-for-sequential-data-Ziv-Lempel",
            "title": {
                "fragments": [],
                "text": "A universal algorithm for sequential data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable- to-block codes designed to match a completely specified source."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 118
                            }
                        ],
                        "text": "It is an area of active research to create models that embody the same capabilities using more efficient computations (Hinton et al., 1995; Dayan et al., 1995; Hinton and Ghahramani, 1997; Hinton, 2001; Hinton and Teh, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9951467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations."
            },
            "slug": "Discovering-Multiple-Constraints-that-are-Satisfied-Hinton-Teh",
            "title": {
                "fragments": [],
                "text": "Discovering Multiple Constraints that are Frequently Approximately Satisfied"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes three methods of learning products of constraints using a heavy-tailed probability distribution for the violations of constraint violations."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 75
                            }
                        ],
                        "text": "Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17285553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9ae39a71308a0bfe12fd5c1ba13165547be3cbd",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of error-correcting codes for the binary symmetric channel. These codes are designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically. The decoding problem involves only very sparse matrices and sparse vectors, and so is a promising candidate for practical decoding."
            },
            "slug": "Good-Codes-Based-on-Very-Sparse-Matrices-Mackay",
            "title": {
                "fragments": [],
                "text": "Good Codes Based on Very Sparse Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new family of error-correcting codes for the binary symmetric channel is presented, designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically."
            },
            "venue": {
                "fragments": [],
                "text": "IMACC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16512130"
                        ],
                        "name": "J. Raviv",
                        "slug": "J.-Raviv",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Raviv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Raviv"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28594190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b51c6a5610be2c5648d1476b6f70e8037e0e8cb8",
            "isKey": false,
            "numCitedBy": 6485,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived."
            },
            "slug": "Optimal-decoding-of-linear-codes-for-minimizing-Bahl-Cocke",
            "title": {
                "fragments": [],
                "text": "Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered and an optimal decoding algorithm is derived."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401345"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Kennedy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 35
                            }
                        ],
                        "text": "The Hamiltonian Monte Carlo method (Duane et al., 1987) is reviewed in Neal (1993b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 36
                            }
                        ],
                        "text": "The Hamiltonian Monte Carlo method (Duane et al., 1987) is reviewed in Neal (1993b). This excellent tome also reviews a huge range of other Monte Carlo methods, including the related topics of simulated annealing and free energy estimation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121101759,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "22ea20339015130099017185e7f36e87933c6a43",
            "isKey": false,
            "numCitedBy": 2583,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hybrid-Monte-Carlo-Kennedy",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 287
                            }
                        ],
                        "text": "Instead of using several models (differing in complexity, for example) and evaluating their relative posterior probabilities, one can make a single hierarchical model having, for example, various continuous hyperparameters which play a role similar to that played by the distinct models (Neal, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 117
                            }
                        ],
                        "text": "Such an input could be said to be irrelevant, as in the automatic relevance determination method for neural networks (MacKay, 1994a; Neal, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 253
                            }
                        ],
                        "text": "For example, if we believe that some input variables in a problem may be irrelevant to the predicted quantity, but we don\u2019t know which, we can define a new model with multiple hyperparameters that captures the idea of uncertain input variable relevance (MacKay, 1994b; Neal, 1996; MacKay, 1995b); these models then infer automatically from the data which are the relevant input variables for a problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 181
                            }
                        ],
                        "text": "As was mentioned in Chapter 41, Bayesian inference for multilayer networks may be implemented by Monte Carlo sampling, or by deterministic methods employing Gaussian approximations (Neal, 1996; MacKay, 1992c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": true,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2358562"
                        ],
                        "name": "Eric Wefald",
                        "slug": "Eric-Wefald",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wefald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Wefald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 252
                            }
                        ],
                        "text": "In a realistic problem such as playing a board game, the tree of possible cogitations and actions that must be considered becomes enormous, and \u2018doing the right thing\u2019 is not simple, because the expected utility of an action cannot be computed exactly (Russell and Wefald, 1991; Baum and Smith, 1993; Baum and Smith, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29045704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f47c8eb5d7a9f6820dae6dc8034dc03d6f9666d",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Limited rationality execution architectures for decision procedures metareasoning architecture rational metareasoning application to game playing application to problem solving search learning the value of computation toward limited rational agents."
            },
            "slug": "Do-the-right-thing-studies-in-limited-rationality-Russell-Wefald",
            "title": {
                "fragments": [],
                "text": "Do the right thing - studies in limited rationality"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Limited rationality execution architectures for decision procedures metareasoning architecture rational metareASONing application to game playing application to problem solving search learning the value of computation toward limited rational agents."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 100
                            }
                        ],
                        "text": "A generalized channel coding theorem, including a cost function for the inputs, can be proved \u2013 see McEliece (1977). The result is a channel capacity C(v\u0304) that is a function of the permitted cost."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60000416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6b51ab12e7a84c1f7a56c0b11bba1944628e0fa",
            "isKey": false,
            "numCitedBy": 729,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Entropy and mutual information 2. Discrete memoryless channels and their capacity-cost functions 3. Discrete memoryless sources and their rate-distortion functions 4. The Gaussian channel and source 5. The source-channel coding theorem 6. Survey of advanced topics for part I 7. Linear codes 8. BCH Goppa, and related codes 9. Convolutional codes 10. Variable-length source coding 11. Survey of advanced topics for part II."
            },
            "slug": "Theory-of-Information-and-Coding-McEliece",
            "title": {
                "fragments": [],
                "text": "Theory of Information and Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This chapter discusses discrete memoryless channels and their capacity-cost functions, and the source-channel coding theorem, which addresses the problem of variable-length source coding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 35
                            }
                        ],
                        "text": "One approach based on the ideas of Skilling (1993) makes approximations to C\u22121t and TraceC\u22121 using iterative methods with cost O(N2) (Gibbs and MacKay, 1996; Gibbs, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118543247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b35e86ce2b9947c23a13fed84cb66ae84bc2f6d",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction. The argument repeatedly goes from the particular to the general, in that if there are general theories then they must apply to special cases. Two such special cases, formalised as the \u201cCox axioms \u201c, lead to the well-known fact that Bayesian probability theory is the only consistent language of inference. Further cases, formalised as the axioms of maximum entropy, show that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy. Finally, a quantified special case shows that this monotonic function must be the exponential, leaving only a single dimensional scaling factor to be determined a posteriori. Many types of distribution, including probability distributions themselves, are positive and additive, so the entropy exponential is very general."
            },
            "slug": "Classic-Maximum-Entropy-Skilling",
            "title": {
                "fragments": [],
                "text": "Classic Maximum Entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction, formalised as the axioms ofmaximum entropy, which shows that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 35
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144977509"
                        ],
                        "name": "Jin Shin",
                        "slug": "Jin-Shin",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109828882"
                        ],
                        "name": "Sang Joon Kim",
                        "slug": "Sang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Kim",
                            "middleNames": [
                                "Joon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang Joon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5747983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d12a1d23b21a9b170118a56386552bc5d4727de",
            "isKey": false,
            "numCitedBy": 47465,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper opened the new area the information theory. Before this paper, most people believed that the only way to make the error probability of transmission as small as desired is to reduce the data rate (such as a long repetition scheme). However, surprisingly this paper revealed that it does not need to reduce the data rate for achieving that much of small errors. It proved that we can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate, which means we cannot achieve the data rate with any encoding scheme that has small enough error probability over the upper bound."
            },
            "slug": "A-Mathematical-Theory-of-Communication-Shin-Kim",
            "title": {
                "fragments": [],
                "text": "A Mathematical Theory of Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is proved that the authors can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate, which means they cannot achieve the data rates with any encoding scheme that has small enough error probability over the upper bound."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 113
                            }
                        ],
                        "text": "A convenient software package for automatic implementation of variational inference in graphical models is VIBES (Bishop et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10346767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "422563a11d1115d70e1b8d2c6cdea6a59a1d25cb",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of probabilistic models. For each new application, however, it is currently necessary first to derive the variational update equations, and then to implement them in application-specific code. Each of these steps is both time consuming and error prone. In this paper we describe a general purpose inference engine called VIBES ('Variational Inference for Bayesian Networks') which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding. New models are specified either through a simple script or via a graphical interface analogous to a drawing package. VIBES then automatically generates and solves the variational equations. We illustrate the power and flexibility of VIBES using examples from Bayesian mixture modelling."
            },
            "slug": "VIBES:-A-Variational-Inference-Engine-for-Bayesian-Bishop-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "VIBES: A Variational Inference Engine for Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A general purpose inference engine called VIBES ('Variational Inference for Bayesian Networks') which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See Pearl (2000) for discussion of many other aspects of causality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16406992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01c3188460d25219433c2dc28629d61b18970d54",
            "isKey": false,
            "numCitedBy": 2319,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "We report theoretical and empirical properties of Gallager's (1963) low density parity check codes on Gaussian channels. It can be proved that, given an optimal decoder, these codes asymptotically approach the Shannon limit. With a practical 'belief propagation' decoder, performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Good-error-correcting-codes-based-on-very-sparse-Mackay",
            "title": {
                "fragments": [],
                "text": "Good Error-Correcting Codes Based on Very Sparse Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "It can be proved that, given an optimal decoder, Gallager's low density parity check codes asymptotically approach the Shannon limit."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52835993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8921b3462a3575b0b5de602a975bd608f6f6652",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": "Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the \"Bethe method\", the \"junction graph method\", the \"cluster variation method\", and the \"region graph method\". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP."
            },
            "slug": "Constructing-free-energy-approximations-and-belief-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Constructing free-energy approximations and generalized belief propagation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work explains how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms, and describes empirical results showing that GBP can significantly outperform BP."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 365,
                                "start": 256
                            }
                        ],
                        "text": "The approximation of posterior probability distributions using variational free energy minimization provides a useful approach to approximating Bayesian inference in a number of fields ranging from neural networks to the decoding of error-correcting codes (Hinton and van Camp, 1993; Hinton and Zemel, 1994; Dayan et al., 1995; Neal and Hinton, 1998; MacKay, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 70
                            }
                        ],
                        "text": "Reinforcement learning addresses approximate methods for this problem (Sutton and Barto, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9166388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "isKey": false,
            "numCitedBy": 32847,
            "numCiting": 636,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning."
            },
            "slug": "Reinforcement-Learning:-An-Introduction-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides a clear and simple account of the key ideas and algorithms of reinforcement learning, which ranges from the history of the field's intellectual foundations to the most recent developments and applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 135
                            }
                        ],
                        "text": "Interesting message-passing algorithms that have different capabilities from the sum\u2013product algorithm include expectation propagation (Minka, 2001) and survey propagation (Braunstein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8632802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf527ca11d7d81a15ff5b5603374a4e9d53b55b6",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, \u201cExpectation Propagation,\u201d unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. \nLoopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction\u2014propagating richer belief states which incorporate correlations between variables. \nThis framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "A-family-of-algorithms-for-approximate-Bayesian-Minka",
            "title": {
                "fragments": [],
                "text": "A family of algorithms for approximate Bayesian inference"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible, and is found to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2524747"
                        ],
                        "name": "E. Ratzer",
                        "slug": "E.-Ratzer",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Ratzer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ratzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14248626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c047c20bb980d4132b705b8d4909557a1bdb54a6",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Low-density parity-check codes are modified to produce transmissions in which the symbols 1 and 0 are used with different frequencies. These codes are good candidates for multiuser channels with cross-talk, such as optical channels."
            },
            "slug": "Sparse-low-density-parity-check-codes-for-channels-Ratzer-Mackay",
            "title": {
                "fragments": [],
                "text": "Sparse low-density parity-check codes for channels with cross-talk"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Low-density parity-check codes are modified to produce transmissions in which the symbols 1 and 0 are used with different frequencies, which are good candidates for multiuser channels with cross-talk, such as optical channels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2003 IEEE Information Theory Workshop (Cat. No.03EX674)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17404261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145512a08a7cd79a0efb1f0503ddc6a4e4ef02dc",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the \"support\" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods."
            },
            "slug": "Fast-Forward-Selection-to-Speed-Up-Sparse-Gaussian-Seeger-Williams",
            "title": {
                "fragments": [],
                "text": "Fast Forward Selection to Speed Up Sparse Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection, which leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47640603"
                        ],
                        "name": "P. Freeman",
                        "slug": "P.-Freeman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Freeman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 76
                            }
                        ],
                        "text": "In simple Gaussian cases it is possible to solve for this optimal precision (Wallace and Freeman, 1987), and it is closely related to the posterior error bars on the parameters, A\u22121, where A = \u2212\u2207\u2207 lnP (w |D,H)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118095811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04eb446825da7a4c2ab3fa6df7ebd377baa66ebe",
            "isKey": false,
            "numCitedBy": 598,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random. The encoded form has two parts. The first states the inferred estimates of the unknown parameters in the model, the second states the data using an optimal code based on the data probability distribution implied by those parameter estimates. Choosing the model and the estimates that give the most compact coding leads to an interesting general inference procedure. In its strict form it has great generality and several nice properties but is computationally infeasible. An approximate form is developed and its relation to other methods is explored."
            },
            "slug": "Estimation-and-Inference-by-Compact-Coding-Wallace-Freeman",
            "title": {
                "fragments": [],
                "text": "Estimation and Inference by Compact Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712569"
                        ],
                        "name": "A. Braunstein",
                        "slug": "A.-Braunstein",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Braunstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Braunstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338183"
                        ],
                        "name": "M. M\u00e9zard",
                        "slug": "M.-M\u00e9zard",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "M\u00e9zard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9zard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719010"
                        ],
                        "name": "R. Zecchina",
                        "slug": "R.-Zecchina",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Zecchina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zecchina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 172
                            }
                        ],
                        "text": "Interesting message-passing algorithms that have different capabilities from the sum\u2013product algorithm include expectation propagation (Minka, 2001) and survey propagation (Braunstein et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6601396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c800d18a9fb1b21c492a643743e4fef32d19b0a6",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the satisfiability of randomly generated formulas formed by M clauses of exactly K literals over N Boolean variables. For a given value of N the problem is known to be most difficult when \u03b1 = M/N is close to the experimental threshold \u03b1c separating the region where almost all formulas are SAT from the region where all formulas are UNSAT. Recent results from a statistical physics analysis suggest that the difficulty is related to the existence of a clustering phenomenon of the solutions when \u03b1 is close to (but smaller than) \u03b1c. We introduce a new type of message passing algorithm which allows to find efficiently a satisfying assignment of the variables in this difficult region. This algorithm is iterative and composed of two main parts. The first is a message\u2010passing procedure which generalizes the usual methods like Sum\u2010Product or Belief Propagation: It passes messages that may be thought of as surveys over clusters of the ordinary messages. The second part uses the detailed probabilistic information obtained from the surveys in order to fix variables and simplify the problem. Eventually, the simplified problem that remains is solved by a conventional heuristic. \u00a9 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2005"
            },
            "slug": "Survey-propagation:-An-algorithm-for-satisfiability-Braunstein-M\u00e9zard",
            "title": {
                "fragments": [],
                "text": "Survey propagation: An algorithm for satisfiability"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new type of message passing algorithm is introduced which allows to find efficiently a satisfying assignment of the variables in this difficult region of randomly generated formulas."
            },
            "venue": {
                "fragments": [],
                "text": "Random Struct. Algorithms"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 53
                            }
                        ],
                        "text": "For a vector-quantization approach to clustering see (Luttrell, 1989; Luttrell, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9727118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46200b14c187e9e7450ad560c598b1af57b3a929",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel derivation is presented of T. Kohonen's topographic mapping training algorithm (Self-Organization and Associative Memory, 1984), based upon an extension of the Linde-Buzo-Gray (LBG) algorithm for vector quantizer design. Thus a vector quantizer is designed by minimizing an L(2) reconstruction distortion measure, including an additional contribution from the effect of code noise which corrupts the output of the vector quantizer. The neighborhood updating scheme of Kohonen's topographic mapping training algorithm emerges as a special case of this code noise model. This formulation of Kohonen's algorithm is a specific instance of the robust hidden layer principle, which stabilizes the internal representations chosen by a network against anticipated noise or distortion processes."
            },
            "slug": "Derivation-of-a-class-of-training-algorithms-Luttrell",
            "title": {
                "fragments": [],
                "text": "Derivation of a class of training algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel derivation is presented of T. Kohonen's topographic mapping training algorithm, based upon an extension of the Linde-Buzo-Gray algorithm for vector quantizer design, which stabilizes the internal representations chosen by a network against anticipated noise or distortion processes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Rumelhart et al. (1986) showed that multilayer perceptrons can be trained, by gradient descent on M(w), to discover solutions to non-trivial problems such as deciding whether an image is symmetric or not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4043,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16430409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a1e1da81b535e1bead3fc2ab6af8b07877823b9",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The elements of the Hessian matrix consist of the second derivatives of the error measure with respect to the weights and thresholds in the network. They are needed in Bayesian estimation of network regularization parameters, for estimation of error bars on the network outputs, for network pruning algorithms, and for fast retraining of the network following a small change in the training data. In this paper we present an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology. Software implementation of the algorithm is straightforward."
            },
            "slug": "Exact-Calculation-of-the-Hessian-Matrix-for-the-Bishop",
            "title": {
                "fragments": [],
                "text": "Exact Calculation of the Hessian Matrix for the Multilayer Perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 9
                            }
                        ],
                        "text": "See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996). Interesting message-passing algorithms that have different capabilities from the sum\u2013product algorithm include expectation propagation (Minka, 2001) and survey propagation (Braunstein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 9
                            }
                        ],
                        "text": "See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10063289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4323cf65589bc509aea260ccaa4ef32a94b2f7e",
            "isKey": true,
            "numCitedBy": 269,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a mean-field algorithm for binary classification with gaussian processes that is based on the TAP approach originally proposed in statistical physics of disordered systems. The theory also yields an approximate leave-one-out estimator for the generalization error, which is computed with no extra computational cost. We show that from the TAP approach, it is possible to derive both a simpler naive mean-field theory and support vector machines (SVMs) as limiting cases. For both mean-field algorithms and support vector machines, simulation results for three small benchmark data sets are presented. They show that one may get state-of-the-art performance by using the leave-one-out estimator for model selection and the built-in leave-one-out estimators are extremely precise when compared to the exact leave-one-out estimate. The second result is taken as strong support for the internal consistency of the mean-field approach."
            },
            "slug": "Gaussian-Processes-for-Classification:-Mean-Field-Opper-Winther",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Classification: Mean-Field Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A mean-field algorithm for binary classification with gaussian processes that is based on the TAP approach originally proposed in statistical physics of disordered systems is derived and an approximate leave-one-out estimator for the generalization error is computed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157745208"
                        ],
                        "name": "Jung-Fu Cheng",
                        "slug": "Jung-Fu-Cheng",
                        "structuredName": {
                            "firstName": "Jung-Fu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Fu Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 4
                            }
                        ],
                        "text": "See (Wiberg, 1996; Frey, 1998; McEliece et al., 1998) for further discussion of the sum\u2013product algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14553992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d953005dd08a863c157b528bbabdf5671d18b6",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the close connection between the now celebrated iterative turbo decoding algorithm of Berrou et al. (1993) and an algorithm that has been well known in the artificial intelligence community for a decade, but which is relatively unknown to information theorists: Pearl's (1982) belief propagation algorithm. We see that if Pearl's algorithm is applied to the \"belief network\" of a parallel concatenation of two or more codes, the turbo decoding algorithm immediately results. Unfortunately, however, this belief diagram has loops, and Pearl only proved that his algorithm works when there are no loops, so an explanation of the experimental performance of turbo decoding is still lacking. However, we also show that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's (1962) low-density parity-check codes, serially concatenated codes, and product codes. Thus, belief propagation provides a very attractive general methodology for devising low-complexity iterative decoding algorithms for hybrid coded systems."
            },
            "slug": "Turbo-Decoding-as-an-Instance-of-Pearl's-\"Belief-McEliece-Mackay",
            "title": {
                "fragments": [],
                "text": "Turbo Decoding as an Instance of Pearl's \"Belief Propagation\" Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's low-density parity-check codes, serially concatenated codes, and product codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410076"
                        ],
                        "name": "R. Yeung",
                        "slug": "R.-Yeung",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Yeung",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Yeung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "With this proviso kept in mind, the interpretation of entropies in terms of sets can be helpful (Yeung, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14157417,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a37eab85f532cdc027260777815d78f164eb93aa",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The author presents a new approach to understanding the underlying mathematical structure of Shannon's information measures, which provides answers to the following two questions for any finite number of random variables. (1) For any information-theoretic identity, is there a corresponding set-theoretic identity via the formal substitution of symbols? (2) For any set-theoretic identity, is there a corresponding information-theoretic identity and, if so, in what sense? The author establishes the analogy between information theory and set theory. Therefore, each information-theoretic operation can formally be viewed as a set-theoretic operation and vice versa. This point of view, which the author believes is of fundamental importance has apparently been overlooked in the past by information theorists. As a consequence the I-diagram, which is a geometrical representation of the relationship among the information measures, is introduced. The I-diagram is analogous to the Venn diagram in set theory. The use of the I-diagram is discussed. >"
            },
            "slug": "A-new-outlook-of-Shannon's-information-measures-Yeung",
            "title": {
                "fragments": [],
                "text": "A new outlook of Shannon's information measures"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The author presents a new approach to understanding the underlying mathematical structure of Shannon's information measures, which provides answers to the following two questions: for any finite number of random variables and for any information-theoretic identity via the formal substitution of symbols."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42794,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6725274"
                        ],
                        "name": "J. Copas",
                        "slug": "J.-Copas",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Copas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Copas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 120
                            }
                        ],
                        "text": "A non-Bayesian approach to this problem is to downweight all predictions uniformly, by an empirically determined factor (Copas, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116430176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c88d7b4f260ab4e66e274e3dda3780ef2148af37",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The fit of a regression predictor to new data is nearly always worse than its fit to the original data. Anticipating this shrinkage leads to Stein-type predictors which, under certain assumptions, give a uniformly lower prediction mean squared error than least squares. Shrinkage can be particularly marked when stepwise fitting is used: the shrinkage is then closer to that expected of the full regression rather than of the subset regression actually fitted. Preshrunk predictors for selected subsets are proposed and tested on a number of practical examples. Both multiple and binary (logistic) regression models are considered."
            },
            "slug": "Regression,-Prediction-and-Shrinkage-Copas",
            "title": {
                "fragments": [],
                "text": "Regression, Prediction and Shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 75
                            }
                        ],
                        "text": "Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122801915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f275df4188cf8d51f3a85bd95ed2afa64196e4",
            "isKey": false,
            "numCitedBy": 2785,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. They show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Near-Shannon-limit-performance-of-low-density-check-Mackay-Neal",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit performance of low density parity check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels, showing that performance substantially better than that of standard convolutional and concatenated codes can be achieved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 37
                            }
                        ],
                        "text": "This remains an active research area (Rasmussen and Ghahramani, 2002; Beal et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5062147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "480d517574a079d3e0159b978cb19b3f014e59a3",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets \u2013 thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach."
            },
            "slug": "Infinite-Mixtures-of-Gaussian-Process-Experts-Rasmussen-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Infinite Mixtures of Gaussian Process Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An extension to the Mixture of Experts model, where the individual experts are Gaussian Process (GP) regression models, using an input-dependent adaptation of the Dirichlet Process to implement a gating network for an infinite number of Experts."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 930666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7eeea351d07ef9cc2d572d857e944ab59ee7c61b",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian committee machine (BCM) is a novel approach to combining estimators that were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators, the main foci are gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for on-line learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to gaussian process regression. Finally, we show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input-dependent combination of estimators."
            },
            "slug": "A-Bayesian-Committee-Machine-Tresp",
            "title": {
                "fragments": [],
                "text": "A Bayesian Committee Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12018209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4c47ebf6454e3c5a8417c580c8ecf694e34ad49",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error under straightforward conditions. General lessons are drawn concerning inference in many dimensions."
            },
            "slug": "Comparison-of-Approximate-Methods-for-Handling-Mackay",
            "title": {
                "fragments": [],
                "text": "Comparison of Approximate Methods for Handling Hyperparameters"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Two approximate methods for computational implementation of Bayesian hierarchical models that include unknown hyperparameters such as regularization constants and noise levels are examined, and the evidence framework is shown to introduce negligible predictive error under straightforward conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 70
                            }
                        ],
                        "text": "ICA was originally derived using an information maximization approach (Bell and Sejnowski, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406409112"
                        ],
                        "name": "A. O'Hagan",
                        "slug": "A.-O'Hagan",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "O'Hagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Hagan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 134
                            }
                        ],
                        "text": "One curious defect of these Monte Carlo methods \u2013 which are widely used by Bayesian statisticians \u2013 is that they are all non-Bayesian (O\u2019Hagan, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124181645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5dae2bf7e59364c9305ca76646336e8e0a8d69f",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We present some fundamental objections to the Monte Carlo method of numerical integra- tion. 1 Background As Bayesian inference is applied to more and more complex and realistic models combined with more and more realistic prior distributions, we become increasingly dependent on numerical methods to explore the resulting complex, high-dimensional, posterior distributions. In particular, there has been considerable interest lately in techniques of numerical integration. The Monte Carlo method, which has long been known to numerical analysts, was brought to the attention of the Bayesian statistics community by Kloek & van Dijk (1978), although Stewart had been using it in this context several years earlier. See Stewart & Johnson (1971). There are many variations and elaborations of Monte Carlo integration, but for our purposes it is enough to study the most basic problem. Consider the one-dimensional integral 00 k= f f(x)dx."
            },
            "slug": "Monte-Carlo-is-fundamentally-unsound-O'Hagan",
            "title": {
                "fragments": [],
                "text": "Monte Carlo is fundamentally unsound"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work presents some fundamental objections to the Monte Carlo method of numerical integration, which has long been known to numerical analysts and was brought to the attention of the Bayesian statistics community by Kloek & van Dijk (1978)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1477649,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "2038e45ef000eeab67ea987d9f148ab13aa0e4b4",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Introducing a new hobby for other people may inspire them to join with you. Reading, as one of mutual hobby, is considered as the very easy hobby to do. But, many people are not interested in this hobby. Why? Boring is the reason of why. However, this feel actually can deal with the book and time of you reading. Yeah, one that we will refer to break the boredom in reading is choosing information theory and coding as the reading material."
            },
            "slug": "Information-Theory-and-Coding-Daugman",
            "title": {
                "fragments": [],
                "text": "Information Theory and Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "One that the authors will refer to break the boredom in reading is choosing information theory and coding as the reading material."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15751235"
                        ],
                        "name": "J. D. Bernal",
                        "slug": "J.-D.-Bernal",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bernal",
                            "middleNames": [
                                "Desmond"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Bernal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4144269,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6f588edb16647f570710a3d3a958fa4d31df890a",
            "isKey": false,
            "numCitedBy": 613,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "N. W. PIRIE'S characteristic and entertaining account of the symposium in Moscow on the origin of life mainly consists of an exposition of his views on the subject and a metaphysical criticism of a minor part of my own contribution to the discussion."
            },
            "slug": "\u201cThe-Origins-of-Life\u201d-Bernal",
            "title": {
                "fragments": [],
                "text": "\u201cThe Origins of Life\u201d"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144880662"
                        ],
                        "name": "J. Propp",
                        "slug": "J.-Propp",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Propp",
                            "middleNames": [
                                "Gary"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Propp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40325691"
                        ],
                        "name": "D. Wilson",
                        "slug": "D.-Wilson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wilson",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 269
                            }
                        ],
                        "text": "The Ising model is a simple model which has been around for a long time, but the task of generating samples from the distribution P (x) = P \u2217(x)/Z is still an active research area; the first \u2018exact\u2019 samples from this distribution were created in the pioneering work of Propp and Wilson (1996), as we\u2019ll describe in Chapter 32."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11456918,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7dd0a09f1d8352c4d38ff549f54f024993c8825d",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "For many applications it is useful to sample from a finite set of objects in accordance with some particular distribution. One approach is to run an ergodic (i.e., irreducible aperiodic) Markov chain whose stationary distribution is the desired distribution on this set; after the Markov chain has run for M steps, with M sufficiently large, the distribution governing the state of the chain approximates the desired distribution. Unfortunately, it can be difficult to determine how large M needs to be. We describe a simple variant of this method that determines on its own when to stop and that outputs samples in exact accordance with the desired distribution. The method uses couplings which have also played a role in other sampling schemes; however, rather than running the coupled chains from the present into the future, one runs from a distant point in the past up until the present, where the distance into the past that one needs to go is determined during the running of the algorithm itself. If the state space has a partial order that is preserved under the moves of the Markov chain, then the coupling is often particularly efficient. Using our approach, one can sample from the Gibbs distributions associated with various statistical mechanics models (including Ising, random-cluster, ice, and dimer) or choose uniformly at random from the elements of a finite distributive lattice. \u00a9 1996 John Wiley & Sons, Inc."
            },
            "slug": "Exact-sampling-with-coupled-Markov-chains-and-to-Propp-Wilson",
            "title": {
                "fragments": [],
                "text": "Exact sampling with coupled Markov chains and applications to statistical mechanics"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work describes a simple variant of this method that determines on its own when to stop and that outputs samples in exact accordance with the desired distribution, and uses couplings which have also played a role in other sampling schemes."
            },
            "venue": {
                "fragments": [],
                "text": "Random Struct. Algorithms"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243483"
                        ],
                        "name": "H. Lappalainen",
                        "slug": "H.-Lappalainen",
                        "structuredName": {
                            "firstName": "Harri",
                            "lastName": "Lappalainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lappalainen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 79
                            }
                        ],
                        "text": "A variational free energy minimization approach to ICA-like models is given in (Miskin, 2001; Miskin and MacKay, 2000; Miskin and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17675382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "370cab058c9bd9f8371d6ff3f91027fd0442d4d3",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a recently developed Bayesian method called ensemble learning is applied to independent component analysis (ICA). Ensemble learning is a computationally eecient approximation for exact Bayesian analysis. In general, the posterior probability density function (pdf) is a complex high dimensional function whose exact treatment is diicult. In ensemble learning, the posterior pdf is approximated by a more simple function and Kullback-Leibler information is used as the criterion for minimising the misst between the actual posterior pdf and its parametric approximation. In this paper, the posterior pdf is approximated by a diagonal Gaus-sian pdf. According to the ICA-model used in this paper, the measurements are generated by a linear mapping from mutually independent source signals whose distributions are mixtures of Gaussians. The measurements are also assumed to have additive Gaussian noise with diagonal covariance. The model structure and all parameters of the distributions are estimated from the data."
            },
            "slug": "Ensemble-learning-for-independent-component-Lappalainen",
            "title": {
                "fragments": [],
                "text": "Ensemble learning for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A recently developed Bayesian method called ensemble learning is applied to independent component analysis (ICA) and the posterior probability density function is approximated by a diagonal Gaus-sian pdf."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "] Implement the infinite Gaussian mixture model of Rasmussen (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16685561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f49a73c42be6dbd851af4599d9911ea1d6ac7f4",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of significance for pairwise comparisons. Two experimental designs are recommended and supported by the DELVE software environment. \nTwo new non-parametric Bayesian learning methods relying on Gaussian process priors over functions are developed. These priors are controlled by hyperparameters which set the characteristic length scale for each input dimension. In the simplest method, these parameters are fit from the data using optimization. In the second, fully Bayesian method, a Markov chain Monte Carlo technique is used to integrate over the hyperparameters. One advantage of these Gaussian process methods is that the priors and hyperparameters of the trained models are easy to interpret. \nThe Gaussian process methods are benchmarked against several other methods, on regression tasks using both real data and data generated from realistic simulations. The experiments show that small datasets are unsuitable for benchmarking purposes because the uncertainties in performance measurements are large. A second set of experiments provide strong evidence that the bagging procedure is advantageous for the Multivariate Adaptive Regression Splines (MARS) method. \nThe simulated datasets have controlled characteristics which make them useful for understanding the relationship between properties of the dataset and the performance of different methods. The dependency of the performance on available computation time is also investigated. It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time. The Gaussian process methods are shown to consistently outperform the more conventional methods."
            },
            "slug": "Evaluation-of-gaussian-processes-and-other-methods-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for non-linear regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752788"
                        ],
                        "name": "D. Boneh",
                        "slug": "D.-Boneh",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Boneh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boneh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37123238"
                        ],
                        "name": "Charles Garrett",
                        "slug": "Charles-Garrett",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Garrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Garrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Baum et al. (1995), analyzing a similar model, show that the population size N should be about \u221a G(logG)2 to make hitchhikers unlikely to arise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6022373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80688843352321e8e7fe500f9bca32ddc7b2971d",
            "isKey": false,
            "numCitedBy": 4550,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the performance of a Genetic Type Algorithm we call Culling and a variety of other algorithms on a problem we refer to as ASP. Culling is near optimal for this problem, highly noise tolerant, and the best known a~~roach . . in some regimes. We show that the problem of learning the Ising perception is reducible to noisy ASP. These results provide an example of a rigorous analysis of GA\u2019s and give insight into when and how C,A\u2019s can beat competing methods. To analyze the genetic algorithm, we view it as a special type of submartingale. We prove some new large deviation bounds on this submartingale w~ich enable us to determine the running time of the algorithm."
            },
            "slug": "On-genetic-algorithms-Baum-Boneh",
            "title": {
                "fragments": [],
                "text": "On genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "C Culling is near optimal for this problem, highly noise tolerant, and the best known a~~roach in some regimes, and some new large deviation bounds on this submartingale enable us to determine the running time of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 8
                            }
                        ],
                        "text": "(2002), Wainwright et al. (2003), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 8
                            }
                        ],
                        "text": "(2002), Wainwright et al. (2003), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996). Interesting message-passing algorithms that have different capabilities from the sum\u2013product algorithm include expectation propagation (Minka, 2001) and survey propagation (Braunstein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 8
                            }
                        ],
                        "text": "(2002), Wainwright et al. (2003), and Forney (2001). See also Pearl (1988). A good reference for the fundamental theory of graphical models is Lauritzen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 492441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f26da30b639ac79f3b74b00b00bea6e527a4bf7c",
            "isKey": true,
            "numCitedBy": 268,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-based reparameterization (TRP) framework that provides a new conceptual view of a large class of algorithms for computing approximate marginals in graphs with cycles. This class includes the belief propagation (BP) or sum-product algorithm as well as variations and extensions of BP. Algorithms in this class can be formulated as a sequence of reparameterization updates, each of which entails refactorizing a portion of the distribution corresponding to an acyclic subgraph (i.e., a tree, or more generally, a hypertree). The ultimate goal is to obtain an alternative but equivalent factorization using functions that represent (exact or approximate) marginal distributions on cliques of the graph. Our framework highlights an important property of the sum-product algorithm and the larger class of reparameterization algorithms: the original distribution on the graph with cycles is not changed. The perspective of tree-based updates gives rise to a simple and intuitive characterization of the fixed points in terms of tree consistency. We develop interpretations of these results in terms of information geometry. The invariance of the distribution, in conjunction with the fixed-point characterization, enables us to derive an exact expression for the difference between the true marginals on an arbitrary graph with cycles, and the approximations provided by belief propagation. More broadly, our analysis applies to any algorithm that minimizes the Bethe free energy. We also develop bounds on the approximation error, which illuminate the conditions that govern their accuracy. Finally, we show how the reparameterization perspective extends naturally to generalizations of BP (e.g., Kikuchi (1951) approximations and variants) via the notion of hypertree reparameterization."
            },
            "slug": "Tree-based-reparameterization-framework-for-of-and-Wainwright-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Tree-based reparameterization framework for analysis of sum-product and related algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A tree-based reparameterization (TRP) framework is presented that provides a new conceptual view of a large class of algorithms for computing approximate marginals in graphs with cycles, which includes the belief propagation or sum-product algorithm as well as variations and extensions of BP."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34788092"
                        ],
                        "name": "S. Brink",
                        "slug": "S.-Brink",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Brink",
                            "middleNames": [
                                "ten"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brink"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 358,
                                "start": 147
                            }
                        ],
                        "text": "There is a growing literature on the practical design of low-density parity-check codes (Mao and Banihashemi, 2000; Mao and Banihashemi, 2001; ten Brink et al., 2002); they are now being adopted for applications from hard drives to satellite communications. For low\u2013density parity\u2013check codes applicable to quantum error-correction, see MacKay et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110295359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a365a598e22c0c1cbfb6a557fb245cff64f8a135",
            "isKey": false,
            "numCitedBy": 604,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel method for visualising the convergence behaviour of iterative decoding schemes is proposed. Each constituent decoder is represented by a mutual information transfer characteristic which describes the flow of extrinsic information through the soft in/soft out decoder. The exchange of extrinsic information between constituent decoders is plotted in an extrinsic information transfer chart. The concepts are illustrated for an iterative demapping and decoding scheme."
            },
            "slug": "Convergence-of-iterative-decoding-Brink",
            "title": {
                "fragments": [],
                "text": "Convergence of iterative decoding"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A novel method for visualising the convergence behaviour of iterative decoding schemes is proposed, which describes the flow of extrinsic information through the soft in/soft out decoder."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2280315,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "93b0eff68081abfa51e499733ffea7188f7a4cf0",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We find that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain."
            },
            "slug": "Bayesian-Monte-Carlo-Rasmussen-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Bayesian Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is found that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549634"
                        ],
                        "name": "E. Offer",
                        "slug": "E.-Offer",
                        "structuredName": {
                            "firstName": "Elke",
                            "lastName": "Offer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Offer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720068"
                        ],
                        "name": "E. Soljanin",
                        "slug": "E.-Soljanin",
                        "structuredName": {
                            "firstName": "Emina",
                            "lastName": "Soljanin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Soljanin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18441753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe81de7d6e3e4d67e998f691192a63c9d39e4e5e",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Several popular, suboptimal algorithms for bit decoding of binary block codes such as turbo decoding, threshold decoding, and message passing for LDPC, were developed almost as a common sense approach to decoding of some specially designed codes. After their introduction, these algorithms have been studied by mathematical tools pertinent more to computer science than the conventional algebraic coding theory. We give an algebraic description of the optimal and suboptimal bit decoders and of the optimal and suboptimal message passing. We explain exactly how suboptimal algorithms approximate the optimal, and show how good these approximations are in some special cases."
            },
            "slug": "AN-ALGEBRAIC-DESCRIPTION-OF-ITERATIVE-DECODING-Offer-Soljanin",
            "title": {
                "fragments": [],
                "text": "AN ALGEBRAIC DESCRIPTION OF ITERATIVE DECODING SCHEMES"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is explained exactly how suboptimal algorithms approximate the optimal, and it is shown how good these approximations are in some special cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16310854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b0ad84493dfc6dd950dc86d18041b72d28d39f3",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using \u2018neural networks\u2019 is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which may actually be irrelevant to the prediction of the output variable. Because a finite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network (even with regularisation or \u2018weight decay\u2019) will not set the coefficients for these junk inputs to zero. Thus the irrelevant variables will hurt the model\u2019s performance."
            },
            "slug": "BAYESIAN-NON-LINEAR-MODELING-FOR-THE-PREDICTION-Mackay",
            "title": {
                "fragments": [],
                "text": "BAYESIAN NON-LINEAR MODELING FOR THE PREDICTION COMPETITION"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The 1993 energy prediction competition involved the prediction of aseries of building energy loads from a series of environmental input variables using non-linear regression using \u2018neural networks\u2019, a popular technique for such modeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144508417"
                        ],
                        "name": "Warren D. Smith",
                        "slug": "Warren-D.-Smith",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "Smith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Warren D. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 252
                            }
                        ],
                        "text": "In a realistic problem such as playing a board game, the tree of possible cogitations and actions that must be considered becomes enormous, and \u2018doing the right thing\u2019 is not simple, because the expected utility of an action cannot be computed exactly (Russell and Wefald, 1991; Baum and Smith, 1993; Baum and Smith, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12427675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a157ece9e259fda728485373234d119addd1fe25",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Bayesian-Approach-to-Relevance-in-Game-Playing-Baum-Smith",
            "title": {
                "fragments": [],
                "text": "A Bayesian Approach to Relevance in Game Playing"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773821"
                        ],
                        "name": "Matthew J. Beal",
                        "slug": "Matthew-J.-Beal",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Beal",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew J. Beal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 37
                            }
                        ],
                        "text": "This remains an active research area (Rasmussen and Ghahramani, 2002; Beal et al., 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 71
                            }
                        ],
                        "text": "ICA was originally derived using an information maximization approach (Bell and Sejnowski, 1995). Another view of ICA, in terms of energy functions, which motivates more general models, is given by Hinton et al. (2001). Another generalization of ICA can be found in Pearlmutter and Parra (1996, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5407275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d315e66c74bd3daa8a8272301fdee1285b01cf5",
            "isKey": false,
            "numCitedBy": 611,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite\u2014 consider, for example, symbols being possible words appearing in English text."
            },
            "slug": "The-Infinite-Hidden-Markov-Model-Beal-Ghahramani",
            "title": {
                "fragments": [],
                "text": "The Infinite Hidden Markov Model"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is shown that it is possible to extend hidden Markov models to have a countably infinite number of hidden states by using the theory of Dirichlet processes to implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36483354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a23da2c14f9355dd63a434b62cf5b28aeebc305",
            "isKey": false,
            "numCitedBy": 3037,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing."
            },
            "slug": "\u201cNeural\u201d-computation-of-decisions-in-optimization-Hopfield-Tank",
            "title": {
                "fragments": [],
                "text": "\u201cNeural\u201d computation of decisions in optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 55
                            }
                        ],
                        "text": "These observations about crosswords were first made by Shannon (1948); I learned about them from Wolf and Siegel (1998). The topic is closely related to the capacity of two-dimensional constrained channels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 0
                            }
                        ],
                        "text": "Shannon, Bell Syst. Tech. J. 30, pp.50\u201364 (1950), but, inexplicably, the great man made numerical errors in it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 55
                            }
                        ],
                        "text": "These observations about crosswords were first made by Shannon (1948); I learned about them from Wolf and Siegel (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "The entropy of English, given a good model, is about one bit per character (Shannon, 1948), so a Huffman code is likely to be highly inefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55379485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a54194422c56399b2923b2ad706b8175c8c48258",
            "isKey": true,
            "numCitedBy": 34823,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases."
            },
            "slug": "A-mathematical-theory-of-communication-Shannon",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of communication"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This final installment of the paper considers the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now."
            },
            "venue": {
                "fragments": [],
                "text": "MOCO"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135122"
                        ],
                        "name": "A. Terras",
                        "slug": "A.-Terras",
                        "structuredName": {
                            "firstName": "Audrey",
                            "lastName": "Terras",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Terras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 66
                            }
                        ],
                        "text": "and Soljanin, 2001); and for background reading on this topic see (Hartmann and Rudolph, 1976; Terras, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "For an accessible introduction to Fourier analysis on finite groups, see Terras (1999). See also MacWilliams and Sloane (1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117514692,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e69d73a9ed476b2a6008dee4f574d09f00184d74",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Cast of characters Part I: 1. Congruences and the quotient ring of the integers mod n 1.2 The discrete Fourier transform on the finite circle 1.3 Graphs of Z/nZ, adjacency operators, eigenvalues 1.4 Four questions about Cayley graphs 1.5 Finite Euclidean graphs and three questions about their spectra 1.6 Random walks on Cayley graphs 1.7 Applications in geometry and analysis 1.8 The quadratic reciprocity law 1.9 The fast Fourier transform 1.10 The DFT on finite Abelian groups - finite tori 1.11 Error-correcting codes 1.12 The Poisson sum formula on a finite Abelian group 1.13 Some applications in chemistry and physics 1.14 The uncertainty principle Part II. Introduction 2.1 Fourier transform and representations of finite groups 2.2 Induced representations 2.3 The finite ax + b group 2.4 Heisenberg group 2.5 Finite symmetric spaces - finite upper half planes Hq 2.6 Special functions on Hq - K-Bessel and spherical 2.7 The general linear group GL(2, Fq) 2.8. Selberg's trace formula and isospectral non-isomorphic graphs 2.9 The trace formula on finite upper half planes 2.10 The trace formula for a tree and Ihara's zeta function."
            },
            "slug": "Fourier-analysis-on-finite-groups-and-applications-Terras",
            "title": {
                "fragments": [],
                "text": "Fourier analysis on finite groups and applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32590599"
                        ],
                        "name": "M. C. Davey",
                        "slug": "M.-C.-Davey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Davey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Davey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 250
                            }
                        ],
                        "text": "From left (best performance) to right: Irregular low-density parity-check code over GF (8), blocklength 48 000 bits (Davey, 1999); JPL turbo code (JPL, 1996) blocklength 65 536; Regular low-density parity-check over GF (16), blocklength 24 448 bits (Davey and MacKay, 1998); Irregular binary low-density paritycheck code, blocklength 16 000 bits (Davey, 1999); Luby et al. (1998) irregular binary lowdensity parity-check code, blocklength 64 000 bits; JPL code for Galileo (in 1992, this was the best known code of rate 1/4); Regular binary low-density parity-check code: blocklength 40 000 bits (MacKay, 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58992663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce4b894c117412d00f7034a379ea8edbd924fbd2",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A new block code is introduced which is capable of correcting multiple insertion, deletion and substitution errors present in a single block. An inner code resilient to synchronisation errors provides soft inputs to an outer code capable of correcting substitution errors. The decoder does not require knowledge of the block boundaries."
            },
            "slug": "Watermark-codes:-reliable-communication-over-Davey-Mackay",
            "title": {
                "fragments": [],
                "text": "Watermark codes: reliable communication over insertion/deletion channels"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An inner code resilient to synchronisation errors provides soft inputs to an outer code capable of correcting substitution errors, and the decoder does not require knowledge of the block boundaries."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Symposium on Information Theory (Cat. No.00CH37060)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 82
                            }
                        ],
                        "text": "Bayesian methods are introduced and contrasted with sampling-theory statistics in (Jaynes, 1983; Gull, 1988; Loredo, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 44
                            }
                        ],
                        "text": "My favourite reading on this topic includes (Jaynes, 1983; Gull, 1988; Loredo, 1990; Berger, 1985; Jaynes, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120668555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065803f662547c47fc46bf7e649847f90e3e90e9",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 227,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, which does not offer clear, compelling criteria for the design of statistical methods. Bayesian probability theory offers unique and demonstrably optimal solutions to well-posed statistical problems, and is historically the original approach to statistics. The reasons for earlier rejection of Bayesian methods are discussed, and it is noted that the work of Cox, Jaynes, and others answers earlier objections, giving Bayesian inference a firm logical and mathematical foundation as the correct mathematical language for quantifying uncertainty. The Bayesian approaches to parameter estimation and model comparison are outlined and illustrated by application to a simple problem based on the gaussian distribution. As further illustrations of the Bayesian paradigm, Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background, and the analysis of the neutrinos detected from supernova SN 1987A. A brief bibliography of astrophysically interesting applications of Bayesian inference is provided."
            },
            "slug": "From-Laplace-to-Supernova-SN-1987A:-Bayesian-in-Loredo",
            "title": {
                "fragments": [],
                "text": "From Laplace to Supernova SN 1987A: Bayesian Inference in Astrophysics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, and Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background and the analysis of the neutrinos detected from supernova SN 1987A."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932728"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143916589"
                        ],
                        "name": "A. Shokrollahi",
                        "slug": "A.-Shokrollahi",
                        "structuredName": {
                            "firstName": "Amin",
                            "lastName": "Shokrollahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shokrollahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727633"
                        ],
                        "name": "R. Urbanke",
                        "slug": "R.-Urbanke",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Urbanke",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urbanke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16228713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02746a7981e1e6a8fd252b80e31bf782f5843eec",
            "isKey": false,
            "numCitedBy": 3128,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We design low-density parity-check (LDPC) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on the work of Richardson and Urbanke (see ibid., vol.47, no.2, p.599-618, 2000). Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds."
            },
            "slug": "Design-of-capacity-approaching-irregular-codes-Richardson-Shokrollahi",
            "title": {
                "fragments": [],
                "text": "Design of capacity-approaching irregular low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work designs low-density parity-check codes that perform at rates extremely close to the Shannon capacity and proves a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 78
                            }
                        ],
                        "text": "This gradient can be efficiently computed using the backpropagation algorithm (Rumelhart et al., 1986), which uses the chain rule to find the derivatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20335,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9346534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
            },
            "slug": "Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp",
            "title": {
                "fragments": [],
                "text": "Keeping the neural networks simple by minimizing the description length of the weights"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units without time-consuming Monte Carlo simulations is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052378377"
                        ],
                        "name": "Ruxandra L. Pinto",
                        "slug": "Ruxandra-L.-Pinto",
                        "structuredName": {
                            "firstName": "Ruxandra",
                            "lastName": "Pinto",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruxandra L. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17806166,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0d42aa182286ffc9f21eabb43efa54f08c464d55",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how large improvements in the accuracy of MCMC estimates for posterior expectations can sometimes be obtained by coupling a Markov chain that samples from the posterior distribution with a chain that samples from a Gaussian approximation to the posterior. Use of this method requires a coupling scheme that produces high correlation between the two chains. An eecient estimator can then be constructed that exploits this correlation, provided an accurate value for the expectation under the Gaussian approximation can be found, which for simple functions can be done analytically. Good coupling schemes are available for many Markov chain samplers, including Gibbs sampling with standard conditional distributions. For many moderate-dimensional problems, the improvement in accuracy using this method will be much greater than the overhead from simulating a second chain."
            },
            "slug": "Improving-Markov-chain-Monte-Carlo-Estimators-by-to-Pinto-Neal",
            "title": {
                "fragments": [],
                "text": "Improving Markov chain Monte Carlo Estimators by Coupling to an Approximating Chain"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is shown how large improvements in the accuracy of MCMC estimates for posterior expectations can sometimes be obtained by coupling a Markov chain that samples from the posterior distribution with a chain that sample from a Gaussian approximation to the posterior."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 83
                            }
                        ],
                        "text": "Further reading on blind separation, including non-ICA algorithms, can be found in (Jutten and Herault, 1991; Comon et al., 1991; Hendin et al., 1994; Amari et al., 1996; Hojen-Sorensen et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3637824"
                        ],
                        "name": "R. M. Tanner",
                        "slug": "R.-M.-Tanner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tanner",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. M. Tanner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 690,
                                "start": 0
                            }
                        ],
                        "text": "Tanner (1981) generalized Gallager\u2019s work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware. Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996). Low-precision decoding algorithms and fast encoding algorithms for Gallager codes are discussed in (Richardson and Urbanke, 2001a; Richardson and Urbanke, 2001b). MacKay and Davey (2000) showed that low\u2013density parity\u2013check codes can outperform Reed\u2013Solomon codes, even on the Reed\u2013Solomon codes\u2019 home turf: high rate and short block lengths."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 12
                            }
                        ],
                        "text": "(1996), and Tanner (1996). In this chapter I will use the word \u2018sample\u2019 in the following sense: a sample from a distribution P (x) is a single realization x whose probability distribution is P (x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Tanner (1981) generalized Gallager\u2019s work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 754232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "157218bae792b6ef550dfd0f73e688d83d98b3d7",
            "isKey": false,
            "numCitedBy": 2971,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for constructing long error-correcting codes from one or more shorter error-correcting codes, referred to as subcodes, and a bipartite graph. A graph is shown which specifies carefully chosen subsets of the digits of the new codes that must be codewords in one of the shorter subcodes. Lower bounds to the rate and the minimum distance of the new code are derived in terms of the parameters of the graph and the subeodes. Both the encoders and decoders proposed are shown to take advantage of the code's explicit decomposition into subcodes to decompose and simplify the associated computational processes. Bounds on the performance of two specific decoding algorithms are established, and the asymptotic growth of the complexity of decoding for two types of codes and decoders is analyzed. The proposed decoders are able to make effective use of probabilistic information supplied by the channel receiver, e.g., reliability information, without greatly increasing the number of computations required. It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities. The construction principles"
            },
            "slug": "A-recursive-approach-to-low-complexity-codes-Tanner",
            "title": {
                "fragments": [],
                "text": "A recursive approach to low complexity codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39274396"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50706548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42bab83cc1aa10e18a31c1a8fb04e165ca8180d1",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The maximum rate at which a synaptic link could theoretically transmit information depends on the type of coding used. In a binary modulation system it depends chiefly on the relaxation time, and the limiting capacity equals the maximum attainable impulse rate. In a system using pulse-interval modulation, temporal precision may be a more important limiting factor. It is shown that in a number of typical cases a system of the second type could transmit several times more information per second through a synaptic link than a binary system, and the relation between relative efficiency, relaxation-time, and temporal resolving power is generalized in graphical form. It is concluded, not that interval modulation rather than binary modulation \u201cought\u201d to be the mode of action of the central nervous system, but that the contrary assumption is unsupported by considerations of efficiency."
            },
            "slug": "The-limiting-information-capacity-of-a-neuronal-Mackay-McCulloch",
            "title": {
                "fragments": [],
                "text": "The limiting information capacity of a neuronal link"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is concluded, not that interval modulation rather than binary modulation \u201cought\u201d to be the mode of action of the central nervous system, but that the contrary assumption is unsupported by considerations of efficiency."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144538257"
                        ],
                        "name": "Y. Weiss",
                        "slug": "Y.-Weiss",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13980420,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "3acaa12ea71ef5569b82d25c97ffbe3a16399de7",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is anupdatedandexpandedversionof TR2000-26,but it is still in draft form. Beliefpropagation(BP)wasonlysupposedtowork for tree-likenetworksbutworks surprisinglywell in many applicationsinvolving networkswith loops,includingturbo codes.However, therehasbeenlittle understandingof thealgorithmor thenatureof thesolutionsit findsfor generalgraphs. We show that BP canonly converge to a stationarypoint of an approximatefree energy, known astheBethefreeenergy in statisticalphysics.This resultcharacterizes BP fixed-pointsand makes connectionswith variationalapproachesto approximate inference. More importantly, our analysislets us build on the progressmadein statistical physicssinceBethe\u2019 s approximationwasintroducedin 1935.Kikuchi andothershave shown how to constructmoreaccuratefree energy approximations,of which Bethe\u2019 s approximationis the simplest. Exploiting the insightsfrom our analysis,we derive generalizedbeliefpropagation(GBP)versionsof theseKikuchi approximations. These new messagepassingalgorithmscanbesignificantlymoreaccuratethanordinaryBP, at anadjustableincreasein complexity. We illustratesucha new GBPalgorithmon a grid Markov network andshow thatit givesmuchmoreaccuratemarginalprobabilities thanthosefoundusingordinaryBP. This work may not be copiedor reproducedin whole or in part for any commercialpurpose.Permissionto copy in wholeor in partwithoutpaymentof feeis grantedfor nonprofiteducational andresearchpurposesprovidedthatall such wholeor partialcopiesincludethefollowing: anoticethatsuchcopying is by permissionof MitsubishiElectricResearch Laboratories, Inc.;anacknowledgmentof theauthorsandindividualcontributionsto thework; andall applicableportions of thecopyright notice.Copying, reproduction,or republishingfor any otherpurposeshallrequirealicensewith payment of feeto MitsubishiElectricResearchLaboratories, Inc. All rightsreserved. Copyright c MitsubishiElectricResearchLaboratories, Inc., 2001 201Broadway, Cambridge,Massachusetts 02139 This is anupdatedandexpandedversionof TR2000-26,but is still in draft form asof its releaseon April 4, 2001.Smallfurtherupdate(section2.3)onMay 15,2001. ! \"$#% '& (\") * + ,. ' \" (\") .' /\" 0 ( & * 132547698;:<6=4?>A@CB D E<FGE<FG6 H$IKJML NPORQTSVUXW(Y(Z9[VY]\\ ^ Y`_ba=UXcdZ9e`f(g Hih O(N=Qkj`l mon W(p=f`qAr=Qts;uvr(N=QwuxsPytz(z {}|;~ \u007f ~9\u007f \u0080P\u0081\u0083\u0082M|`\u0084G\u0085\u0087\u0086 \u0088w\u0089R\u0082 \u008a\u008bFG\u008c\u008d\u008cGF\u008d6=\u008e\u0090\u008f\u0091@C\u00923\u0093tD D \u008e/6=4 H$IKJ\u0094L NPORQTSVUXW(Y`Z=[VY]\\ ^ Y`_ba=UXc\u0095Z=e`f`g Hih O(N9Q;j`l mKn W`pRf`qAr=Qts;uvr(N9Q}u\u0096s`y`N's \u0097 \u0084d|}|`\u0082\u0098\u0080'\u0099t\u0081\u0083\u0082\u0098|P\u0084\u009a\u0085\u0087\u0086 \u0088}\u0089=\u0082 B 6=F\u008d\u0093\u009b\u008a\u009cD F\u008d\u009d;\u009d ^ W`_\u009f\u009e= 9\u00a1Xf}U\u0098\u00a2=\u00a3wc\u0095f}pR\u00a3}f\u00a5\u00a4\u0098c\u0095\u00a6'cd\u00a7 \u0308c\u00a9W(p a ^ S\u0083fkU \u0308\u00ab(f}\u00ac\u0095f}\\(g'z'\u00ad(y\u009f\u00a2 W9Z=Y\u009f\u00aeMY`\u00ac\u00a9\u00ac SVf}UX\u00ab`fk\u00ac\u00a9fk\\`g ^ h l`z sPN`Otu \u0304Q]s(str mon W`p=f(qAy=QkOPu\u00b0r`z'N]u\u0096yPO'NPl {k\u00b1o|t\u007f 2x2 \u0304\u0081C\u0088w2k\u0086\u00873P|P\u0084\u009a \u0301\u03bc|P\u0085 |}{k\u0086 |;~=\u00b6 \u00b71 \u0327To`\u00bb91\u20444=1\u20442A3\u20444\u00bf\u00bb SVf}\u00ac\u0095c\u00a9f}\u00c0M\u009e=UXW`\u009eRY`e(Yt\u00a1Xc\u00a9W(p\u00c2\u00c1\u008dS mV\u00c3 [\u00c4Y`\u00a7\u00c5W`p=\u00ac\u0095\\/\u00a7 \u0308 =\u009e=\u009e\u00bfW(\u00a7 \u0308fkZ \u00a1XW![\u0083W(U \u0308\u00ab\u00c6\u00c0\u00c7W`U \u00a1 \u0308UXf}f}u\u009a\u00ac\u0095c\u0095\u00ab`f p=fw\u00a1\u0096[VW`UX\u00ab9\u00a7 a= 9\u00a1<[\u0083W(U \u0308\u00ab9\u00a73\u00a7 \u0308 =UX\u009e=U \u0308cd\u00a7 \u0308c\u00a9p=e(\u00ac\u00a9\\\u0098[\u0083fk\u00ac\u00a9\u00ac(c\u00a9pb_\u009fY`p \\\u0098YP\u009eR\u009e=\u00ac\u00a9cd\u00a3}YP\u00a1 \u0308c\u0095W`pR\u00a7\u00c8c\u00a9p \u00a6(W`\u00ac\u0095\u00a6'c\u0095p=e p=fw\u00a1\u0096[VW`UX\u00ab9\u00a70[\u00c9c\u00a9\u00a1 n \u00ac\u00a9W W(\u009eR\u00a7}gKc\u00a9pR\u00a3}\u00ac\u00a9 5Z9c\u00a9pRe\u00c6\u00a1X =UXa5W!\u00a3wW9Z9f;\u00a7}\u00ca/\u00aeMWt[\u0083fk\u00a6`f}U;gC\u00a1 n f}UXf n Y(\u00a7 a5fkf}p%\u00ac\u00a9c\u00a9\u00a1x\u00a1X\u00ac\u00a9f\u00cb =pRZ9fkUX\u00a7x\u00a1XY`pRZ9c\u0095p=e W`\u00c0 \u00a1 n f!YP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _\u00ccW(U\u009b\u00a1 n f\u00c6p5Yt\u00a1 \u0308 RU \u0308f\u00c6WP\u00c00\u00a1 n f \u00a7xW(\u00ac\u00a9 =\u00a1 \u0308c\u0095W`pR\u00a7\u00c4c\u00a9\u00a1\u00c9\u00cdRpRZ=\u00a7\u00c9\u00c0\u00c7W`U\u00c9e`fkp=f}U \u0304YP\u00ac\u00cee(UXY`\u009e n \u00a7k\u00ca \u00cf f \u00a7 n Wt[-\u00a1 n YP\u00a1\u009fS m \u00a3kYPp/W(p=\u00ac\u00a9\\ \u00a3}W`p \u00a6`fkU \u0308e(f\u009f\u00a1 \u0308W\u00c6Y\u00c6\u00a7\u0096\u00a1 \u0304Yt\u00a1Xc\u00a9W(pRYPUX\\\u00c6\u009e\u00bfW`c\u0095p'\u00a1\u00c5W`\u00c0MYPp YP\u009e=\u009eRU \u0308W]\u00d09c\u0095_\u009fYP\u00a1 \u0308fM\u00c0\u00c7U \u0308fkf\u0098fkp=f}UXe`\\(g'\u00ab p=Wt[\u00c9p Y`\u00a7\u0083\u00a1 n f\u00a5SVf}\u00a1 n fT\u00c0\u00c7UXf}fTf}p=fkU \u0308e(\\\u0091c\u00a9p \u00a7\u0096\u00a1 \u0304Yt\u00a1Xc\u0095\u00a7xu \u00a1 \u0308cd\u00a3}Y`\u00ac3\u009e n \\9\u00a7xcd\u00a3}\u00a7k\u00ca\u0083\u00d1 n cd\u00a7\u0094UXfk\u00a7 \u0308 =\u00ac\u00a9\u00a1M\u00a3 n YPU \u0304Y`\u00a3w\u00a1 \u0308fkU \u0308c\u0095\u00d2}f;\u00a7VS m \u00cd=\u00d09f;Z u\u009a\u009e\u00bfW`c\u0095p'\u00a1X\u00a7\u0094Y`pRZ _\u0091YP\u00ab`f;\u00a7 \u00a3wW(p=p=fk\u00a3w\u00a1 \u0308c\u0095W`pR\u00a7\u00c4[\u00c9c\u00a9\u00a1 n \u00a6tYPUXcdYt\u00a1 \u0308c\u0095W`p5YP\u00ac YP\u009e=\u009eRU \u0308W'Y`\u00a3 n f;\u00a7o\u00a1XW\u009fY`\u009e=\u009e=UXW]\u00d09c\u00a9_\u0091Yt\u00a1Xf\u0098c\u0095p9\u00c0\u00c7fkU \u0308fkpR\u00a3wf(\u00ca H W`UXf\u00a5c\u00a9_\u009f\u009e\u00bfW`U \u0308\u00a1XY`p(\u00a1X\u00ac\u00a9\\(g=W` RU\u0094YPpRY`\u00ac\u00a9\\9\u00a7 \u0308c\u0095\u00a7\u00c4\u00ac\u0095fw\u00a1 \u0304\u00a7\u00c9 R\u00a7\u00c9a= Rc\u00a9\u00acdZ W`p\u00d3\u00a1 n fb\u009e=U \u0308W(e`UXfk\u00a7X\u00a7K_\u0091Y(Z9f c\u00a9p/\u00a7x\u00a1XYP\u00a1 \u0308cd\u00a7\u0096\u00a1Xc\u0095\u00a3kYP\u00acA\u009e n \\9\u00a7 \u0308c\u0095\u00a3k\u00a70\u00a7xc\u0095pR\u00a3}f\u009bS\u0083f}\u00a1 n f(\u00d4 \u00a7\u00c5YP\u009e=\u009eRU \u0308W]\u00d09c\u0095_\u009fYP\u00a1 \u0308c\u0095W`p\u00c6[VY(\u00a7\u00a5c\u00a9p'\u00a1 \u0308UXW9Z9 R\u00a3}fkZ c\u00a9p Q;l`j'y9\u00ca<\u00d5\u00a5c\u0095\u00ab  R\u00a3 n cRY`pRZ\u009fWP\u00a1 n f}U \u0304\u00a7 n Y]\u00a6(f\u0094\u00a7 n Wt[\u00c9p n Wt[)\u00a1XW0\u00a3wW(pR\u00a7\u0096\u00a1XU \u0308 5\u00a3\u03bc\u00a1K_\u009fW`UXf\u0094Y`\u00a3wu \u00a3w =U \u0304Yt\u00a1XfK\u00c0\u00c7UXf}f\u00c4fkp=f}UXe`\\0Y`\u009e=\u009e=UXW]\u00d0 c\u0095_\u0091Yt\u00a1Xc\u00a9W(pR\u00a7}g]W`\u00c0R[ n cd\u00a3 n SVfw\u00a1 n f`\u00d4 \u00a7AYP\u009eR\u009e=U \u0308W]\u00d09c\u0095_\u0091Yt\u00a1 \u0308c\u0095W`p c\u0095\u00a7T\u00a1 n f\u009b\u00a7 \u0308c\u00a9_\u009f\u009e=\u00ac\u0095fk\u00a7x\u00a1k\u00ca I \u00d09\u009e=\u00ac\u0095W`c\u00a9\u00a1 \u0308c\u0095p=e\u00d3\u00a1 n f\u009fc\u0095pR\u00a7 \u0308c\u00a9e n \u00a1 \u0304\u00a7T\u00c0\u00c7U \u0308W(_\u00d6W` RU\u00a5Y`pRYP\u00ac\u0095\\9\u00a7xcd\u00a7kg3[\u0083f\u009fZ9f}u U \u0308c\u0095\u00a6`fVe`fkp=f}U \u0304YP\u00ac\u0095c\u0095\u00d2}fkZ0a5fk\u00ac\u00a9c\u0095fw\u00c0\u00bf\u009e=U \u0308W(\u009eRYPe'Yt\u00a1Xc\u00a9W(p\u00d7\u00c1G\u00d8\u00a5S m\u0083\u00c3 \u00a6(f}U \u0304\u00a7xc\u0095W`pR\u00a7\u00c8WP\u00c05\u00a1 n f;\u00a7xf\u00c9\u00d5\u00a5c\u0095\u00ab' 5\u00a3 n c YP\u009e=\u009eRU \u0308W]\u00d09c\u0095_\u009fYP\u00a1 \u0308c\u0095W`pR\u00a7k\u00ca \u00d1 n f;\u00a7xf\u00c6p=f}[\u00d9_\u009ffk\u00a7X\u00a7XYPe`fi\u009eRY`\u00a7X\u00a7xc\u0095p=e Y`\u00ac\u00a9e(W`UXc\u0087\u00a1 n _\u0091\u00a7\u00d7\u00a3kYPp\u00daa\u00bff \u00a7xc\u0095e`pRc\u0087\u00cd5\u00a3kYPp'\u00a1 \u0308\u00ac\u0095\\\u009b_\u009fW`UXf\u00a5Y`\u00a3}\u00a3} =U \u0304Yt\u00a1 \u0308fT\u00a1 n YPp W`U \u0304Z9c\u0095pRYPUX\\\u009bS m g=Yt\u00a1MYPp Y`Zt\u00db\u0096 R\u00a7\u0096\u00a1 \u0304YPa=\u00ac\u0095f\u00a5c\u00a9p=u \u00a3wUXfkY(\u00a7xf\u00a5c\u0095pi\u00a3wW(_ \u009eR\u00ac\u00a9f}\u00d0 c\u00a9\u00a1\u0096\\`\u00ca \u00cf fbc\u0095\u00ac\u00a9\u00ac\u0095 R\u00a7x\u00a1 \u0308U \u0304Yt\u00a1 \u0308f\u00c5\u00a7x R\u00a3 n Y\u0091pRf}[ \u00d8\u00a5S m YP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _\u00dcW`p Y0e(U \u0308cdZ H Y`U \u0308\u00ab(Wt\u00a6bp=f}\u00a1\u0096[\u0083W(U \u0308\u00ab\u009fYPpRZ\u00d7\u00a7 n Wt[1\u00a1 n Yt\u00a1Vc\u0087\u00a1\u00c4e`c\u0095\u00a6`f;\u00a7K_\u00c5 R\u00a3 n _\u009fW`UXfMY(\u00a3}\u00a3w RUXYP\u00a1 \u0308f _\u0091YPUXe`c\u0095pRYP\u00ac\u00ce\u009e=UXW`aRY`a=c\u0095\u00ac\u00a9c\u00a9\u00a1 \u0308c\u0095fk\u00a7V\u00a1 n YPp \u00a1 n W(\u00a7 \u0308f\u0098\u00c0\u00c7W( =pRZ  R\u00a7 \u0308c\u00a9pRe W(UXZ=c\u00a9pRY`U \u0308\\\u009bS m \u00ca \u00dd \u00de(\u00df \u00bb91\u20444=\u00e0\u00c9\u00e1M\u00e2M3\u20444\u00bf\u00bb=\u00e3x\u00e0 \u00df L W9\u00a3kYP\u00acM\u00e4 \u0308a5fk\u00ac\u00a9c\u0095fw\u00c0\u0083\u009e=U \u0308W(\u009eRYPe'Yt\u00a1Xc\u00a9W(pR\u00e5 \u00c1\u008dS m\u0083\u00c3 YP\u00ac\u0095e`W`UXc\u00a9\u00a1 n _\u0091\u00a7M\u00a7 \u0308 R\u00a3 n Y(\u00a7\u0094\u00a1 n W(\u00a7 \u0308f0c\u0095p(\u00a1XU \u0308W9Z9 5\u00a3wfkZ$a'\\ m f;YPUX\u00ac\u00c8YPUXf e` 5YPU \u0304YPp'\u00a1 \u0308fkfkZ \u00a1XW$\u00a3wW(p'\u00a6(f}UXe`fb\u00a1 \u0308W \u00a1 n f\u009b\u00a3wW`UXUXfk\u00a3\u03bc\u00a1\u00a5_\u0091YPUXe`c\u0095pRYP\u00acA\u009e\u00bfW(\u00a7x\u00a1 \u0308fkU \u0308c\u0095W`U\u00a5\u009e=UXW`a5YPa=c\u0095\u00ac\u00a9c\u00a9\u00a1 \u0308c\u0095fk\u00a7\u00a5c\u0095p\u00cb\u00a1 \u0308UXf}f}u\u009a\u00ac\u0095c\u00a9\u00ab(f e`U \u0304YP\u009e n c\u0095\u00a3kYP\u00ac5_ W9Z9fk\u00ac\u0095\u00a7k\u00caA\u00e6=W(U\u0083e(f}p=fkUXY`\u00acRp=f}\u00a1\u0096[\u0083W(U \u0308\u00ab9\u00a7A[\u00c9c\u00a9\u00a1 n \u00ac\u00a9W W`\u009e5\u00a7}g'\u00a1 n f\u0098\u00a7 \u0308c\u0087\u00a1X RYt\u00a1Xc\u00a9W(p\u00d7cd\u00a7K_\u00c5 R\u00a3 n \u00ac\u0095fk\u00a7X\u00a7\u0083\u00a3}\u00ac\u00a9f;YPU;\u00ca \u00e7 p1\u00a1 n fiW`pRf n YPpRZ3gMY!p  =_ba\u00bff}U\u00d3WP\u00c0 U \u0308f;\u00a7xf;YPU \u0304\u00a3 n f}U \u0304\u00a7 n Y]\u00a6(f f}_\u009f\u009e=c\u0095U \u0308cd\u00a3}Y`\u00ac\u00a9\u00ac\u0095\\)Z9fk_\u009fW`pR\u00a7x\u00a1 \u0308U \u0304Yt\u00a1XfkZ1e`W W9Z \u009e\u00bff}U \u0308\u00c0\u00c7W`UX_\u009fY`pR\u00a3wf\u0098\u00c0\u00c7W`U\u00c9S m YP\u00ac\u0095e`W`UXc\u00a9\u00a1 n _\u0091\u00a7VYP\u009e=\u009e=\u00ac\u0095c\u0095fkZ\u00d7\u00a1 \u0308W e`U \u0304YP\u009e n \u00a7\u0083[\u00c9c\u00a9\u00a1 n \u00ac\u00a9W W`\u009e5\u00a7}\u00ca \u00e7 p=f Z9U \u0304YP_\u0091Yt\u00a1Xc\u0095\u00a3\u0098\u00a3}Y(\u00a7xfTc\u0095\u00a7 \u00a1 n f\u00c9p=fkY`U\u0083\u00a2 n Y`p=p=W`p=u\u009a\u00ac\u0095c\u00a9_\u009fc\u00a9\u00a1o\u009e5fkUx\u00c0\u00c7W(U \u0308_\u0091Y`pR\u00a3wf\u00c4WP\u00c0\u00c4\u00e4 \u0308\u00d1< =UXa5W \u00a3}W Z=fk\u00a7X\u00e5=gP[ n W'\u00a7xf\u0094Z=fk\u00a3wW9Z9c\u0095p=e YP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _ c\u0095\u00a7 fk\u00e8' =c\u0095\u00a6tYP\u00ac\u0095f}p'\u00a1V\u00a1 \u0308W\u009fS m W`p Y0\u00ac\u0095W W`\u009e \\\u0091e`U \u0304YP\u009e n\u00c6\u00e9 j9g Q;O]\u00ea\u00b0\u00ca \u00e7 \u00a1 n f}U\u00c9\u00a7 \u0308 R\u00a3}\u00a3}fk\u00a7X\u00a7\u0096\u00c0\u00c7 R\u00ac\u00ce\u00a3kY`\u00a7 \u0308fk\u00a7\u0083c\u00a9p5\u00a3w\u00ac\u0095 RZ9f\u00a5\u00a3wW`_\u009f\u009e= =\u00a1 \u0308f}U \u00a6 c\u0095\u00a7 \u0308c\u0095W`p \u009e=UXW`a=\u00ac\u0095f}_\u0091\u00a7\u00c5YPpRZ!_\u009ffkZ=c\u0095\u00a3kYP\u00ac\u0083Z=c\u0095Y`e`W`p5\u00a7xcd\u00a7 \u00e9 j=gAN g\u0083Q]N;\u00eaV\u00a7 \u0308 =e(e`fk\u00a7x\u00a1 \u0308c\u0095p=e \u00a1 n f\u00d3\u00a7 \u0308 R\u00a3}\u00a3}fk\u00a7X\u00a70W`\u00c0\u00c9S m W(p \u00ac\u0095W'W(\u009e \\ie`U \u0304YP\u009e n \u00a7\u00a5c\u0095\u00a70p=WP\u00a10\u00ac\u0095c\u00a9_\u009fc\u00a9\u00a1 \u0308f;Z!\u00a1 \u0308W$\u00a3}W Z=c\u00a9p=e$Y`\u009e=\u009e=\u00ac\u0095c\u0095\u00a3kYt\u00a1Xc\u00a9W(pR\u00a7}\u00ca \u00e7 p\u00cb\u00a1 n f\u009bW`\u00a1 n fkU n Y`pRZ g<\u00c0\u00c7W`U0W`\u00a1 n fkU e`U \u0304YP\u009e n \u00a7V[\u00c9c\u0087\u00a1 n \u00ac\u0095W W`\u009eR\u00a7kg=S m _\u009fY]\\\u00d7e`c\u0095\u00a6`f\u00a5\u009e5W W(U\u00c9U \u0308f;\u00a7x =\u00ac\u00a9\u00a1X\u00a7\u00c4W(U\u00c4\u00c0\u008dYPc\u0095\u00ac\u00ce\u00a1 \u0308W\u009b\u00a3}W`p \u00a6`fkU \u0308e(f \u00e9 Q;N;\u00ea\u00b0\u00ca \u00e6=W(U Y e`fkp=f}U \u0304YP\u00ac\u0098e`U \u0304YP\u009e n gM\u00ac\u00a9c\u00a9\u00a1x\u00a1X\u00ac\u00a9f n Y(\u00a7\u00d7a\u00bff}fkp\u00c2 =pRZ=f}U \u0304\u00a7\u0096\u00a1XW'W9Z\u00daY`a5W( 9\u00a1\u00d3[ n Yt\u00a1 YP\u009e=\u009e=UXW]\u00d09c\u00a9_\u0091YP\u00a1 \u0308c\u0095W`p\u00daS m UXf}\u009e=UXfk\u00a7 \u0308f}p'\u00a1X\u00a7kgoYPp5Z n Wt[\u00dcc\u0087\u00a1\u0091_\u009fc\u00a9e n \u00a1 a\u00bff c\u00a9_\u009f\u009e=UXWt\u00a6`f;Z \u00ca \u00d1 n cd\u00a7 \u009eRYP\u009e\u00bff}U;\u00d4 \u00a7 e(W(YP\u00acVcd\u00a7 \u00a1 \u0308W\u00cb\u009e=U \u0308Wt\u00a6 cdZ9f\u00d3\u00a1 n YP\u00a1 =pRZ=f}U \u0304\u00a7\u0096\u00a1 \u0304YPpRZ9c\u0095p=e!YPpRZ c\u00a9p'\u00a1XU \u0308W9Z9 R\u00a3}f Y\u00cb\u00a7 \u0308fw\u00a1\u009fWP\u00c0Mp=fk[\u00dcYP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _\u009f\u00a7\u00c5UXfk\u00a7 \u0308 =\u00ac\u0087\u00a1Xc\u00a9pRei\u00c0\u00c7UXW`_\u00eb\u00a1 n Yt\u00a1\u0091 =pRZ=f}U \u0308u \u00a7x\u00a1XYPp5Z9c\u00a9pReR\u00ca \u00cf f\u00d7\u00a7 n Wt[ \u00a1 n Yt\u00a1 S m cd\u00a7 \u00a1 n f\u009f\u00cdRU \u0304\u00a7\u0096\u00a1bc\u0095p Y \u009e=UXW`e`UXfk\u00a7X\u00a7 \u0308c\u00a9W(piWP\u00c0\u00c4\u00ac\u0095W \u00a3kYP\u00aco_\u009ff;\u00a7 \u0308\u00a7XYPe(fwu\u00b0\u009eRY`\u00a7X\u00a7xc\u0095p=e YP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _\u009f\u00a7kg3f;Y`\u00a3 n e`c\u0095\u00a6'c\u0095p=e fk\u00e8' =c\u0095\u00a6tYP\u00ac\u0095f}p'\u00a10U \u0308f;\u00a7x R\u00ac\u0087\u00a1 \u0304\u00a7T\u00a1 \u0308W$Y \u00a3wW(U \u0308UXfk\u00a7 \u0308\u009e5W(pRZ9c\u0095p=e YP\u009e=\u009e=UXW]\u00d09c\u00a9_\u0091YP\u00a1 \u0308c\u0095W`p \u00c0\u00c7UXW`_ \u00a7x\u00a1XYt\u00a1Xc\u0095\u00a7x\u00a1 \u0308cd\u00a3}Y`\u00ac5\u009e n \\9\u00a7 \u0308c\u0095\u00a3k\u00a7V\u00ab'pRWt[\u00c9p\u00d7Y`\u00a7K\u00a1 n f \u00e4X\u00d5\u00a5c\u00a9\u00ab  R\u00a3 n cd\u00e50Y`\u009e=\u009e=UXW]\u00d0 c\u0095_\u0091Yt\u00a1Xc\u00a9W(p\u009f\u00a1 \u0308W\u00c5\u00a1 n f \u00d8Tc\u00a9aRaR\u00a7\u0083\u00c0\u00c7U \u0308fkf\u0098fkp=f}UXe`\\(\u00ca \u00d1 n f;\u00a7xf YP\u00ac\u0095e`W(U \u0308c\u00a9\u00a1 n _\u0091\u00a7 n Y]\u00a6`f\u00d3\u00a1 n f YP\u00a1x\u00a1XUXY(\u00a3\u03bc\u00a1 \u0308c\u0095\u00a6`f\u00d3\u009e=UXW`\u009e\u00bff}U \u0308\u00a1\u0096\\\u00cbW`\u00c0\u0098a\u00bff}c\u0095p=e\u00cb R\u00a7xfkUxuvY`Zt\u00db\u0096 R\u00a7\u0096\u00a1 \u0304YPa=\u00ac\u0095f`q\u00d3a \\ \u009eRY]\\'u c\u0095p=e\u00d7\u00a7 \u0308W`_\u009ff0Y`Z=Z=c\u0087\u00a1Xc\u00a9W(pRYP\u00ac7\u00a3wW`_\u009f\u009e= =\u00a1XYt\u00a1Xc\u00a9W(pRYP\u00ac<\u00a3wW(\u00a7x\u00a1kg5W`p=f\u00c5\u00a3}YPp W(a9\u00a1XY`c\u00a9p$\u00a3}W`pR\u00a7 \u0308cdZ9f}U \u0304YPa=\u00ac\u0095f\u00a5c\u00a9_\u009f\u009e=UXWt\u00a6`fk_\u009ff}p'\u00a1 c\u0095p \u00a1 n f$Y`\u00a3k\u00a3w =U \u0304Y`\u00a3}\\\u00cbW`\u00c0\u0098W(p=f`\u00d4 \u00a7\u009fYP\u009e=\u009e=UXW]\u00d09c\u00a9_\u0091YP\u00a1 \u0308c\u0095W`p3gKY`pRZ\u00ec\u00a3}YPp\u00ec\u00a7xW(_\u009ffw\u00a1 \u0308c\u0095_\u009ffk\u00a7\u009fW(a9\u00a1XY`c\u00a9p1Y\u00c6\u00a3}W`p \u00a6`fkU \u0308e(f}p'\u00a1 _\u009ffk\u00a7X\u00a7 \u0308Y`e`f}u\u009a\u009eRY(\u00a7 \u0308\u00a7 \u0308c\u0095p=e\u00c5Y`\u00ac\u00a9e(W`UXc\u0087\u00a1 n _\u00ed[ n f}p W(UXZ9c\u0095pRY`U \u0308\\\u009bS m Z9W fk\u00a7\u00c4p=W`\u00a1M\u00a3wW(p'\u00a6(f}UXe`f(\u00ca \u00ee \u00ef!\u00f0\u00c8\u00f1 \u00e3 \u00f03\u00f2\u00c5\u00f3 1\u20444=\u00e0 \u00f3 1\u204427\u00f4K1\u20442<\u00bb=\u00e3x\u00e0 \u00df\u00f6\u00f5\u0098\u00f7\u00c9\u00f0 \u00e1\u00c4\u00f8 \u00f3 \u00e0K\u00e3 \u00df \u00bb=o 1\u20442C1\u20444 \u00f0\u00c2\u00f9 \u00f0 1\u20444=\u00e0 \u00f4o1\u20444=1\u20442A\u00e1\u0098\u00e3 \u00f0\u00c8\u00df \u00bb \u00f3 \u00e0K\u00e3 \u00df \u00bb=o\u00cb\u00e0 \u00f2 \u00bb9\u00fa \u00f0?\u00ef!\u00f0 \u00bb9\u00fa \u00f0 \u00f2 1\u20444 \u00f03\u00f0%\u00f0<\u00df\u0094\u00f0 1\u20444=\u00f4o\u00fb \u00e6=W(U \u00a1 n f\u0091\u009e= =UX\u009e5W'\u00a7xf\u0091W`\u00c0\u00c4Y`pRYP\u00ac\u0095\\ \u00d2}c\u0095p=e YPpRZ!Z9f;\u00a7 \u0308\u00a3}U \u0308c\u0095a=c\u0095p=e S m [Vf\u009bY`\u00a7X\u00a7 \u0308 =_\u009ff\u00c5\u00a1 n Yt\u00a1\u00c5[\u0083f\u009bY`U \u0308f\u009fe`c\u0095\u00a6`fkp\u00cbY`p =pRZ=c\u00a9UXfk\u00a3w\u00a1 \u0308fkZ\u009fe(UXY`\u009e n cd\u00a3}Y`\u00ac9_ W9Z9fk\u00ac=WP\u00c03\u00fc\u00fdp=W9Z9f;\u00a7o[\u00c9c\u00a9\u00a1 n \u009eRY`c\u00a9UX[\u00c9c\u0095\u00a7 \u0308f\u00c4\u009e5W`\u00a1 \u0308fkp(\u00a1Xc\u0095Y`\u00ac\u0095\u00a7M\u00c1\u008dY \u009eRY`c\u00a9UX[\u00c9cd\u00a7xf H Y`U \u0308\u00ab(Wt\u00a6 J Y`pRZ9W`_\u00f6\u00e67c\u0095f}\u00acdZ \u00c3 \u00ca h p \\\u009be`U \u0304YP\u009e n c\u0095\u00a3kYP\u00ac\u00bf_\u009fW Z=f}\u00ac3\u00a3}Y`p\u00d7a\u00bff0\u00a3wW(p'\u00a6(f}U \u0308\u00a1 \u0308f;Z\u0091c\u00a9p'\u00a1XW\u00c5\u00a1 n cd\u00a7V\u00c0\u00c7W`UX_\u00eda5f}\u00c0\u00c7W`UXf\u00a5Z=W`c\u0095p=e c\u0095p9\u00c0\u00c7f}UXf}pR\u00a3}f\u00c4\u00a1 n UXW` Re n Y \u00a7x =c\u00a9\u00a1XY`a=\u00ac\u0095f\u0094\u00a3w\u00ac\u0095 R\u00a7\u0096\u00a1Xf}UXc\u00a9pRe WP\u00c0\u00cep=W9Z9fk\u00a7Ac\u0095p'\u00a1 \u0308W \u00acdYPUXe`fkU7pRW Z=fk\u00a7 \u00e9 Q]sk\u00ea\u00b0\u00caA\u00d1 n UXW` =e n \u00a1 n c\u0095\u00a7 \u00a1 \u0308U \u0304YPp5\u00a7\u0096\u00c0\u00c7W(U \u0308_\u0091Yt\u00a1Xc\u00a9W(p3g\u00c8W(p=f\u009b\u00a3}Y`p/YP\u009e=\u009eR\u00ac\u00a9\\\u00cbYP\u00ac\u0095\u00aco\u00a1 n f\u009bU \u0308f;\u00a7x R\u00ac\u0087\u00a1 \u0304\u00a7 c\u00a9p/\u00a1 n cd\u00a70\u009eRY`\u009e5fkU \u00a1 \u0308W\u00c6YPUXa=c\u0087\u00a1XUXY`U \u0308\\ie`U \u0304YP\u009e n cd\u00a3}Y`\u00ac _\u009fW9Z9f}\u00acd\u00a7Kc\u0095pR\u00a3}\u00ac\u00a9 RZ=c\u00a9p=eb\u00a1 n W'\u00a7xfM[\u00c9c\u0087\u00a1 n\u00d7n c\u0095e n fkUxu\u00b0W`U \u0304Z9fkUA\u009e5W`\u00a1 \u0308f}p'\u00a1Xc\u0095Y`\u00ac\u0095\u00a7k\u00ca \u00cf f\u0098[\u00c9c\u00a9\u00ac\u0095\u00ac5e(c\u00a9\u00a6(f\u0094fw\u00d09\u009e=\u00ac\u0095cd\u00a3wc\u00a9\u00a1\u0083fw\u00d0=Y`_ \u009eR\u00ac\u00a9f;\u00a7 WP\u00c07\u00a1 n c\u0095\u00a7\u00c4c\u0095p \u00ac\u0095YP\u00a1 \u0308f}U\u0094\u00a7 \u0308fk\u00a3w\u00a1 \u0308c\u0095W`pR\u00a7k\u00ca \u00d1 n f\u009f\u00a7\u0096\u00a1 \u0304Yt\u00a1Xf\u00c5W`\u00c0KfkY(\u00a3 n  =p=W(aR\u00a7xfkU \u0308\u00a6(fkZ p=W9Z9f \u00fe\u00c9cd\u00a7\u0098Z9fkp=WP\u00a1XfkZia'\\ \u00ff xg3Y`pRZ$[Vf\u00c5Y(\u00a7 \u0308\u00a7 \u0308 =_\u009ffbf;Y`\u00a3 n  Rp=W`a9u \u00a7 \u0308f}UX\u00a6`fkZ\u009fpRW Z=fMcd\u00a7\u0083\u00a3}W`p=p=f;\u00a3\u03bc\u00a1XfkZ\u009f\u00a1XW\u00c5YPp\u00d7W`aR\u00a7 \u0308f}UX\u00a6`f;Z\u009fp=W9Z9f x\u00caA\u00d1 n fK\u00db\u0096W(c\u00a9p'\u00a1V\u009e=U \u0308W(aRYPaRc\u00a9\u00ac\u0095c\u0087\u00a1\u0096\\\u009fZ9cd\u00a7\u0096\u00a1XU \u0308c\u0095a= 9\u00a1Xc\u00a9W(p \u00c0\u00c7 =pR\u00a3w\u00a1 \u0308c\u0095W`p cd\u00a7\u00c9e`c\u0095\u00a6`fkp\u00d7a \\ \u00c1\u00c7\u00ff x\u00ff  \u0308\u00ff \u00c3 Q \u00c1\u00c7\u00ff x\u00ff \u00c3 \u00c1\u008d\u00ff \u00c3 \u00c1\u0096Q \u00c3 [ n fkU \u0308f  \u0308\u00c1\u008d\u00ff \u00c3 c\u0095\u00a7T\u00a1 n f\u009f\u00ac\u00a9W9\u00a3}Y`\u00ac \u00e4xfk\u00a6'cdZ9fkpR\u00a3wf;\u00e5\u0091\u00c0\u00c7W`U pRW Z=f \u00fe \u0304g (\u00c1\u00c7\u00ff  \u0308\u00ff! \u00c3 cd\u00a7T\u00a1 n f\u0091\u00a3wW(_\u009f\u009eRYt\u00a1Xc\u00a9a=c\u0095\u00ac\u0095c\u0087\u00a1\u0096\\ _\u0091Yt\u00a1XU \u0308c\u00a9\u00d0\u009ba5f}\u00a1\u0096[\u0083fkf}p p=W9Z9fk\u00a7\u00c4\u00fe\u0083YPpRZ \"5g=YPpRZ c\u0095\u00a7\u00c9Y p=W(U \u0308_\u0091YP\u00ac\u0095c\u0095\u00d2kYt\u00a1Xc\u00a9W(p\u00d7\u00a3wW(pR\u00a7x\u00a1XYPp'\u00a1;\u00ca$#vp [ n YP\u00a1V\u00c0\u00c7W(\u00ac\u00a9\u00ac\u0095Wt[\u0094\u00a7kg [Vf\u00a5\u00a7 \u0308c\u0095_ \u009eR\u00ac\u00a9c\u00a9\u00c0\u00c7\\\u00d7p=W`\u00a1XYP\u00a1 \u0308c\u0095W`p YPpRZ [\u00c9UXc\u0087\u00a1Xf X\u00c1\u00c7\u00ff \u00c3 Y(\u00a7\u00c9\u00a7 n W(Ux\u00a1 n YPp5Z\u009b\u00c0\u00c7W(U X\u00c1\u00c7\u00ff \u00c3 \u00ca \u00d1 n f0\u00a7x\u00a1XY`pRZ=YPU \u0304Z\u00d7S m  =\u009e\u00ceZ=Yt\u00a1Xf U \u0308 R\u00ac\u00a9f;\u00a7\u00c9YPUXf`q % (\u00c1\u008d\u00ff! \u00c3'& (*),+.'\u00c1\u00c7\u00ff  \u0308\u00ff! \u00c3 X\u00c1\u00c7\u00ff \u00c3 /.0 21 3546 % / X\u00c1\u00c7\u00ff \u00c3 \u00c1GN \u00c3 7  \u0308\u00c1\u00c7\u00ff \u00c3'& (  \u0308\u00c1\u008d\u00ff \u00c3 /.0 21 3 % /  \u0308\u00c1\u008d\u00ff \u00c3 \u00c1\u008dj \u00c3 [ n fkU \u0308f ( Z=f}p=W`\u00a1 \u0308fk\u00a7\u00c4Y\u00c5p=W`UX_\u0091YP\u00ac\u0095c\u00a9\u00d2;Yt\u00a1Xc\u00a9W(p\u00d3\u00a3wW(pR\u00a7x\u00a1XYPp'\u00a1\u00c9YPpRZ\u00d3\u00fc/\u00c1\u00c7\u00fe \u00c3 8 \" _\u009ffkY`pR\u00a7\u00c9YP\u00ac\u0095\u00ac\u00cep=W9Z9fk\u00a7Vp=fkc\u00a9e n a\u00bfW`UXc\u0095p=e p=W9Z9f \u00fe \u0304g5f}\u00d09\u00a3}f}\u009e9\u00a1*\"5\u00caT\u00aeMf}UXf % \u00c5U \u0308f}\u00c0\u00c7f}U \u0304\u00a7\u00c9\u00a1XW\u009b\u00a1 n f\u00c5_\u009ffk\u00a7X\u00a7XYPe`f0\u00a1 n YP\u00a1Tp=W9Z9f \u00fe\u00c9\u00a7xfkpRZ=\u00a7M\u00a1 \u0308W\u00d3p=W9Z9f9\" Y`pRZ 7 7cd\u00a7K\u00a1 n f\u00a5a5"
            },
            "slug": "Bethe-free-energy,-Kikuchi-approximations,-and-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Bethe free energy, Kikuchi approximations, and belief propagation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown that BP canonly converge to a stationarypoint of an approximatefree energy, known astheBethefreeenergy in statisticalphysics, as of its release on April 4, 2001."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145881044"
                        ],
                        "name": "M. Luby",
                        "slug": "M.-Luby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Luby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745699"
                        ],
                        "name": "M. Mitzenmacher",
                        "slug": "M.-Mitzenmacher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mitzenmacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mitzenmacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143916589"
                        ],
                        "name": "A. Shokrollahi",
                        "slug": "A.-Shokrollahi",
                        "structuredName": {
                            "firstName": "Amin",
                            "lastName": "Shokrollahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shokrollahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798326"
                        ],
                        "name": "V. Stemann",
                        "slug": "V.-Stemann",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Stemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Stemann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8625981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a11da39c2db260e946899c871bb073b81fa2760a",
            "isKey": false,
            "numCitedBy": 786,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present randomized constructions of linear-time encodable and decodable codes that can transmit over lossy channels at rates extremely close to capacity. The encod-ing and decoding algorithms for these codes have fast and simple software implementations. Partial implementationsof our algorithms are faster by orders of magnitude than the best software implementations of any previous algorithm forthis problem. We expect these codes will be extremely useful for applications such as real-time audio and video transmission over the Internet, where lossy channels are common and fast decoding is a requirement. Despite the simplicity of the algorithms, their design andanalysis are mathematically intricate. The design requires the careful choice of a random irregular bipartite graph,where the structure of the irregular graph is extremely important. We model the progress of the decoding algorithmby a set of differential equations. The solution to these equations can then be expressed as polynomials in one variable with coefficients determined by the graph structure. Based on these polynomials, we design a graph structure that guarantees successful decoding with high probability"
            },
            "slug": "Practical-loss-resilient-codes-Luby-Mitzenmacher",
            "title": {
                "fragments": [],
                "text": "Practical loss-resilient codes"
            },
            "venue": {
                "fragments": [],
                "text": "STOC '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2989237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f3d6c191d658229e97ed42828c67cc0ddb11585",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Nearest-neighbor interpolation algorithms have many useful properties for applications to learning, but they often exhibit poor generalization. In this paper, it is shown that much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size. The resulting method is called variable-kernel similarity metric (VSM) learning. It has been tested on several standard classification data sets, and on these problems it shows better generalization than backpropagation and most other learning methods. The number of parameters that must be determined through optimization are orders of magnitude less than for backpropagation or radial basis function (RBF) networks, which may indicate that the method better captures the essential degrees of variation in learning. Other features of VSM learning are discussed that make it relevant to models for biological learning in the brain."
            },
            "slug": "Similarity-Metric-Learning-for-a-Variable-Kernel-Lowe",
            "title": {
                "fragments": [],
                "text": "Similarity Metric Learning for a Variable-Kernel Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size to create a variable-kernel similarity metric (VSM) learning."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16378222,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9bfe7080107d3bdc21bd937593f91932ea40a524",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a natural way of defining prior distributions over functions of one or more input variables. In a simple nonparametric regression problem, where such a function gives the mean of a Gaussian distribution for an observed response, a Gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases. Hyperparameters that define the covariance function of the Gaussian process can be sampled using Markov chain methods. Regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations. Software is now available that implements these methods using covariance functions with hierarchical parameterizations. Models defined in this way can discover high-level properties of the data, such as which inputs are relevant to predicting the response."
            },
            "slug": "Monte-Carlo-Implementation-of-Gaussian-Process-for-Neal",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Software is now available that implements Gaussian process methods using covariance functions with hierarchical parameterizations, which can discover high-level properties of the data, such as which inputs are relevant to predicting the response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187514"
                        ],
                        "name": "R. Durbin",
                        "slug": "R.-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708077"
                        ],
                        "name": "S. Eddy",
                        "slug": "S.-Eddy",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Eddy",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 262
                            }
                        ],
                        "text": "Examples of latent variable models include Chapter 22\u2019s mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang, 1986; Durbin et al., 1998); and factor analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2852254,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "571f5bbecd3a083a2bb6844f59a3f8cea237252e",
            "isKey": false,
            "numCitedBy": 4477,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field."
            },
            "slug": "Biological-Sequence-Analysis:-Probabilistic-Models-Durbin-Eddy",
            "title": {
                "fragments": [],
                "text": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "112917405"
                        ],
                        "name": "A.",
                        "slug": "A.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "A.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 60
                            }
                        ],
                        "text": "For a review of the history of spread-spectrum methods, see Scholtz (1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 51396490,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a931ac1b3ef11235d4516f341b3b5bd8b1ee330e",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstrucz-This monograph reviews events, circa 1920-1960, leading to the development of spread-spectrum communication systems. The WHYN, Hush-Up, BLADES, F9C-A/Rake, CODORAC, and ARC-SO systems are featured, along with a description of the prior art in secure  communications,  and  introductions  to  other  early spreadspectrum  communication  efforts.  References  to he available literature from this period are included."
            },
            "slug": "The-Origins-of-Spread-Spectrum-Communications-A.",
            "title": {
                "fragments": [],
                "text": "The Origins of Spread-Spectrum Communications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145881044"
                        ],
                        "name": "M. Luby",
                        "slug": "M.-Luby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Luby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150202208"
                        ],
                        "name": "M. Amin Shokrolloahi",
                        "slug": "M.-Amin-Shokrolloahi",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Amin Shokrolloahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Amin Shokrolloahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150222719"
                        ],
                        "name": "M. Mizenmacher",
                        "slug": "M.-Mizenmacher",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Mizenmacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mizenmacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122821434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d45dac1b8dd39c985a6ee4c911c78ef5d464897d",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct new families of low-density parity-check codes, which we call irregular codes. When decoded using belief propagation, our codes can correct more errors than previously known low-density codes. Our improved performance comes from using codes based on irregular random bipartite graphs, based on the work of Luby et al. (1997). Previously studied low-density codes have been derived from regular bipartite graphs. Initial experimental results for our irregular codes suggest that, with improvements, irregular codes may be able to match turbo code performance."
            },
            "slug": "Improved-low-density-parity-check-codes-using-and-Luby-Shokrolloahi",
            "title": {
                "fragments": [],
                "text": "Improved low-density parity-check codes using irregular graphs and belief propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Initial experimental results for the new families of low-density parity-check codes constructed based on irregular random bipartite graphs suggest that, with improvements, irregular codes may be able to match turbo code performance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Symposium on Information Theory (Cat. No.98CH36252)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 4
                            }
                        ],
                        "text": "See (Wiberg, 1996; Frey, 1998; McEliece et al., 1998) for further discussion of the sum-product algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 75
                            }
                        ],
                        "text": "Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 31
                            }
                        ],
                        "text": "They were rediscovered in 1995 (MacKay and Neal, 1996; Wiberg, 1996) and shown to have outstanding theoretical and practical properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115168171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb44d50bce92b4ce2c0ea53bd8ede95f628ee3cb",
            "isKey": false,
            "numCitedBy": 1007,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the \u201cTanner graph\u201d. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations."
            },
            "slug": "Codes-and-Decoding-on-General-Graphs-Wiberg",
            "title": {
                "fragments": [],
                "text": "Codes and Decoding on General Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932728"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727633"
                        ],
                        "name": "R. Urbanke",
                        "slug": "R.-Urbanke",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Urbanke",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urbanke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11770980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ab6692caf7cc6b25e27fb59531c3a193c80d032",
            "isKey": false,
            "numCitedBy": 1030,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Low-density parity-check (LDPC) codes can be considered serious competitors to turbo codes in terms of performance and complexity and they are based on a similar philosophy: constrained random code ensembles and iterative decoding algorithms. We consider the encoding problem for LDPC codes. More generally we consider the encoding problem for codes specified by sparse parity-check matrices. We show how to exploit the sparseness of the parity-check matrix to obtain efficient encoders. For the (3,6)-regular LDPC code, for example, the complexity of encoding is essentially quadratic in the block length. However, we show that the associated coefficient can be made quite small, so that encoding codes even of length n/spl sime/100000 is still quite practical. More importantly, we show that \"optimized\" codes actually admit linear time encoding."
            },
            "slug": "Efficient-encoding-of-low-density-parity-check-Richardson-Urbanke",
            "title": {
                "fragments": [],
                "text": "Efficient encoding of low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how to exploit the sparseness of the parity-check matrix to obtain efficient encoders and it is shown that \"optimized\" codes actually admit linear time encoding."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9374636,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8c9e9e4e8caf7d788b3a14c50b2881f4cb21c115",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We hypothesize that the origin of the genetic code is associated with the structure of the tRNA that existed in primal cells. The sequences of modern tRNA contain correlations which can be understood as \"fossil\" evidence of the secondary structure of primal tRNA. Kinetic proofreading through diffusion can amplify a low level of intrinsic selectivity of tRNA for its amino acid. Experimental tests of the theory are suggested."
            },
            "slug": "Origin-of-the-genetic-code:-a-testable-hypothesis-Hopfield",
            "title": {
                "fragments": [],
                "text": "Origin of the genetic code: a testable hypothesis based on tRNA structure, sequence, and kinetic proofreading."
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "It is hypothesized that the origin of the genetic code is associated with the structure of the tRNA that existed in primal cells andKinetic proofreading through diffusion can amplify a low level of intrinsic selectivity of tRNA for its amino acid."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145469967"
                        ],
                        "name": "M. Huber",
                        "slug": "M.-Huber",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Huber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Huber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6955448,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "50472409437fcd35643a34ed622bdb43ea3306b4",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two algorithms for uniformly sampling from the proper colorings of a graph using colors. We use exact sampling from the stationary distribution of a Markov chain with states that are the -colorings of a graph with maximum degree . As opposed to approximate sampling algorithms based on rapid mixing, these algorithms have termination criteria that allow them to stop on some inputs much more quickly than in the worst case running time bound. For the first algorithm we show that when , the algorithm has an upper limit on the expected running time that is polynomial. For the second algorithm we show that for where is an integer that satisfies , the running time is polynomial. Previously, Jerrum showed that it was possible to approximately sample uniformly in polynomial time from the set of -colorings when but our algorithm is the first polynomial time exact sampling algorithm for this problem. Using approximate sampling, Jerrum also showed how to approximately count the number of -colorings. We give a new procedure for approximately counting the number of -colorings that improves the running time of the procedure of Jerrum by a factor of when , where is the number of nodes in the graph to be colored and is the number of edges. In addition, we present an improved analysis of the chain of Luby and Vigoda for exact sampling from the independent sets of a graph. Finally, we present the first polynomial time method for exactly sampling from the sink free orientations of a graph. Bubley and Dyer showed how to approximately sample from this state space in ! \" $#&%(') +*-,/.0 1 time, our algorithm takes 23 45 expected time. 6 This work was partially supported by an Office of Naval Research Fellowship and NSF grants CCR-9307391, CCR-9700029, and DMS-9505155, and ONR grant N00014-96-1-00500 1 Introduction Recently a number of exciting results have appeared in the area of Monte Carlo Markov Chain (MCMC) theory. One such result is the procedure of Propp and Wilson [10] known as coupling from the past (CFTP), which allows us to sample directly from the stationary distribution of certain Markov chains without visiting each state in the chain. Many chains that arise naturally out of statistical mechanics and approximate counting problems have a number of states exponential in the size of the input. Although this makes it impossible to efficiently compute the entire stationary distribution, using CFTP we can still sample efficiently from the stationary distribution. The state space we are primarily interested in sampling from here is the set of proper colorings of a graph 798 ;: using colors. A proper coloring of a graph 7 is an assignment of colors to nodes so that no two neighboring nodes receive the same color. This state space is a special case of a framework from statistical mechanics known as the Potts model. The ability to sample efficiently from state spaces such as the Potts model leads to better approximate counting algorithms and has applications in statistical mechanics (see [1]). The -coloring problem is of interest in complexity theory. Jerrum, Valiant, and Vazirani [6] showed that for a class of problems which includes -colorings that a method for efficient approximate sampling from the state space could be used to construct an efficient method for approximating the size of the state space. Counting the number of -colorings of a graph is a ?A@ -complete problem, making it unlikely that an efficient algorithm will be found to solve it exactly. In the next section we describe the Potts model in more detail, after which we present a brief description of CFTP, along with our first algorithm for exact sampling from the -colorings of a graph. In section 5 we present our exact coloring sampling algorithm, which is the first to run in polynomial time. We then present a second exact sampling algorithm for the -coloring chain that uses both CFTP and rejection sampling, and has polynomial running time. We then briefly discuss the extension of these methods to the general Potts model. We then present an algorithm for approximately B counting the number of -colorings that improves upon the running time of the previous method (due to Jerrum) by a factor of 23 $ C C ED Finally we present the first polynomial time method for exact sampling from the sink free orientations of a graph. This algorithm has a longer running time than the previous approximate sampling algorithm due to Bubley and Dyer [2]. We use the total variation distance to quantify what we mean by approximate and exact sampling. If the distributions F and G put probability mass on a finite set, the total variation distance between them is H F IJG H KML 8 N O P H F \u008d D These algorithms are weaker than Jerrum\u2019s in that they require more colors to run in polynomial time. However, they are exact sampling algorithms whereas Jerrum\u2019s method only generates an approximate sample. Moreover, these algorithms might finish running before the bounds given on the the running time would indicate. In the algorithms which rely on rapid mixing, the algorithm must always take the same worst case amount of steps in the Markov chain. In algorithms like ours, which are based on CFTP, we have termination criteria that allow us to end the algorithm before the worst case analysis would indicate. In addition, our algorithms are exact samplers, and so the running time does not depend on * , making them faster than Jerrum\u2019s method by a factor of %(') N *E . Unlike Jerrum\u2019s method, however, the running time of these algorithms is random, and to ensure that the algorithm terminates with a probability of at least N I | , it is necessary to run for an extra factor of %(') N | time. Note that * , which bounds the total variation, will usually be much smaller than | which bounds the probability our algorithm does not complete on schedule. Since these sampling algorithms are often run many times (for example in the counting applications) the running time is often even more closely concentrated around the expected running time. In the chains we consider here, there is a color set \u008e , a vertex set : , and the state space of the Markov chain is \u008f\u0091\u0090 \u008e L . For example, in the -colorings of a graph, : is the vertex set of our graph 7m8\u0092 ;: , and \u008f is the set of proper colorings of the nodes of : . Some examples of chains in this class include the hard core gas model and the sink free orientations of a graph, which are discussed in section 2."
            },
            "slug": "Exact-sampling-and-approximate-counting-techniques-Huber",
            "title": {
                "fragments": [],
                "text": "Exact sampling and approximate counting techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work presents two algorithms for uniformly sampling from the proper colorings of a graph using colors using the Potts model, and presents the first polynomial time exact sampling algorithm for this problem, which is the first to run in polynometric time."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2202717"
                        ],
                        "name": "Linda C. Bauman Peto",
                        "slug": "Linda-C.-Bauman-Peto",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Peto",
                            "middleNames": [
                                "C.",
                                "Bauman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda C. Bauman Peto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11426560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fa57bd91f731522c861404d29e4604ba6ac6d3",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as 'smoothing'. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. The new algorithm is compared with smoothing on a two million word corpus. The methods prove to be about equally accurate, with the hierarchical model using fewer computational resources."
            },
            "slug": "A-hierarchical-Dirichlet-language-model-Mackay-Peto",
            "title": {
                "fragments": [],
                "text": "A hierarchical Dirichlet language model"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as 'smoothing' is discussed, and the methods prove to be about equally accurate, with the hierarchical model using fewer computational resources."
            },
            "venue": {
                "fragments": [],
                "text": "Nat. Lang. Eng."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1834562"
                        ],
                        "name": "H. Omre",
                        "slug": "H.-Omre",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "Omre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Omre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 137
                            }
                        ],
                        "text": "Kriging has been developed considerably in the last thirty years (see Cressie (1993) for a review) including several Bayesian treatments (Omre, 1987; Kitanidis, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120589524,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "bab80db95e06b666bce04c9991f4f2cffb4c7766",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Frequently a user wants to merge general knowledge of the regionalized variable under study with available observations. Introduction of fake observations is the usual way of doing this. Bayesian kriging allows the user to specify a qualified guess, associated with uncertainty, for the expected surface. The method will provide predictions which are based on both observations and this qualified guess."
            },
            "slug": "Bayesian-kriging\u2014Merging-observations-and-qualified-Omre",
            "title": {
                "fragments": [],
                "text": "Bayesian kriging\u2014Merging observations and qualified guesses in kriging"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3334426"
                        ],
                        "name": "S. Adler",
                        "slug": "S.-Adler",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Adler",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Adler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In Adler\u2019s (1981) overrelaxation method, one instead samples x (t+1) i from a Gaussian that is biased to the opposite side of the conditional distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120579440,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5bc46f7416ac3d6e7d80a8a4552c0969093291f7",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I formulate a successive over-relaxation (SOR) procedure for the Monte Carlo evaluation of the Euclidean partition function for multiquadratic actions (such as the Yang-Mills action with canonical gauge fixing). A convergence analysis for the quadratic-action (Abelian) case shows that as thermalization proceeds the mean nodal fields relax according to the difference equation arising from the standard SOR analysis of the associated classical Euclidean field equation. Hence, SOR should accelerate the thermalization process, just as it accelerates convergence in the numerical solution of second-order elliptic differential equations."
            },
            "slug": "Over-relaxation-method-for-the-Monte-Carlo-of-the-Adler",
            "title": {
                "fragments": [],
                "text": "Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29936138"
                        ],
                        "name": "E. Marinari",
                        "slug": "E.-Marinari",
                        "structuredName": {
                            "firstName": "Enzo",
                            "lastName": "Marinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marinari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145052965"
                        ],
                        "name": "G. Parisi",
                        "slug": "G.-Parisi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Parisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 50
                            }
                        ],
                        "text": "The closely related \u2018simulated tempering\u2019 method (Marinari and Parisi, 1992) corrects the biases introduced by the annealing process by making the temperature itself a random variable that is updated in Metropolis fashion during the simulation. Neal\u2019s (1998) \u2018annealed importance sampling\u2019 method removes the biases introduced by annealing by computing importance weights for each generated point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 49
                            }
                        ],
                        "text": "The closely related \u2018simulated tempering\u2019 method (Marinari and Parisi, 1992) corrects the biases introduced by the annealing process by making the temperature itself a random variable that is updated in Metropolis fashion during the simulation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12321327,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f909aeaef41c781f8b4e27d5c3cc854dd3ddb3cc",
            "isKey": false,
            "numCitedBy": 1513,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new global optimization method (Simulated Tempering) for simulating effectively a system with a rough free-energy landscape (i.e., many coexisting states) at finite nonzero temperature. This method is related to simulated annealing, but here the temperature becomes a dynamic variable, and the system is always kept at equilibrium. We analyse the method on the Random Field Ising Model, and we find a dramatic improvement over conventional Metropolis and cluster methods. We analyse and discuss the conditions under which the method has optimal performances."
            },
            "slug": "Simulated-tempering:-a-new-Monte-Carlo-scheme-Marinari-Parisi",
            "title": {
                "fragments": [],
                "text": "Simulated tempering: a new Monte Carlo scheme"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144529123"
                        ],
                        "name": "N. J. Cohen",
                        "slug": "N.-J.-Cohen",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Cohen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. J. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38234690"
                        ],
                        "name": "N. Weinberger",
                        "slug": "N.-Weinberger",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Weinberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12960317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fbe65823f0cea82a001a5a4504a66ab73e68f8",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "T h e Boltzmann machine is a nonlinear network of stochastic binary processing units t ha t interact pairwise through symmetric connection strengths. In a third-order Boltzmann machine, triples of units interact through symmetric conjunctive interactions. The Boltzmann learning algorithm is generalized t o higher-order interactions. The rate of learning for internal representations in a higher-order Boltzmann machine should be much faster t han for a second-order Boltzmann machine based on pairwise interactions. I N T R O D U C T I O N Thousands of hours of practice are required by humans t o become experts in domains such as chess, mathematics and physics1. Learning in these domains requires the mastery of a large number of highly interrelated ideas, and a deep understanding requires generalization as well as memorization. There are two traditions in the literature on learning in neural network models. One class of models is based on the problem of content-addressable memory and emphasizes a fast, one-shot form of learning. The second class of models uses slow, incremental learning, which requires many repetitions of examples. I t is difficult in humans t o s tudy fast and slow learning in isolation. In some amnesics, however, the long-term retention of facts is severely impaired, but the slow acquisition of skills, including cognitive skills, is spared2. Thus, it is possible tha t separate memory mechanisms are used t o implement fast learning and slow learning. Long practice is required t o become a n expert, bu t expert performance is swift and difficult to analyze; with more practice there is faster performance1. Why is slow learning so slow? One possibility is t h a t the expert develops internal representations tha t allow fast parallel searches for solutions t o problems in the task domain, in contrast t o a novice who must apply knowledge piecemeal. An internal representation is a mental model of the task domain; t h a t is, internal degrees of freedom between the sensory inputs and motor outputs t h a t efficiently encode the variables relevant t o the solution of the problem. This approach can be made more precise by specifying neural network models and showing how they incorporate internal representations. L E A R N I N G IN NETWORK M O D E L S Network models of fast learning include linear correlation-matrix m o d e l ~ ~ ~ ~ ~ ~ l ~ and the more recent nonlinear autoassociative m o d e ~ s ~ ~ ~ ~ ~ ~ ' ~ . These models use the Hebb learning rule t o store information t h a t can be retrieved by the completion of partially specified input patterns. New patterns are stored by imposing the pattern on the network and altering the connection strengths between the pairs of units that are above threshold. The information that is stored therefore concerns the correlations, .or second-order relationships between the components of the pattern. The internal model is built from correlations. Network models of slow learning include the perceptronll and adaline12. These networks can classify input patterns given only examples of inputs and desired outputs. The connection strengths are changed incrementally during the training and the network gradually converges to a set of weights tha t solves the problem if such as set of weights exists. Unfortunately, there are many difficult problems that cannot be solved with these networks, such as the prediction of parity'3. The perceptron and adaline are limited because they have only one layer of modifiable connection strengths and can only implement linear discriminant functions. Higher-order problems like parity cannot be solved by storing the desired patterns using the class of contentaddressable algorithms based on the Hebb learning rule. These models are limited because the metric of similarity is based on Hamming distance and only correlations can be used t o access patterns. The first network model to demonstrably learn t o solve higher-order problems was the Boltzmann machine, which overcame the limitations of previous network models by introducing hidden units14*15116. Hidden units are added t o the network t o mediate between the input and output units; they provide the extra internal degrees of freedom needed t o form internal representations. The Boltzmann learning algorithm incrementally modifies internal connections in the network to build higher-order pattern detectors. The hidden units can be recruited t o form internal representations for any problem; however, the learning may require an extremely large number of training examples and can be excessively slow. One way t o speed up the learning is to use hidden units that have higher-order interactions with other units. THIRD-ORDER BOLTZMANN MACHINES Consider a Boltzmann machine with a cubic global energy function: where si is the state of the i t h binary unit and w;p is a weight between triples of units. This type of interaction generalizes the pairwise interactions in Hopfield networkslo and Boltzmann machines, which contribute a quadratic term t o the energy. Fig. 1 shows an interpretation of the cubic term as conjunctive synapses. Each unit in the network updates its binary state asynchronously with probability where T is a parameter the i t h unit is given by analagous t o the temperature and the total input to If wijk is symmetric on all pairs of indices then the energy of the network is nonincreasing. It can be shown that in equilibrium the probabilities of global states P, follow a Boltzmann distribution Fig. 1. Third-order interactions between three units. In the diagram the lines between units represent reciprocal interactions that are activated only when the third unit is in the on state. The third unit acts presynaptically to conjunctively control the painvise interactions. There are two forms of the Boltzmann learning algorithm, one for networks with inputs and outputs treated identically, and a second for networks where the input units are always clamped15. The former learning algorithm will be generalized for third-order interactions. The learning metric on weight space remains the same: where P , is the probability of a global state with both the inputs and outputs clamped, and Pd, is the probability of a global state when the network is allowed to run freely. I t can be shown that the gradient of G is given by where p i jk is the ensemble average probability of three units all being in the on state when the input and output units are clamped, and pi;k is the corresponding probability when the network is running freely. T o minimize G , it is sufficient to measure the time averaged triple co-occurence probabilities when the network is in equilibrium under the two conditions and t o change each weight according to where c scales the size of each weight change. HIGHER-ORDER BOLTZMANN MACHINES Define the energy of a k -th order Boltzmann machine as where w 7172 . . . 7t is a k -dimensional weight indices. The G matrix can be minimized by matrix symmetric on all pairs of gradient descent: where P ~ , ~ . . . is the probability of the k-tuple co-occurence of the I (s 71 ,S 72 , . . s 7r ) when the inputs and outputs are clamped, and p 7172. . . 7L is the corresponding probability when the network is freely running. In general, the energy for a Boltzmann machine is the sum over all orders of interaction and the learning algorithm is a linear combination of terms from each order. This is a Markov random field with polynomial interactions17."
            },
            "slug": "Higher-Order-Boltzmann-Machines-Cohen-Weinberger",
            "title": {
                "fragments": [],
                "text": "Higher-Order Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work presents the Boltzmann machine, a nonlinear network of stochastic binary processing units, which overcame the limitations of previous network models by introducing hidden units and shows how they incorporate internal representations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1869302"
                        ],
                        "name": "W. Teahan",
                        "slug": "W.-Teahan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Teahan",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Teahan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 4
                            }
                        ],
                        "text": "PPM (Teahan, 1995) is a leading method for text compression, and it uses arithmetic coding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16164126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0f36536c610912c7a24e45d5eea9c666603716e",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "by carefully fine-tuning the techniques even further, but we are in danger of over-fitting the algorithm to suit the test data. Important questions have also been raised by this research that require further investigation. For example, why are bounded context models just as effective as unbounded models? Why is an entropy-based method effective for selecting between multiple probability estimators, but less effective when applied to the problem of local order estimation? Are there better methods of blending the context models rather than the PPM method of estimating an escape probability? A greater understanding of these questions is necessary before substantial improvement in probability estimation is possible. [5] P.G. Howard. The design and analysis of efficient lossless data compression systems. scaling factor of 1.1. Combined results for the best bounded and unbounded approaches (PPM 3 and PPM* 4) are shown in Table 3. Results for PPM 3 are taken from Table 1 and for PPM* 4 from Table 2. The results in the column labelled \" Combined \" uses whichever of the two is smaller, and leads to a final compression ratio of 2.25 bpc for the whole corpus, which is a further improvement of 1% over each of the two methods taken separately. Interestingly, neither of the two approaches performs consistently better than the other, instead outperforming the other on about half of the files (PPM* 4 performing noticeably better on program source code files such as progl and progp for example). A straightforward implementation of the combined approach would be to compress each file using both methods in parallel, and then discard the output file for whichever method was inferior. An adaptive implementation of such a scheme, which selects the better method for each context as the file is being compressed exploiting the fact that the same context trie can be used to implement both methods has yet to be investigated. Four new methods that improve the probability estimation for the PPM data compression scheme have been described. As well, two different approaches that use bounded and unbounded length context models have been compared. Each of the new methods is effective at improving the performance of the prediction for both of these approaches, leading to an 8% improvement for both approaches over the standard implementation PPMC. The two approaches combined lead to a further 1% improvement in performance. While these gains are not large, we are in an area \u2026"
            },
            "slug": "Probability-estimation-for-PPM-Teahan",
            "title": {
                "fragments": [],
                "text": "Probability estimation for PPM"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Four new methods that improve the probability estimation for the PPM data compression scheme have been described, leading to an 8% improvement for both approaches over the standard implementation PPMC."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144267648"
                        ],
                        "name": "C. Berzuini",
                        "slug": "C.-Berzuini",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Berzuini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berzuini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124058301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "446ee311cbefdf5a07d2b4a409c9d4229cc59739",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov chain Monte Carlo (MCMC) sampling is a numerically intensive simulation technique which has greatly improved the practicality of Bayesian inference and prediction. However, MCMC sampling is too slow to be of practical use in problems involving a large number of posterior (target) distributions, as in dynamic modelling and predictive model selection. Alternative simulation techniques for tracking moving target distributions, known as particle filters, which combine importance sampling, importance resampling and MCMC sampling, tend to suffer from a progressive degeneration as the target sequence evolves. We propose a new technique, based on these same simulation methodologies, which does not suffer from this progressive degeneration."
            },
            "slug": "Following-a-moving-target\u2014Monte-Carlo-inference-for-Gilks-Berzuini",
            "title": {
                "fragments": [],
                "text": "Following a moving target\u2014Monte Carlo inference for dynamic Bayesian models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a new technique for tracking moving target distributions, known as particle filters, which does not suffer from a progressive degeneration as the target sequence evolves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069357329"
                        ],
                        "name": "P. Elias",
                        "slug": "P.-Elias",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Elias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Elias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 72
                            }
                        ],
                        "text": "These defects are rectified by arithmetic codes, which were invented by Elias, by Rissanen and by Pasco, and subsequently made practical by Witten et al. (1987). In an arithmetic code, the probabilistic modelling is clearly separated from the encoding operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 119
                            }
                        ],
                        "text": "Another code which elegantly transcends the sequence of self-delimiting codes is Elias\u2019s \u2018universal code for integers\u2019 (Elias, 1975), which effectively chooses from all the codes C\u03b1, C\u03b2 , ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6752631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ecd86899c752f1e41ec4792cff22a4c7fbfc048",
            "isKey": false,
            "numCitedBy": 1206,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Countable prefix codeword sets are constructed with the universal property that assigning messages in order of decreasing probability to codewords in order of increasing length gives an average code-word length, for any message set with positive entropy, less than a constant times the optimal average codeword length for that source. Some of the sets also have the asymptotically optimal property that the ratio of average codeword length to entropy approaches one uniformly as entropy increases. An application is the construction of a uniformly universal sequence of codes for countable memoryless sources, in which the n th code has a ratio of average codeword length to source rate bounded by a function of n for all sources with positive rate; the bound is less than two for n = 0 and approaches one as n increases."
            },
            "slug": "Universal-codeword-sets-and-representations-of-the-Elias",
            "title": {
                "fragments": [],
                "text": "Universal codeword sets and representations of the integers"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An application is the construction of a uniformly universal sequence of codes for countable memoryless sources, in which the n th code has a ratio of average codeword length to source rate bounded by a function of n for all sources with positive rate."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2678443"
                        ],
                        "name": "D. Warren",
                        "slug": "D.-Warren",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warren",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Warren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 252
                            }
                        ],
                        "text": "In a realistic problem such as playing a board game, the tree of possible cogitations and actions that must be considered becomes enormous, and \u2018doing the right thing\u2019 is not simple, because the expected utility of an action cannot be computed exactly (Russell and Wefald, 1991; Baum and Smith, 1993; Baum and Smith, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14246174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5838432c92c8905c7066962400c55ddc8803f11a",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. This has been eeective in many games because of the computational eeciency of the Alpha-beta algorithm. But as Bayesians, we want to know the best way to use the inexact statistical information provided by the leaf evaluator to choose our next move. We add a model of uncertainty to the standard evaluation function. Within such a formal model, there is an optimal tree growth procedure and an optimal method of valuing the tree. We describe how to optimally value the tree within our model, and how to eeciently approximate the optimal tree to search. Our tree growth procedure provably approximates the contribution of each leaf to the utility in the limit where we grow a large tree, taking explicit account of the interactions between expanding diierent leaves. Our algorithms run (under reasonable assumptions) in linear time and hence except for a small constant factor, are as time eecient as Alpha-beta. In a given amount of time our algorithm can thus, at least in principle, grow a tree several times as deep as Alpha-beta along the lines judged most relevant, but at a cost of expanding suuciently less along lines judged less relevant so that a small constant factor fewer nodes are searched in total. Our algorithm will apportion a greater fraction of its thinking time to more relevant moves, and will appropriately weigh the relevance of each leaf in choosing its move once it is nished searching. Empirical evidence of the eecacy of this approach is presented in part 2."
            },
            "slug": "Best-Play-for-Imperfect-Players-and-Game-Tree-part-Baum-Warren",
            "title": {
                "fragments": [],
                "text": "Best Play for Imperfect Players and Game Tree Search; part I - theory"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work adds a model of uncertainty to the standard evaluation function of game tree search, and describes how to optimally value the tree within the model, and how to eeciently approximate the optimal tree to search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 16694,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118984000,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "575bfe84f91b6b4234451c5f3f41932b53949e8b",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov chain Monte Carlo methods such as Gibbs sampling and simple forms of the Metropolis algorithm typically move about the distribution being sampled via a random walk. For the complex, high-dimensional distributions commonly encountered in Bayesian inference and statistical physics, the distance moved in each iteration of these algorithms will usually be small, because it is difficult or impossible to transform the problem to eliminate dependencies between variables. The inefficiency inherent in taking such small steps is greatly exacerbated when the algorithm operates via a random walk, as in such a case moving to a point n steps away will typically take around n 2 iterations. Such random walks can sometimes be suppressed using \u201coverrelaxed\u201d variants of Gibbs sampling (a.k.a. the heatbath algorithm), but such methods have hitherto been largely restricted to problems where all the full conditional distributions are Gaussian. I present an overrelaxed Markov chain Monte Carlo algorithm based on order statistics that is more widely applicable. In particular, the algorithm can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution functions can be efficiently computed. The method is demonstrated on an inference problem for a simple hierarchical Bayesian model."
            },
            "slug": "Suppressing-Random-Walks-in-Markov-Chain-Monte-Neal",
            "title": {
                "fragments": [],
                "text": "Suppressing Random Walks in Markov Chain Monte Carlo Using Ordered Overrelaxation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An overrelaxed Markov chain Monte Carlo algorithm based on order statistics that can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution function can be efficiently computed."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068387850"
                        ],
                        "name": "Amit",
                        "slug": "Amit",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30058673"
                        ],
                        "name": "Gutfreund",
                        "slug": "Gutfreund",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Gutfreund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gutfreund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30053069"
                        ],
                        "name": "Sompolinsky",
                        "slug": "Sompolinsky",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sompolinsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 26
                            }
                        ],
                        "text": "For the limit of large I, Amit et al. (1985) have used methods from statistical physics to find numerically the transition between these two behaviours."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2906691,
            "fieldsOfStudy": [
                "Materials Science",
                "Mathematics"
            ],
            "id": "b0f93cf5b155e9deccec9ddd6978fb3b4775878b",
            "isKey": false,
            "numCitedBy": 779,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hopfield model for a neural network is studied in the limit when the number $p$ of stored patterns increases with the size $N$ of the network, as $p=\\ensuremath{\\alpha}N$. It is shown that, despite its spin-glass features, the model exhibits associative memory for $\\ensuremath{\\alpha}l{\\ensuremath{\\alpha}}_{c}$, ${\\ensuremath{\\alpha}}_{c}\\ensuremath{\\gtrsim}0.14$. This is a result of the existence at low temperature of $2p$ dynamically stable degenerate states, each of which is almost fully correlated with one of the patterns. These states become ground states at $\\ensuremath{\\alpha}l0.05$. The phase diagram of this rich spin-glass is described."
            },
            "slug": "Storing-infinite-numbers-of-patterns-in-a-model-of-Amit-Gutfreund",
            "title": {
                "fragments": [],
                "text": "Storing infinite numbers of patterns in a spin-glass model of neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The Hopfield model for a neural network is studied in the limit when the number of stored patterns increases with the size of the network, as $p=\\ensuremath{\\alpha}N$, and it is shown that, despite its spin-glass features, the model exhibits associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143681410"
                        ],
                        "name": "H. Loeliger",
                        "slug": "H.-Loeliger",
                        "structuredName": {
                            "firstName": "Hans-Andrea",
                            "lastName": "Loeliger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Loeliger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7715701"
                        ],
                        "name": "R. Koetter",
                        "slug": "R.-Koetter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Koetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koetter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 75
                            }
                        ],
                        "text": "Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 195
                            }
                        ],
                        "text": "From left to right: Irregular LDPCC over GF (8), blocklength 48000 bits (Davey, 1999); JPL turbo code (JPL, 1996) blocklength 65536; Regular LDPCC over GF (16), blocklength 24448 bits (Davey and MacKay, 1998); Irregular binary LDPCC, blocklength 16000 bits (Davey, 1999); Luby et al. (1998) irregular binary LDPCC, blocklength 64000 bits; JPL code for Galileo (in 1992, this was the best known code of rate 1/4); Regular binary LDPCC: blocklength 40000 bits (MacKay, 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36630145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "848822f6c3446842730587cb4373a53f69e38720",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, most known decoding procedures for error-correcting codes were based either on algebraically calculating the error pattern or on some sort of tree or trellis search. With the advent of turbo coding, a third decoding principle has finally had its breakthrough: iterative decoding. With respect to Viterbi decoding, a code is most naturally described by means of a trellis diagram. The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis. More precisely, it is the \"time axis\" of a trellis that is generalized to a Tanner graph."
            },
            "slug": "Codes-and-iterative-decoding-on-general-graphs-Wiberg-Loeliger",
            "title": {
                "fragments": [],
                "text": "Codes and iterative decoding on general graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis."
            },
            "venue": {
                "fragments": [],
                "text": "Eur. Trans. Telecommun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17493705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c958128419f40636645d3e2c7a8c88b1073b7c4c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data. The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA. 1. ICA AS A CAUSAL GENERATIVE MODEL Factor analysis is based on a causal generative model in which an observation vector is generated in three stages. First, the activities of the factors (also known as latent or hidden variables) are chosen independently from one dimensional Gaussian priors. Next, these hidden activities are multiplied by a matrix of weights (the \u201cfactor loading\u201d matrix) to produce a noise-free observation vector. Finally, independent Gaussian \u201csensor noise\u201d is added to each component of the noise-free observation vector. Given an observation vector and a factor loading matrix, it is tractable to compute the posterior distribution of the hidden activities because this distribution is a Gaussian, though it generally has off-diagonal terms in the covariance matrix so it is not as simple as the prior distribution over hidden activities. ICA can also be viewed as a causal generative model [1, 2] that differs from factor analysis in two ways. First, the priors over the hidden activities remain independent but they are non-Gaussian. By itself, this modification would make it intractable to compute the posterior distribution over hidden activities. Tractability is restored by eliminating sensor noise and by using the same number of factors as input dimensions. This ensures that the posterior distribution over hidden activities collapses to a point. Interpreting ICA as a type of causal generative model suggests a number of ways in which it might be generalized, for instance to deal with more hidden units than input dimensions. Most of these generalizations retain marginal independence of the hidden activities and add sensor noise, but fail to preserve the property that the posterior distribution collapses to a point. As Funded by the Wellcome Trust and the Gatsby Charitable Foundation. a result inference is intractable and crude approximations are needed to model the posterior distribution, e.g., a MAP estimate in [3], a Laplace approximation in [4, 5] or more sophisticated variational approximations in [6]. 2. ICA AS AN ENERGY-BASED DENSITY MODEL We now describe a very different way of interpreting ICA as a probability density model. In the next section we describe how we can fit the model to data. The advantage of our energy-based view is that it suggests different generalizations of the basic ICA algorithm which preserve the computationally attractive property that the hidden activities are a simple deterministic function of the observed data. Instead of viewing the hidden factors as stochastic latent variables in a causal generative model, we view them as deterministic functions of the data with parameters . The hidden factors are then used for assigning an energy , to each possible observation vector :"
            },
            "slug": "A-New-View-of-ICA-Hinton-Welling",
            "title": {
                "fragments": [],
                "text": "A New View of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new way of interpreting ICA as a probability density model and anew way of fitting this model to data are presented, which suggests different generalizations of the basic ICA algorithm which preserve the computationally attractive property that the hidden activities are a simple deterministic function of the observed data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145881044"
                        ],
                        "name": "M. Luby",
                        "slug": "M.-Luby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Luby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745699"
                        ],
                        "name": "M. Mitzenmacher",
                        "slug": "M.-Mitzenmacher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mitzenmacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mitzenmacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143916589"
                        ],
                        "name": "A. Shokrollahi",
                        "slug": "A.-Shokrollahi",
                        "structuredName": {
                            "firstName": "Amin",
                            "lastName": "Shokrollahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shokrollahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3148302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57beaeccb087051c23af77ab444ba4a42fd4c7cd",
            "isKey": false,
            "numCitedBy": 1213,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a simple erasure recovery algorithm for codes derived from cascades of sparse bipartite graphs and analyze the algorithm by analyzing a corresponding discrete-time random process. As a result, we obtain a simple criterion involving the fractions of nodes of different degrees on both sides of the graph which is necessary and sufficient for the decoding process to finish successfully with high probability. By carefully designing these graphs we can construct for any given rate R and any given real number /spl epsiv/ a family of linear codes of rate R which can be encoded in time proportional to ln(1//spl epsiv/) times their block length n. Furthermore, a codeword can be recovered with high probability from a portion of its entries of length (1+/spl epsiv/)Rn or more. The recovery algorithm also runs in time proportional to n ln(1//spl epsiv/). Our algorithms have been implemented and work well in practice; various implementation issues are discussed."
            },
            "slug": "Efficient-erasure-correcting-codes-Luby-Mitzenmacher",
            "title": {
                "fragments": [],
                "text": "Efficient erasure correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A simple erasure recovery algorithm for codes derived from cascades of sparse bipartite graphs is introduced and a simple criterion involving the fractions of nodes of different degrees on both sides of the graph is obtained which is necessary and sufficient for the decoding process to finish successfully with high probability."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 386,
                                "start": 75
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. A longer review of Gaussian processes is in (MacKay, 1998). A review paper on regression with parametric models and complexity control using hierarchical Bayesian models is (MacKay, 1992a). Neal (1997b) discusses various ways of connecting Gaussian processes to classification models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152716904"
                        ],
                        "name": "Simon T. Wilson",
                        "slug": "Simon-T.-Wilson",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Wilson",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon T. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32590599"
                        ],
                        "name": "M. C. Davey",
                        "slug": "M.-C.-Davey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Davey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Davey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 205
                            }
                        ],
                        "text": "How did a high-information-content self-replicating system ever emerge in the first place? In the general area of the origins of life and other tricky questions about evolution, I highly recommend Maynard Smith and Sz\u00e1thmary (1995), Maynard Smith and Sz\u00e1thmary (1999), Kondrashov (1988), Maynard Smith (1988), Ridley (2000), Dyson (1985), Cairns-Smith (1985), and Hopfield (1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 160
                            }
                        ],
                        "text": "The encoding complexity can be reduced by clever encoding tricks invented by Richardson and Urbanke (2001b) or by specially constructing the paritycheck matrix (MacKay et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14190291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45ab27a22bbb4d4c34492560d656037ab0a62204",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The low-density parity check codes whose performance is closest to the Shannon limit are \"Gallager codes\" based on irregular graphs. We compare alternative methods for constructing these graphs and present two results. First, we find a \"super-Poisson\" construction which gives a small improvement in empirical performance over a random construction. Second, whereas Gallager codes normally take N/sup 2/ time to encode, we investigate constructions of regular and irregular Gallager codes that allow more rapid encoding and have smaller memory requirements in the encoder. We find that these \"fast encoding\" Gallager codes have equally good performance."
            },
            "slug": "Comparison-of-constructions-of-irregular-Gallager-Mackay-Wilson",
            "title": {
                "fragments": [],
                "text": "Comparison of constructions of irregular Gallager codes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A \"super-Poisson\" construction is found which gives a small improvement in empirical performance over a random construction and constructions of regular and irregular Gallager codes that allow more rapid encoding and have smaller memory requirements in the encoder are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Importance sampling (reviewed by Neal (1993b)) and annealed importance sampling (Neal, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11112994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "isKey": false,
            "numCitedBy": 1212,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers."
            },
            "slug": "Annealed-importance-sampling-Neal",
            "title": {
                "fragments": [],
                "text": "Annealed importance sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler, which can be seen as a generalization of a recently-proposed variant of sequential importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32590599"
                        ],
                        "name": "M. C. Davey",
                        "slug": "M.-C.-Davey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Davey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Davey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": ", 1997); codes for reliable communication over channels with synchronization errors remain an active research area (Davey and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15146592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47fa313b426aeb55a61a89617433fe24bfeb95e2",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A new block code is introduced which is capable of correcting multiple insertion, deletion, and substitution errors. The code consists of nonlinear inner codes, which we call \"watermark\"\" codes, concatenated with low-density parity-check codes over nonbinary fields. The inner code allows probabilistic resynchronization and provides soft outputs for the outer decoder, which then completes decoding. We present codes of rate 0.7 and transmitted length 5000 bits that can correct 30 insertion/deletion errors per block. We also present codes of rate 3/14 and length 4600 bits that can correct 450 insertion/deletion errors per block."
            },
            "slug": "Reliable-communication-over-channels-with-and-Davey-Mackay",
            "title": {
                "fragments": [],
                "text": "Reliable communication over channels with insertions, deletions, and substitutions"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A new block code is introduced which is capable of correcting multiple insertion, deletion, and substitution errors, and consists of nonlinear inner codes, which is called \"watermark\"\" codes, concatenated with low-density parity-check codes over nonbinary fields."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932728"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727633"
                        ],
                        "name": "R. Urbanke",
                        "slug": "R.-Urbanke",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Urbanke",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urbanke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 106
                            }
                        ],
                        "text": "Thresholds fmax for regular low-density parity-check codes, assuming sum\u2013product decoding algorithm, from Richardson and Urbanke (2001a). The Shannon limit for rate-1/2 codes is fmax = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 72
                            }
                        ],
                        "text": "] Implement an optimal digital fountain decoder that uses the method of Richardson and Urbanke (2001b) derived for fast encoding of sparse-graph codes (section 47."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 56
                            }
                        ],
                        "text": "Fast encoding of general low-density parity-check codes Richardson and Urbanke (2001b) demonstrated an elegant method by which the encoding cost of any low-density parity-check code can be reduced from the straightforward method\u2019s M 2 to a cost of N + g2, where g, the gap, is hopefully a small constant, and in the worst cases scales as a small fraction of N ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 514746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2b32222ec8955c4378fd2804757f4f8449d9241",
            "isKey": false,
            "numCitedBy": 3014,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general method for determining the capacity of low-density parity-check (LDPC) codes under message-passing decoding when used over any binary-input memoryless channel with discrete or continuous output alphabets. Transmitting at rates below this capacity, a randomly chosen element of the given ensemble will achieve an arbitrarily small target probability of error with a probability that approaches one exponentially fast in the length of the code. (By concatenating with an appropriate outer code one can achieve a probability of error that approaches zero exponentially fast in the length of the code with arbitrarily small loss in rate.) Conversely, transmitting at rates above this capacity the probability of error is bounded away from zero by a strictly positive constant which is independent of the length of the code and of the number of iterations performed. Our results are based on the observation that the concentration of the performance of the decoder around its average performance, as observed by Luby et al. in the case of a binary-symmetric channel and a binary message-passing algorithm, is a general phenomenon. For the particularly important case of belief-propagation decoders, we provide an effective algorithm to determine the corresponding capacity to any desired degree of accuracy. The ideas presented in this paper are broadly applicable and extensions of the general method to low-density parity-check codes over larger alphabets, turbo codes, and other concatenated coding schemes are outlined."
            },
            "slug": "The-capacity-of-low-density-parity-check-codes-Richardson-Urbanke",
            "title": {
                "fragments": [],
                "text": "The capacity of low-density parity-check codes under message-passing decoding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results are based on the observation that the concentration of the performance of the decoder around its average performance, as observed by Luby et al. in the case of a binary-symmetric channel and a binary message-passing algorithm, is a general phenomenon."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25682944"
                        ],
                        "name": "P. L. Mcfadden",
                        "slug": "P.-L.-Mcfadden",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Mcfadden",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. L. Mcfadden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10569808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16b4fe161b0642ddd1447abd7b8802c4ff7d039a",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse-graph codes appropriate for use in quantum error-correction are presented. Quantum error-correcting codes based on sparse graphs are of interest for three reasons. First, the best codes currently known for classical channels are based on sparse graphs. Second, sparse-graph codes keep the number of quantum interactions associated with the quantum error-correction process small: a constant number per quantum bit, independent of the block length. Third, sparse-graph codes often offer great flexibility with respect to block length and rate. We believe some of the codes we present are unsurpassed by previously published quantum error-correcting codes."
            },
            "slug": "Sparse-graph-codes-for-quantum-error-correction-Mackay-Mitchison",
            "title": {
                "fragments": [],
                "text": "Sparse-graph codes for quantum error correction"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Sparse-graph codes appropriate for use in quantum error-correction are presented and some of the codes are believed to be unsurpassed by previously publishedquantum error-correcting codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144267648"
                        ],
                        "name": "C. Berzuini",
                        "slug": "C.-Berzuini",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Berzuini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berzuini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34949056"
                        ],
                        "name": "N. G. Best",
                        "slug": "N.-G.-Best",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Best",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. G. Best"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755614"
                        ],
                        "name": "C. Larizza",
                        "slug": "C.-Larizza",
                        "structuredName": {
                            "firstName": "Cristiana",
                            "lastName": "Larizza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Larizza"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "See Isard and Blake (1996), Isard and Blake (1998), Berzuini et al. (1997), Berzuini and Gilks (2001), Doucet et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 52
                            }
                        ],
                        "text": "See Isard and Blake (1996), Isard and Blake (1998), Berzuini et al. (1997), Berzuini and Gilks (2001), Doucet et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16664482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9c9af999255f0e33c3d0ce30183b926c3a54f09",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In dynamic statistical modeling situations, observations arise sequentially, causing the model to expand by progressive incorporation of new data items and new unknown parameters. For example, in clinical monitoring, patients and data arrive sequentially, and new patient-specific parameters are introduced with each new patient. Markov chain Monte Carlo (MCMC) might be used for continuous updating of the evolving posterior distribution, but would need to be restarted from scratch at each expansion stage. Thus MCMC methods are often too slow for real-time inference in dynamic contexts. By combining MCMC with importance resampling, we show how real-time sequential updating of posterior distributions can be effected. The proposed dynamic sampling algorithms use posterior samples from previous updating stages and exploit conditional independence between groups of parameters to allow samples of parameters no longer of interest to be discarded, such as when a patient dies or is discharged. We apply the ..."
            },
            "slug": "Dynamic-conditional-independence-models-and-Markov-Berzuini-Best",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional independence models and Markov chain Monte Carlo methods"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed dynamic sampling algorithms use posterior samples from previous updating stages and exploit conditional independence between groups of parameters to allow samples of parameters no longer of interest to be discarded, such as when a patient dies or is discharged."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018201"
                        ],
                        "name": "G. Forney",
                        "slug": "G.-Forney",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Forney",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Forney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195868257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74a8959a5971b56dcda05cdde57b724906bf28aa",
            "isKey": false,
            "numCitedBy": 498,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Wiberg et al. (see European Transactions on Telelecommunications, vol.6, p.513-25, Sept./Oct. 1995) proposed graphical code realizations using three kinds of elements: symbol variables, state variables and local constraints. We focus on normal realizations, namely Wiberg-type realizations in which all symbol variables have degree 1 and state variables have degree 2. A natural graphical model of a normal realization represents states by leaf edges, states by ordinary edges, and local constraints by vertices. Any such graph may be decoded by message-passing (the sum-product algorithm). We show that any Wiberg-type realization may be put into normal form without essential change in its graph or its decoding complexity. Group or linear codes are realized by group or linear realizations. We show that an appropriately defined dual of a group or linear normal realization realizes the dual group or linear code. The symbol variables, state variables and graph topology of the dual realization are unchanged, while local constraints are replaced by their duals."
            },
            "slug": "Codes-on-graphs:-normal-realizations-Forney",
            "title": {
                "fragments": [],
                "text": "Codes on graphs: normal realizations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work shows that any Wiberg-type realization may be put into normal form without essential change in its graph or its decoding complexity, and shows that an appropriately defined dual of a group or linear normal realization realizes the dual group orlinear code."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Symposium on Information Theory (Cat. No.00CH37060)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1833925"
                        ],
                        "name": "C. Berrou",
                        "slug": "C.-Berrou",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Berrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1870588"
                        ],
                        "name": "A. Glavieux",
                        "slug": "A.-Glavieux",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Glavieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glavieux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952051"
                        ],
                        "name": "P. Thitimajshima",
                        "slug": "P.-Thitimajshima",
                        "structuredName": {
                            "firstName": "Punya",
                            "lastName": "Thitimajshima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thitimajshima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17770377,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "3ba9baa534a8ea39a31c69e72ada959aaa6a4dc1",
            "isKey": false,
            "numCitedBy": 8239,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders.<<ETX>>"
            },
            "slug": "Near-Shannon-limit-error-correcting-coding-and-1-Berrou-Glavieux",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICC '93 - IEEE International Conference on Communications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720405"
                        ],
                        "name": "J. Ziv",
                        "slug": "J.-Ziv",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50154247"
                        ],
                        "name": "A. Lempel",
                        "slug": "A.-Lempel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Lempel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lempel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20900807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5be4e0eccca2892d31406a03b0c485f7a395fe5a",
            "isKey": false,
            "numCitedBy": 3487,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity \\rho(x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of \\rho(x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences."
            },
            "slug": "Compression-of-individual-sequences-via-coding-Ziv-Lempel",
            "title": {
                "fragments": [],
                "text": "Compression of individual sequences via variable-rate coding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed concept of compressibility is shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35912187"
                        ],
                        "name": "P. Feldman",
                        "slug": "P.-Feldman",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Feldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2690270,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "18f9be9dfd1df07c651f9aa9332bd4bd55ea77cb",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding Homosexuality : Its Biological and Psychological Bases.By J. A. Loraine. Pp. 217. (Medical and Technical Publishing: Lancaster, October 1974.) \u00a36.50."
            },
            "slug": "Evolution-of-sex-Feldman",
            "title": {
                "fragments": [],
                "text": "Evolution of sex"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Understanding Homosexuality : Its Biological and Psychological Bases by J. A. Loraine."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15736158,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f22bf9d37c49a7bbbca45d3a331b56a4a4143c5",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to sample from a distribution is to sample uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizonta\u00ecslice' deened by the current vertical position. Variations on such`slice sampling' methods can easily be implemented for univariate distributions , and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling, and may be more eecient than easily-constructed versions of the Metropolis algorithm. Slice sampling is therefore attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates a Markov chain sampler from a model specii-cation. One can also easily devise overrelaxed versions of slice sampling, which sometimes greatly improve sampling eeciency by suppressing random walk behaviour. Random walks can also be avoided in some slice sampling schemes that simultaneously update all variables."
            },
            "slug": "Markov-Chain-Monte-Carlo-Methods-Based-on-`Slicing'-Neal",
            "title": {
                "fragments": [],
                "text": "Markov Chain Monte Carlo Methods Based on `Slicing' the Density Function"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Slice sampling is attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates aMarkov chain sampler from a model specii-cation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2663587"
                        ],
                        "name": "E. Berlekamp",
                        "slug": "E.-Berlekamp",
                        "structuredName": {
                            "firstName": "Elwyn",
                            "lastName": "Berlekamp",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Berlekamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 5
                            }
                        ],
                        "text": "[See Berlekamp (1968) or Lin and Costello (1983) for further information; Reed\u2013Solomon codes exist for N < q."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 3
                            }
                        ],
                        "text": "So Berlekamp (1980) suggests that the sensible way to approach error-correction is to design encoding-decoding systems and plot their performance on a variety of idealized channels as a function of the channel\u2019s noise level."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eb75f6c270e0aa505da0e0b1bd47841cf7b84ad",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a survey of error-correcting codes, with emphasis on the costs of encoders and decoders, and the relationship of these costs to various important system parameters such as speed and delay. Following an introductory overview, the remainder of this paper is divided into three sections corresponding to the three major types of channel noise: white Gaussian noise, interference, and digital errors of the sort which occur in secondary memories such as disks. Appendix A presents some of the more important facts about modern implementations of decoders for long high-rate Reed-Solomon codes, which play an important role throughout the paper. Appendix B investigates some important aspects of the tradeoffs between error correction and error detection."
            },
            "slug": "The-technology-of-error-correcting-codes-Berlekamp",
            "title": {
                "fragments": [],
                "text": "The technology of error-correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper is a survey of error-correcting codes, with emphasis on the costs of encoders and decoders, and the relationship of these costs to various important system parameters such as speed and delay."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231485"
                        ],
                        "name": "S. Aji",
                        "slug": "S.-Aji",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Aji",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39802221"
                        ],
                        "name": "Hui Jin",
                        "slug": "Hui-Jin",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078918528"
                        ],
                        "name": "A. Khandekar",
                        "slug": "A.-Khandekar",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Khandekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khandekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 127
                            }
                        ],
                        "text": "Notes This method for proving that codes are good can be applied to other linear codes, such as low-density parity-check codes (MacKay, 1999b; Aji et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18547519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "242d47f83ca11ac84bbbdb6c910f032f1f8b0981",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we develop a method for closely estimating noise threshold values for ensembles of binary linear codes on the binary symmetric channel. Our method, based on the \u201ctypical pairs\u201d decoding algorithm pioneered by Shannon, completely decouples the channel from the code ensemble. In this, it resembles the classical union bound, but unlike the union bound, our method is powerful enough to prove Shannon\u2019s theorem for the ensemble of random linear codes. We apply our method to find numerical thresholds for the ensembles of low-density parity-check codes, and \u201crepeat-accumulate\u201d codes."
            },
            "slug": "BSC-Thresholds-for-Code-Ensembles-Based-on-\u201cTypical-Aji-Jin",
            "title": {
                "fragments": [],
                "text": "BSC Thresholds for Code Ensembles Based on \u201cTypical Pairs\u201d Decoding"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A method for closely estimating noise threshold values for ensembles of binary linear codes on the binary symmetric channel, based on the \u201ctypical pairs\u201d decoding algorithm pioneered by Shannon, is developed, powerful enough to prove Shannon\u2019s theorem for the ensemble of random linear codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 83
                            }
                        ],
                        "text": ", 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996). Low-precision decoding algorithms and fast encoding algorithms for Gallager codes are discussed in (Richardson and Urbanke, 2001a; Richardson and Urbanke, 2001b). MacKay and Davey (2000) showed that low\u2013density parity\u2013check codes can outperform Reed\u2013Solomon codes, even on the Reed\u2013Solomon codes\u2019 home turf: high rate and short blocklengths."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 74
                            }
                        ],
                        "text": "[The design of good codes for erasure channels is an active research area (Spielman, 1996; Byers et al., 1998); see also Chapter 50."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 75
                            }
                        ],
                        "text": "Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2029658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdf1a72765e0841ac18329d7b9bf8d0d9331be4c",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "Ab,stract-We present a new class of asymptotically good, linear error-correcting codes. These codes can be both encoded and decoded in linear time. They can also be encoded by logarithmicdepth circuits of linear size and decoded by logarithmic depth circuits of size 0 (n log n) . We present both randomized and explicit constructions of these codes. Zndex Terms- Asymptotically good error-correcting code, linear-time, expander graph, superconcentrators."
            },
            "slug": "Linear-time-encodable-and-decodable-codes-Spielman",
            "title": {
                "fragments": [],
                "text": "Linear-time encodable and decodable error-correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work presents a new class of asymptotically good, linear error-correcting codes that can be both encoded and decoded in linear time and presents both randomized and explicit constructions of these codes."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102295411"
                        ],
                        "name": "Zhen Luo",
                        "slug": "Zhen-Luo",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 114
                            }
                        ],
                        "text": "449] In their landmark paper demonstrating that bacteria could mutate from virus sensitivity to virus resistance, Luria and Delbr\u00fcck (1943) wanted to estimate the mutation rate in an exponentially-growing population from the total number of mutants found at the end of the experiment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14759690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ba3f2e75ce0def9fb7b9037d3a6867caa3a7470",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An adaptive spline method for smoothing is proposed that combines features from both regression spline and smoothing spline approaches. One of its advantages is the ability to vary the amount of smoothing in response to the inhomogeneous \u201ccurvature\u201d of true functions at different locations. This method can be applied to many multivariate function estimation problems, which is illustrated by an application to smoothing temperature data on the globe. The method's performance in a simulation study is found to be comparable to the wavelet shrinkage methods proposed by Donoho and Johnstone. The problem of how to count the degrees of freedom for an adaptively chosen set of basis functions is addressed. This issue arises also in the MARS procedure proposed by Friedman and other adaptive regression spline procedures."
            },
            "slug": "Hybrid-Adaptive-Splines-Luo-Wahba",
            "title": {
                "fragments": [],
                "text": "Hybrid Adaptive Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500274"
                        ],
                        "name": "D. Gottlieb",
                        "slug": "D.-Gottlieb",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gottlieb",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gottlieb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347536"
                        ],
                        "name": "S. Orszag",
                        "slug": "S.-Orszag",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Orszag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Orszag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48897539"
                        ],
                        "name": "P. J. Huber",
                        "slug": "P.-J.-Huber",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Huber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Huber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2175970"
                        ],
                        "name": "F. Roberts",
                        "slug": "F.-Roberts",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Roberts",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Roberts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63602833,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab91f8b2372ec432d8f93c86b545cd9729446291",
            "isKey": false,
            "numCitedBy": 1583,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral Methods in Fluid DynamicsNumerical Methods for Partial Differential EquationsNumerical Analysis of Partial Differential EquationsNumerical analysis of spectral methods : theory and applicationsSpectral Methods And Their ApplicationsA Brief Introduction to Numerical AnalysisA First Course in the Numerical Analysis of Differential Equations South Asian EditionConvergence of Spectral Methods for Hyperbolic Initial-boundary Value SystemsReview of Some Approximation Operators for the Numerical Analysis of Spectral MethodsSpectral Methods in MATLABA Modified Spectral Method in Phase SpaceThe Birth of Numerical AnalysisSpectral Methods for Non-Standard Eigenvalue ProblemsPartial Differential EquationsNumerical Analysis of Spectral MethodsNumerical Analysis of Partial Differential Equations Using Maple and MATLABSpectral MethodsSpectral Methods for NonStandard Eigenvalue ProblemsAn Introduction to the Numerical Analysis of Spectral MethodsSpectral Methods in Time for Parabolic ProblemsSpectral Methods in Chemistry and PhysicsA First Course in the Numerical Analysis of Differential Equations South Asian EditionSummary of Research in Applied Mathematics, Numerical Analysis and Computer Science at the Institute for Computer Applications in Science and EngineeringNumerical AnalysisSpectral Methods for Compressible Flow ProblemsA First Course in the Numerical Analysis of Differential EquationsSummary of Research in Applied Mathematics, Numerical Analysis, and Computer SciencesA Theoretical Introduction to Numerical AnalysisNumerical AnalysisRiemann-Hilbert Problems, Their Numerical Solution, and the Computation of Nonlinear Special FunctionsSpectral MethodsSpectral Methods for Uncertainty QuantificationSpectral Methods and Their ApplicationsNumerical Analysis of Spectral Methods: Theory and ApplicatonsSpectral Methods for Incompressible Viscous FlowAdvances in Numerical Analysis: Nonlinear partial differential equations and dynamical systemsSpectral Methods Using Multivariate Polynomials on the Unit BallA First Course in the Numerical Analysis of Differential EquationsFundamentals of Engineering Numerical AnalysisSpectral Methods for Time-Dependent Problems"
            },
            "slug": "CBMS-NSF-REGIONAL-CONFERENCE-SERIES-IN-APPLIED-Gottlieb-Orszag",
            "title": {
                "fragments": [],
                "text": "CBMS-NSF REGIONAL CONFERENCE SERIES IN APPLIED MATHEMATICS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2343452"
                        ],
                        "name": "Shu Lin",
                        "slug": "Shu-Lin",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690091"
                        ],
                        "name": "D. Costello",
                        "slug": "D.-Costello",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Costello",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Costello"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28213032,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "003643595acb19117ddff6c8ed854ddd30c1aff5",
            "isKey": false,
            "numCitedBy": 4443,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Coding for Reliable Digital Transmission and Storage. 2. Introduction to Algebra. 3. Linear Block Codes. 4. Important Linear Block Codes. 5. Cyclic Codes. 6. Binary BCH Codes. 7. Nonbinary BCH Codes, Reed-Solomon Codes, and Decoding Algorithms. 8. Majority-Logic Decodable Codes. 9. Trellises for Linear Block Codes. 10. Reliability-Based Soft-Decision Decoding Algorithms for Linear Block Codes. 11. Convolutional Codes. 12. Trellis-Based Decoding Algorithms for Convolutional Codes. 13. Sequential and Threshold Decoding of Convolutional Codes. 14. Trellis-Based Soft-Decision Algorithms for Linear Block Codes. 15. Concatenated Coding, Code Decomposition ad Multistage Decoding. 16. Turbo Coding. 17. Low Density Parity Check Codes. 18. Trellis Coded Modulation. 19. Block Coded Modulation. 20. Burst-Error-Correcting Codes. 21. Automatic-Repeat-Request Strategies."
            },
            "slug": "Error-control-coding-fundamentals-and-applications-Lin-Costello",
            "title": {
                "fragments": [],
                "text": "Error control coding - fundamentals and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This book explains coding for Reliable Digital Transmission and Storage using Trellis-Based Soft-Decision Decoding Algorithms for Linear Block Codes and Convolutional Codes, and some of the techniques used in this work."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall computer applications in electrical engineering series"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399079966"
                        ],
                        "name": "Pedro A. d. F. R. H\u00f8jen-S\u00f8rensen",
                        "slug": "Pedro-A.-d.-F.-R.-H\u00f8jen-S\u00f8rensen",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "H\u00f8jen-S\u00f8rensen",
                            "middleNames": [
                                "A.",
                                "d.",
                                "F.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro A. d. F. R. H\u00f8jen-S\u00f8rensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145579972"
                        ],
                        "name": "L. K. Hansen",
                        "slug": "L.-K.-Hansen",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Hansen",
                            "middleNames": [
                                "Kai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. K. Hansen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 83
                            }
                        ],
                        "text": "Further reading on blind separation, including non-ICA algorithms, can be found in (Jutten and Herault, 1991; Comon et al., 1991; Hendin et al., 1994; Amari et al., 1996; Hojen-Sorensen et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40589161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62573e769b4e5771723315c69a900d1a29af6077",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop mean-field approaches for probabilistic independent component analysis (ICA). The sources are estimated from the mean of their posterior distribution and the mixing matrix (and noise level) is estimated by maximum a posteriori (MAP). The latter requires the computation of (a good approximation to) the correlations between sources. For this purpose, we investigate three increasingly advanced mean-field methods: the variational (also known as naive mean field) approach, linear response corrections, and an adaptive version of the Thouless, Anderson and Palmer (1977) (TAP) mean-field approach, which is due to Opper and Winther (2001). The resulting algorithms are tested on a number of problems. On synthetic data, the advanced mean-field approaches are able to recover the correct mixing matrix in cases where the variational mean-field theory fails. For handwritten digits, sparse encoding is achieved using nonnegative source and mixing priors. For speech, the mean-field method is able to separate in the underdetermined (overcomplete) case of two sensors and three sources. One major advantage of the proposed method is its generality and algorithmic simplicity. Finally, we point out several possible extensions of the approaches developed here."
            },
            "slug": "Mean-Field-Approaches-to-Independent-Component-H\u00f8jen-S\u00f8rensen-Winther",
            "title": {
                "fragments": [],
                "text": "Mean-Field Approaches to Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Three increasingly advanced mean-field methods are investigated: the variational (also known as naive mean field) approach, linear response corrections, and an adaptive version of the Thouless, Anderson and Palmer (1977) (TAP) mean- field approach, which is due to Opper and Winther (2001)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1833925"
                        ],
                        "name": "C. Berrou",
                        "slug": "C.-Berrou",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Berrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1870588"
                        ],
                        "name": "A. Glavieux",
                        "slug": "A.-Glavieux",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Glavieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glavieux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12044273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "388fa95178e9b142b0ddec1dbc5dacbbab53ad8f",
            "isKey": false,
            "numCitedBy": 2930,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new family of convolutional codes, nicknamed turbo-codes, built from a particular concatenation of two recursive systematic codes, linked together by nonuniform interleaving. Decoding calls on iterative processing in which each component decoder takes advantage of the work of the other at the previous step, with the aid of the original concept of extrinsic information. For sufficiently large interleaving sizes, the correcting performance of turbo-codes, investigated by simulation, appears to be close to the theoretical limit predicted by Shannon."
            },
            "slug": "Near-optimum-error-correcting-coding-and-decoding:-Berrou-Glavieux",
            "title": {
                "fragments": [],
                "text": "Near optimum error correcting coding and decoding: turbo-codes"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new family of convolutional codes, nicknamed turbo-codes, built from a particular concatenation of two recursive systematic codes, linked together by nonuniform interleaving appears to be close to the theoretical limit predicted by Shannon."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2576061"
                        ],
                        "name": "T. Welch",
                        "slug": "T.-Welch",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Welch",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2055321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15957c2a8dc85e5066e393da9ab70883876521ea",
            "isKey": false,
            "numCitedBy": 2303,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Data stored on disks and tapes or transferred over communications links in commercial computer systems generally contains significant redundancy. A mechanism or procedure which recodes the data to lessen the redundancy could possibly double or triple the effective data densitites in stored or communicated data. Moreover, if compression is automatic, it can also aid in the rise of software development costs. A transparent compression mechanism could permit the use of \"sloppy\" data structures, in that empty space or sparse encoding of data would not greatly expand the use of storage space or transfer time; however , that requires a good compression procedure. Several problems encountered when common compression methods are integrated into computer systems have prevented the widespread use of automatic data compression. For example (1) poor runtime execution speeds interfere in the attainment of very high data rates; (2) most compression techniques are not flexible enough to process different types of redundancy; (3) blocks of compressed data that have unpredictable lengths present storage space management problems. Each compression ' This article was written while Welch was employed at Sperry Research Center; he is now employed with Digital Equipment Corporation. 8 m, 2 /R4/OflAb l strategy poses a different set of these problems and, consequently , the use of each strategy is restricted to applications where its inherent weaknesses present no critical problems. This article introduces a new compression algorithm that is based on principles not found in existing commercial methods. This algorithm avoids many of the problems associated with older methods in that it dynamically adapts to the redundancy characteristics of the data being compressed. An investigation into possible application of this algorithm yields insight into the compressibility of various types of data and serves to illustrate system problems inherent in using any compression scheme. For readers interested in simple but subtle procedures, some details of this algorithm and its implementations are also described. The focus throughout this article will be on transparent compression in which the computer programmer is not aware of the existence of compression except in system performance. This form of compression is \"noiseless,\" the decompressed data is an exact replica of the input data, and the compression apparatus is given no special program information, such as data type or usage statistics. Transparency is perceived to be important because putting an extra burden on the application programmer would cause"
            },
            "slug": "A-Technique-for-High-Performance-Data-Compression-Welch",
            "title": {
                "fragments": [],
                "text": "A Technique for High-Performance Data Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new compression algorithm is introduced that is based on principles not found in existing commercial methods in that it dynamically adapts to the redundancy characteristics of the data being compressed, and serves to illustrate system problems inherent in using any compression scheme."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 53
                            }
                        ],
                        "text": "For a vector-quantization approach to clustering see (Luttrell, 1989; Luttrell, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61300466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d922fc2e408c0a3996fd77c4a24f42af663768",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method of vector quantisation which trades off accuracy for speed of encoding. We achieve this by hierarchically structuring a multistage encoder so that each stage encodes low dimensional input vectors. Such hierarchical encoders may easily be realised as a set of fast table look-up operations. We demonstrate how the Euclidean distortion in such a multistage encoder is approximately minimised by using Kohonen's topographic mapping learning algorithm from neural network theory. We also demonstrate the performance of the technique on various stochastic time series. We find that there is little loss in encoding accuracy, when compared with the exact nearest neighbour encoding using an equivalent single stage encoder."
            },
            "slug": "Hierarchical-vector-quantisation-Luttrell",
            "title": {
                "fragments": [],
                "text": "Hierarchical vector quantisation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method of vector quantisation which trades off accuracy for speed of encoding is presented, which finds that there is little loss in encoding accuracy, when compared with the exact nearest neighbour encoding using an equivalent single stage encoder."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144664658"
                        ],
                        "name": "J. Wolf",
                        "slug": "J.-Wolf",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Wolf",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739020"
                        ],
                        "name": "P. Siegel",
                        "slug": "P.-Siegel",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Siegel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Siegel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 90
                            }
                        ],
                        "text": "These observations about crosswords were first made by Shannon; I learned about them from Wolf and Siegel (1998). The topic is closely related to the capacity of two-dimensional constrained channels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1091040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11509943697f97da24b0d289a4f7d95c2fa9ee82",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider large two-dimensional arrays where each row and each column must satisfy certain constraints. We relate this problem to an assertion made by Shannon regarding the existence of large two-dimensional crossword puzzles. One-dimensional constrained sequences. We begin by considering an alphabet containing M symbols and one-dimensional sequences of these symbols that satisfy a set of well defined constraints. To narrow the focus of this paper we will concentrate on a special class of such one-dimensional constrained sequences which consist of concatenations of isolated words from some dictionary. Details of this special class follow. We first assume that one of the M symbols is of special significance and is called the space symbol. The collection of the remaining (M-1) symbols will be called non-space symbols. We assume that we have a dictionary of Q allowable words. The words in this dictionary are assumed to be sequences of the (M-1) non-space symbols. We assume that the i-th word of the dictionary is of length Li and that all the words in the dictionary are distinct. Finally we assume that the constraints allow for every concatenation of words from the dictionary provided that consecutive words are separated by one (or in some cases more than one) space symbol. One example of such a constrained system would be the sequences of English words in the rows or columns of a crossword puzzle as contained in many American newspapers or magazines. Here M=27, the black space between words is considered the space symbol and there are 26 non-space symbols. In this case, we allow for one or more than one space symbol between words in the dictionary to allow for one or more than one black square between consecutive words. A second example of such a constrained system would be binary (d,k) sequences. The alphabet consists of the two symbols 0 and 1. The usual description of such binary sequences is that the symbols must satisfy the following two constraints. (1) d-constraint: Two consecutive 1\u2019s must be separated by a run of at least d consecutive 0\u2019s. (2) k-constraint: The maximum length of a run of consecutive 0\u2019s is k. Note, however, the following equivalent description of such (d,k) binary sequences which applies when d > 1. The M=2 binary symbols are the space symbol \u201c1\u201d and the non-space symbol \u201c0\u201d. The dictionary contains the Q=(k-d+1) code words consisting of runs of j 0\u2019s where j takes on the (k-d+1) values from d to k. Furthermore, every word in the dictionary can be followed by exactly one space symbol (i.e., one \u201c1\u201d). The modifications to this description that must be made for the case of d=0 are straightforward. If any concatenation of the M symbols is allowable, there are exactly M distinct sequences of length n. It is desirable to know how many distinct sequences of length n can be formed for the constrained system. Shannon [1] provided us with several methods of computing this quantity. One method is very easy to understand in terms of our dictionary description of these constrained sequences. Assume first the case where there is one and only one space between adjacent code words. Let N(n) be the number of distinct sequences of length n in our constrained system. Note that any sequence of length n can be thought of as a sequence of length (n-j-1) followed by a space followed by a code word of length j > 1. Then, if N(n-j-1) is the number of distinct sequences of length (n-j-1) and if Aj denotes the number of distinct words in our dictionary of length j, then the number of distinct sequences of length N(n) would equal the sum over all code word lengths j of the product of Aj and N(n-j-1). From the standard theory of linear difference equations one knows that the solution for N(n) is a sum of exponentials of the form \u03b3 which for large n is dominated by the largest real root of an algebraic equation found by substituting \u03b3 for N(j) in the original linear difference"
            },
            "slug": "On-Two-Dimensional-Arrays-and-Crossword-Puzzles-Wolf-Siegel",
            "title": {
                "fragments": [],
                "text": "On Two-Dimensional Arrays and Crossword Puzzles"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper considers large two-dimensional arrays where each row and each column must satisfy certain constraints, and considers an alphabet containing M symbols and one-dimensional sequences of these symbols that satisfy a set of well defined constraints."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 57
                            }
                        ],
                        "text": "For further reading about Turing and Bletchley Park, see Hodges (1983) and Good (1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 365,
                                "start": 256
                            }
                        ],
                        "text": "The approximation of posterior probability distributions using variational free energy minimization provides a useful approach to approximating Bayesian inference in a number of fields ranging from neural networks to the decoding of error-correcting codes (Hinton and van Camp, 1993; Hinton and Zemel, 1994; Dayan et al., 1995; Neal and Hinton, 1998; MacKay, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2445072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f",
            "isKey": false,
            "numCitedBy": 958,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes."
            },
            "slug": "Autoencoders,-Minimum-Description-Length-and-Free-Hinton-Zemel",
            "title": {
                "fragments": [],
                "text": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152251452"
                        ],
                        "name": "John Maynard Smith",
                        "slug": "John-Maynard-Smith",
                        "structuredName": {
                            "firstName": "John Maynard",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Maynard Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865864"
                        ],
                        "name": "E. Szathm\u00e1ry",
                        "slug": "E.-Szathm\u00e1ry",
                        "structuredName": {
                            "firstName": "E\u00f6rs",
                            "lastName": "Szathm\u00e1ry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Szathm\u00e1ry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13088743,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1b2616784c1defb11abd0e8a88fa804f397c061e",
            "isKey": false,
            "numCitedBy": 3946,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "List of Tables Preface 1. Introduction 2. What is Life? 3. Chemical evolution 4. The evolution of templates 5. The chicken and egg problem 6. The origin of translation and the genetic code 7. The origin of protocells 8. The origin of eukaryotes 9. The origin of sex and the nature of species 10. Intragenomic conflict 11. Symbiosis 12. Development in simple organisms 13. Gene regulation and cell heredity 14. The development of spatial patterns 15. Development and evolution 16. The origins of societies 17. The origins of language References Index"
            },
            "slug": "The-Major-Transitions-in-Evolution-Smith-Szathm\u00e1ry",
            "title": {
                "fragments": [],
                "text": "The Major Transitions in Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book discusses the origins of societies, development and evolution, and the development of spatial patterns in simple organisms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59693498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c91b92ed2c09c1873979108da6e3402cb56e24",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard runlength-limiting codes \u2013 nonlinear codes defined by trellises \u2013 have the disadvantage that they disconnect the outer error-correcting code from the bit-bybit likelihoods that come out of the channel. The normal motivation for runlength limits is to prevent loss of synchronization between the transmitter and receiver. I suggest mapping the user data to transmitted symbols in such a way that timing errors lead not to errors of synchronization, but to symbol errors. In magnetic recording channels with high bit densities, there may be constraints not only on the maximum runlength but also on the number of successive transitions permitted. The coding method proposed here is compatible with these constraints."
            },
            "slug": "An-Alternative-to-Runlength-limiting-Codes:-Turn-Mackay",
            "title": {
                "fragments": [],
                "text": "An Alternative to Runlength-limiting Codes: Turn Timing Errors into Substitution Errors"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work suggests mapping the user data to transmitted symbols in such a way that timing errors lead not to errors of synchronization, but to symbol errors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145605452"
                        ],
                        "name": "B. McMillan",
                        "slug": "B.-McMillan",
                        "structuredName": {
                            "firstName": "Brockway",
                            "lastName": "McMillan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. McMillan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 72
                            }
                        ],
                        "text": "The Kraft inequality might be more accurately referred to as the Kraft\u2013 McMillan inequality: Kraft proved that if the inequality is satisfied, then a prefix code exists with the given lengths. McMillan (1956) proved the converse, that unique decodeability implies that the inequality holds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37706511,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "32da762acabe9200ce2ffce835cbfd4c055e61fc",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a list of b words, each word being a string of letters from a given fixed alphabet of a letters. If every string of words drawn from this list, when written out in letters without additional space marks to separate the words, is uniquely decipherable, then \\begin{equation} a^{-l_1} + a^{-l_2} + \\cdots + a^{-l_b} \\leq 1, \\qquad \\qquad (1) \\end{equation} where l_i, 1 \\leq i \\leq b , is the length of the i th word in the list. This result extends a remark of J. L. Doob, who derived the same inequality for lists of a more restricted kind. A consequence of (1) and work of Shannon is that this more restricted kind of list suffices in the search for codes with specified amounts of redundancy."
            },
            "slug": "Two-inequalities-implied-by-unique-decipherability-McMillan",
            "title": {
                "fragments": [],
                "text": "Two inequalities implied by unique decipherability"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A consequence of (1) and work of Shannon is that this more restricted kind of list suffices in the search for codes with specified amounts of redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134862099"
                        ],
                        "name": "W. C. Ferreira",
                        "slug": "W.-C.-Ferreira",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Ferreira",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. C. Ferreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144566299"
                        ],
                        "name": "W. A. Clarke",
                        "slug": "W.-A.-Clarke",
                        "structuredName": {
                            "firstName": "Willem",
                            "lastName": "Clarke",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413100"
                        ],
                        "name": "A. Helberg",
                        "slug": "A.-Helberg",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Helberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Helberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395830389"
                        ],
                        "name": "K. Abdel-Ghaffar",
                        "slug": "K.-Abdel-Ghaffar",
                        "structuredName": {
                            "firstName": "Khaled",
                            "lastName": "Abdel-Ghaffar",
                            "middleNames": [
                                "A.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Abdel-Ghaffar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144909932"
                        ],
                        "name": "A. Vinck",
                        "slug": "A.-Vinck",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Vinck",
                            "middleNames": [
                                "J.",
                                "Han"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vinck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 71
                            }
                        ],
                        "text": "Not even the capacity of channels with synchronization errors is known (Levenshtein, 1966; Ferreira et al., 1997); codes for reliable communication over channels with synchronization errors remain an active research area (Davey and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15799941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71a92c278fff14ed11312b78cd0568584878f190",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Levenshtein (1966) proposed a class of single insertion/deletion correcting codes, based on the number-theoretic construction due to Varshamov and Tenengolt's (1965). We present several interesting results on the binary structure of these codes, and their relation to constrained codes with nulls in the power spectral density function. One surprising result is that the higher order spectral null codes of Immink and Beenker (1987) are sub-codes of balanced Levenshtein codes. Other spectral null sub-codes with similar coding rates, may also be constructed. We furthermore present some coding schemes and spectral shaping markers which alleviate the fundamental restriction on Levenshtein's codes that the boundaries of each codeword should be known before insertion/deletion correction can be effected."
            },
            "slug": "Insertion/deletion-correction-with-spectral-nulls-Ferreira-Clarke",
            "title": {
                "fragments": [],
                "text": "Insertion/deletion correction with spectral nulls"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Some coding schemes and spectral shaping markers are presented which alleviate the fundamental restriction on Levenshtein's codes that the boundaries of each codeword should be known before insertion/deletion correction can be effected."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9704838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cea9d59691410447bde0f39a028ffb3e21181a3",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result xi(t) of mixing n unknown independent sources si(t) through an unknown n \u00d7 n mixing matrix A(t) of causal linear filters: xi = \u03a3j aij * sj. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm \"Contextual ICA,\" after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "slug": "Maximum-Likelihood-Blind-Source-Separation:-A-of-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting algorithm is called cICA, after the (Bell and Sejnowski 1995) Infomax algorithm, which is able to separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 104
                            }
                        ],
                        "text": "These networks have been successfully applied to real-world tasks as varied as pronouncing English text (Sejnowski and Rosenberg, 1987) and focussing multiple-mirror telescopes (Angel et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748112"
                        ],
                        "name": "P. Howard",
                        "slug": "P.-Howard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 100
                            }
                        ],
                        "text": "4 It uses a carefully designed approximate arithmetic coder for binary alphabets called the Z-coder (Bottou et al., 1998), which is much faster than the arithmetic coding software described above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16439611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b8c203bacd40a69f27bc7345bb94c88591a76fc",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Z-coder, a new adaptive data compression coder for coding binary data. The Z-coder is derived from the Golomb (1966) run-length coder, and retains most of the speed and simplicity of the earlier coder. The Z-coder can also be thought of as a multiplication-free approximate arithmetic coder, showing the close relationship between run-length coding and arithmetic coding. The Z-coder improves upon existing arithmetic coders by its speed and its principled design. We present a derivation of the Z-coder as well as details of the construction of its adaptive probability estimation table."
            },
            "slug": "The-Z-coder-adaptive-binary-coder-Bottou-Howard",
            "title": {
                "fragments": [],
                "text": "The Z-coder adaptive binary coder"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The Z-coder is a new adaptive data compression coder for coding binary data, derived from the Golomb (1966) run-length coder, and retains most of the speed and simplicity of the earlier coder."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings DCC '98 Data Compression Conference (Cat. No.98TB100225)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 18
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton, 1968) states that one should prefer models that can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61324799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d76ee9b6fb4d441e31abcd4dadc4e44c576017",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"
            },
            "slug": "An-Information-Measure-for-Classification-Wallace-Boulton",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is suggested that the best classification is that which results in the briefest recording of all the attribute information, and the measurements of each thing are regarded as being a message about that thing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47913669"
                        ],
                        "name": "G. Matheron",
                        "slug": "G.-Matheron",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Matheron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Matheron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "The two main approaches to implementing Bayesian inference for neural networks are the Monte Carlo methods developed by Neal (1996) and the Gaussian approximation methods developed by MacKay (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 128762998,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "0dc2f3ea76b9627497b8c7b1dba92c4037c5d4f0",
            "isKey": false,
            "numCitedBy": 4009,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge of ore grades and ore reserves as well as error estimation of these values, is fundamental for mining engineers and mining geologists. Until now no appropriate scientific approach to those estimation problems has existed: geostatistics, the principles of which are summarized in this paper, constitutes a new science leading to such an approach. The author criticizes classical statistical methods still in use, and shows some of the main results given by geostatistics. Any ore deposit evaluation as well as proper decision of starting mining operations should be preceded by a geostatistical investigation which may avoid economic failures."
            },
            "slug": "Principles-of-geostatistics-Matheron",
            "title": {
                "fragments": [],
                "text": "Principles of geostatistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 56
                            }
                        ],
                        "text": "Gaussian processes and support vector learning machines (Scholkopf et al., 1995; Vapnik, 1995) have a lot in common."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144758838"
                        ],
                        "name": "M. Ridley",
                        "slug": "M.-Ridley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ridley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ridley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 82498805,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "539768bdcbe50236515e17e91d0383389576a83a",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The existence of complex life is one of the great mysteries of evolution, for complexity is neither inevitable nor necessary. Indeed, as Mark Ridley shows in this important and thought-provoking book, two major biological hurdles had to be overcome to allow living complexity to evolve. Complex life is constructed from more genes than simple life. But as gene numbers increase, so too do the number of copying errors - it is easier to make a mistake copying the Bible than copying an advertising slogan. Similarly, natural selection encourages gene selfishness, and genes could easily evolve to subvert complex life forms. In retracing the history of life on our planet - from the initial wobbly replicating molecules, through microbes, worms and flies and ultimately to humans - Ridley reveals how life has evolved as a series of steps to deal with error and coerce genes to co-operate within each body. Mendel's Demon offers startling novel perspectives on matters as disparate as the origins of sex and gender, potential cures for AIDS, corporate mergers and acquisitions, and the long-term perils of human cloning."
            },
            "slug": "Mendel's-Demon:-Gene-Justice-and-the-Complexity-of-Ridley",
            "title": {
                "fragments": [],
                "text": "Mendel's Demon: Gene Justice and the Complexity of Life"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In retracing the history of life on the authors' planet - from the initial wobbly replicating molecules, through microbes, worms and flies and ultimately to humans - Ridley reveals how life has evolved as a series of steps to deal with error and coerce genes to co- cooperate within each body."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1122,
                                "start": 53
                            }
                        ],
                        "text": "This fragment is appropriate for a regular j=3, k= 4 Gallager code. of a regular network with parameters j, k, each new pair {r, x} in the list at the ith iteration is created by drawing the new x from its distribution and drawing at random with replacement (j\u22121)(k\u22121) pairs {r, x} from the list at the (i\u22121)th iteration; these are assembled into a tree fragment (figure 47.12) and the sum\u2013product algorithm is run from top to bottom to find the new r value associated with the new node. As an example, the results of runs with j=4, k=8 and noise densities f between 0.01 and 0.10, using 10 000 samples at each iteration, are shown in figure 47.10. Runs with low enough noise level show a collapse to zero entropy after a small number of iterations, and those with high noise level decrease to a non-zero entropy corresponding to a failure to decode. The boundary between these two behaviours is called the threshold of the decoding algorithm for the binary symmetric channel. Figure 47.10 shows by Monte Carlo simulation that the threshold for regular (j, k) = (4, 8) codes is about 0.075. Richardson and Urbanke (2001a) have derived thresholds for regular codes by a tour de force of direct analytic methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 36
                            }
                        ],
                        "text": "The following results are proved in Gallager (1963) and MacKay (1999b). Low-density parity-check codes, in spite of their simple construction, are good codes, given an optimal decoder (good codes in the sense of section 11."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 54
                            }
                        ],
                        "text": "For further reading about rate-distortion theory, see Gallager (1968), p. 451, or McEliece (2002), p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 20
                            }
                        ],
                        "text": "(For solutions, see Gallager (1963) and MacKay (1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 53
                            }
                        ],
                        "text": "Make the graph irregular The second way of improving Gallager codes, introduced by Luby et al. (2001b), is to make their graphs irregular."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 901,
                                "start": 4
                            }
                        ],
                        "text": "5 4 Gallager(273,82) DSC(273,82) Figure 47.18. An algebraically constructed low-density parity-check code satisfying many redundant constraints outperforms an equivalent random Gallager code. The table shows the N , M , K, distance d, and row weight k of some difference-set cyclic codes, highlighting the codes that have large d/N , small k, and large N/M . In the comparison the Gallager code had (j, k) = (4, 13), and rate identical to the N = 273 difference-set cyclic code. Vertical axis: block error probability. Horizontal axis: signal-to-noise ratio Eb/N0 (dB). about 0.6 dB; and Matthew Davey\u2019s code that combines both these features \u2013 it\u2019s irregular over GF (8) \u2013 gives a win of about 0.9 dB over the regular binary Gallager code. Methods for optimizing the profile of a Gallager code (that is, its number of rows and columns of each degree), have been developed by Richardson et al. (2001) and have led to low-density parity-check codes whose performance, when decoded by the sum\u2013product algorithm, is within a hair\u2019s breadth of the Shannon limit."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 67
                            }
                        ],
                        "text": "For a precise statement of this result and further references, see Gallager (1968), p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "The following results are proved in Gallager (1963) and MacKay (1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "For further reading about rate-distortion theory, see Gallager (1968), p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1142,
                                "start": 132
                            }
                        ],
                        "text": "One way to study the decoding algorithm is to imagine it running on an infinite tree-like graph with the same local topology as the Gallager code\u2019s graph. Figure 47.11. Local topology of the graph of a Gallager code with column weight j = 3 and row weight k = 4. White nodes represent bits, xl; black nodes represent checks, zm; each edge corresponds to a 1 in H. The larger the matrix H, the closer its decoding properties should approach those of the infinite graph. Imagine an infinite belief network with no loops, in which every bit xn connects to j checks and every check zm connects to k bits (figure 47.11). We consider the iterative flow of information in this network, and examine the average entropy of one bit as a function of number of iterations. At each iteration, a bit has accumulated information from its local network out to a radius equal to the number of iterations. Successful decoding will occur only if the average entropy of a bit decreases to zero as the number of iterations increases. The iterations of an infinite belief network can be simulated by Monte Carlo methods \u2013 a technique first used by Gallager (1963). Imagine a network of radius I (the total number of iterations) centred on one bit."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12709402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206f827fad201506c315d40c1469b41a45141893",
            "isKey": true,
            "numCitedBy": 10568,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed number j \\geq 3 of l's and each row contains a small fixed number k > j of l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixed j . When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixed j . A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. For j > 3 and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound."
            },
            "slug": "Low-density-parity-check-codes-Gallager",
            "title": {
                "fragments": [],
                "text": "Low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described and the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2033521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65e6ef044d951d2c3cbf7512b6bfa83a8b2b0739",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In honor of the twenty-fifth anniversary of Huffman coding, four new results about Huffman codes are presented. The first result shows that a binary prefix condition code is a Huffman code iff the intermediate and terminal nodes in the code tree can be listed by nonincreasing probability so that each node in the list is adjacent to its sibling. The second result upper bounds the redundancy (expected length minus entropy) of a binary Huffman code by P_{1}+ \\log_{2}[2(\\log_{2}e)/e]=P_{1}+0.086 , where P_{1} is the probability of the most likely source letter. The third result shows that one can always leave a codeword of length two unused and still have a redundancy of at most one. The fourth result is a simple algorithm for adapting a Huffman code to slowly varying esthnates of the source probabilities. In essence, one maintains a running count of uses of each node in the code tree and lists the nodes in order of these counts. Whenever the occurrence of a message increases a node count above the count of the next node in the list, the nodes, with their attached subtrees, are interchanged."
            },
            "slug": "Variations-on-a-theme-by-Huffman-Gallager",
            "title": {
                "fragments": [],
                "text": "Variations on a theme by Huffman"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Four new results about Huffman codes are presented and a simple algorithm for adapting a Huffman code to slowly varying esthnates of the source probabilities is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32590599"
                        ],
                        "name": "M. C. Davey",
                        "slug": "M.-C.-Davey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Davey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Davey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15660576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0756c8a5db4635b4c1b787646a7cfdc88d5a4649",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Gallager codes with large block length and low rate (e.g., N \u2243 10,000\u201340,000, R \u2243 0.25\u20130.5) have been shown to have record-breaking performance for low signal-to-noise applications. In this paper we study Gallager codes at the other end of the spectrum. We first explore the theoretical properties of binary Gallager codes with very high rates and observe that Gallager codes of any rate offer runlength-limiting properties at no additional cost."
            },
            "slug": "Evaluation-of-Gallager-Codes-for-Short-Block-Length-Mackay-Davey",
            "title": {
                "fragments": [],
                "text": "Evaluation of Gallager Codes for Short Block Length and High Rate Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper first explores the theoretical properties of binary Gallager codes with very high rates and observes that Gallager code of any rate offer runlength-limiting properties at no additional cost."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115778631"
                        ],
                        "name": "Illtyd Trethowan",
                        "slug": "Illtyd-Trethowan",
                        "structuredName": {
                            "firstName": "Illtyd",
                            "lastName": "Trethowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illtyd Trethowan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 77
                            }
                        ],
                        "text": "The encoding complexity can be reduced by clever encoding tricks invented by Richardson and Urbanke (2001b) or by specially constructing the paritycheck matrix (MacKay et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 220419906,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "a0117b72a4f68d0a134e24f674ca7fd0b42663b7",
            "isKey": false,
            "numCitedBy": 1989,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "IT is a rather depressing task for a Catholic to write a technical philosophical essay, for it is improbable that the philosophers whom he criticises wi l l read it . F r Hawkins therefore deserves our thanks in a special w a y ; such work as this is necessary and its effects, though slow, are sure. There are a number of excellences about the book. I t reveals a wide knowledge of contem\u00ad porary English thought on the subject ; an index would have shown many references to Mind, to the Proceedings of the Aristotelian Society and to works of modern logic (the well-arranged table of contents gives some idea of the book's scope in this respect). Recent attempts to eliminate the notion of causality are very carefully repulsed. The writing is almost invariably attractive, and there are no slips or misprints obscuring the argument, which must be something like a record for Catholic philosophical books in recent years. These last virtues unfortunately are not shared by the only review (to the writer's present knowledge) which has had anything in particular to say about F r Hawkins's book, that in the Tablet (October 23rd, 1937). I t wi l l be of advantage to consider this article, for the criticisms which it em\u00ad bodies deserve to be made clear, and it wi l l therefore be quoted later in some detail. T h e main criticism that we have to offer here affects the policy which F r Hawkins has adopted. He has written a small book (as someone said of Kant's Critique, it would have been quicker to read had it been longer) and by far the greater part is taken up by the historical setting of the question and"
            },
            "slug": "Causality-Trethowan",
            "title": {
                "fragments": [],
                "text": "Causality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1938
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636368"
                        ],
                        "name": "C. Brody",
                        "slug": "C.-Brody",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Brody",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Brody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 125
                            }
                        ],
                        "text": "The problem of finding a large subset of variables that are approximately equal can be solved with a neural network approach (Hopfield and Brody, 2000; Hopfield and Brody, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14219816,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "406d968077fdb21f7d44e5b0c095ff1d66bcd329",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "A previous paper described a network of simple integrate-and-fire neurons that contained output neurons selective for specific spatiotemporal patterns of inputs; only experimental results were described. We now present the principles behind the operation of this network and discuss how these principles point to a general class of computational operations that can be carried out easily and naturally by networks of spiking neurons. Transient synchrony of the action potentials of a group of neurons is used to signal \"recognition\" of a space-time pattern across the inputs of those neurons. Appropriate synaptic coupling produces synchrony when the inputs to these neurons are nearly equal, leaving the neurons unsynchronized or only weakly synchronized for other input circumstances. When the input to this system comes from timed past events represented by decaying delay activity, the pattern of synaptic connections can be set such that synchronization occurs only for selected spatiotemporal patterns. We show how the recognition is invariant to uniform time warp and uniform intensity change of the input events. The fundamental recognition event is a transient collective synchronization, representing \"many neurons now agree,\" an event that is then detected easily by a cell with a small time constant. If such synchronization is used in neurobiological computation, its hallmark will be a brief burst of gamma-band electroencephalogram noise when and where such a recognition event or decision occurs."
            },
            "slug": "What-is-a-moment-Transient-synchrony-as-a-mechanism-Hopfield-Brody",
            "title": {
                "fragments": [],
                "text": "What is a moment? Transient synchrony as a collective mechanism for spatiotemporal integration."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The principles behind the operation of a network of simple integrate-and-fire neurons that contained output neurons selective for specific spatiotemporal patterns of inputs are presented and it is shown how the recognition is invariant to uniform time warp and uniform intensity change of the input events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636368"
                        ],
                        "name": "C. Brody",
                        "slug": "C.-Brody",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Brody",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Brody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 125
                            }
                        ],
                        "text": "The problem of finding a large subset of variables that are approximately equal can be solved with a neural network approach (Hopfield and Brody, 2000; Hopfield and Brody, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2795465,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "42fd26e1ec3c485d5fd3db8d78a8ce1228efac7d",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of complex temporal sequences is a general sensory problem that requires integration of information over time. We describe a very simple \"organism\" that performs this task, exemplified here by recognition of spoken monosyllables. The network's computation can be understood through the application of simple but generally unexploited principles describing neural activity. The organism is a network of very simple neurons and synapses; the experiments are simulations. The network's recognition capabilities are robust to variations across speakers, simple masking noises, and large variations in system parameters. The network principles underlying recognition of short temporal sequences are applied here to speech, but similar ideas can be applied to aspects of vision, touch, and olfaction. In this article, we describe only properties of the system that could be measured if it were a real biological organism. We delay publication of the principles behind the network's operation as an intellectual challenge: the essential principles of operation can be deduced based on the experimental results presented here alone. An interactive web site (http://neuron.princeton.edu/ approximately moment) is available to allow readers to design and carry out their own experiments on the organism."
            },
            "slug": "What-is-a-moment-\"Cortical\"-sensory-integration-a-Hopfield-Brody",
            "title": {
                "fragments": [],
                "text": "What is a moment? \"Cortical\" sensory integration over a brief interval."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A very simple \"organism\" that performs this task, exemplified here by recognition of spoken monosyllables, is described, which is a network of very simple neurons and synapses; the experiments are simulations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 44
                            }
                        ],
                        "text": "How do cells perform error correction? (See Hopfield (1974), Hopfield (1980))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17477578,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "657591945c8ce123226f729713b7ecb8182df103",
            "isKey": false,
            "numCitedBy": 1205,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The specificity with which the genetic code is read in protein synthesis, and with which other highly specific biosynthetic reactions take place, can be increased above the level available from free energy differences in intermediates or kinetic barriers by a process defined here as kinetic proofreading. A simple kinetic pathway is described which results in this proofreading when the reaction is strongly but nonspecifically driven, e.g., by phosphate hydrolysis. Protein synthesis, amino acid recognition, and DNA replication, all exhibit the features of this model. In each case, known reactions which otherwise appear to be useless or deleterious complications are seen to be essential to the proofreading function."
            },
            "slug": "Kinetic-proofreading:-a-new-mechanism-for-reducing-Hopfield",
            "title": {
                "fragments": [],
                "text": "Kinetic proofreading: a new mechanism for reducing errors in biosynthetic processes requiring high specificity."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The specificity with which the genetic code is read in protein synthesis, and with which other highly specific biosynthetic reactions take place, can be increased above the level available from free energy differences in intermediates or kinetic barriers by a process defined here as kinetic proofreading."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2224611,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "db0b909610a19c58fd35b7c921145797fae5bc41",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A mechanism for proofreading biosynthetic processes requiring high accuracy is described. The previously understood \"kinetic proofreading\" mechanism of enhancing accuracy has distinguishing characteristics such as the non-stoichiometric use of substrate or cosubstrate that have allowed its identification in aspects of DNA and protein synthesis. The proofreading scheme developed here, though generically related, lacks all the previous identifying features. A DNA polymerase proofreading in this manner need neither generate dNMP nor have a 3' leads to 5' exonuclease activity. Protein synthesis could be proofread even with stoichiometric GTP consumption or without elongation factor Tu . GTP. The kinetic scheme that generates this proofreading makes use of an \"energy relay\" from previous substrate molecules and is a representative of a class of nonequilibrium processes displaying dynamic cooperativity. This proofreading mechanism has its own identifying characteristics, which are sufficiently subtle that they would have generally escaped notice or defied interpretation."
            },
            "slug": "The-energy-relay:-a-proofreading-scheme-based-on-of-Hopfield",
            "title": {
                "fragments": [],
                "text": "The energy relay: a proofreading scheme based on dynamic cooperativity and lacking all characteristic symptoms of kinetic proofreading in DNA replication and protein synthesis."
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A mechanism for proofreading biosynthetic processes requiring high accuracy, which makes use of an \"energy relay\" from previous substrate molecules and is a representative of a class of nonequilibrium processes displaying dynamic cooperativity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103152315"
                        ],
                        "name": "Lurias",
                        "slug": "Lurias",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Lurias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lurias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932699022"
                        ],
                        "name": "M. Delbrock",
                        "slug": "M.-Delbrock",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Delbrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Delbrock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10688611,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c891ca3891bf4aeb95f17b3ddc79cc1120d55c1c",
            "isKey": false,
            "numCitedBy": 1085,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "HEN a pure bacterial culture is. attacked by a bacterial virus, the culW ture will clear after a few hours due to destruction of the sensitive ceIls by the virus. However, after further incubation for a few hours, or sometimes days, the culture will often become turbid again, due to the growth of a bacterial variant which is resistant to the action of the virus. This variant can be isolated and freed from the virus and will in many cases retain its resistance to the action of the virus even if subcultured through many generations in the absence of the virus. While the sensitive strain adsorbed the virus readily, the resistant variant will generally not show any affinity to it. The resistant bacterial variants appear readily in cultures grown from a single cell. They were, therefore, certainly not present when the culture was started. Their resistance is generally rather specific. It does not extend to viruses that are found to differ by other criteria from the strain in whose presence the resistant culture developed. The variant may differ from the original strain in morphological or metabolic characteristics, or in serological type or in colony type. Most often, however, no such correlated changes are apparent, and the variant may be distinguished from the original strain only by its resistance to the inciting strain of virus. The nature of these variants and the manner in which they originate have been discussed by many authors, and numerous attempts have been made to correlate the phenomenon with other instances of bacterial variation. The net effect of the addition of virus consists of the appearance of a variant strain, characterized by a new stable character-namely, resistance to the inciting virus. The situation has often been expressed by saying that bacterial viruses are powerful \u201cdissociating agents.\u201d While this expression summarizes adequately the net effect, it must not be taken to imply anything about the mechanism by which the result is brought about. A moment\u2019s reflection will show that there are greatly differing mechanisms which might produce the same end result. D\u2019HERELLE (1926) and many other investigators believed that the virus by direct action induced the resistant variants. GRATIA (1921), BURNET (I929), and others, on the other hand, believed that the resistant bacterial variants are produced by mutation in the culture prior to the addition of virus. The"
            },
            "slug": "MUTATIONS-OF-BACTERIA-FROM-VIRUS-SENSITIVITY-TO-Lurias-Delbrock",
            "title": {
                "fragments": [],
                "text": "MUTATIONS OF BACTERIA FROM VIRUS SENSITIVITY TO VIRUS RESISTANCE\u2019-\u2019"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The net effect of the addition of virus consists of the appearance of a variant strain, characterized by a new stable character-namely, resistance to the inciting virus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399438082"
                        ],
                        "name": "J. Angel",
                        "slug": "J.-Angel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Angel",
                            "middleNames": [
                                "Roger",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Angel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6613822"
                        ],
                        "name": "P. Wizinowich",
                        "slug": "P.-Wizinowich",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wizinowich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wizinowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389862660"
                        ],
                        "name": "M. Lloyd-Hart",
                        "slug": "M.-Lloyd-Hart",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lloyd-Hart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lloyd-Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8952596"
                        ],
                        "name": "D. Sandler",
                        "slug": "D.-Sandler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sandler",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sandler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 177
                            }
                        ],
                        "text": "These networks have been successfully applied to real-world tasks as varied as pronouncing English text (Sejnowski and Rosenberg, 1987) and focussing multiple-mirror telescopes (Angel et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4277029,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8bc55121991997ddac98eec923d9ea616621d731",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "IMAGES formed by ground-based telescopes are marred by atmospheric 'seeing9. The plane wavefront from an unresolved star is distorted by continually changing turbulent fluctuations in the air's refractive index. Diffraction-limited performance can in principle be recovered through the methods of adaptive optics, in which the instantaneous wavefront shape is sensed and corrected in real-time by deformable optics that cancel the distortion1,2. The highest resolution will be achieved when this technique is applied to multiple-telescope arrays. For such arrays, the biggest errors caused by seeing at infrared wavelengths are the variations in pathlength and wavefront tilt between array elements. We show here that these errors can be derived by an artificial neural network, given only a pair of simultaneous in-focus and out-of-focus images of a reference star formed at the combined focus of all the array elements. We have optimized a neural network appropriate for 2.2-\u03bcm wavelength imaging at the Multiple Mirror Telescope in Arizona. Corrections made by moving the beam-combining mirrors will largely recover the diffraction-limited profile, with a resolution of 0.06 arcsec."
            },
            "slug": "Adaptive-optics-for-array-telescopes-using-Angel-Wizinowich",
            "title": {
                "fragments": [],
                "text": "Adaptive optics for array telescopes using neural-network techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222408"
                        ],
                        "name": "S. Dolinar",
                        "slug": "S.-Dolinar",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Dolinar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dolinar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "A code using the same format but using a longer constraint length \u2013 15 \u2013 for its convolutional code and a larger Reed\u2013 Solomon code was developed by the Jet Propulsion Laboratory (Swanson, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Tanner (1981) generalized Gallager\u2019s work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 109755523,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ed2d7372248a392428d3200a8991c92861aed99e",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past six to eight years, an extensive research effort was conducted to investigate advanced coding techniques which promised to yield more coding gain than is available with current NASA standard codes. The delay in Galileo's launch due to the temporary suspension of the shuttle program provided the Galileo project with an opportunity to evaluate the possibility of including some version of the advanced codes as a mission enhancement option. A study was initiated last summer to determine if substantial coding gain was feasible for Galileo and, is so, to recommend a suitable experimental code for use as a switchable alternative to the current NASA-standard code. The Galileo experimental code study resulted in the selection of a code with constant length 15 and rate 1/4. The code parameters were chosen to optimize performance within cost and risk constraints consistent with retrofitting the new code into the existing Galileo system design and launch schedule. The particular code was recommended after a very limited search among good codes with the chosen parameters. It will theoretically yield about 1.5 dB enhancement under idealizing assumptions relative to the current NASA-standard code at Galileo's desired bit error rates. This ideal predicted gain includes enough cushion to meet the project's target of at least 1 dB enhancement under real, non-ideal conditions."
            },
            "slug": "A-New-Code-for-Galileo-Dolinar",
            "title": {
                "fragments": [],
                "text": "A New Code for Galileo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401097482"
                        ],
                        "name": "A. Eyre-Walker",
                        "slug": "A.-Eyre-Walker",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Eyre-Walker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Eyre-Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111640"
                        ],
                        "name": "P. Keightley",
                        "slug": "P.-Keightley",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Keightley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Keightley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4314159,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ca5673dae49c65d41ff4948a9d138390dc056186",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been suggested that humans may suffer a high genomic deleterious mutation rate,. Here we test this hypothesis by applying a variant of a molecular approach to estimate the deleterious mutation rate in hominids from the level of selective constraint in DNA sequences. Under conservative assumptions, we estimate that an average of 4.2 amino-acid-altering mutations per diploid per generation have occurred in the human lineage since humans separated from chimpanzees. Of these mutations, we estimate that at least 38% have been eliminated by natural selection, indicating that there have been more than 1.6 new deleterious mutations per diploid genome per generation. Thus, the deleterious mutation rate specific to protein-coding sequences alone is close to the upper limit tolerable by a species such as humans that has a low reproductive rate, indicating that the effects of deleterious mutations may have combined synergistically. Furthermore, the level of selective constraint in hominid protein-coding sequences is atypically low. A large number of slightly deleterious mutations may therefore have become fixed in hominid lineages."
            },
            "slug": "High-genomic-deleterious-mutation-rates-in-hominids-Eyre-Walker-Keightley",
            "title": {
                "fragments": [],
                "text": "High genomic deleterious mutation rates in hominids"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The deleterious mutation rate specific to protein-coding sequences alone is close to the upper limit tolerable by a species such as humans that has a low reproductive rate, indicating that the effects of deleteriously mutations may have combined synergistically."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3517194"
                        ],
                        "name": "E. Capaldi",
                        "slug": "E.-Capaldi",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Capaldi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Capaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 29
                            }
                        ],
                        "text": "A simple model due to Donald Hebb (1949) captures the idea of associative memory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2085961,
            "fieldsOfStudy": [
                "Education",
                "Medicine"
            ],
            "id": "3cea0c3d350d78bd3d9bd557c1fe56c59c9bf0e2",
            "isKey": false,
            "numCitedBy": 4823,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd the organization of behavior as the choice of reading, you can find here."
            },
            "slug": "The-organization-of-behavior.-Capaldi",
            "title": {
                "fragments": [],
                "text": "The organization of behavior."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of applied behavior analysis"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104685446"
                        ],
                        "name": "C. McCollough",
                        "slug": "C.-McCollough",
                        "structuredName": {
                            "firstName": "Celeste",
                            "lastName": "McCollough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McCollough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 106
                            }
                        ],
                        "text": "I speculate that the McCollough effect, an extremely long-lasting association of colours with orientation (McCollough, 1965; MacKay and MacKay, 1974), is produced by the adaptation mechanism that tunes our chromaticaberration-deconvolution circuits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29075991,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b533059ce46d811804f5416434c15866fa599b16",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An aftereffect of color which depends on the orientation of lines in the test field may be obtained by presenting a horizontal grating of one color alternately with a vertical grating of a different color. Like the aftereffect of adaptation to chromatic fringes produced by prismatic spectacles, this aftereffect is visible in monochromatic light and fails to show inter-ocular transfer. It is suggested that both effects are to be understood in terms of color adaptation of orientation-specific edge-detectors."
            },
            "slug": "Color-Adaptation-of-Edge-Detectors-in-the-Human-McCollough",
            "title": {
                "fragments": [],
                "text": "Color Adaptation of Edge-Detectors in the Human Visual System"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An aftereffect of color which depends on the orientation of lines in the test field may be obtained by presenting a horizontal grating of one color alternately with a vertical grates of a different color."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143679541"
                        ],
                        "name": "David J. Ward",
                        "slug": "David-J.-Ward",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ward",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784157"
                        ],
                        "name": "A. Blackwell",
                        "slug": "A.-Blackwell",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Blackwell",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blackwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057742060"
                        ],
                        "name": "D. MacKay",
                        "slug": "D.-MacKay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "MacKay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. MacKay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "By inverting an arithmetic coder, we can obtain an information-efficient text entry device that is driven by continuous pointing gestures (Ward et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 189874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e07d1711f14d11d99f4826c364bc6ca6e35fcff0",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing devices for communicating information to computers are bulky, slow to use, or unreliable. Dasher is a new interface incorporating language modelling and driven by continuous two-dimensional gestures, e.g. a mouse, touchscreen, or eye-tracker. Tests have shown that this device can be used to enter text at a rate of up to 34 words per minute, compared with typical ten-finger keyboard typing of 40-60 words per minute. Although the interface is slower than a conventional keyboard, it is small and simple, and could be used on personal data assistants and by motion-impaired computer users."
            },
            "slug": "Dasher\u2014a-data-entry-interface-using-continuous-and-Ward-Blackwell",
            "title": {
                "fragments": [],
                "text": "Dasher\u2014a data entry interface using continuous gestures and language models"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Dasher is a new interface incorporating language modelling and driven by continuous two-dimensional gestures, e.g. a mouse, touchscreen, or eye-tracker that could be used on personal data assistants and by motion-impaired computer users."
            },
            "venue": {
                "fragments": [],
                "text": "UIST '00"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 712,
                                "start": 141
                            }
                        ],
                        "text": "For high-dimensional problems, the most widely used random sampling methods are Markov chain Monte Carlo methods like the Metropolis method, Gibbs sampling, and slice sampling. The problem with all these methods is this: yes, a given algorithm can be guaranteed to produce samples from the target density P (x) asymptotically, \u2018once the chain has converged to the equilibrium distribution\u2019. But if one runs the chain for too short a time T , then the samples will come from some other distribution P (T )(x). For how long must the Markov chain be run before it has \u2018converged\u2019? As was mentioned in Chapter 29, this question is usually very hard to answer. However, the pioneering work of Propp and Wilson (1996) allows one, for certain chains, to answer this very question; furthermore Propp and Wilson show how to obtain \u2018exact\u2019 samples from the target density."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 777,
                                "start": 110
                            }
                        ],
                        "text": "This chapter describes a sequence of methods: importance sampling, rejection sampling, the Metropolis method, Gibbs sampling and slice sampling. For each method, we discuss whether the method is expected to be useful for high-dimensional problems such as arise in inference with graphical models. [A graphical model is a probabilistic model in which dependencies and independencies of variables are represented by edges in a graph whose nodes are the variables.] Along the way, the terminology of Markov chain Monte Carlo methods is presented. The subsequent chapter discusses advanced methods for reducing random walk behaviour. For details of Monte Carlo methods, theorems and proofs and a full list of references, the reader is directed to Neal (1993b), Gilks et al. (1996), and Tanner (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1621,
                                "start": 16
                            }
                        ],
                        "text": "Discuss whether Gibbs sampling has similar properties. Exercise 29.20. ] Markov chain Monte Carlo methods do not compute partition functions Z, yet they allow ratios of quantities like Z to be estimated. For example, consider a random-walk Metropolis algorithm in a state space where the energy is zero in a connected accessible region, and infinitely large everywhere else; and imagine that the accessible space can be chopped into two regions connected by one or more corridor states. The fraction of times spent in each region at equilibrium is proportional to the volume of the region. How does the Monte Carlo method manage to do this without measuring the volumes? Exercise 29.21. ] Philosophy. One curious defect of these Monte Carlo methods \u2013 which are widely used by Bayesian statisticians \u2013 is that they are all non-Bayesian (O\u2019Hagan, 1987). They involve computer experiments from which estimators of quantities of interest are derived. These estimators depend on the proposal distributions that were used to generate the samples and on the random numbers that happened to come out of our random number generator. In contrast, an alternative Bayesian approach to the problem would use the results of our computer experiments to infer the properties of the target function P (x) and generate predictive distributions for quantities of interest such as \u03a6. This approach would give answers that would depend only on the computed values of P \u2217(x(r)) at the points {x(r)}; the answers would not depend on how those points were chosen. Can you make a Bayesian Monte Carlo method? (See Rasmussen and Ghahramani (2003) for a practical attempt."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 134
                            }
                        ],
                        "text": "One approach based on the ideas of Skilling (1993) makes approximations to C\u22121t and TraceC\u22121 using iterative methods with cost O(N 2) (Gibbs and MacKay, 1996; Gibbs, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 756,
                                "start": 110
                            }
                        ],
                        "text": "This chapter describes a sequence of methods: importance sampling, rejection sampling, the Metropolis method, Gibbs sampling and slice sampling. For each method, we discuss whether the method is expected to be useful for high-dimensional problems such as arise in inference with graphical models. [A graphical model is a probabilistic model in which dependencies and independencies of variables are represented by edges in a graph whose nodes are the variables.] Along the way, the terminology of Markov chain Monte Carlo methods is presented. The subsequent chapter discusses advanced methods for reducing random walk behaviour. For details of Monte Carlo methods, theorems and proofs and a full list of references, the reader is directed to Neal (1993b), Gilks et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 796,
                                "start": 110
                            }
                        ],
                        "text": "This chapter describes a sequence of methods: importance sampling, rejection sampling, the Metropolis method, Gibbs sampling and slice sampling. For each method, we discuss whether the method is expected to be useful for high-dimensional problems such as arise in inference with graphical models. [A graphical model is a probabilistic model in which dependencies and independencies of variables are represented by edges in a graph whose nodes are the variables.] Along the way, the terminology of Markov chain Monte Carlo methods is presented. The subsequent chapter discusses advanced methods for reducing random walk behaviour. For details of Monte Carlo methods, theorems and proofs and a full list of references, the reader is directed to Neal (1993b), Gilks et al. (1996), and Tanner (1996). In this chapter I will use the word \u2018sample\u2019 in the following sense: a sample from a distribution P (x) is a single realization x whose probability distribution is P (x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1554,
                                "start": 269
                            }
                        ],
                        "text": "The update rule at each step of the Markov chain takes a single spin, enumerates all possible states of the neighbouring spins that are compatible with the current summary state, and, for each of these local scenarios, computes the new value (+ or -) of the spin using Gibbs sampling (coupled to a random number u as in algorithm 32.4). If all these new values agree, then the new value of the updated spin in the summary state is set to the unanimous value (+ or -). Otherwise, the new value of the spin in the summary state is \u2018?\u2019. The initial condition, at time T0, is given by setting all the spins in the summary state to \u2018?\u2019, which corresponds to considering all possible start configurations. In the case of a spin system with positive couplings, this summary state simulation will be identical to the simulation of the uppermost state and lowermost states, in the style of Propp and Wilson, with coalescence occuring when all the \u2018?\u2019 symbols have disappeared. The summary state method can be applied to general spin systems with any couplings. The only shortcoming of this method is that the envelope may describe an unnecessarily large set of states, so there is no guarantee that the summary state algorithm will converge; the time for coalescence to be detected may be considerably larger than the actual time taken for the underlying Markov chain to coalesce. The summary state scheme has been applied to exact sampling in belief networks by Harvey and Neal (2000), and to the triangular antiferromagnetic Ising model by Childs et al. (2001). Summary state methods were first introduced by Huber (1998); they also go by the names sandwiching methods and bounding chains."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 388,
                                "start": 28
                            }
                        ],
                        "text": "As an example, consider the Gibbs sampling method applied to a ferromagnetic Ising spin system, with the partial ordering of states being defined thus: state x is \u2018greater than or equal to\u2019 state y if xi \u2265 yi for all spins i. The maximal and minimal states are the the all-up and all-down states. The Markov chains are coupled together as shown in algorithm 32.4. Propp and Wilson (1996) show that exact samples can be generated for this system, although the time to find exact samples is large if the Ising model is below its critical temperature, since the Gibbs sampling method itself is slowly-mixing under these conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1477,
                                "start": 269
                            }
                        ],
                        "text": "The update rule at each step of the Markov chain takes a single spin, enumerates all possible states of the neighbouring spins that are compatible with the current summary state, and, for each of these local scenarios, computes the new value (+ or -) of the spin using Gibbs sampling (coupled to a random number u as in algorithm 32.4). If all these new values agree, then the new value of the updated spin in the summary state is set to the unanimous value (+ or -). Otherwise, the new value of the spin in the summary state is \u2018?\u2019. The initial condition, at time T0, is given by setting all the spins in the summary state to \u2018?\u2019, which corresponds to considering all possible start configurations. In the case of a spin system with positive couplings, this summary state simulation will be identical to the simulation of the uppermost state and lowermost states, in the style of Propp and Wilson, with coalescence occuring when all the \u2018?\u2019 symbols have disappeared. The summary state method can be applied to general spin systems with any couplings. The only shortcoming of this method is that the envelope may describe an unnecessarily large set of states, so there is no guarantee that the summary state algorithm will converge; the time for coalescence to be detected may be considerably larger than the actual time taken for the underlying Markov chain to coalesce. The summary state scheme has been applied to exact sampling in belief networks by Harvey and Neal (2000), and to the triangular antiferromagnetic Ising model by Childs et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Gaussian Processes for Regression and Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Cambridge Univ. PhD dissertation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150346573"
                        ],
                        "name": "Y. Bar-Shalom",
                        "slug": "Y.-Bar-Shalom",
                        "structuredName": {
                            "firstName": "Yaakov",
                            "lastName": "Bar-Shalom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bar-Shalom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 400,
                                "start": 161
                            }
                        ],
                        "text": "Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes, and are popular models for speech and music modelling (Bar-Shalom and Fortmann, 1988). Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 667,
                                "start": 161
                            }
                        ],
                        "text": "Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes, and are popular models for speech and music modelling (Bar-Shalom and Fortmann, 1988). Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparison of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996). Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 541,
                                "start": 161
                            }
                        ],
                        "text": "Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes, and are popular models for speech and music modelling (Bar-Shalom and Fortmann, 1988). Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparison of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 160
                            }
                        ],
                        "text": "Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes, and are popular models for speech and music modelling (Bar-Shalom and Fortmann, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 126190349,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d72d3003f5b9596473ae848995e789e80ed965e",
            "isKey": true,
            "numCitedBy": 4423,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tracking-and-data-association-Bar-Shalom",
            "title": {
                "fragments": [],
                "text": "Tracking and data association"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 392,
                                "start": 37
                            }
                        ],
                        "text": "[This remarkable 1/n law is known as Zipf\u2019s law, and applies to the word frequencies of many languages (Zipf, 1949).] If we assume that English is generated by picking words at random according to this distribution, what is the entropy of English (per word)? [This calculation can be found in \u2018Prediction and entropy of printed English\u2019, C.E. Shannon, Bell Syst. Tech. J. 30, pp.50\u201364 (1950), but, inexplicably, the great man made numerical errors in it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "[This remarkable 1/n law is known as Zipf\u2019s law, and applies to the word frequencies of many languages (Zipf, 1949)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "What is the nature of this distribution over words? Zipf\u2019s law (Zipf, 1949) asserts that the probability of the rth most probable word in a language is approximately"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 13
                            }
                        ],
                        "text": "According to Zipf, a log\u2013log plot of frequency versus word-rank should show a straight line with slope \u2212\u03b1. Mandelbrot\u2019s (1982) modification of Zipf\u2019s law introduces a third parameter v, asserting that the probabilities are given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141120597,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac",
            "isKey": true,
            "numCitedBy": 7038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-behavior-and-the-principle-of-least-effort-Zipf",
            "title": {
                "fragments": [],
                "text": "Human behavior and the principle of least effort"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Gibbs and MacKay (2000) have implemented another cheap and cheerful approach based on the methods of Jaakkola and Jordan (section 35."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 184
                            }
                        ],
                        "text": "The two main approaches to implementing Bayesian inference for neural networks are the Monte Carlo methods developed by Neal (1996) and the Gaussian approximation methods developed by MacKay (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 40
                            }
                        ],
                        "text": "(For solutions, see Gallager (1963) and MacKay (1999b).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 56
                            }
                        ],
                        "text": "The following results are proved in Gallager (1963) and MacKay (1999b). Low-density parity-check codes, in spite of their simple construction, are good codes, given an optimal decoder (good in the sense of section 11."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Draft 3.1415"
            },
            "venue": {
                "fragments": [],
                "text": "January 12,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "The encoding complexity can be reduced by clever encoding tricks invented by Richardson and Urbanke (2001b) or by specially constructing the paritycheck matrix (MacKay et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 75
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 71
                            }
                        ],
                        "text": "Fast encoding of general low-density parity-check codes Richardson and Urbanke (2001b) demonstrated an elegant method by which the encoding cost of any low-density parity-check code can be reduced from the straightforward method\u2019s M 2 to a cost of N + g2, where g, the gap, is hopefully a small constant, and in the worst cases scales as a small fraction of N ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 348,
                                "start": 75
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparison of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 15
                            }
                        ],
                        "text": "Richardson and Urbanke (2001a) have derived thresholds for regular codes by a tour de force of direct analytic methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "Thresholds fmax for regular low\u2013density parity\u2013check codes, assuming sum\u2013product decoding algorithm, from Richardson and Urbanke (2001a). The Shannon limit for rate-1/2 codes is fmax = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 87
                            }
                        ],
                        "text": "] Implement an optimal digital fountain decoder that uses the method of Richardson and Urbanke (2001b) derived for fast encoding of sparse-graph codes (section 47."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 81
                            }
                        ],
                        "text": "Useful tools for the design of irregular low\u2013density parity\u2013 check codes include (Chung et al., 1999; Urbanke, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 474,
                                "start": 75
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes. See also O\u2019Hagan (1978). The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b). A thorough comparison of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996). Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LdpcOpt \u2013 a fast and accurate degree distribution optimizer for LDPC code ensembles. lthcwww.epfl.ch/ research/ldpcopt"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 11
                            }
                        ],
                        "text": "And indeed Neal (1996) showed that the properties of a neural network with one hidden layer (as in equation (45."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 62
                            }
                        ],
                        "text": "We now give a more general view of K-means clustering, due to Neal and Hinton (1998), in which the algorithm is shown to optimize a variational objective function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 33
                            }
                        ],
                        "text": "Importance sampling (reviewed by Neal (1993b)) and annealed importance sampling (Neal, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 10
                            }
                        ],
                        "text": "Pinto and Neal (2001) have shown that the accuracy of estimates obtained from a Markov chain Monte Carlo simulation (the second problem discussed in section 29."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "Radford Neal (1996) has also shown that in the limit as H \u2192 \u221e the statistical properties of the functions generated by randomizing the weights are independent of the number of hidden units; so, interestingly, the complexity of the functions becomes independent of the number of parameters in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "\u2018Thermodynamic integration\u2019 during simulated annealing, the \u2018acceptance ratio\u2019 method, and \u2018umbrella sampling\u2019 (reviewed by Neal (1993b))."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian mixture modelling by Monte Carlo simulation"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CRG\u2013TR\u201391\u20132,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 55
                            }
                        ],
                        "text": "Bits back encoding has been turned into a practical compression method for data modelled with latent variable models by Frey (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 119
                            }
                        ],
                        "text": ", one that is memoryless on sufficiently long timescales), the Lempel\u2013Ziv algorithm can be proven asymptotically to compress down to the entropy of the source. This is why it is called a \u2018universal\u2019 compression algorithm. For a proof of this property, see Cover and Thomas (1991). It achieves its compression, however, only by memorizing substrings that have happened so that it has a short name for them the next time they occur."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2324,
                                "start": 2
                            }
                        ],
                        "text": "express the Propp and Wilson algorithm in a way that generalizes to the case of spin systems with negative couplings. The idea the summary state version of the exact sampling method is still that we keep track of bounds on the set of all trajectories, and detect when these bounds are equal, so as to find exact samples. But the bounds will not themselves be actual trajectories, and they will not necessarily be tight bounds. Instead of simulating two trajectories, each of which moves in a state space {\u22121,+1}N , we simulate one trajectory envelope in an augmented state space {\u22121,+1, ?}N , where the symbol ? denotes \u2018either \u22121 or +1\u2019. We call the state of this augmented system the \u2018summary state\u2019. An example summary state of a six-spin system is ++-?+?. This summary state is shorthand for the set of states ++-+++, ++-++-, ++--++, ++--+- . The update rule at each step of the Markov chain takes a single spin, enumerates all possible states of the neighbouring spins that are compatible with the current summary state, and, for each of these local scenarios, computes the new value (+ or -) of the spin using Gibbs sampling (coupled to a random number u as in algorithm 32.4). If all these new values agree, then the new value of the updated spin in the summary state is set to the unanimous value (+ or -). Otherwise, the new value of the spin in the summary state is \u2018?\u2019. The initial condition, at time T0, is given by setting all the spins in the summary state to \u2018?\u2019, which corresponds to considering all possible start configurations. In the case of a spin system with positive couplings, this summary state simulation will be identical to the simulation of the uppermost state and lowermost states, in the style of Propp and Wilson, with coalescence occuring when all the \u2018?\u2019 symbols have disappeared. The summary state method can be applied to general spin systems with any couplings. The only shortcoming of this method is that the envelope may describe an unnecessarily large set of states, so there is no guarantee that the summary state algorithm will converge; the time for coalescence to be detected may be considerably larger than the actual time taken for the underlying Markov chain to coalesce. The summary state scheme has been applied to exact sampling in belief networks by Harvey and Neal (2000), and to the triangular antiferromagnetic Ising model by Childs et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On-screen viewing permitted"
            },
            "venue": {
                "fragments": [],
                "text": "Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. Index \u0393, 598 \u03a6(z), 514"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "For low-density parity-check codes applicable to quantum error-correction, see MacKay et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 4
                            }
                        ],
                        "text": "See MacKay (1998a) for discussion of this exercise."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 184
                            }
                        ],
                        "text": "The two main approaches to implementing Bayesian inference for neural networks are the Monte Carlo methods developed by Neal (1996) and the Gaussian approximation methods developed by MacKay (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 116
                            }
                        ],
                        "text": "] What is the capacity of the axon of a spiking neuron, viewed as a communication channel, in bits per second? [See MacKay and McCulloch (1952) for an early publication on this topic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "Some practical codes for multi-user channels are presented in Ratzer and MacKay (2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Adaptive Models. California Institute of Technology PhD dissertation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 89
                            }
                        ],
                        "text": "For articles on this idea, and new approximate inference algorithms motivated by it, see Yedidia (2000); Yedidia et al. (2000c); Welling and Teh (2001); Yuille (2001); Yedidia et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 89
                            }
                        ],
                        "text": "For articles on this idea, and new approximate inference algorithms motivated by it, see Yedidia (2000); Yedidia et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 226
                            }
                        ],
                        "text": "We\u2019ll assume that the channel is a memoryless channel (though more complex channels can easily be handled by running the sum\u2013product algorithm on a more complex graph that represents the expected correlations among the errors (Worthen and Stark, 1998))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Low-density parity check codes for fading channels with memory"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 36th Allerton Conf. on Communication,"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052840456"
                        ],
                        "name": "A. Bhattacharyya",
                        "slug": "A.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bhattacharyya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 235941388,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "26953a65d5136ce54ca43ca916e9454de5aac084",
            "isKey": false,
            "numCitedBy": 1616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-a-measure-of-divergence-between-two-statistical-Bhattacharyya",
            "title": {
                "fragments": [],
                "text": "On a measure of divergence between two statistical populations defined by their probability distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1943
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145447865"
                        ],
                        "name": "A. Mansour",
                        "slug": "A.-Mansour",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mansour"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 83
                            }
                        ],
                        "text": "Further reading on blind separation, including non-ICA algorithms, can be found in (Jutten and Herault, 1991; Comon et al., 1991; Hendin et al., 1994; Amari et al., 1996; Hojen-Sorensen et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 208963683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3af7f1ecb210ad315aeae4ffae932d17f9f85d47",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-Separation-of-Sources-Mansour",
            "title": {
                "fragments": [],
                "text": "Blind Separation of Sources"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850958"
                        ],
                        "name": "J. Patrick",
                        "slug": "J.-Patrick",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Patrick",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Patrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 90
                            }
                        ],
                        "text": "Although some of the earliest work on complex model comparison involved the MDL framework (Patrick and Wallace, 1982), MDL has no apparent advantages over the direct probabilistic approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 190222497,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0dced67b882a610063ce9de08af102978d1288ec",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Archaeoastronomy-in-the-Old-World:-STONE-CIRCLE-AN-Patrick-Wallace",
            "title": {
                "fragments": [],
                "text": "Archaeoastronomy in the Old World: STONE CIRCLE GEOMETRIES: AN INFORMATION THEORY APPROACH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31781461"
                        ],
                        "name": "M. Tanner",
                        "slug": "M.-Tanner",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Tanner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 12
                            }
                        ],
                        "text": "(1996), and Tanner (1996). In this chapter I will use the word \u2018sample\u2019 in the following sense: a sample from a distribution P (x) is a single realization x whose probability distribution is P (x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124725107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e160cdd387ccca0ed150e5ed2a87c56f576f6e35",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tools-for-Statistical-Inference:-Methods-for-the-of-Tanner",
            "title": {
                "fragments": [],
                "text": "Tools for Statistical Inference: Methods for the Exploration of Posterior Distributions and Likelihood Functions, 3rd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32352062"
                        ],
                        "name": "C. Heyde",
                        "slug": "C.-Heyde",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Heyde",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Heyde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704728"
                        ],
                        "name": "E. Seneta",
                        "slug": "E.-Seneta",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Seneta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Seneta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "For further reading about Turing and Bletchley Park, see Hodges (1983) and Good (1979). For an in-depth read about cryptography, Schneier\u2019s (1996) book is highly recommended."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1096,
                                "start": 111
                            }
                        ],
                        "text": "To deduce the state of the machine, \u2018it was therefore necessary to find about 129 decibans from somewhere\u2019, as Good puts it. Banburismus was aimed not at deducing the entire state of the machine, but only at figuring out which wheels were in use; the logic-based bombes, fed with guesses of the plaintext (cribs), were then used to crack what the settings of the wheels were. The Enigma machine, once its wheels and plugs were put in place, implemented a continually-changing permutation cypher that wandered deterministically through a state space of 263 permutations. Because an enormous number of messages were sent each day, there was a good chance that whatever state one machine was in when sending one character of a message, there would be another machine in the same state while sending a particular character in another message. Because the evolution of the machine\u2019s state was deterministic, the two machines would remain in the same state as each other (1)I\u2019ve been most helped by descriptions given by Tony Sale (http://www. codesandciphers.org.uk/lectures/) and by Jack Good (1979), who worked with Turing at Bletchley."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122546577,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "13f68dfb1615f4edd53fc971f0ebec86614afdc1",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Studies-in-the-History-of-Probability-and-XXXI.-The-Heyde-Seneta",
            "title": {
                "fragments": [],
                "text": "Studies in the History of Probability and Statistics. XXXI. The simple branching process, a turning point test and a fundamental inequality: A historical note on I. J. Bienaym\u00e9"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120208815,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "603bdbb17ba1f909280405a076455ac4f878fbf3",
            "isKey": false,
            "numCitedBy": 2773,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Inference-for-Probabilistic-Functions-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "Statistical Inference for Probabilistic Functions of Finite State Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105524883"
                        ],
                        "name": "S. Barnett",
                        "slug": "S.-Barnett",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Barnett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Barnett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "We can write C\u22121 N+1 in terms of CN and C \u22121 N using the partitioned inverse equations (Barnett, 1979):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117450169,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "3225cc94a4a1ac541035705b7d0dd3fd0617c94b",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-Methods-for-Engineers-and-Scientists-Barnett",
            "title": {
                "fragments": [],
                "text": "Matrix Methods for Engineers and Scientists"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369815"
                        ],
                        "name": "K. Ritter",
                        "slug": "K.-Ritter",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118940380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee67b18db12698aea68331c5f2c61cb8fd0f6510",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-numerical-analysis-Ritter",
            "title": {
                "fragments": [],
                "text": "Bayesian numerical analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143834737"
                        ],
                        "name": "D. A. Bell",
                        "slug": "D.-A.-Bell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bell",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. A. Bell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109410157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e163fb03f549ab2bb1dbbf746005553ea15a575",
            "isKey": false,
            "numCitedBy": 2797,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-Theory-and-Reliable-Communication-Bell",
            "title": {
                "fragments": [],
                "text": "Information Theory and Reliable Communication"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154179"
                        ],
                        "name": "V. Levenshtein",
                        "slug": "V.-Levenshtein",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Levenshtein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Levenshtein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 71
                            }
                        ],
                        "text": "Not even the capacity of channels with synchronization errors is known (Levenshtein, 1966; Ferreira et al., 1997); codes for reliable communication over channels with synchronization errors remain an active research area (Davey and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60827152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd",
            "isKey": false,
            "numCitedBy": 10971,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Binary-codes-capable-of-correcting-deletions,-and-Levenshtein",
            "title": {
                "fragments": [],
                "text": "Binary codes capable of correcting deletions, insertions, and reversals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47976263"
                        ],
                        "name": "F. Mosteller",
                        "slug": "F.-Mosteller",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Mosteller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mosteller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144087709"
                        ],
                        "name": "D. L. Wallace",
                        "slug": "D.-L.-Wallace",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wallace",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 407,
                                "start": 217
                            }
                        ],
                        "text": "So, how should such infinite models be implemented in finite computers? And how should we set up our Bayesian models so as to avoid getting silly answers? Infinite mixture models for categorical data are presented in Neal (1991), along with a Monte Carlo method for simulating inferences and predictions. Infinite Gaussian mixture models with a flat hierarchical structure are presented in Rasmussen (2000). Neal (2001) shows how to use Dirichlet diffusion trees to define models of hierarchical clusters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 217
                            }
                        ],
                        "text": "So, how should such infinite models be implemented in finite computers? And how should we set up our Bayesian models so as to avoid getting silly answers? Infinite mixture models for categorical data are presented in Neal (1991), along with a Monte Carlo method for simulating inferences and predictions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60749965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af0c05cad7d09a5d7bd296dd24f6172ca8e84cf7",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applied-Bayesian-and-classical-inference-:-the-case-Mosteller-Wallace",
            "title": {
                "fragments": [],
                "text": "Applied Bayesian and classical inference : the case of the Federalist papers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703768"
                        ],
                        "name": "A. Andrew",
                        "slug": "A.-Andrew",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Andrew",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Andrew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54143952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efa45eb0eddad36f7ff570a51eb5fd5047c5d0af",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Andrew",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805342"
                        ],
                        "name": "R. Blahut",
                        "slug": "R.-Blahut",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Blahut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blahut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53897375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d789b834bcb9734199b52fb589d9d4ad614a0b45",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-and-practice-of-information-theory-Blahut",
            "title": {
                "fragments": [],
                "text": "Principles and practice of information theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4956880"
                        ],
                        "name": "F. Ratliff",
                        "slug": "F.-Ratliff",
                        "structuredName": {
                            "firstName": "Floyd",
                            "lastName": "Ratliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1947963"
                        ],
                        "name": "L. Riggs",
                        "slug": "L.-Riggs",
                        "structuredName": {
                            "firstName": "Lorrin",
                            "lastName": "Riggs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Riggs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 57
                            }
                        ],
                        "text": "The typical size of a microsaccade is 5\u201310minutes of arc (Ratliff and Riggs, 1950)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37993804,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "f225bf94866a5a372188f909764a0ffdcc352a50",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Involuntary-motions-of-the-eye-during-monocular-Ratliff-Riggs",
            "title": {
                "fragments": [],
                "text": "Involuntary motions of the eye during monocular fixation."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology"
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39274396"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21321333"
                        ],
                        "name": "V. Mackay",
                        "slug": "V.-Mackay",
                        "structuredName": {
                            "firstName": "Valerie",
                            "lastName": "Mackay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 106
                            }
                        ],
                        "text": "I speculate that the McCollough effect, an extremely long-lasting association of colours with orientation (McCollough, 1965; MacKay and MacKay, 1974), is produced by the adaptation mechanism that tunes our chromaticaberration-deconvolution circuits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5920647,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "3aa3f26504c5aa4574338c8f4777786cd616dcf4",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-time-course-of-the-McCollough-effect-and-its-Mackay-Mackay",
            "title": {
                "fragments": [],
                "text": "The time course of the McCollough effect and its physiological implications."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143679541"
                        ],
                        "name": "David J. Ward",
                        "slug": "David-J.-Ward",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ward",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057742060"
                        ],
                        "name": "D. MacKay",
                        "slug": "D.-MacKay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "MacKay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. MacKay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 67
                            }
                        ],
                        "text": "words per minute, hands-free, using gaze direction to drive Dasher (Ward and MacKay, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 210117974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0d7aa750e67254ed3d3aec84d2ea313fe91c3ad",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-Hands-free-Writing-by-Gaze-Direction-Ward-MacKay",
            "title": {
                "fragments": [],
                "text": "Fast Hands-free Writing by Gaze Direction"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "isKey": false,
            "numCitedBy": 2170,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m \u226a n without any significant decrease in the accuracy of the solution."
            },
            "slug": "Using-the-Nystr\u00f6m-Method-to-Speed-Up-Kernel-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems) and the computational complexity of a predictor using this approximation is O(m2n)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143732395"
                        ],
                        "name": "C. Holmes",
                        "slug": "C.-Holmes",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Holmes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830755"
                        ],
                        "name": "D. Denison",
                        "slug": "D.-Denison",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Denison",
                            "middleNames": [
                                "G.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Denison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 74
                            }
                        ],
                        "text": "] Investigate the application of perfect sampling to linear regression in Holmes and Mallick (1998) or Holmes and Denison (2002) and try to generalize it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33779812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03c5379dbfedd6dd0ae0d2d0d38ce456b745302f",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The coupling from the past (CFTP) procedure is a protocol for finite-state Markov chain Monte Carlo (MCMC) methods whereby the algorithm itself can determine the necessary runtime to convergence. In this paper, we demonstrate how this protocol can be applied to the problem of signal reconstruction using Bayesian wavelet analysis where the dimensionality of the wavelet basis set is unknown, and the observations are distorted by Gaussian white noise of unknown variance. MCMC simulation is used to account for model uncertainty by drawing samples of wavelet bases for approximating integrals (or summations) on the model space that are either too complex or too computationally demanding to perform analytically. We extend the CFTP protocol by making use of the central limit theorem to show how the algorithm can also monitor its own approximation error induced by MCMC. In this way, we can assess the number of MCMC samples needed to approximate the integral to within a user specified tolerance level. Hence, the method automatically ensures convergence and determines the necessary number of iterations needed to meet the error criteria."
            },
            "slug": "Perfect-sampling-for-the-wavelet-reconstruction-of-Holmes-Denison",
            "title": {
                "fragments": [],
                "text": "Perfect sampling for the wavelet reconstruction of signals"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper extends the CFTP protocol by making use of the central limit theorem to show how the algorithm can also monitor its own approximation error induced by MCMC, and assess the number of MCMC samples needed to approximate the integral to within a user specified tolerance level."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115452860,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3fc40b83ad9066c7b547167c851dcefab2ec9ad2",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Double-Loop-Algorithm-to-Minimize-the-Bethe-and-Yuille",
            "title": {
                "fragments": [],
                "text": "A Double-Loop Algorithm to Minimize the Bethe and Kikuchi Free Energies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 109
                            }
                        ],
                        "text": "The general decoding problem (find the maximum likelihood s in the equation Gs+n = r) is in fact NP-complete (Berlekamp et al., 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the intractability of certain coding problems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Info. Theory"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ensemble learning for hidden Markov models. www.inference.phy.cam.ac.uk/mackay/abstracts/ ensemblePaper.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of Information Theory, Inference, and Learning Algorithms with Harry Potter"
            },
            "venue": {
                "fragments": [],
                "text": "Comparison of Information Theory, Inference, and Learning Algorithms with Harry Potter"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neurons with graded response properties have collective computational properties like those of two-state neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Natl. Acad. Sci. USA"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 19
                            }
                        ],
                        "text": "The Baldwin effect (Baldwin, 1896; Hinton and Nowlan, 1987) has been widely studied as a mechanism whereby learning guides evolution, and it could also act at the level of transcription and translation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How learning can guide"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Neuroscience Unit,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Edited by G"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 62
                            }
                        ],
                        "text": "This chapter\u2019s material is originally due to Polya (1954) and Cover (1965) and the exposition that follows is Yaser Abu-Mostafa\u2019s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometrical and statistical properties"
            },
            "venue": {
                "fragments": [],
                "text": "discussion). J. R. Statist. Soc. B"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Slice sampling \u2013 a binary implementation"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 135
                            }
                        ],
                        "text": "An excellent software package, BUGS, makes it easy to set up almost arbitrary probabilistic models and simulate them by Gibbs sampling (Thomas et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BUGS: A program to perform Bayesian inference using Gibbs sampling"
            },
            "venue": {
                "fragments": [],
                "text": "In Bayesian Statistics"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 88
                            }
                        ],
                        "text": "There is a growing literature on the practical design of low-density parity-check codes (Mao and Banihashemi, 2000; Mao and Banihashemi, 2001; ten Brink et al., 2002); they are now being adopted for applications from hard drives to satellite communications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A heuristic search for good LDPC codes at short block lengths"
            },
            "venue": {
                "fragments": [],
                "text": "In IEEE International Conf. on Communications"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 88
                            }
                        ],
                        "text": "There is a growing literature on the practical design of low-density parity-check codes (Mao and Banihashemi, 2000; Mao and Banihashemi, 2001; ten Brink et al., 2002); they are now being adopted for applications from hard drives to satellite communications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Design of good LDPC codes using girth distribution"
            },
            "venue": {
                "fragments": [],
                "text": "In IEEE International Symposium on Info. Theory, Italy,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A review paper on regression with complexity control using hierarchical Bayesian models is"
            },
            "venue": {
                "fragments": [],
                "text": "A longer review of Gaussian processes is in"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Defining priors for distributions using Dirichlet diffusion trees"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 0104,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative probabilistic decoding of low density parity check codes. Animations available on world wide web. www.inference.phy.cam.ac.uk/mackay/codes/gifs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "468 39 The Single Neuron as a"
            },
            "venue": {
                "fragments": [],
                "text": "You can buy this book for 30 pounds or $50 437 35 Random Inference Topics . . . . . . . . . . . . . . . . . . . 445 36 Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . 451 37 Bayesian Inference and Sampling Theory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 70
                            }
                        ],
                        "text": "For a more comprehensive view of trellises, the reader should consult Kschischang and Sorokine (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the trel"
            },
            "venue": {
                "fragments": [],
                "text": "Info. Theory"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Shokrollahi (2003) presents raptor codes, which are an extension of LT codes with linear-time encoding and decoding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Raptor codes. Technical report, Laboratoire d\u2019algorithmique, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland. Available from algo.epfl.ch"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik, 1995) have a lot in common. Both are kernel-based predictors, the kernel being another name for the covariance function. A Bayesian version of support vectors"
            },
            "venue": {
                "fragments": [],
                "text": "Gaussian processes and support vector learning machines"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 109
                            }
                        ],
                        "text": "bzip is a block-sorting file compressor, which makes use of a neat hack called the Burrows\u2013Wheeler transform (Burrows and Wheeler, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A block-sorting loss"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000; Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger et al., 2003; Opper and Winther, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reduced rank Gaussian process learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 269
                            }
                        ],
                        "text": "How did a high-information-content self-replicating system ever emerge in the first place? In the general area of the origins of life and other tricky questions about evolution, I highly recommend Maynard Smith and Sz\u00e1thmary (1995), Maynard Smith and Sz\u00e1thmary (1999), Kondrashov (1988), Maynard Smith (1988), Ridley (2000), Dyson (1985), Cairns-Smith (1985), and Hopfield (1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 406,
                                "start": 355
                            }
                        ],
                        "text": "One of the neglected features of our visual system is that the raw image falling on the retina is severely blurred: while most people can see with a resolution of about 1 arcminute (one sixtieth of a degree) under any daylight conditions, bright or dim, the image on our retina is blurred through a point spread function of width as large as 5 arcminutes (Wald and Griffin, 1947; Howarth and Bradley, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The longitudinal aberration of the human eye and its correction"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Res"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A problem with variational free energy minimization. www.inference.phy.cam.ac.uk/mackay/ abstracts/minima.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A contextsensitive generalization of ICA"
            },
            "venue": {
                "fragments": [],
                "text": "In International Conf. on Neural Information Processing,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995a) Free energy minimization algorithm for decoding and cryptanalysis"
            },
            "venue": {
                "fragments": [],
                "text": "Electronics Letters"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 829,
                                "start": 67
                            }
                        ],
                        "text": "Low-density parity-check codes codes were first studied in 1962 by Gallager, then were generally forgotten by the coding theory community. Tanner (1981) generalized Gallager\u2019s work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware. Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman, 1996; Sipser and Spielman, 1996). Low-precision decoding algorithms and fast encoding algorithms for Gallager codes are discussed in (Richardson and Urbanke, 2001a; Richardson and Urbanke, 2001b). MacKay and Davey (2000) showed that low-density parity-check codes can outperform Reed\u2013Solomon codes, even on the Reed\u2013Solomon codes\u2019 home turf: high rate and short blocklengths."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 67
                            }
                        ],
                        "text": "Low-density parity-check codes codes were first studied in 1962 by Gallager, then were generally forgotten by the coding theory community. Tanner (1981) generalized Gallager\u2019s work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Mechanics. Addison\u2013Wesley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deleterious mutations and the evo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 273
                            }
                        ],
                        "text": "These results quantify the well known argument for why species reproduce by sex with recombination, namely that recombination allows useful mutations to spread more rapidly through the species and allows deleterious mutations to be more rapidly cleared from the population (Maynard Smith, 1978; Felsenstein, 1985; Maynard Smith, 1988; Maynard Smith and Sz\u00e1thmary, 1995)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 774,
                                "start": 295
                            }
                        ],
                        "text": "These results quantify the well known argument for why species reproduce by sex with recombination, namely that recombination allows useful mutations to spread more rapidly through the species and allows deleterious mutations to be more rapidly cleared from the population (Maynard Smith, 1978; Felsenstein, 1985; Maynard Smith, 1988; Maynard Smith and Sz\u00e1thmary, 1995). A population that reproduces by recombination can acquire information from natural selection at a rate of order \u221a G times faster than a parthenogenetic population, and it can tolerate a mutation rate that is of order \u221a G times greater. For genomes of size G ' 108 coding nucleotides, this factor of \u221a G is substantial. This enormous advantage conferred by sex has been noted before by Kondrashov (1988), but this meme, which Kondrashov calls \u2018the deterministic mutation hypothesis\u2019, does not seem to have diffused throughout the evolutionary research community, as there are still numerous papers in which the prevalence of sex is viewed as a mystery to be explained by elaborate mechanisms."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recombination and sex: is Maynard Smith necessary"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 134
                            }
                        ],
                        "text": "One approach based on the ideas of Skilling (1993) makes approximations to C\u22121t and TraceC\u22121 using iterative methods with cost O(N 2) (Gibbs and MacKay, 1996; Gibbs, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient implementation of Gaussian processes for interpolation. www.inference.phy.cam.ac.uk/mackay/abstracts/ gpros.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 8
                            }
                        ],
                        "text": "(2001), Yedidia et al. (2000b), Yedidia et al. (2000a), Yedidia et al. (2002), and Forney (2001). A good reference for the fundamental theory of graphical models is Lauritzen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 8
                            }
                        ],
                        "text": "(2001), Yedidia et al. (2000b), Yedidia et al. (2000a), Yedidia et al. (2002), and Forney (2001). A good reference for the fundamental theory of graphical models is Lauritzen (1996). A readable introduction to Bayesian networks is given by Jensen (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2000a) Bethe free energy, Kikuchi approximations and belief propagation algorithms. Technical report, Mitsubishi"
            },
            "venue": {
                "fragments": [],
                "text": "MERL TR-2001-16"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "A bible of exact marginalization is Bretthorst\u2019s (1988) book on Bayesian spectrum analysis and parameter estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Spectrum Analysis and Param"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Characterization of belief propagation and its generalizations"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Mitsubishi Electric Research Laboratories. MERL TR-2001-15"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 184
                            }
                        ],
                        "text": "Conditional distributions that are not of standard form may still be sampled from by adaptive rejection sampling if the conditional distribution satisfies certain convexity properties (Gilks and Wild, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive rejection sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 406,
                                "start": 355
                            }
                        ],
                        "text": "One of the neglected features of our visual system is that the raw image falling on the retina is severely blurred: while most people can see with a resolution of about 1 arcminute (one sixtieth of a degree) under any daylight conditions, bright or dim, the image on our retina is blurred through a point spread function of width as large as 5 arcminutes (Wald and Griffin, 1947; Howarth and Bradley, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The change in refractive"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 71
                            }
                        ],
                        "text": "Treatises on Bayesian statistics from the statistics community include (Box and Tiao, 1973; O\u2019Hagan, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Inference in Statistical Analysis. Addison\u2013Wesley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data. Society for Industrial and Applied Mathematics"
            },
            "venue": {
                "fragments": [],
                "text": "CBMS-NSF Regional Conf. series in applied mathematics"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 406,
                                "start": 355
                            }
                        ],
                        "text": "One of the neglected features of our visual system is that the raw image falling on the retina is severely blurred: while most people can see with a resolution of about 1 arcminute (one sixtieth of a degree) under any daylight conditions, bright or dim, the image on our retina is blurred through a point spread function of width as large as 5 arcminutes (Wald and Griffin, 1947; Howarth and Bradley, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The change in refractive power of the eye in bright and dim light"
            },
            "venue": {
                "fragments": [],
                "text": "J. Opt. Soc. Am"
            },
            "year": 1947
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Repeat\u2013accumulate codes were studied by Divsalar et al. (1998) for theoretical purposes, as simple turbo-like codes that might be more amenable to analysis than messy turbo codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coding theorems for \u2018turbo-like"
            },
            "venue": {
                "fragments": [],
                "text": "codes. In Proc. 36th Allerton Conf. on Communication,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On-screen viewing permitted. Printing not permitted"
            },
            "venue": {
                "fragments": [],
                "text": "On-screen viewing permitted. Printing not permitted"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The best detection of pulses; Bell Laboratories memorandum"
            },
            "venue": {
                "fragments": [],
                "text": "The best detection of pulses; Bell Laboratories memorandum"
            },
            "year": 1944
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Games, Sex and Evolution. Harvester\u2013Wheatsheaf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 104
                            }
                        ],
                        "text": "An encyclopaedic paper on Gaussian processes giving many valid covariance functions has been written by Abrahamsen (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A review of Gaussian random fields and correlation functions"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 917, Norwegian Computing Center, Blindern,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Programming Pearls. Addison-Wesley, second edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "241 17 Communication over Constrained Noiseless Channels . . . 248 18"
            },
            "venue": {
                "fragments": [],
                "text": "Why have Sex? Information Acquisition and Evolution . . 269 IV Probabilities and Inference . . . . . . . . . . . . . . . . . . 281 20 An Example Inference Task"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Ordered Overrelaxation The overrelaxation method has been generalized by Neal (1995) whose ordered overrelaxation method is applicable to any system where Gibbs sampling is used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 79
                            }
                        ],
                        "text": "A variational free energy minimization approach to ICA-like models is given in (Miskin, 2001; Miskin and MacKay, 2000; Miskin and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ensemble learning for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "In ICA: Principles and Practice, ed"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Ordered Overrelaxation The overrelaxation method has been generalized by Neal (1995) whose ordered overrelaxation method is applicable to any system where Gibbs sampling is used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 79
                            }
                        ],
                        "text": "A variational free energy minimization approach to ICA-like models is given in (Miskin, 2001; Miskin and MacKay, 2000; Miskin and MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ensemble learning for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "In ICA: Principles and Practice, ed"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 42
                            }
                        ],
                        "text": "Actually in my elementary statistics book (Spiegel, 1988) I find Yates\u2019s correction: If you want to know about Yates\u2019s correction, read a sampling theory textbook."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistics. Schaum\u2019s outline series. New York: McGraw-Hill, 2 edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 137
                            }
                        ],
                        "text": "Kriging has been developed considerably in the last thirty years (see Cressie (1993) for a review) including several Bayesian treatments (Omre, 1987; Kitanidis, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parameter uncertainty in estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Math. Statistics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 71
                            }
                        ],
                        "text": "Treatises on Bayesian statistics from the statistics community include (Box and Tiao, 1973; O\u2019Hagan, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Inference, volume 2B of Kendall\u2019s"
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Theory of Statistics. Edward Arnold"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 92
                            }
                        ],
                        "text": "This calculation can also be carried out for the ensemble of low-density parity-check codes (Gallager, 1963; MacKay, 1999b; Litsyn and Shevelev, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Low Density Parity Check Codes. Number 21 in MIT Research monograph series. MIT Press. Available from www.inference.phy.cam.ac.uk/mackay/gallager/ papers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On curve fitting and optimal design for regression"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Society, B"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 71
                            }
                        ],
                        "text": "Further reading These observations about crosswords were first made by Shannon; I learned about them from Wolf and Siegel (1998). The topic is closely related to the capacity of two-dimensional constrained channels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 0
                            }
                        ],
                        "text": "Shannon, Bell Syst. Tech. J. 30, pp.50-64 (1950), but, inexplicably, the great man made numerical errors in it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The best detection of pulses"
            },
            "venue": {
                "fragments": [],
                "text": "In Collected Papers of Claude Shannon,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 18
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton, 1968) states that one should prefer models that can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 23
                            }
                        ],
                        "text": "446) and the reference (Kepler and Oprea, 2001), in which sampling theory estimates and confidence intervals for a mutation rate are constructed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 221
                            }
                        ],
                        "text": "One of these estimators (based on the mean number of mutated bacteria, averaging over several experiments) has appallingly large variance, yet sampling theorists continue to use it and base confidence intervals around it (Kepler and Oprea, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved inference of muta"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 103,
            "methodology": 68,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 249,
        "totalPages": 25
    },
    "page_url": "https://www.semanticscholar.org/paper/Information-Theory,-Inference,-and-Learning-Mackay/f7f15848cd0fbb3d08f351595da833b1627de9c3?sort=total-citations"
}