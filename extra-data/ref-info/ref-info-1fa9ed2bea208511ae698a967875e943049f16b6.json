{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 7
                            }
                        ],
                        "text": "\u2022 BERT (Devlin et al. (2018)) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 109
                            }
                        ],
                        "text": "Interpretability and diversity There is a growing field of study, sometimes referred as BERTology from BERT (Devlin et al., 2018), concerned with investigating the inner working of large-scale pretrained models and trying to build a science on top of these empirical results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 278
                            }
                        ],
                        "text": "\u2026models provided in Transformers are pretrained with a general purpose training objective, usually a variant of language modeling like standard (sometime called causal) language modeling as used for instance in Radford et al. (2019) or masked language modeling as introduced in Devlin et al. (2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026tokenizers and models) was inspired by the design of the tensor2tensor library (Vaswani et al., 2018) and the original code repository of Bert (Devlin et al., 2018) from Google Research while concept of providing easy caching for pretrained models steamed from features of the AllenNLP library\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 49
                            }
                        ],
                        "text": ", 2018) and the original code repository of Bert (Devlin et al., 2018) from Google Research while concept of providing easy caching for pretrained models steamed from features of the AllenNLP library (Gardner et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33754,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40695489"
                        ],
                        "name": "Joel Grus",
                        "slug": "Joel-Grus",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Grus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Grus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3385516"
                        ],
                        "name": "Oyvind Tafjord",
                        "slug": "Oyvind-Tafjord",
                        "structuredName": {
                            "firstName": "Oyvind",
                            "lastName": "Tafjord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oyvind Tafjord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697425"
                        ],
                        "name": "Pradeep Dasigi",
                        "slug": "Pradeep-Dasigi",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Dasigi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Dasigi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22243769"
                        ],
                        "name": "Nelson F. Liu",
                        "slug": "Nelson-F.-Liu",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Liu",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nelson F. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874222"
                        ],
                        "name": "Michael Schmitz",
                        "slug": "Michael-Schmitz",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Schmitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Schmitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 208
                            }
                        ],
                        "text": "\u2026et al., 2018) and the original code repository of Bert (Devlin et al., 2018) from Google Research while concept of providing easy caching for pretrained models steamed from features of the AllenNLP library (Gardner et al., 2018) open-sourced by the Allen Institute for Artificial Intelligence (AI2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 125
                            }
                        ],
                        "text": "At the time of writing, the authors have been mostly aware of FastBert9, FARM10, flair (Akbik et al., 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 158
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 137
                            }
                        ],
                        "text": ", 2018) from Google Research while concept of providing easy caching for pretrained models steamed from features of the AllenNLP library (Gardner et al., 2018) open-sourced by the Allen Institute for Artificial Intelligence (AI2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 159
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 86
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": ", 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3994096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b4cc549a1bc4bc112189da36c318193d05d806",
            "isKey": true,
            "numCitedBy": 893,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field."
            },
            "slug": "AllenNLP:-A-Deep-Semantic-Natural-Language-Platform-Gardner-Grus",
            "title": {
                "fragments": [],
                "text": "AllenNLP: A Deep Semantic Natural Language Processing Platform"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "AllenNLP is described, a library for applying deep learning methods to NLP research that addresses issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143808231"
                        ],
                        "name": "Nikita Kitaev",
                        "slug": "Nikita-Kitaev",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Kitaev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Kitaev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6639036"
                        ],
                        "name": "Anselm Levskaya",
                        "slug": "Anselm-Levskaya",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Levskaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anselm Levskaya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Reformer (Kitaev et al., 2020) Longformer (Beltagy et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 209315300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "isKey": false,
            "numCitedBy": 726,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
            },
            "slug": "Reformer:-The-Efficient-Transformer-Kitaev-Kaiser",
            "title": {
                "fragments": [],
                "text": "Reformer: The Efficient Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402716"
                        ],
                        "name": "Colin Raffel",
                        "slug": "Colin-Raffel",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Raffel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Raffel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625142"
                        ],
                        "name": "Adam Roberts",
                        "slug": "Adam-Roberts",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3844009"
                        ],
                        "name": "Katherine Lee",
                        "slug": "Katherine-Lee",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617804"
                        ],
                        "name": "Sharan Narang",
                        "slug": "Sharan-Narang",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Narang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Narang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380243217"
                        ],
                        "name": "Michael Matena",
                        "slug": "Michael-Matena",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Matena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Matena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389316"
                        ],
                        "name": "Yanqi Zhou",
                        "slug": "Yanqi-Zhou",
                        "structuredName": {
                            "firstName": "Yanqi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanqi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157338362"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2019) T5 (Raffel et al., 2019) MarianMT (J."
                    },
                    "intents": []
                }
            ],
            "corpusId": 204838007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
            "isKey": false,
            "numCitedBy": 3764,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
            },
            "slug": "Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380459402"
                        ],
                        "name": "Lysandre Debut",
                        "slug": "Lysandre-Debut",
                        "structuredName": {
                            "firstName": "Lysandre",
                            "lastName": "Debut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lysandre Debut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "The distillation of large models (e.g. DistilBERT (Sanh et al., 2019)) is one of the most promising directions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 11
                            }
                        ],
                        "text": "DistilBERT (Sanh et al., 2019)) is one of the most promising directions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 13
                            }
                        ],
                        "text": "\u2022 DistilBERT (Sanh et al. (2019)) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 203626972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "isKey": false,
            "numCitedBy": 2086,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
            },
            "slug": "DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut",
            "title": {
                "fragments": [],
                "text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and introduces a triple loss combining language modeling, distillation and cosine-distance losses."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362534"
                        ],
                        "name": "Zhenzhong Lan",
                        "slug": "Zhenzhong-Lan",
                        "structuredName": {
                            "firstName": "Zhenzhong",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenzhong Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46221498"
                        ],
                        "name": "Mingda Chen",
                        "slug": "Mingda-Chen",
                        "structuredName": {
                            "firstName": "Mingda",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingda Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7685850"
                        ],
                        "name": "Sebastian Goodman",
                        "slug": "Sebastian-Goodman",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48267618"
                        ],
                        "name": "Piyush Sharma",
                        "slug": "Piyush-Sharma",
                        "structuredName": {
                            "firstName": "Piyush",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piyush Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ALBERT (Lan et al., 2019) Electra (Clark et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 202888986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "isKey": false,
            "numCitedBy": 2706,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL."
            },
            "slug": "ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen",
            "title": {
                "fragments": [],
                "text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100984698"
                        ],
                        "name": "Yada Pruksachatkun",
                        "slug": "Yada-Pruksachatkun",
                        "structuredName": {
                            "firstName": "Yada",
                            "lastName": "Pruksachatkun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yada Pruksachatkun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10666396"
                        ],
                        "name": "Nikita Nangia",
                        "slug": "Nikita-Nangia",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Nangia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Nangia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "Transformers notably includes pre-processors and fine-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al. (2019)) and SQuAD1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 111
                            }
                        ],
                        "text": "Transformers includes several tools and scripts to evaluate models on GLUE (Wang et al. (2018)) and SuperGLUE (Wang et al. (2019))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 14
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " pretrained model is often evaluated using wide-range language understanding benchmarks. Transformers includes several tools and scripts to evaluate models on GLUE (Wang et al. (2018)) and SuperGLUE (Wang et al. (2019)). These two benchmarks gather a variety of datasets to evaluate natural language understanding systems. Details of the datasets can be found in the Appendix on page 7. 4 Figure 1: Write With Transfor"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 16
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "re the performances of several architectures on a common language understanding benchmark. Transformers notably includes pre-processors and \ufb01ne-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al. (2019)) and SQuAD1.1 (Rajpurkar et al., 2016). Pushing best practices forward Transformers seeks a balance between sticking to the original authors\u2019 code-base for reliability and providing clear and readabl"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143424870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9f6ada77448664b71128bb19df15765336974a6",
            "isKey": true,
            "numCitedBy": 823,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL."
            },
            "slug": "SuperGLUE:-A-Stickier-Benchmark-for-General-Purpose-Wang-Pruksachatkun",
            "title": {
                "fragments": [],
                "text": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new benchmark styled after GLUE is presented, a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 17
                            }
                        ],
                        "text": "\u2022 Transformer-XL (Dai et al. (2019)) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57759363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "isKey": false,
            "numCitedBy": 1771,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
            },
            "slug": "Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang",
            "title": {
                "fragments": [],
                "text": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061407009"
                        ],
                        "name": "Benjamin Hoover",
                        "slug": "Benjamin-Hoover",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Hoover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Hoover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879705"
                        ],
                        "name": "Hendrik Strobelt",
                        "slug": "Hendrik-Strobelt",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Strobelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hendrik Strobelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3159346"
                        ],
                        "name": "Sebastian Gehrmann",
                        "slug": "Sebastian-Gehrmann",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Gehrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Gehrmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, model pages can link to exBERT (Hoover et al., 2019), a Transformer visualization library."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204402756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "327d7e55d64cb34d55bd3a3fe58233c238a312cd",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques. To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process. exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets. By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models."
            },
            "slug": "exBERT:-A-Visual-Analysis-Tool-to-Explore-Learned-Hoover-Strobelt",
            "title": {
                "fragments": [],
                "text": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "ExBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets, and can quickly replicate findings from literature and extend them to previously not analyzed models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884561"
                        ],
                        "name": "Sebastian Ruder",
                        "slug": "Sebastian-Ruder",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ruder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ruder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2705113"
                        ],
                        "name": "Swabha Swayamdipta",
                        "slug": "Swabha-Swayamdipta",
                        "structuredName": {
                            "firstName": "Swabha",
                            "lastName": "Swayamdipta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Swabha Swayamdipta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145662441"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "In the past 18 months, advances on many Natural Language Processing (NLP) tasks have been dominated by deep learning models and, more specifically, the use of Transfer Learning methods (Ruder et al., 2019) in which a deep neural network language model is pretrained on a web-scale unlabelled text dataset with a general-purpose training objective before being fine-tuned on various downstream tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Natural Language Processing (NLP) tasks have been dominated by deep learning models and, more specifically, the use of Transfer Learning methods (Ruder et al., 2019) in which a deep neural network language model is pretrained on a web-scale unlabelled text dataset with a general-purpose\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 186206211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 656,
            "paperAbstract": {
                "fragments": [],
                "text": "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks."
            },
            "slug": "Transfer-Learning-in-Natural-Language-Processing-Ruder-Peters",
            "title": {
                "fragments": [],
                "text": "Transfer Learning in Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403712"
                        ],
                        "name": "A. Akbik",
                        "slug": "A.-Akbik",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Akbik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akbik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077245166"
                        ],
                        "name": "Tanja Bergmann",
                        "slug": "Tanja-Bergmann",
                        "structuredName": {
                            "firstName": "Tanja",
                            "lastName": "Bergmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanja Bergmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2725328"
                        ],
                        "name": "Duncan A. J. Blythe",
                        "slug": "Duncan-A.-J.-Blythe",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Blythe",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duncan A. J. Blythe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4565995"
                        ],
                        "name": "Kashif Rasul",
                        "slug": "Kashif-Rasul",
                        "structuredName": {
                            "firstName": "Kashif",
                            "lastName": "Rasul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kashif Rasul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134757625"
                        ],
                        "name": "Stefan Schweter",
                        "slug": "Stefan-Schweter",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schweter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Schweter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742129"
                        ],
                        "name": "Roland Vollgraf",
                        "slug": "Roland-Vollgraf",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Vollgraf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roland Vollgraf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2018), flair (Akbik et al., 2019), and Stanza (Qi et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "At the time of writing, the authors have been mostly aware of FastBert9, FARM10, flair (Akbik et al., 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 157
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 181704107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a7bbc46795929f0eace82b64c44c92a48682fb5",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to \u201cmix and match\u201d various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a \u201cmodel zoo\u201d of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair ."
            },
            "slug": "FLAIR:-An-Easy-to-Use-Framework-for-NLP-Akbik-Bergmann",
            "title": {
                "fragments": [],
                "text": "FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The core idea of the FLAIR framework is to present a simple, unified interface for conceptually very different types of word and document embeddings, which effectively hides all embedding-specific engineering complexity and allows researchers to \u201cmix and match\u201d variousembeddings with little effort."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 78
                            }
                        ],
                        "text": "Transformers notably includes pre-processors and fine-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 79
                            }
                        ],
                        "text": "Transformers notably includes pre-processors and fine-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al. (2019)) and SQuAD1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 76
                            }
                        ],
                        "text": "Transformers includes several tools and scripts to evaluate models on GLUE (Wang et al. (2018)) and SuperGLUE (Wang et al. (2019))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 14
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5034059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b8da28d006415866bf48f9a6e06b5242129195",
            "isKey": true,
            "numCitedBy": 2635,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
            },
            "slug": "GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh",
            "title": {
                "fragments": [],
                "text": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 99
                            }
                        ],
                        "text": ", 2017) has repeatedly advanced the state-of-the-art on NLP tasks ranging from text classification (Yang et al., 2019), language understanding (Liu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 228
                            }
                        ],
                        "text": "\u2026et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al., 2017) has repeatedly advanced the state-of-the-art on NLP tasks ranging from text classification (Yang et al., 2019), language understanding (Liu et al.,\nar X\niv :1\n91 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 8
                            }
                        ],
                        "text": "\u2022 XLNet (Yang et al. (2019)) builds upon Transformer-XL and proposes an autoregressive pretraining scheme combining BERT\u2019s bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195069387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "isKey": true,
            "numCitedBy": 4227,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
            },
            "slug": "XLNet:-Generalized-Autoregressive-Pretraining-for-Yang-Dai",
            "title": {
                "fragments": [],
                "text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autore progressive formulation."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387725"
                        ],
                        "name": "Jeff Wu",
                        "slug": "Jeff-Wu",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48422824"
                        ],
                        "name": "Rewon Child",
                        "slug": "Rewon-Child",
                        "structuredName": {
                            "firstName": "Rewon",
                            "lastName": "Child",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rewon Child"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150970919"
                        ],
                        "name": "D. Luan",
                        "slug": "D.-Luan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2698777"
                        ],
                        "name": "Dario Amodei",
                        "slug": "Dario-Amodei",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Amodei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Amodei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": ", 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 126
                            }
                        ],
                        "text": "Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models (Radford et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "GPT2 showcased\n8https://github.com/huggingface/swift-coreml-transformers 9https://github.com/kaushaltrivedi/fast-bert\n10https://github.com/deepset-ai/FARM 11https://github.com/facebookresearch/pytext\nzero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "\u2022 GPT (Radford et al. (2018)) and GPT2 (Radford et al. (2019)) are two large autoregressive language models pretrained with language modeling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "Built by the authors on top of Transformers, Write with Transformer5 is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 211
                            }
                        ],
                        "text": "\u2026models provided in Transformers are pretrained with a general purpose training objective, usually a variant of language modeling like standard (sometime called causal) language modeling as used for instance in Radford et al. (2019) or masked language modeling as introduced in Devlin et al. (2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 160025533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "isKey": true,
            "numCitedBy": 6284,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "slug": "Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu",
            "title": {
                "fragments": [],
                "text": "Language Models are Unsupervised Multitask Learners"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144951032"
                        ],
                        "name": "Sergey Golovanov",
                        "slug": "Sergey-Golovanov",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Golovanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Golovanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056786460"
                        ],
                        "name": "R. Kurbanov",
                        "slug": "R.-Kurbanov",
                        "structuredName": {
                            "firstName": "Rauf",
                            "lastName": "Kurbanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kurbanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742235"
                        ],
                        "name": "S. Nikolenko",
                        "slug": "S.-Nikolenko",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Nikolenko",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nikolenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150179297"
                        ],
                        "name": "Kyryl Truskovskyi",
                        "slug": "Kyryl-Truskovskyi",
                        "structuredName": {
                            "firstName": "Kyryl",
                            "lastName": "Truskovskyi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyryl Truskovskyi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30779443"
                        ],
                        "name": "Alexander Tselousov",
                        "slug": "Alexander-Tselousov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Tselousov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Tselousov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "-second Annual Conference on Neural Information Processing Systems (NIPS 2018). The approach used to reach 5https://transformer.huggingface.co 5 these performances is described in Wolf et al. (2019); Golovanov et al. (2019) and the code and pretrained models, based on the Transformers library, are available online6. Using in production To facilitate the transition from research to production, all the models in the libra"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 82
                            }
                        ],
                        "text": "The approach used to reach these performances is described in Wolf et al. (2019); Golovanov et al. (2019) and the code and pretrained models, based on the Transformers library, are available online6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 196184953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "791c3c30f2af10ac06f4fbc5b1e8960064aacbc7",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results."
            },
            "slug": "Large-Scale-Transfer-Learning-for-Natural-Language-Golovanov-Kurbanov",
            "title": {
                "fragments": [],
                "text": "Large-Scale Transfer Learning for Natural Language Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work focuses in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": ", 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al., 2017) has repeatedly advanced the state-of-the-art on NLP tasks ranging from text classification (Yang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Howard and Ruder, 2018; Peters et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al., 2017) has repeatedly advanced the state-of-the-art on NLP tasks ranging from text classification (Yang et al., 2019), language\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": false,
            "numCitedBy": 35157,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46181066"
                        ],
                        "name": "Iz Beltagy",
                        "slug": "Iz-Beltagy",
                        "structuredName": {
                            "firstName": "Iz",
                            "lastName": "Beltagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iz Beltagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527954"
                        ],
                        "name": "Arman Cohan",
                        "slug": "Arman-Cohan",
                        "structuredName": {
                            "firstName": "Arman",
                            "lastName": "Cohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arman Cohan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2020) Longformer (Beltagy et al., 2020)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 215737171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b6394ad5654f5cd0fba763768ba4e523f7bbca",
            "isKey": false,
            "numCitedBy": 852,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset."
            },
            "slug": "Longformer:-The-Long-Document-Transformer-Beltagy-Peters",
            "title": {
                "fragments": [],
                "text": "Longformer: The Long-Document Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911755"
                        ],
                        "name": "M. Shoeybi",
                        "slug": "M.-Shoeybi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Shoeybi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shoeybi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66870756"
                        ],
                        "name": "M. Patwary",
                        "slug": "M.-Patwary",
                        "structuredName": {
                            "firstName": "Mostofa",
                            "lastName": "Patwary",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Patwary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41158993"
                        ],
                        "name": "Raul Puri",
                        "slug": "Raul-Puri",
                        "structuredName": {
                            "firstName": "Raul",
                            "lastName": "Puri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raul Puri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081566"
                        ],
                        "name": "P. LeGresley",
                        "slug": "P.-LeGresley",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "LeGresley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. LeGresley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48991386"
                        ],
                        "name": "J. Casper",
                        "slug": "J.-Casper",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Casper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Casper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "\u2026on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes (Liu et al., 2019; Shoeybi et al., 2019) has made it both difficult and costly for researchers and practitioners with limited computational resources to benefit from\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202660670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "isKey": false,
            "numCitedBy": 498,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%)."
            },
            "slug": "Megatron-LM:-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary",
            "title": {
                "fragments": [],
                "text": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters and shows that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50531624"
                        ],
                        "name": "Peng Qi",
                        "slug": "Peng-Qi",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49889487"
                        ],
                        "name": "Yuhao Zhang",
                        "slug": "Yuhao-Zhang",
                        "structuredName": {
                            "firstName": "Yuhao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49889860"
                        ],
                        "name": "Yuhui Zhang",
                        "slug": "Yuhui-Zhang",
                        "structuredName": {
                            "firstName": "Yuhui",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhui Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40756403"
                        ],
                        "name": "Jason Bolton",
                        "slug": "Jason-Bolton",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Bolton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Bolton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 212725611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42e863f93203d37a2518da381beaf06e4c70fb3d",
            "isKey": false,
            "numCitedBy": 552,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/."
            },
            "slug": "Stanza:-A-Python-Natural-Language-Processing-for-Qi-Zhang",
            "title": {
                "fragments": [],
                "text": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This work introduces Stanza, an open-source Python natural language processing toolkit supporting 66 human languages that features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2320509"
                        ],
                        "name": "Marjan Ghazvininejad",
                        "slug": "Marjan-Ghazvininejad",
                        "structuredName": {
                            "firstName": "Marjan",
                            "lastName": "Ghazvininejad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marjan Ghazvininejad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113947684"
                        ],
                        "name": "Abdelrahman Mohamed",
                        "slug": "Abdelrahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdelrahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdelrahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 204960716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "isKey": false,
            "numCitedBy": 2422,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
            },
            "slug": "BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu",
            "title": {
                "fragments": [],
                "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models, which matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144958935"
                        ],
                        "name": "Karthik Narasimhan",
                        "slug": "Karthik-Narasimhan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Narasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Narasimhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "GPT2 showcased\n8https://github.com/huggingface/swift-coreml-transformers 9https://github.com/kaushaltrivedi/fast-bert\n10https://github.com/deepset-ai/FARM 11https://github.com/facebookresearch/pytext\nzero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "Built by the authors on top of Transformers, Write with Transformer5 is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 6
                            }
                        ],
                        "text": "\u2022 GPT (Radford et al. (2018)) and GPT2 (Radford et al. (2019)) are two large autoregressive language models pretrained with language modeling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49313245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "isKey": true,
            "numCitedBy": 3533,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI)."
            },
            "slug": "Improving-Language-Understanding-by-Generative-Radford-Narasimhan",
            "title": {
                "fragments": [],
                "text": "Improving Language Understanding by Generative Pre-Training"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, improving upon the state of the art in 9 out of the 12 tasks studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46181066"
                        ],
                        "name": "Iz Beltagy",
                        "slug": "Iz-Beltagy",
                        "structuredName": {
                            "firstName": "Iz",
                            "lastName": "Beltagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iz Beltagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46258841"
                        ],
                        "name": "Kyle Lo",
                        "slug": "Kyle-Lo",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Lo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Lo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527954"
                        ],
                        "name": "Arman Cohan",
                        "slug": "Arman-Cohan",
                        "structuredName": {
                            "firstName": "Arman",
                            "lastName": "Cohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arman Cohan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Left) Example of a model page and model card for SciBERT (Beltagy et al., 2019), a pretrained model targeting extraction from scientific literature submitted by a community contributor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Case 1: Model Architects AllenAI, a major NLP research lab, developed a new pretrained model for improved extraction from biomedical texts called SciBERT (Beltagy et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202558505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e98fe2163640da8ab9695b9ee9c433bb30f5353",
            "isKey": false,
            "numCitedBy": 1019,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."
            },
            "slug": "SciBERT:-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo",
            "title": {
                "fragments": [],
                "text": "SciBERT: A Pretrained Language Model for Scientific Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks and demonstrates statistically significant improvements over BERT."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91955511"
                        ],
                        "name": "Hang Le",
                        "slug": "Hang-Le",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27421936"
                        ],
                        "name": "Lo\u00efc Vial",
                        "slug": "Lo\u00efc-Vial",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Vial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lo\u00efc Vial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35342746"
                        ],
                        "name": "Jibril Frej",
                        "slug": "Jibril-Frej",
                        "structuredName": {
                            "firstName": "Jibril",
                            "lastName": "Frej",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jibril Frej"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "139097094"
                        ],
                        "name": "Vincent Segonne",
                        "slug": "Vincent-Segonne",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Segonne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Segonne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443469"
                        ],
                        "name": "Maximin Coavoux",
                        "slug": "Maximin-Coavoux",
                        "structuredName": {
                            "firstName": "Maximin",
                            "lastName": "Coavoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maximin Coavoux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752710"
                        ],
                        "name": "B. Lecouteux",
                        "slug": "B.-Lecouteux",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lecouteux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lecouteux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064411934"
                        ],
                        "name": "Benoit Crabb'e",
                        "slug": "Benoit-Crabb'e",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Crabb'e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benoit Crabb'e"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143823463"
                        ],
                        "name": "L. Besacier",
                        "slug": "L.-Besacier",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Besacier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Besacier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2504683"
                        ],
                        "name": "D. Schwab",
                        "slug": "D.-Schwab",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Schwab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Schwab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To load FlauBERT (Le et al., 2020), a BERT model pretrained on a French training corpus, the command is:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 209202658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3b8528104baeb0b979f94810064eea701099703",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP."
            },
            "slug": "FlauBERT:-Unsupervised-Language-Model-Pre-training-Le-Vial",
            "title": {
                "fragments": [],
                "text": "FlauBERT: Unsupervised Language Model Pre-training for French"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces and shares FlauBERT, a model learned on a very large and heterogeneous French corpus and applies it to diverse NLP tasks and shows that most of the time they outperform other pre-training approaches."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093348519"
                        ],
                        "name": "Jeremy Howard",
                        "slug": "Jeremy-Howard",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Howard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884561"
                        ],
                        "name": "Sebastian Ruder",
                        "slug": "Sebastian-Ruder",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ruder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ruder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 84
                            }
                        ],
                        "text": "Following noticeable improvements using Long Short-Term Memory (LSTM) architectures (Howard and Ruder, 2018; Peters et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "Following noticeable improvements using Long Short-Term Memory (LSTM) architectures (Howard and Ruder, 2018; Peters et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al., 2017) has repeatedly advanced the state-of-the-art on\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40100965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "isKey": true,
            "numCitedBy": 2251,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code."
            },
            "slug": "Universal-Language-Model-Fine-tuning-for-Text-Howard-Ruder",
            "title": {
                "fragments": [],
                "text": "Universal Language Model Fine-tuning for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduces techniques that are key for fine- Tuning a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830914"
                        ],
                        "name": "Guillaume Lample",
                        "slug": "Guillaume-Lample",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Lample",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Lample"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480903"
                        ],
                        "name": "A. Conneau",
                        "slug": "A.-Conneau",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Conneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conneau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 35
                            }
                        ],
                        "text": ", 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 6
                            }
                        ],
                        "text": "\u2022 XLM (Lample and Conneau (2019)) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 61
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58981712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
            "isKey": true,
            "numCitedBy": 1510,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u201916 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available."
            },
            "slug": "Cross-lingual-Language-Model-Pretraining-Lample-Conneau",
            "title": {
                "fragments": [],
                "text": "Cross-lingual Language Model Pretraining"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingsual language model objective."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100984698"
                        ],
                        "name": "Yada Pruksachatkun",
                        "slug": "Yada-Pruksachatkun",
                        "structuredName": {
                            "firstName": "Yada",
                            "lastName": "Pruksachatkun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yada Pruksachatkun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10734735"
                        ],
                        "name": "Philip Yeres",
                        "slug": "Philip-Yeres",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Yeres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Yeres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48447436"
                        ],
                        "name": "Haokun Liu",
                        "slug": "Haokun-Liu",
                        "structuredName": {
                            "firstName": "Haokun",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haokun Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80842917"
                        ],
                        "name": "Jason Phang",
                        "slug": "Jason-Phang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Phang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Phang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41022736"
                        ],
                        "name": "Phu Mon Htut",
                        "slug": "Phu-Mon-Htut",
                        "structuredName": {
                            "firstName": "Phu",
                            "lastName": "Htut",
                            "middleNames": [
                                "Mon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phu Mon Htut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6117577"
                        ],
                        "name": "Ian Tenney",
                        "slug": "Ian-Tenney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Tenney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Tenney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 211990050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00b30ed463625da04166eb78ca617539b41a9846",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT."
            },
            "slug": "jiant:-A-Software-Toolkit-for-Research-on-Text-Pruksachatkun-Yeres",
            "title": {
                "fragments": [],
                "text": "jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Jiant is introduced, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks and it is demonstrated that jiant reproduces published performance on a variety of tasks and models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40899333"
                        ],
                        "name": "Clement Delangue",
                        "slug": "Clement-Delangue",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Delangue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement Delangue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 62
                            }
                        ],
                        "text": "The approach used to reach these performances is described in Wolf et al. (2019); Golovanov et al. (2019) and the code and pretrained models, based on the Transformers library, are available online6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 held at the Thirty-second Annual Conference on Neural Information Processing Systems (NIPS 2018). The approach used to reach 5https://transformer.huggingface.co 5 these performances is described in Wolf et al. (2019); Golovanov et al. (2019) and the code and pretrained models, based on the Transformers library, are available online6. Using in production To facilitate the transition from research to production, al"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59222757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e81b50f68265b84d55d03dab3c296b9fd4516857",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 % absolute improvement), 80.7 (46 % absolute improvement) and 19.5 (20 % absolute improvement)."
            },
            "slug": "TransferTransfo:-A-Transfer-Learning-Approach-for-Wolf-Sanh",
            "title": {
                "fragments": [],
                "text": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo is introduced which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model which shows strong improvements over the current state-of-the-art end-to-end conversational models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144397625"
                        ],
                        "name": "Paul Michel",
                        "slug": "Paul-Michel",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Michel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Michel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700325"
                        ],
                        "name": "Graham Neubig",
                        "slug": "Graham-Neubig",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Neubig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham Neubig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026giving easy access to the inner representations of these models, notably the hidden states, the attention weights or heads importance as defined in Michel et al. (2019) and (ii) providing different models in a unified API to prevent overfitting to a specific architecture (and set of pretrained\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "Some examples include Tenney et al. (2019), Michel et al. (2019), Clark et al. (2019b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 166227946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention \"head\" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention."
            },
            "slug": "Are-Sixteen-Heads-Really-Better-than-One-Michel-Levy",
            "title": {
                "fragments": [],
                "text": "Are Sixteen Heads Really Better than One?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is made the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6117577"
                        ],
                        "name": "Ian Tenney",
                        "slug": "Ian-Tenney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Tenney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Tenney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790066"
                        ],
                        "name": "Dipanjan Das",
                        "slug": "Dipanjan-Das",
                        "structuredName": {
                            "firstName": "Dipanjan",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipanjan Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949185"
                        ],
                        "name": "Ellie Pavlick",
                        "slug": "Ellie-Pavlick",
                        "structuredName": {
                            "firstName": "Ellie",
                            "lastName": "Pavlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellie Pavlick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They used the Transformers API as a generic front-end and performed fine-tuning on a variety of different models, leading to research on the structure of BERT (Tenney et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 22
                            }
                        ],
                        "text": "Some examples include Tenney et al. (2019), Michel et al. (2019), Clark et al. (2019b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 155092004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
            "isKey": false,
            "numCitedBy": 678,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."
            },
            "slug": "BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das",
            "title": {
                "fragments": [],
                "text": "BERT Rediscovers the Classical NLP Pipeline"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work finds that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691021"
                        ],
                        "name": "Antoine Bosselut",
                        "slug": "Antoine-Bosselut",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bosselut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bosselut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516777"
                        ],
                        "name": "Hannah Rashkin",
                        "slug": "Hannah-Rashkin",
                        "structuredName": {
                            "firstName": "Hannah",
                            "lastName": "Rashkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannah Rashkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729164"
                        ],
                        "name": "Maarten Sap",
                        "slug": "Maarten-Sap",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Sap",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maarten Sap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8805254"
                        ],
                        "name": "Chaitanya Malaviya",
                        "slug": "Chaitanya-Malaviya",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Malaviya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya Malaviya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709797"
                        ],
                        "name": "Asli Celikyilmaz",
                        "slug": "Asli-Celikyilmaz",
                        "structuredName": {
                            "firstName": "Asli",
                            "lastName": "Celikyilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asli Celikyilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 224
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": ", 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 189762527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods."
            },
            "slug": "COMET:-Commonsense-Transformers-for-Automatic-Graph-Bosselut-Rashkin",
            "title": {
                "fragments": [],
                "text": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs, and suggests that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123052390"
                        ],
                        "name": "Michael Collins",
                        "slug": "Michael-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "Some examples include Tenney et al. (2019), Michel et al. (2019), Clark et al. (2019b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 165163607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9770fff7379a7ab9006b48939462354dda9a2053",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study yes/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work."
            },
            "slug": "BoolQ:-Exploring-the-Surprising-Difficulty-of-Clark-Lee",
            "title": {
                "fragments": [],
                "text": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511414"
                        ],
                        "name": "Myle Ott",
                        "slug": "Myle-Ott",
                        "structuredName": {
                            "firstName": "Myle",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myle Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048577"
                        ],
                        "name": "Jingfei Du",
                        "slug": "Jingfei-Du",
                        "structuredName": {
                            "firstName": "Jingfei",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingfei Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 145
                            }
                        ],
                        "text": "\u2026improvements on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes (Liu et al., 2019; Shoeybi et al., 2019) has made it both difficult and costly for researchers and practitioners with limited computational resources\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 10
                            }
                        ],
                        "text": "\u2022 RoBERTa (Liu et al. (2019)) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 184
                            }
                        ],
                        "text": "While this approach has shown impressive improvements on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes (Liu et al., 2019; Shoeybi et al., 2019) has made it both difficult and costly for researchers and practitioners with limited computational resources to benefit from these models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 23
                            }
                        ],
                        "text": "For instance, RoBERTa (Liu et al., 2019) was trained on 160 GB of text using 1024 32GB V100."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 160
                            }
                        ],
                        "text": "These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 198953378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "isKey": true,
            "numCitedBy": 7267,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            "slug": "RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott",
            "title": {
                "fragments": [],
                "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, and the best model achieves state-of-the-art results on GLUE, RACE and SQuAD."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144358401"
                        ],
                        "name": "Kevin Clark",
                        "slug": "Kevin-Clark",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030219"
                        ],
                        "name": "Urvashi Khandelwal",
                        "slug": "Urvashi-Khandelwal",
                        "structuredName": {
                            "firstName": "Urvashi",
                            "lastName": "Khandelwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urvashi Khandelwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ith investigating the inner working of large-scale pretrained models and trying to build a science on top of these empirical results. Some examples include Tenney et al. (2019), Michel et al. (2019), Clark et al. (2019b). Transformers aims at facilitating and increasing the scope of these studies by (i) giving easy access to the inner representations of these models, notably the hidden states, the attention weights"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "Some examples include Tenney et al. (2019), Michel et al. (2019), Clark et al. (2019b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 184486746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95a251513853c6032bdecebd4b74e15795662986",
            "isKey": false,
            "numCitedBy": 755,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT\u2019s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT\u2019s attention."
            },
            "slug": "What-Does-BERT-Look-at-An-Analysis-of-BERT\u2019s-Clark-Khandelwal",
            "title": {
                "fragments": [],
                "text": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that certain attention heads correspond well to linguistic notions of syntax and coreference, and an attention-based probing classifier is proposed and used to demonstrate that substantial syntactic information is captured in BERT\u2019s attention."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24590005"
                        ],
                        "name": "Alex Perelygin",
                        "slug": "Alex-Perelygin",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Perelygin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Perelygin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110402830"
                        ],
                        "name": "Jean Wu",
                        "slug": "Jean-Wu",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964541"
                        ],
                        "name": "Jason Chuang",
                        "slug": "Jason-Chuang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922861"
                        ],
                        "name": "Christopher Potts",
                        "slug": "Christopher-Potts",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Potts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Potts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 990233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "isKey": false,
            "numCitedBy": 5366,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases."
            },
            "slug": "Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin",
            "title": {
                "fragments": [],
                "text": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Sentiment Treebank that includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality, and introduces the Recursive Neural Tensor Network."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 84
                            }
                        ],
                        "text": "Following noticeable improvements using Long Short-Term Memory (LSTM) architectures (Howard and Ruder, 2018; Peters et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 109
                            }
                        ],
                        "text": "Following noticeable improvements using Long Short-Term Memory (LSTM) architectures (Howard and Ruder, 2018; Peters et al., 2018), a series of works combining Transfer Learning methods with large-scale Transformer architectures (Vaswani et al., 2017) has repeatedly advanced the state-of-the-art on\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3626819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "isKey": true,
            "numCitedBy": 7987,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
            },
            "slug": "Deep-Contextualized-Word-Representations-Peters-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep Contextualized Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403712"
                        ],
                        "name": "A. Akbik",
                        "slug": "A.-Akbik",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Akbik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akbik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077245166"
                        ],
                        "name": "Tanja Bergmann",
                        "slug": "Tanja-Bergmann",
                        "structuredName": {
                            "firstName": "Tanja",
                            "lastName": "Bergmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanja Bergmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742129"
                        ],
                        "name": "Roland Vollgraf",
                        "slug": "Roland-Vollgraf",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Vollgraf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roland Vollgraf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "At the time of writing, the authors have been mostly aware of FastBert9, FARM10, flair (Akbik et al., 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 157
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174799702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edfe9dd16316618e694cd087d0d418dac91eb48c",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a \u201dglobal\u201d word representation from all contextualized instances. We evaluate these \u201dpooled contextualized embeddings\u201d on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction."
            },
            "slug": "Pooled-Contextualized-Embeddings-for-Named-Entity-Akbik-Bergmann",
            "title": {
                "fragments": [],
                "text": "Pooled Contextualized Embeddings for Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a method in which it dynamically aggregate contextualized embeddings of each unique string that the authors encounter and uses a pooling operation to distill a \u201dglobal\u201d word representation from all contextualized instances."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 38
                            }
                        ],
                        "text": ", 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 177
                            }
                        ],
                        "text": "C L\n] 1\n2019; Wang et al., 2018, 2019), machine translation (Lample and Conneau, 2019), and zero-short language generation (Radford et al., 2019) up to co-reference resolution (Joshi et al., 2019) and commonsense inference (Bosselut et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 198229624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81f5810fbbab9b7203b9556f4ce3c741875407bc",
            "isKey": false,
            "numCitedBy": 879,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1"
            },
            "slug": "SpanBERT:-Improving-Pre-training-by-Representing-Joshi-Chen",
            "title": {
                "fragments": [],
                "text": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The approach extends BERT by masking contiguous random spans, rather than random tokens, and training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403712"
                        ],
                        "name": "A. Akbik",
                        "slug": "A.-Akbik",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Akbik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akbik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2725328"
                        ],
                        "name": "Duncan A. J. Blythe",
                        "slug": "Duncan-A.-J.-Blythe",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Blythe",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duncan A. J. Blythe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742129"
                        ],
                        "name": "Roland Vollgraf",
                        "slug": "Roland-Vollgraf",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Vollgraf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roland Vollgraf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "At the time of writing, the authors have been mostly aware of FastBert9, FARM10, flair (Akbik et al., 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "een created on top of Transformers or have integrated Transformers as a package dependency or through wrappers. At the time of writing, the authors have been mostly aware of FastBert9, FARM10, flair (Akbik et al., 2018, 2019), AllenNLP (Gardner et al., 2018) and PyText11 but there are likely more interesting developments to be found, from research and internal projects to production packages. 5 Architectures Here i"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 191
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 157
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ection of which stands the present library. The \ufb01rst direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16. These libraries precede Transformers and target somewhat different use-cases, for instance those with a particular focus on research for AllenNLP or a strong attention to producti"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ibrary1. Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018). Transformers is an ongoing effort maintained by the team of engineers and research scientists at Hugging Face2, with support from a vibrant community of more than 120 external contributors. We are c"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52010710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "421fc2556836a6b441de806d7b393a35b6eaea58",
            "isKey": true,
            "numCitedBy": 862,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair"
            },
            "slug": "Contextual-String-Embeddings-for-Sequence-Labeling-Akbik-Blythe",
            "title": {
                "fragments": [],
                "text": "Contextual String Embeddings for Sequence Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes to leverage the internal states of a trained character language model to produce a novel type of word embedding which they refer to as contextual string embeddings, which are fundamentally model words as sequences of characters and are contextualized by their surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775536"
                        ],
                        "name": "Bryan McCann",
                        "slug": "Bryan-McCann",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "McCann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan McCann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40518045"
                        ],
                        "name": "James Bradbury",
                        "slug": "James-Bradbury",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bradbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Model pretraining (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018) allows models to be trained on generic corpora and subsequently be easily adapted to specific tasks with strong performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9447219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "isKey": false,
            "numCitedBy": 710,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art."
            },
            "slug": "Learned-in-Translation:-Contextualized-Word-Vectors-McCann-Bradbury",
            "title": {
                "fragments": [],
                "text": "Learned in Translation: Contextualized Word Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Adding context vectors to a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation to contextualize word vectors improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717641"
                        ],
                        "name": "Mohammad Taher Pilehvar",
                        "slug": "Mohammad-Taher-Pilehvar",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Pilehvar",
                            "middleNames": [
                                "Taher"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammad Taher Pilehvar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387447871"
                        ],
                        "name": "Jos\u00e9 Camacho-Collados",
                        "slug": "Jos\u00e9-Camacho-Collados",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Camacho-Collados",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 Camacho-Collados"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 102353817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a925f818f787e142c5f6bcb7bbd7ede2deb34860",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "By design, word embeddings are unable to model the dynamic nature of words\u2019 semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/."
            },
            "slug": "WiC:-the-Word-in-Context-Dataset-for-Evaluating-Pilehvar-Camacho-Collados",
            "title": {
                "fragments": [],
                "text": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations, and shows that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81840293"
                        ],
                        "name": "Adina Williams",
                        "slug": "Adina-Williams",
                        "structuredName": {
                            "firstName": "Adina",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adina Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10666396"
                        ],
                        "name": "Nikita Nangia",
                        "slug": "Nikita-Nangia",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Nangia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Nangia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 315
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 145
                            }
                        ],
                        "text": "\u2026(2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3432876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "isKey": false,
            "numCitedBy": 2037,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."
            },
            "slug": "A-Broad-Coverage-Challenge-Corpus-for-Sentence-Williams-Nangia",
            "title": {
                "fragments": [],
                "text": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Multi-Genre Natural Language Inference corpus is introduced, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding and shows that it represents a substantially more difficult task than does the Stanford NLI corpus."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3407277"
                        ],
                        "name": "Adam Paszke",
                        "slug": "Adam-Paszke",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Paszke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Paszke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793298"
                        ],
                        "name": "S. Gross",
                        "slug": "S.-Gross",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114250963"
                        ],
                        "name": "Gregory Chanan",
                        "slug": "Gregory-Chanan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Chanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Chanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052812305"
                        ],
                        "name": "E. Yang",
                        "slug": "E.-Yang",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2375710"
                        ],
                        "name": "Zach DeVito",
                        "slug": "Zach-DeVito",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "DeVito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach DeVito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3370429"
                        ],
                        "name": "Zeming Lin",
                        "slug": "Zeming-Lin",
                        "structuredName": {
                            "firstName": "Zeming",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeming Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050846"
                        ],
                        "name": "Alban Desmaison",
                        "slug": "Alban-Desmaison",
                        "structuredName": {
                            "firstName": "Alban",
                            "lastName": "Desmaison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alban Desmaison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029482"
                        ],
                        "name": "L. Antiga",
                        "slug": "L.-Antiga",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Antiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Antiga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977806"
                        ],
                        "name": "Adam Lerer",
                        "slug": "Adam-Lerer",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lerer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "To accommodate a large community of practitioners and researchers, the library is deeply compatible with (and actually makes compatible) two major deep learning frameworks: PyTorch (Paszke et al., 2017) and TensorFlow (from release 2.0) (Abadi et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 80
                            }
                        ],
                        "text": "The second direction concerns lower-level deep-learning frameworks like PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 81
                            }
                        ],
                        "text": "The second direction concerns lower-level deep-learning frameworks like PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) which have both been extended with model sharing capabilities or hubs, respectively called TensorFlow Hub18 and PyTorch Hub19."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 181
                            }
                        ],
                        "text": "To accommodate a large community of practitioners and researchers, the library is deeply compatible with (and actually makes compatible) two major deep learning frameworks: PyTorch (Paszke et al., 2017) and TensorFlow (from release 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40027675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "isKey": true,
            "numCitedBy": 10325,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we describe an automatic differentiation module of PyTorch \u2014 a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features."
            },
            "slug": "Automatic-differentiation-in-PyTorch-Paszke-Gross",
            "title": {
                "fragments": [],
                "text": "Automatic differentiation in PyTorch"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An automatic differentiation module of PyTorch is described \u2014 a library designed to enable rapid research on machine learning models that focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511414"
                        ],
                        "name": "Myle Ott",
                        "slug": "Myle-Ott",
                        "structuredName": {
                            "firstName": "Myle",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myle Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068070"
                        ],
                        "name": "Sergey Edunov",
                        "slug": "Sergey-Edunov",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Edunov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Edunov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51428394"
                        ],
                        "name": "Alexei Baevski",
                        "slug": "Alexei-Baevski",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Baevski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei Baevski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144270981"
                        ],
                        "name": "Angela Fan",
                        "slug": "Angela-Fan",
                        "structuredName": {
                            "firstName": "Angela",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angela Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793298"
                        ],
                        "name": "S. Gross",
                        "slug": "S.-Gross",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055124"
                        ],
                        "name": "Nathan Ng",
                        "slug": "Nathan-Ng",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2529182"
                        ],
                        "name": "David Grangier",
                        "slug": "David-Grangier",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Grangier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Grangier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325985"
                        ],
                        "name": "Michael Auli",
                        "slug": "Michael-Auli",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Auli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Auli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 19
                            }
                        ],
                        "text": ", 2018), fairseq21 (Ott et al., 2019) and Megatron-LM22."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "Typical examples are the tensor2tensor20 library (Vaswani et al., 2018), fairseq21 (Ott et al., 2019) and Megatron-LM22."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 91184134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
            "isKey": false,
            "numCitedBy": 1565,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto"
            },
            "slug": "fairseq:-A-Fast,-Extensible-Toolkit-for-Sequence-Ott-Edunov",
            "title": {
                "fragments": [],
                "text": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks and supports distributed training across multiple GPUs and machines."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783281"
                        ],
                        "name": "Daniel Khashabi",
                        "slug": "Daniel-Khashabi",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Khashabi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Khashabi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37202877"
                        ],
                        "name": "Snigdha Chaturvedi",
                        "slug": "Snigdha-Chaturvedi",
                        "structuredName": {
                            "firstName": "Snigdha",
                            "lastName": "Chaturvedi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Snigdha Chaturvedi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617131"
                        ],
                        "name": "Michael Roth",
                        "slug": "Michael-Roth",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33145619"
                        ],
                        "name": "Shyam Upadhyay",
                        "slug": "Shyam-Upadhyay",
                        "structuredName": {
                            "firstName": "Shyam",
                            "lastName": "Upadhyay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shyam Upadhyay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5112038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99ad0533f84c110da2d0713d5798e6e14080b159",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills."
            },
            "slug": "Looking-Beyond-the-Surface:-A-Challenge-Set-for-Khashabi-Chaturvedi",
            "title": {
                "fragments": [],
                "text": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills, and finds human solvers to achieve an F1-score of 88.1%."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151810148"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787620"
                        ],
                        "name": "Konstantin Lopyrev",
                        "slug": "Konstantin-Lopyrev",
                        "structuredName": {
                            "firstName": "Konstantin",
                            "lastName": "Lopyrev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantin Lopyrev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026(STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 3
                            }
                        ],
                        "text": "1 (Rajpurkar et al. (2016)) and SQuAD2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 363,
                                "start": 359
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "hitectures on a common language understanding benchmark. Transformers notably includes pre-processors and \ufb01ne-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al. (2019)) and SQuAD1.1 (Rajpurkar et al., 2016). Pushing best practices forward Transformers seeks a balance between sticking to the original authors\u2019 code-base for reliability and providing clear and readable implementations featuring best practi"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "1 (Rajpurkar et al., 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ing systems. Details of the datasets can be found in the Appendix on page 7. 4 Figure 1: Write With Transformer Regarding the machine comprehension tasks, the library feature evaluations on SQuAD1.1 (Rajpurkar et al. (2016)) and SQuAD2.0 (Rajpurkar et al. (2018)). Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018)). 4.2 Language model \ufb01ne-tun"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11816014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05dd7254b632376973f3a1b4d39485da17814df5",
            "isKey": true,
            "numCitedBy": 4263,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
            },
            "slug": "SQuAD:-100,000+-Questions-for-Machine-Comprehension-Rajpurkar-Zhang",
            "title": {
                "fragments": [],
                "text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879705"
                        ],
                        "name": "Hendrik Strobelt",
                        "slug": "Hendrik-Strobelt",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Strobelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hendrik Strobelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3159346"
                        ],
                        "name": "Sebastian Gehrmann",
                        "slug": "Sebastian-Gehrmann",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Gehrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Gehrmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758236"
                        ],
                        "name": "H. Pfister",
                        "slug": "H.-Pfister",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We highlight specific systems developed by users with different goals following the architect, trainer, and end-user distinction of Strobelt et al. (2017):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25127323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a798c13da2d500dd76c4ff76f18ded43df0d59bc",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community."
            },
            "slug": "LSTMVis:-A-Tool-for-Visual-Analysis-of-Hidden-State-Strobelt-Gehrmann",
            "title": {
                "fragments": [],
                "text": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics, and describes the domain, the different stakeholders, and their goals and tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48926630"
                        ],
                        "name": "Adam Poliak",
                        "slug": "Adam-Poliak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Poliak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Poliak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28926612"
                        ],
                        "name": "Aparajita Haldar",
                        "slug": "Aparajita-Haldar",
                        "structuredName": {
                            "firstName": "Aparajita",
                            "lastName": "Haldar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aparajita Haldar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2034613"
                        ],
                        "name": "Rachel Rudinger",
                        "slug": "Rachel-Rudinger",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Rudinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Rudinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157840220"
                        ],
                        "name": "J. E. Hu",
                        "slug": "J.-E.-Hu",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hu",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949185"
                        ],
                        "name": "Ellie Pavlick",
                        "slug": "Ellie-Pavlick",
                        "structuredName": {
                            "firstName": "Ellie",
                            "lastName": "Pavlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellie Pavlick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2352617"
                        ],
                        "name": "Aaron Steven White",
                        "slug": "Aaron-Steven-White",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "White",
                            "middleNames": [
                                "Steven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Steven White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 514,
                                "start": 511
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 52123220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74d8a800d73fc68a398f92ed0536912d1b7a32f3",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources."
            },
            "slug": "Collecting-Diverse-Natural-Language-Inference-for-Poliak-Haldar",
            "title": {
                "fragments": [],
                "text": "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A large-scale collection of diverse natural language inference datasets that help provide insight into how well a sentence representation captures distinct types of reasoning are presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545335"
                        ],
                        "name": "Rowan Zellers",
                        "slug": "Rowan-Zellers",
                        "structuredName": {
                            "firstName": "Rowan",
                            "lastName": "Zellers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rowan Zellers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312309"
                        ],
                        "name": "Yonatan Bisk",
                        "slug": "Yonatan-Bisk",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Bisk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Bisk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4671928"
                        ],
                        "name": "Roy Schwartz",
                        "slug": "Roy-Schwartz",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ding the machine comprehension tasks, the library feature evaluations on SQuAD1.1 (Rajpurkar et al. (2016)) and SQuAD2.0 (Rajpurkar et al. (2018)). Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018)). 4.2 Language model \ufb01ne-tuning Fine-tuning a language model on a downstream text corpus usually leads to signi\ufb01cant gains for tasks on this co"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 52
                            }
                        ],
                        "text": "Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52019251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
            "isKey": true,
            "numCitedBy": 456,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a partial description like \u201cshe opened the hood of the car,\u201d humans can reason about the situation and anticipate what might come next (\u201dthen, she examined the engine\u201d). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research."
            },
            "slug": "SWAG:-A-Large-Scale-Adversarial-Dataset-for-Zellers-Bisk",
            "title": {
                "fragments": [],
                "text": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper introduces the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning, and proposes Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3390191"
                        ],
                        "name": "Isaac Cowhey",
                        "slug": "Isaac-Cowhey",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Cowhey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isaac Cowhey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236429"
                        ],
                        "name": "Tushar Khot",
                        "slug": "Tushar-Khot",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Khot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tushar Khot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48229640"
                        ],
                        "name": "Ashish Sabharwal",
                        "slug": "Ashish-Sabharwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Sabharwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Sabharwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393851"
                        ],
                        "name": "Carissa Schoenick",
                        "slug": "Carissa-Schoenick",
                        "structuredName": {
                            "firstName": "Carissa",
                            "lastName": "Schoenick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carissa Schoenick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3385516"
                        ],
                        "name": "Oyvind Tafjord",
                        "slug": "Oyvind-Tafjord",
                        "structuredName": {
                            "firstName": "Oyvind",
                            "lastName": "Tafjord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oyvind Tafjord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3922816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community."
            },
            "slug": "Think-you-have-Solved-Question-Answering-Try-ARC,-Clark-Cowhey",
            "title": {
                "fragments": [],
                "text": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760868"
                        ],
                        "name": "M. Surdeanu",
                        "slug": "M.-Surdeanu",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Surdeanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Surdeanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661918"
                        ],
                        "name": "John Bauer",
                        "slug": "John-Bauer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784228"
                        ],
                        "name": "J. Finkel",
                        "slug": "J.-Finkel",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Finkel",
                            "middleNames": [
                                "Rose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Finkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105138"
                        ],
                        "name": "Steven Bethard",
                        "slug": "Steven-Bethard",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bethard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bethard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240597"
                        ],
                        "name": "David McClosky",
                        "slug": "David-McClosky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McClosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David McClosky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Two core libraries are NLTK (Loper and Bird, 2002) and Stanford CoreNLP (Manning et al., 2014), which collect a variety of different approaches to NLP in a single package."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14068874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "isKey": false,
            "numCitedBy": 6057,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "slug": "The-Stanford-CoreNLP-Natural-Language-Processing-Manning-Surdeanu",
            "title": {
                "fragments": [],
                "text": "The Stanford CoreNLP Natural Language Processing Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The design and use of the Stanford CoreNLP toolkit is described, an extensible pipeline that provides core natural language analysis, and it is suggested that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14487640"
                        ],
                        "name": "Ari Holtzman",
                        "slug": "Ari-Holtzman",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Holtzman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari Holtzman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144685020"
                        ],
                        "name": "Jan Buys",
                        "slug": "Jan-Buys",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Buys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Buys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39191185"
                        ],
                        "name": "Maxwell Forbes",
                        "slug": "Maxwell-Forbes",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Forbes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maxwell Forbes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": ") on generation quality (Holtzman et al., 2019), Write with Transformer offers various options to dynamically tweak the decoding algorithm and investigate the resulting samples from the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on generation quality (Holtzman et al., 2019), Write with Transformer offers various options to dynamically tweak the decoding algorithm and investigate the resulting samples from the model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 127986954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
            "isKey": false,
            "numCitedBy": 924,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence."
            },
            "slug": "The-Curious-Case-of-Neural-Text-Degeneration-Holtzman-Buys",
            "title": {
                "fragments": [],
                "text": "The Curious Case of Neural Text Degeneration"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39861444"
                        ],
                        "name": "G. Klein",
                        "slug": "G.-Klein",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152847918"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505751"
                        ],
                        "name": "Yuntian Deng",
                        "slug": "Yuntian-Deng",
                        "structuredName": {
                            "firstName": "Yuntian",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntian Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053934"
                        ],
                        "name": "Jean Senellart",
                        "slug": "Jean-Senellart",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Senellart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Senellart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16538528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab5002a22b9b4244a8329b140bd0a86021aa2d1",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
            },
            "slug": "OpenNMT:-Open-Source-Toolkit-for-Neural-Machine-Klein-Kim",
            "title": {
                "fragments": [],
                "text": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486762"
                        ],
                        "name": "L. Bentivogli",
                        "slug": "L.-Bentivogli",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Bentivogli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bentivogli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 224
                            }
                        ],
                        "text": "\u2026Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 228
                            }
                        ],
                        "text": "\u2026(QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 858065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db8885a0037fe47d973ade79d696586453710233",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge. This year a major innovation was introduced, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario. A subtask was also proposed, aimed at detecting novel information. To continue the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was set up, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48 submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs). As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory effort started in RTE-5 to perform resource evaluation through ablation tests was not only reiterated in RTE-6, but also extended to tools."
            },
            "slug": "The-Sixth-PASCAL-Recognizing-Textual-Entailment-Bentivogli-Clark",
            "title": {
                "fragments": [],
                "text": "The Sixth PASCAL Recognizing Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which TextualEntailment is performed on a real corpus in the Update Summarization scenario."
            },
            "venue": {
                "fragments": [],
                "text": "TAC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913774"
                        ],
                        "name": "Tianqi Chen",
                        "slug": "Tianqi-Chen",
                        "structuredName": {
                            "firstName": "Tianqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47108160"
                        ],
                        "name": "T. Moreau",
                        "slug": "T.-Moreau",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Moreau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moreau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732910"
                        ],
                        "name": "Ziheng Jiang",
                        "slug": "Ziheng-Jiang",
                        "structuredName": {
                            "firstName": "Ziheng",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziheng Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149970173"
                        ],
                        "name": "Lianmin Zheng",
                        "slug": "Lianmin-Zheng",
                        "structuredName": {
                            "firstName": "Lianmin",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianmin Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621619"
                        ],
                        "name": "Eddie Q. Yan",
                        "slug": "Eddie-Q.-Yan",
                        "structuredName": {
                            "firstName": "Eddie",
                            "lastName": "Yan",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eddie Q. Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050154"
                        ],
                        "name": "Haichen Shen",
                        "slug": "Haichen-Shen",
                        "structuredName": {
                            "firstName": "Haichen",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haichen Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37270394"
                        ],
                        "name": "M. Cowan",
                        "slug": "M.-Cowan",
                        "structuredName": {
                            "firstName": "Meghan",
                            "lastName": "Cowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cowan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185540"
                        ],
                        "name": "Leyuan Wang",
                        "slug": "Leyuan-Wang",
                        "structuredName": {
                            "firstName": "Leyuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leyuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49994783"
                        ],
                        "name": "Yuwei Hu",
                        "slug": "Yuwei-Hu",
                        "structuredName": {
                            "firstName": "Yuwei",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuwei Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717411"
                        ],
                        "name": "L. Ceze",
                        "slug": "L.-Ceze",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ceze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ceze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695691"
                        ],
                        "name": "A. Krishnamurthy",
                        "slug": "A.-Krishnamurthy",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Krishnamurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnamurthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2018) and TVM (Chen et al., 2018)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52939079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df013a17ab84d5403361da4538a04d574f58be83",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies."
            },
            "slug": "TVM:-An-Automated-End-to-End-Optimizing-Compiler-Chen-Moreau",
            "title": {
                "fragments": [],
                "text": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "TVM is a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends and automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "OSDI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486762"
                        ],
                        "name": "L. Bentivogli",
                        "slug": "L.-Bentivogli",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Bentivogli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bentivogli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5791809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f8468de03ee9f12d693237bec87916311bf1c24",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge. This year\u2019s challenge replicated the exercise proposed in RTE-6, consisting of a Main Task, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario; a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Thirteen teams participated in the Main Task (submitting 33 runs) and 5 in the Novelty Detection Subtask (submitting 13 runs). The KBP Validation Task was undertaken by 2 participants which submitted 5 runs. The ablation test experiment, introduced in RTE-5 to evaluate the impact of knowledge resources used by the systems participating in the Main Task and extended also to tools in RTE-6, was also repeated in RTE-7."
            },
            "slug": "The-Seventh-PASCAL-Recognizing-Textual-Entailment-Bentivogli-Clark",
            "title": {
                "fragments": [],
                "text": "The Seventh PASCAL Recognizing Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge, which replicated the exercise proposed in RTE-6, consisting of a Main Task, a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task."
            },
            "venue": {
                "fragments": [],
                "text": "TAC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403835575"
                        ],
                        "name": "Roy Bar-Haim",
                        "slug": "Roy-Bar-Haim",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Bar-Haim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Bar-Haim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66648221"
                        ],
                        "name": "Bill Dolan",
                        "slug": "Bill-Dolan",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Dolan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36405930"
                        ],
                        "name": "L. Ferro",
                        "slug": "L.-Ferro",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Ferro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ferro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711977"
                        ],
                        "name": "Idan Szpektor",
                        "slug": "Idan-Szpektor",
                        "structuredName": {
                            "firstName": "Idan",
                            "lastName": "Szpektor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Idan Szpektor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 451,
                                "start": 448
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 177
                            }
                        ],
                        "text": "\u2026Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 599,
                                "start": 596
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "\u2026(QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13385138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "136326377c122560768db674e35f5bcd6de3bc40",
            "isKey": true,
            "numCitedBy": 408,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Second PASCAL Recognising Textual Entailment Challenge (RTE-2). 1 We describe the RTE2 dataset and overview the submissions for the challenge. One of the main goals for this year\u2019s dataset was to provide more \u201crealistic\u201d text-hypothesis examples, based mostly on outputs of actual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year\u2019s state of the art."
            },
            "slug": "The-Second-PASCAL-Recognising-Textual-Entailment-Bar-Haim-Dagan",
            "title": {
                "fragments": [],
                "text": "The Second PASCAL Recognising Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The RTE2 dataset is described and the submissions for the challenge are overviewed, to provide more \u201crealistic\u201d text-hypothesis examples, based mostly on outputs of actual systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125776"
                        ],
                        "name": "Chris Brockett",
                        "slug": "Chris-Brockett",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brockett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brockett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "\u2026in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16639476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "475354f10798f110d34792b6d88f31d6d5cb099e",
            "isKey": false,
            "numCitedBy": 834,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters."
            },
            "slug": "Automatically-Constructing-a-Corpus-of-Sentential-Dolan-Brockett",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing a Corpus of Sentential Paraphrases"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase, is described."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46236380"
                        ],
                        "name": "Alex Warstadt",
                        "slug": "Alex-Warstadt",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Warstadt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Warstadt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 32
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44072099,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.\u2019s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions."
            },
            "slug": "Neural-Network-Acceptability-Judgments-Warstadt-Singh",
            "title": {
                "fragments": [],
                "text": "Neural Network Acceptability Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper introduces the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature, and trains several recurrent neural network models on acceptability classification, and finds that the authors' models outperform unsupervised models by Lau et al. (2016) on CoLA."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857734"
                        ],
                        "name": "Guokun Lai",
                        "slug": "Guokun-Lai",
                        "structuredName": {
                            "firstName": "Guokun",
                            "lastName": "Lai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guokun Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912046"
                        ],
                        "name": "Qizhe Xie",
                        "slug": "Qizhe-Xie",
                        "structuredName": {
                            "firstName": "Qizhe",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qizhe Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391802"
                        ],
                        "name": "Hanxiao Liu",
                        "slug": "Hanxiao-Liu",
                        "structuredName": {
                            "firstName": "Hanxiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanxiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " tasks, the library feature evaluations on SQuAD1.1 (Rajpurkar et al. (2016)) and SQuAD2.0 (Rajpurkar et al. (2018)). Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018)). 4.2 Language model \ufb01ne-tuning Fine-tuning a language model on a downstream text corpus usually leads to signi\ufb01cant gains for tasks on this corpus, in particular when t"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6826032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "636a79420d838eabe4af7fb25d6437de45ab64e8",
            "isKey": true,
            "numCitedBy": 698,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students\u2019 ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines."
            },
            "slug": "RACE:-Large-scale-ReAding-Comprehension-Dataset-Lai-Xie",
            "title": {
                "fragments": [],
                "text": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models and the ceiling human performance."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 119
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 17
                            }
                        ],
                        "text": ", 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
            },
            "venue": {
                "fragments": [],
                "text": "To appear."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The additional optimizer currently available is the Adam optimizer (Kingma and Ba, 2014) with an additional weight decay fix, also known as \u2018AdamW\u2018 (Loshchilov and Hutter, 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90063,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422908"
                        ],
                        "name": "Robin Jia",
                        "slug": "Robin-Jia",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n be found in the Appendix on page 7. 4 Figure 1: Write With Transformer Regarding the machine comprehension tasks, the library feature evaluations on SQuAD1.1 (Rajpurkar et al. (2016)) and SQuAD2.0 (Rajpurkar et al. (2018)). Others currently-supported benchmarks include SWAG (Zellers et al. (2018)), RACE (Lai et al. (2017)) and ARC (Clark et al. (2018)). 4.2 Language model \ufb01ne-tuning Fine-tuning a language model on a d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 3
                            }
                        ],
                        "text": "0 (Rajpurkar et al. (2018))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 47018994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
            "isKey": true,
            "numCitedBy": 1398,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
            },
            "slug": "Know-What-You-Don\u2019t-Know:-Unanswerable-Questions-Rajpurkar-Jia",
            "title": {
                "fragments": [],
                "text": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SQuadRUn is a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21308992"
                        ],
                        "name": "Steven Bird",
                        "slug": "Steven-Bird",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Two core libraries are NLTK (Loper and Bird, 2002) and Stanford CoreNLP (Manning et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1438450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a660ec8aa995a88a00bfb41cb86c022047a9db",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset."
            },
            "slug": "NLTK:-The-Natural-Language-Toolkit-Bird",
            "title": {
                "fragments": [],
                "text": "NLTK: The Natural Language Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware that covers symbolic and statistical natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678656"
                        ],
                        "name": "I. Loshchilov",
                        "slug": "I.-Loshchilov",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Loshchilov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Loshchilov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 147
                            }
                        ],
                        "text": "The additional optimizer currently available is the Adam optimizer (Kingma and Ba, 2014) with an additional weight decay fix, also known as \u2018AdamW\u2018 (Loshchilov and Hutter, 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3312944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a",
            "isKey": false,
            "numCitedBy": 679,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process."
            },
            "slug": "Fixing-Weight-Decay-Regularization-in-Adam-Loshchilov-Hutter",
            "title": {
                "fragments": [],
                "text": "Fixing Weight Decay Regularization in Adam"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678656"
                        ],
                        "name": "I. Loshchilov",
                        "slug": "I.-Loshchilov",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Loshchilov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Loshchilov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 147
                            }
                        ],
                        "text": "The additional optimizer currently available is the Adam optimizer (Kingma and Ba, 2014) with an additional weight decay fix, also known as \u2018AdamW\u2018 (Loshchilov and Hutter, 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53592270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "isKey": false,
            "numCitedBy": 3476,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL"
            },
            "slug": "Decoupled-Weight-Decay-Regularization-Loshchilov-Hutter",
            "title": {
                "fragments": [],
                "text": "Decoupled Weight Decay Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function, and provides empirical evidence that this modification substantially improves Adam's generalization performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72655349"
                        ],
                        "name": "Sheng Zhang",
                        "slug": "Sheng-Zhang",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108860856"
                        ],
                        "name": "Xiaodong Liu",
                        "slug": "Xiaodong-Liu",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46700348"
                        ],
                        "name": "Jingjing Liu",
                        "slug": "Jingjing-Liu",
                        "structuredName": {
                            "firstName": "Jingjing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingjing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800354"
                        ],
                        "name": "Kevin Duh",
                        "slug": "Kevin-Duh",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Duh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Duh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53116244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5b66ee341cb990f7f70a124b5fab3316d3b7e27",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at this http URL"
            },
            "slug": "ReCoRD:-Bridging-the-Gap-between-Human-and-Machine-Zhang-Liu",
            "title": {
                "fragments": [],
                "text": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work presents a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning, and demonstrates that the performance of state-of-the-art MRC systems fall far behind human performance."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743722"
                        ],
                        "name": "Douwe Kiela",
                        "slug": "Douwe-Kiela",
                        "structuredName": {
                            "firstName": "Douwe",
                            "lastName": "Kiela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douwe Kiela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47686397"
                        ],
                        "name": "Suvrat Bhooshan",
                        "slug": "Suvrat-Bhooshan",
                        "structuredName": {
                            "firstName": "Suvrat",
                            "lastName": "Bhooshan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suvrat Bhooshan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22593971"
                        ],
                        "name": "Hamed Firooz",
                        "slug": "Hamed-Firooz",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Firooz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hamed Firooz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389630028"
                        ],
                        "name": "Davide Testuggine",
                        "slug": "Davide-Testuggine",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Testuggine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Davide Testuggine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MMBT (Kiela et al., 2019)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 202539204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a7620afb12870a5df0e178dd175d37a5cbc8c0c",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks. The modern digital world is increasingly multimodal, however, and textual information is often accompanied by other modalities such as images. We introduce a supervised multimodal bitransformer model that fuses information from text and image encoders, and obtain state-of-the-art performance on various multimodal classification benchmark tasks, outperforming strong baselines, including on hard test sets specifically designed to measure multimodal performance."
            },
            "slug": "Supervised-Multimodal-Bitransformers-for-Images-and-Kiela-Bhooshan",
            "title": {
                "fragments": [],
                "text": "Supervised Multimodal Bitransformers for Classifying Images and Text"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work introduces a supervised multimodal bitransformer model that fuses information from text and image encoders, and obtains state-of-the-art performance on various multi-modal classification benchmark tasks, outperforming strong baselines, including on hard test sets specifically designed to measure multimodals performance."
            },
            "venue": {
                "fragments": [],
                "text": "ViGIL@NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634377"
                        ],
                        "name": "H. Levesque",
                        "slug": "H.-Levesque",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Levesque",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Levesque"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883814"
                        ],
                        "name": "E. Davis",
                        "slug": "E.-Davis",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40429476"
                        ],
                        "name": "L. Morgenstern",
                        "slug": "L.-Morgenstern",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Morgenstern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Morgenstern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 546,
                                "start": 542
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 273
                            }
                        ],
                        "text": "\u2026Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 694,
                                "start": 690
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 277
                            }
                        ],
                        "text": "\u2026(QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15710851,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "128cb6b891aee1b5df099acb48e2efecfcff689f",
            "isKey": true,
            "numCitedBy": 691,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test."
            },
            "slug": "The-Winograd-Schema-Challenge-Levesque-Davis",
            "title": {
                "fragments": [],
                "text": "The Winograd Schema Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents an alternative to the Turing Test that has some conceptual and practical advantages, and English-speaking adults will have no difficulty with it, and the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person."
            },
            "venue": {
                "fragments": [],
                "text": "KR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445241"
                        ],
                        "name": "E. Brevdo",
                        "slug": "E.-Brevdo",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Brevdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brevdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1565641737"
                        ],
                        "name": "Fran\u00e7ois Chollet",
                        "slug": "Fran\u00e7ois-Chollet",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Chollet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Chollet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776283"
                        ],
                        "name": "Stephan Gouws",
                        "slug": "Stephan-Gouws",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Gouws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Gouws"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35474601"
                        ],
                        "name": "Ryan Sepassi",
                        "slug": "Ryan-Sepassi",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Sepassi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Sepassi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "Typical examples are the tensor2tensor20 library (Vaswani et al., 2018), fairseq21 (Ott et al., 2019) and Megatron-LM22."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 49
                            }
                        ],
                        "text": "Typical examples are the tensor2tensor20 library (Vaswani et al., 2018), fairseq21 (Ott et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 163
                            }
                        ],
                        "text": "More precisely, organizing the modules around three main components (configuration, tokenizers and models) was inspired by the design of the tensor2tensor library (Vaswani et al., 2018) and the original code repository of Bert (Devlin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026organizing the modules around three main components (configuration, tokenizers and models) was inspired by the design of the tensor2tensor library (Vaswani et al., 2018) and the original code repository of Bert (Devlin et al., 2018) from Google Research while concept of providing easy caching for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3988816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642c1b4a9da95ea4239708afc5929a5007a1870d",
            "isKey": true,
            "numCitedBy": 376,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model."
            },
            "slug": "Tensor2Tensor-for-Neural-Machine-Translation-Vaswani-Bengio",
            "title": {
                "fragments": [],
                "text": "Tensor2Tensor for Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model."
            },
            "venue": {
                "fragments": [],
                "text": "AMTA"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 119
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 17
                            }
                        ],
                        "text": ", 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
            },
            "venue": {
                "fragments": [],
                "text": "To appear."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35168946"
                        ],
                        "name": "A. Gordon",
                        "slug": "A.-Gordon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gordon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714932"
                        ],
                        "name": "Zornitsa Kozareva",
                        "slug": "Zornitsa-Kozareva",
                        "structuredName": {
                            "firstName": "Zornitsa",
                            "lastName": "Kozareva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zornitsa Kozareva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316824"
                        ],
                        "name": "Melissa Roemmele",
                        "slug": "Melissa-Roemmele",
                        "structuredName": {
                            "firstName": "Melissa",
                            "lastName": "Roemmele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Melissa Roemmele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 141
                            }
                        ],
                        "text": "\u2026are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 434646,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fb0b11046474b8f1c810f947f313c7c7229a988f",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "SemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we describe the development of this task and its motivation. We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future."
            },
            "slug": "SemEval-2012-Task-7:-Choice-of-Plausible-An-of-Gordon-Kozareva",
            "title": {
                "fragments": [],
                "text": "SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The two systems that competed in this task as part of SemEval-2012 are described, and their results are compared to those achieved in previously published research."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49501003"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81120201"
                        ],
                        "name": "Simone Wu",
                        "slug": "Simone-Wu",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simone Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064225225"
                        ],
                        "name": "Andrew Zaldivar",
                        "slug": "Andrew-Zaldivar",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zaldivar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zaldivar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80940648"
                        ],
                        "name": "Parker Barnes",
                        "slug": "Parker-Barnes",
                        "structuredName": {
                            "firstName": "Parker",
                            "lastName": "Barnes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Parker Barnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177877"
                        ],
                        "name": "Lucy Vasserman",
                        "slug": "Lucy-Vasserman",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vasserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vasserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083807"
                        ],
                        "name": "B. Hutchinson",
                        "slug": "B.-Hutchinson",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Hutchinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hutchinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79542084"
                        ],
                        "name": "Elena Spitzer",
                        "slug": "Elena-Spitzer",
                        "structuredName": {
                            "firstName": "Elena",
                            "lastName": "Spitzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elena Spitzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81316798"
                        ],
                        "name": "Inioluwa Deborah Raji",
                        "slug": "Inioluwa-Deborah-Raji",
                        "structuredName": {
                            "firstName": "Inioluwa",
                            "lastName": "Raji",
                            "middleNames": [
                                "Deborah"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Inioluwa Deborah Raji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076288"
                        ],
                        "name": "Timnit Gebru",
                        "slug": "Timnit-Gebru",
                        "structuredName": {
                            "firstName": "Timnit",
                            "lastName": "Gebru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timnit Gebru"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Additional model-specific metadata can be provided via a model card (Mitchell et al., 2018) that describes properties of its training, a citation to the work, datasets used during pretraining, and any caveats about known biases in the model and its predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52946140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7365f887c938ca21a6adbef08b5a520ebbd4638f",
            "isKey": false,
            "numCitedBy": 667,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation."
            },
            "slug": "Model-Cards-for-Model-Reporting-Mitchell-Wu",
            "title": {
                "fragments": [],
                "text": "Model Cards for Model Reporting"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes model cards, a framework that can be used to document any trained machine learning model in the application fields of computer vision and natural language processing, and provides cards for two supervised models: One trained to detect smiling faces in images, and one training to detect toxic comments in text."
            },
            "venue": {
                "fragments": [],
                "text": "FAT"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2034613"
                        ],
                        "name": "Rachel Rudinger",
                        "slug": "Rachel-Rudinger",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Rudinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Rudinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300343"
                        ],
                        "name": "Jason Naradowsky",
                        "slug": "Jason-Naradowsky",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Naradowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Naradowsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056398837"
                        ],
                        "name": "Brian Leonard",
                        "slug": "Brian-Leonard",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Leonard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Leonard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 141
                            }
                        ],
                        "text": "\u2026Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 438,
                                "start": 435
                            }
                        ],
                        "text": "The datasets in SuperGLUE are: Boolean Questions (BoolQ) Clark et al. (2019a), CommitmentBank (CB) De Marneffe et al. (2019), Choice of Plausible Alternatives (COPA) Roemmele et al. (2011), Multi-Sentence Reading Comprehension (MultiRC) Khashabi et al. (2018), Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) Zhang et al. (2018), Word-in-Context (WiC) Pilehvar and Camacho-Collados (2019), Winograd Schema Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 13756572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \u201cWinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics."
            },
            "slug": "Gender-Bias-in-Coreference-Resolution-Rudinger-Naradowsky",
            "title": {
                "fragments": [],
                "text": "Gender Bias in Coreference Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender are introduced, and systematic gender bias in three publicly-available coreference resolution systems is evaluated and confirmed."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al., 2018) or Flair (Akbik et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 119
                            }
                        ],
                        "text": "Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy (Honnibal and Montani, 2017), AllenNLP (Gardner et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 17
                            }
                        ],
                        "text": ", 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "The first direction includes Natural Language Processing libraries such as AllenNLP13 (Gardner et al., 2018), SpaCy14 (Honnibal and Montani, 2017), flair15 (Akbik et al., 2018, 2019) or PyText16."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
            },
            "venue": {
                "fragments": [],
                "text": "To appear."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 197
                            }
                        ],
                        "text": "\u2026Challenge (WSC) Rudinger et al. (2018), Diverse Natural Language Inference Collection (DNC) Poliak et al. (2018), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 201
                            }
                        ],
                        "text": "\u2026(QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195352006,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Third-PASCAL-Recognizing-Textual-Entailment-Giampiccolo-Magnini",
            "title": {
                "fragments": [],
                "text": "The Third PASCAL Recognizing Textual Entailment Challenge"
            },
            "venue": {
                "fragments": [],
                "text": "ACL-PASCAL@ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177568"
                        ],
                        "name": "J. Q. Candela",
                        "slug": "J.-Q.-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Candela",
                            "middleNames": [
                                "Qui\u00f1onero"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Q. Candela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69440470"
                        ],
                        "name": "Magnini B Dagan I",
                        "slug": "Magnini-B-Dagan-I",
                        "structuredName": {
                            "firstName": "Magnini",
                            "lastName": "Dagan I",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnini B Dagan I"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11393449"
                        ],
                        "name": "F. Lauria",
                        "slug": "F.-Lauria",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Lauria",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lauria"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8564414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e03d300581e16f6664157d2c1c6ceec33ec528ce",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Learning-Challenges.-Evaluating-Predictive-Candela-MagniniBDagan",
            "title": {
                "fragments": [],
                "text": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)"
            },
            "venue": {
                "fragments": [],
                "text": "Diverse Natural Language Inference Collection (DNC)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TensorFlow: Large-scale machine learning on heterogeneous systems"
            },
            "venue": {
                "fragments": [],
                "text": "Software available from tensorflow.org."
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reading Comprehension with Commonsense Reasoning Dataset"
            },
            "venue": {
                "fragments": [],
                "text": "Winograd Schema Challenge (WSC) Rudinger et al"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 273
                            }
                        ],
                        "text": "The datasets in GLUE are: CoLA (Warstadt et al. (2018)), Stanford Sentiment Treebank (SST) (Socher et al. (2013)), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2017), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "First quora dataset release: Question pairs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 45,
            "methodology": 35
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 80,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/HuggingFace's-Transformers:-State-of-the-art-Wolf-Debut/1fa9ed2bea208511ae698a967875e943049f16b6?sort=total-citations"
}