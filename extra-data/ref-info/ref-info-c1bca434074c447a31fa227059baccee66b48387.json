{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938043"
                        ],
                        "name": "Suman V. Ravuri",
                        "slug": "Suman-V.-Ravuri",
                        "structuredName": {
                            "firstName": "Suman",
                            "lastName": "Ravuri",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suman V. Ravuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "The tandem network trained on clean and noisy data outperforms a tandem approach trained on clean data only as reported in [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "We also compare to a tandem approach, where a neural network is trained to output phoneme posteriors [10]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1413462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbee79ac1865cd42780215d8fb2da4bc2ab7f381",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we show how new training principles and optimization techniques for neural networks can be used for different network structures. In particular, we revisit the Recurrent Neural Network (RNN), which explicitly models the Markovian dynamics of a set of observations through a non-linear function with a much larger hidden state space than traditional sequence models such as an HMM. We apply pretraining principles used for Deep Neural Networks (DNNs) and second-order optimization techniques to train an RNN. Moreover, we explore its application in the Aurora2 speech recognition task under mismatched noise conditions using a Tandem approach. We observe top performance on clean speech, and under high noise conditions, compared to multi-layer perceptrons (MLPs) and DNNs, with the added benefit of being a \u201cdeeper\u201d model than an MLP but more compact than a DNN."
            },
            "slug": "Revisiting-Recurrent-Neural-Networks-for-robust-ASR-Vinyals-Ravuri",
            "title": {
                "fragments": [],
                "text": "Revisiting Recurrent Neural Networks for robust ASR"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Recurrent Neural Network is revisited, which explicitly models the Markovian dynamics of a set of observations through a non-linear function with a much larger hidden state space than traditional sequence models such as an HMM."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66980418"
                        ],
                        "name": "S. Tamura",
                        "slug": "S.-Tamura",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Neural network models have also been applied to learn a function to map a noisy utterance x to a clean version of the utterance y [3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195708690,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "f258b49b4a65d5b30a9bd539067ded2a3b2c5531",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using a back propagation network learning algorithm, a four-layered feed-forward network is trained on learning samples to realize a mapping from the set of noisy signals a set of noise-free signals. Computer experiments were carried out on 12 kHz sampled Japanese speech data, using stationary and nonstationary noise. The experiments showed that the network can indeed learn to perform noise reduction. Even for noisy speech signals that had not been part of the training data, the network successfully produced noise-suppressed output signals.<<ETX>>"
            },
            "slug": "Noise-reduction-using-connectionist-models-Tamura-Waibel",
            "title": {
                "fragments": [],
                "text": "Noise reduction using connectionist models"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Using a back propagation network learning algorithm, a four-layered feed-forward network is trained on learning samples to realize a mapping from the set of noisy signals a set of noise-free signals, showing that the network can indeed learn to perform noise reduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894961"
                        ],
                        "name": "Shahla Parveen",
                        "slug": "Shahla-Parveen",
                        "structuredName": {
                            "firstName": "Shahla",
                            "lastName": "Parveen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shahla Parveen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144220752"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Green",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 192
                            }
                        ],
                        "text": "Previous work also introduced neural network models which capture the temporal nature of speech using recurrent connections, and directly estimate word identity for isolated digit recognition [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2084645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a0be8ede3fee193b69802d5ec0a044de66530f",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In the 'missing data' approach to improving the robustness of automatic speech recognition to added noise, an initial process identifies spectral-temporal regions which are dominated by the speech source. The remaining regions are considered to be 'missing'. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classification with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task."
            },
            "slug": "Speech-Recognition-with-Missing-Data-using-Neural-Parveen-Green",
            "title": {
                "fragments": [],
                "text": "Speech Recognition with Missing Data using Recurrent Neural Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks, which allows for the use of long-term time constraints and to make the problems of classification with incomplete data and imputing missing values interact."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "A neural network which attempts to reconstruct a clean version of its own noisy input is known in the literature as a denoising autoencoder (DAE) [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5465,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46263938"
                        ],
                        "name": "Li Jiang",
                        "slug": "Li-Jiang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755472"
                        ],
                        "name": "J. Droppo",
                        "slug": "J.-Droppo",
                        "structuredName": {
                            "firstName": "Jasha",
                            "lastName": "Droppo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Droppo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Training noise reduction models using stereo (clean and noisy) data has been used successfully with the SPLICE algorithm [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14891079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b77bfe687300f727c58efd28bac944808d7264e1",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel technique of SPLICE (Stereo-based Piecewise Linear Compensation for Environments) for high performance robust speech recognition. It is an efficient noise reduction and channel distortion compensation technique that makes effective use of stereo training data. We present a new version of SPLICE using the minimum-mean-square-error decision, and describe an extension by training clusters of hidden Markov models (HMMs) with SPLICE processing. Comprehensive results using a Wall Street Journal large vocabulary recognition task and with a wide range of noise types demonstrate the superior performance of the SPLICE technique over that under noisy matched conditions (19% word error rate reduction). The new technique is also shown to consistently outperform the spectral-subtraction noise reduction technique, and is currently being integrated into the Microsoft MiPad, a new generation PDA prototype."
            },
            "slug": "High-performance-robust-speech-recognition-using-Deng-Acero",
            "title": {
                "fragments": [],
                "text": "High-performance robust speech recognition using stereo training data"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A novel technique of SPLICE (Stereo-based Piecewise Linear Compensation for Environments) for high performance robust speech recognition is described, an efficient noise reduction and channel distortion compensation technique that makes effective use of stereo training data."
            },
            "venue": {
                "fragments": [],
                "text": "2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Robust automatic speech recognition (ASR), that with background noise and channel distortion, is a fundamental problem as ASR increasingly moves to mobile devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "For robust ASR we would like a function f(x) \u2192 y which maps the noisy utterance x to a clean utterance y with background noise and distortion removed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "[2] A. Ragni and M. Gales, \u201cDerivative Kernels for Noise Robust ASR,\u201d in ASRU, 2011."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "However, our feature denoising outperforms the tandem approach, which suggests that for robust ASR it is better to predict clean features as opposed to phoneme posteriors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "Existing state-of-the-art methods for robust ASR use specialized domain knowledge to denoise the speech signal [1] or train a word-segment discriminative model robust to noise [2]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "[10] O. Vinyals, S. Ravuri, and D. Povey, \u201cRevisiting Recurrent Neural Networks for Robust ASR,\u201d in ICASSP, 2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "We demonstrate our model\u2019s ability to produce cleaned utterances for the Aurora2 robust ASR task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "We also train the same 3 layer with 500 hidden units each DRDAE model as used in the ASR experiment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "We perform robust ASR experiments using the Aurora2 corpus [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "In this way, our work shares underlying principles with deep neural net acoustic models, which recently yielded substantial improvements in ASR [5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6703261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2446e8f2012f23176ff602be633c0ed2b956d66c",
            "isKey": true,
            "numCitedBy": 190,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The context-independent deep belief network (DBN) hidden Markov model (HMM) hybrid architecture has recently achieved promising results for phone recognition. In this work, we propose a context-dependent DBN-HMM system that dramatically outperforms strong Gaussian mixture model (GMM)-HMM baselines on a challenging, large vocabulary, spontaneous speech recognition dataset from the Bing mobile voice search task. Our system achieves absolute sentence accuracy improvements of 5.8% and 9.2% over GMM-HMMs trained using the minimum phone error rate (MPE) and maximum likelihood (ML) criteria, respectively, which translate to relative error reductions of 16.0% and 23.2%."
            },
            "slug": "Large-vocabulary-continuous-speech-recognition-with-Dahl-Yu",
            "title": {
                "fragments": [],
                "text": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMS"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a context-dependent DBN-HMM system that dramatically outperforms strong Gaussian mixture model (GMM)-HMM baselines on a challenging, large vocabulary, spontaneous speech recognition dataset from the Bing mobile voice search task."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145381506"
                        ],
                        "name": "D. Pearce",
                        "slug": "D.-Pearce",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pearce",
                            "middleNames": [
                                "J.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pearce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848037"
                        ],
                        "name": "H. Hirsch",
                        "slug": "H.-Hirsch",
                        "structuredName": {
                            "firstName": "Hans-G\u00fcnter",
                            "lastName": "Hirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hirsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "We perform robust ASR experiments using the Aurora2 corpus [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb59dffc2f366518ea13a1399d76ce22c422ddf6",
            "isKey": false,
            "numCitedBy": 1980,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a database designed to evaluate the performance of speech recognition algorithms in noisy conditions. The database may either be used for the evaluation of front-end feature extraction algorithms using a defined HMM recognition back-end or complete recognition systems. The source speech for this database is the TIdigits, consisting of connected digits task spoken by American English talkers (downsampled to 8kHz) . A selection of 8 different real-world noises have been added to the speech over a range of signal to noise ratios and special care has been taken to control the filtering of both the speech and noise. The framework was prepared as a contribution to the ETSI STQ-AURORA DSR Working Group [1]. Aurora is developing standards for Distributed Speech Recognition (DSR) where the speech analysis is done in the telecommunication terminal and the recognition at a central location in the telecom network. The framework is currently being used to evaluate alternative proposals for front-end feature extraction. The database has been made publicly available through ELRA so that other speech researchers can evaluate and compare the performance of noise robust algorithms. Recognition results are presented for the first standard DSR feature extraction scheme that is based on a cepstral analysis."
            },
            "slug": "The-aurora-experimental-framework-for-the-of-speech-Pearce-Hirsch",
            "title": {
                "fragments": [],
                "text": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A database designed to evaluate the performance of speech recognition algorithms in noisy conditions and recognition results are presented for the first standard DSR feature extraction scheme that is based on a cepstral analysis."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804969"
                        ],
                        "name": "A. Ragni",
                        "slug": "A.-Ragni",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Ragni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ragni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 176
                            }
                        ],
                        "text": "Existing state-of-the-art methods for robust ASR use specialized domain knowledge to denoise the speech signal [1] or train a word-segment discriminative model robust to noise [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14500288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02841e798c9e75983970ed6a2f4756aab704197e",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been interest in combining generative and discriminative classifiers. In these classifiers features for the discriminative models are derived from the generative kernels. One advantage of using generative kernels is that systematic approaches exist to introduce complex dependencies into the feature-space. Furthermore, as the features are based on generative models standard model-based compensation and adaptation techniques can be applied to make discriminative models robust to noise and speaker conditions. This paper extends previous work in this framework in several directions. First, it introduces derivative kernels based on context-dependent generative models. Second, it describes how derivative kernels can be incorporated in structured discriminative models. Third, it addresses the issues associated with large number of classes and parameters when context-dependent models and high-dimensional feature-spaces of derivative kernels are used. The approach is evaluated on two noise-corrupted tasks: small vocabulary AURORA 2 and medium-to-large vocabulary AURORA 4 task."
            },
            "slug": "Derivative-kernels-for-noise-robust-ASR-Ragni-Gales",
            "title": {
                "fragments": [],
                "text": "Derivative kernels for noise robust ASR"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper introduces derivative kernels based on context-dependent generative models and describes how derivative kernels can be incorporated in structured discriminative models to address the issues associated with large number of classes and parameters when context- dependent models and high-dimensional feature-spaces of derivative kernels are used."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47778994"
                        ],
                        "name": "A. Lahiri",
                        "slug": "A.-Lahiri",
                        "structuredName": {
                            "firstName": "Ahbik",
                            "lastName": "Lahiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41227297"
                        ],
                        "name": "B. Prochnow",
                        "slug": "B.-Prochnow",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Prochnow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Prochnow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "We find this batch optimization technique to perform as well as the pre-training and stochastic gradient fine-tuning approach used in other deep learning work [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6076653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "053912e76e50c9f923a1fc1c173f1365776060cc",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining."
            },
            "slug": "On-optimization-methods-for-deep-learning-Le-Ngiam",
            "title": {
                "fragments": [],
                "text": "On optimization methods for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "Existing state-of-the-art methods for robust ASR use specialized domain knowledge to denoise the speech signal [1] or train a word-segment discriminative model robust to noise [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 171
                            }
                        ],
                        "text": "For comparison we include the error rates when using the original MFCC features, as well as the features produced by the ETSI2 advanced front end (AFE) denoising system [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced front-end feature extraction algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Advanced front-end feature extraction algorithm"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced front - end feature extraction algorithm , \u201d Technical Report"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Existing state-of-the-art methods for robust ASR use specialized domain knowledge to denoise the speech signal [1] or train a word-segment discriminative model robust to noise [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 169
                            }
                        ],
                        "text": "For comparison we include the error rates when using the original MFCC features, as well as the features produced by the ETSI2 advanced front end (AFE) denoising system [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced front-end feature extraction algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report. ETSI ES 202 050, 2007."
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 7,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 12,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Recurrent-Neural-Networks-for-Noise-Reduction-in-Maas-Le/c1bca434074c447a31fa227059baccee66b48387?sort=total-citations"
}