{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16685561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f49a73c42be6dbd851af4599d9911ea1d6ac7f4",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of significance for pairwise comparisons. Two experimental designs are recommended and supported by the DELVE software environment. \nTwo new non-parametric Bayesian learning methods relying on Gaussian process priors over functions are developed. These priors are controlled by hyperparameters which set the characteristic length scale for each input dimension. In the simplest method, these parameters are fit from the data using optimization. In the second, fully Bayesian method, a Markov chain Monte Carlo technique is used to integrate over the hyperparameters. One advantage of these Gaussian process methods is that the priors and hyperparameters of the trained models are easy to interpret. \nThe Gaussian process methods are benchmarked against several other methods, on regression tasks using both real data and data generated from realistic simulations. The experiments show that small datasets are unsuitable for benchmarking purposes because the uncertainties in performance measurements are large. A second set of experiments provide strong evidence that the bagging procedure is advantageous for the Multivariate Adaptive Regression Splines (MARS) method. \nThe simulated datasets have controlled characteristics which make them useful for understanding the relationship between properties of the dataset and the performance of different methods. The dependency of the performance on available computation time is also investigated. It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time. The Gaussian process methods are shown to consistently outperform the more conventional methods."
            },
            "slug": "Evaluation-of-gaussian-processes-and-other-methods-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for non-linear regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Kohavi (1995) shows that stratified 10- fold cross-validation produces fairly good estimates in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Question 6: Given a learning algorithm A and a small data set S, what is the accuracy of the classifiers produced by A when A is trained on new training sets of the same size as S? Kohavi (1995) shows that stratified 10fold cross-validation produces fairly good estimates in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 154
                            }
                        ],
                        "text": "Hence, we can measure the accuracy of C on the test set and construct a binomial confidence interval (Snedecor & Cochran, 1989; Efron & Tibshirani, 1993; Kohavi, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60538272,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "49f6fe73703ecad9271b23697da8902fe49348b4",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 360,
            "paperAbstract": {
                "fragments": [],
                "text": "In this doctoral dissertation, we study three basic problems in machine learning and two new hypothesis spaces with corresponding learning algorithms. The problems we investigate are: accuracy estimation, feature subset selection, and parameter tuning. The latter two problems are related and are studied under the wrapper approach. The hypothesis spaces we investigate are: decision tables with a default majority rule (DTMs) and oblivious read-once decision graphs (OODGs). For accuracy estimation, we investigate cross-validation and the~.632 bootstrap. We show examples where they fail and conduct a large scale study comparing them. We conclude that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters. We define the wrapper approach and use it for feature subset selection and parameter tuning. We relate definitions of feature relevancy to the set of optimal features, which is defined with respect to both a concept and an induction algorithm. The wrapper approach requires a search space, operators, a search engine, and an evaluation function. We investigate all of them in detail and introduce compound operators for feature subset selection. Finally, we abstract the search problem into search with probabilistic estimates. We introduce decision tables with a default majority rule (DTMs) to test the conjecture that feature subset selection is a very powerful bias. The accuracy of induced DTMs is surprisingly powerful, and we concluded that this bias is extremely important for many real-world datasets. We show that the resulting decision tables are very small and can be succinctly displayed. We study properties of oblivious read-once decision graphs (OODGs) and show that they do not suffer from some inherent limitations of decision trees. We describe a a general framework for constructing OODGs bottom-up and specialize it using the wrapper approach. We show that the graphs produced are use less features than C4.5, the state-of-the-art decision tree induction algorithm, and are usually easier for humans to comprehend."
            },
            "slug": "Wrappers-for-Performance-Enhancements-and-Oblivious-Kohavi",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance Enhancements and Oblivious Decision Graphs."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This doctoral dissertation concludes that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913246"
                        ],
                        "name": "E. B. Kong",
                        "slug": "E.-B.-Kong",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kong",
                            "middleNames": [
                                "Bae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. B. Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 73
                            }
                        ],
                        "text": "We then selected three difficult problems: the EXP6 problem developed by Kong and Dietterich (1995), the letter recognition data set (Frey & Slate, 1991), and the Pima Indians diabetes task (Merz & Murphy, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17043461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25744dbb4294fe7abb2d9b1b0d39006482ebb4ab",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Error-Correcting-Output-Coding-Corrects-Bias-and-Kong-Dietterich",
            "title": {
                "fragments": [],
                "text": "Error-Correcting Output Coding Corrects Bias and Variance"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Breiman (1994, 1996) has called this behavior \u201cinstability,\u201d and he has shown that this is a serious problem for the decision tree algorithms, such as CART (Breiman, Friedman, Olshen, & Stone, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 50814231,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4783d05ae9dd82a53883d33a767ef4ab0af789b",
            "isKey": false,
            "numCitedBy": 1122,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In model selection, usually a best predictor is chosen from a collection {\u03bc(.,s)} of predictors where \u03bc(.,s) is the minimum least-squares predictor in a collection U s of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in U s . If L is the data used to derive the sequence {\u03bc(., s)}, the procedure is called unstable if a small change in L can cause large changes in {\u03bc(., s)}. With a crystal ball, one could pick the predictor in {\u03bc(.,s)} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\u03bc(., s)} and then averaging over many such predictor sequences."
            },
            "slug": "Heuristics-of-instability-and-stabilization-in-Breiman",
            "title": {
                "fragments": [],
                "text": "Heuristics of instability and stabilization in model selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207705714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f31d46fdeb84cb848c1e3f19c3182f0b481963d2",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an entropic prior for multinomial parameter estimation problems and solve for its maximum a posteriori (MAP) estimator. The prior is a bias for maximally structured and minimally ambiguous models. In conditional probability models with hidden state, iterative MAP estimation drives weakly supported parameters toward extinction, effectively turning them off. Thus, structure discovery is folded into parameter estimation. We then establish criteria for simplifying a probabilistic model's graphical structure by trimming parameters and states, with a guarantee that any such deletion will increase the posterior probability of the model. Trimming accelerates learning by sparsifying the model. All operations monotonically and maximally increase the posterior probability, yielding structure-learning algorithms only slightly slower than parameter estimation via expectation-maximization and orders of magnitude faster than search-based structure induction. When applied to hidden Markov model training, the resulting models show superior generalization to held-out test data. In many cases the resulting models are so sparse and concise that they are interpretable, with hidden states that strongly correlate with meaningful categories."
            },
            "slug": "Structure-Learning-in-Conditional-Probability-via-Brand",
            "title": {
                "fragments": [],
                "text": "Structure Learning in Conditional Probability Models via an Entropic Prior and Parameter Extinction"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "An entropic prior is introduced for multinomial parameter estimation problems and the resulting models show superior generalization to held-out test data, and a guarantee that any such deletion will increase the posterior probability of the model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1437248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "957687487abf4f8a3bf6c61d5e4e7f62e10da2ab",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics. The advantage of our theory over the well-established Vapnik-Chervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior of learning curves. This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory. The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes.We illustrate our results with many concrete examples of learning curve bounds derived from our theory."
            },
            "slug": "Rigorous-Learning-Curve-Bounds-from-Statistical-Haussler-Kearns",
            "title": {
                "fragments": [],
                "text": "Rigorous Learning Curve Bounds from Statistical Mechanics"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics is introduced, which can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713687"
                        ],
                        "name": "M. Kub\u00e1t",
                        "slug": "M.-Kub\u00e1t",
                        "structuredName": {
                            "firstName": "Miroslav",
                            "lastName": "Kub\u00e1t",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kub\u00e1t"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22521683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eebec8fcef7f3ec4deabf50c5182eb21dd1f2000",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Successful implementations of radial-basis function (RBF) networks for classification tasks must deal with architectural issues, the burden of irrelevant attributes, scaling, and some other problems. This paper addresses these issues by initializing RBF networks with decision trees that define relatively pure regions in the instance space; each of these regions then determines one basis function. The resulting network is compact, easy to induce, and has favorable classification accuracy."
            },
            "slug": "Decision-trees-can-initialize-radial-basis-function-Kub\u00e1t",
            "title": {
                "fragments": [],
                "text": "Decision trees can initialize radial-basis function networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper addresses issues by initializing RBF networks with decision trees that define relatively pure regions in the instance space; each of these regions then determines one basis function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583597"
                        ],
                        "name": "J. Kolen",
                        "slug": "J.-Kolen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kolen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kolen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Then the null hypothesis can be written as\nPrR,x[ f\u0302A(x) = f (x)] = PrR,x[ f\u0302B(x) = f (x)], where the notation PrR,x indicates the probability taken with respect to the random draws of the training set R and the test example x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "The resulting learned network depends critically on the random starting state (Kolen & Pollack, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13373157,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4cb3e976daead7e2a69b315015d9672f1c2f5e7c",
            "isKey": false,
            "numCitedBy": 367,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result, additional deterministic experiments were performed. The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration."
            },
            "slug": "Backpropagation-is-Sensitive-to-Initial-Conditions-Kolen-Pollack",
            "title": {
                "fragments": [],
                "text": "Backpropagation is Sensitive to Initial Conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique and the magnitude of the initial condition vector is a very significant parameter in convergence time variability is explored."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70641663"
                        ],
                        "name": "H. Lindman",
                        "slug": "H.-Lindman",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Lindman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lindman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "The Quasi-F test (Lindman, 1992) is applied to determine whether the effect due to the choice of learning algorithms is significantly nonzero."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Then the null hypothesis can be written as\nPrR,x[ f\u0302A(x) = f (x)] = PrR,x[ f\u0302B(x) = f (x)], where the notation PrR,x indicates the probability taken with respect to the random draws of the training set R and the test example x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62013388,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "c1835cadb938f71f361ac615b8c3b6ea151c08ee",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is an introductory text on the analysis of variance and also a reference tool for the researcher into variance analysis. It stresses application rather than theory, covers major techniques including post hoc testing and discriminates between different research designs. The book is intended for graduate students in any field where statistics is used."
            },
            "slug": "Analysis-of-Variance-in-Experimental-Design-Lindman",
            "title": {
                "fragments": [],
                "text": "Analysis of Variance in Experimental Design"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is intended for graduate students in any field where statistics is used and covers major techniques including post hoc testing and discriminates between different research designs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84729464"
                        ],
                        "name": "W. J. Langford",
                        "slug": "W.-J.-Langford",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Langford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. J. Langford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "This means that we must use some form of resampling (i.e., cross-validation or the bootstrap) to perform the statistical analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "Hence, we can measure the accuracy of C on the test set and construct a binomial confidence interval (Snedecor & Cochran, 1989; Efron & Tibshirani, 1993; Kohavi, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "To evaluate the type I error rates of our four statistical tests with real learning algorithms, we needed to find two learning algorithms that had identical performance when trained on training sets of a given size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "A second simple statistical test is based on measuring the difference between the error rate of algorithm A and the error rate of algorithm B (Snedecor & Cochran, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 31
                            }
                        ],
                        "text": "For each size of training set (270, 200, and 150), we adjusted the distance metric for NN so that the average performance (over all 1000 data sets, measured on the 22,801 calibration examples) matched the average performance of C4.5 to within 0.1%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 80
                            }
                        ],
                        "text": "This estimate focuses on the probability of disagreement of the two algorithms (Snedecor & Cochran, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4147901,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "cb802c6630216808f956c3729af88ffe7ae1f45b",
            "isKey": true,
            "numCitedBy": 9654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Principles of Statistical TechniquesA First Course, from the Beginnings, for Schools and Universities. By P. G. Moore. Pp. viii + 239. (Cambridge: At the University Press, 1958.) 22s. 6d. net."
            },
            "slug": "Statistical-Methods-Langford",
            "title": {
                "fragments": [],
                "text": "Statistical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Principles of Statistical TechniquesA First Course, from the Beginnings, for Schools and Universities, for schools and Universities."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66128781"
                        ],
                        "name": "J. J. Biundo",
                        "slug": "J.-J.-Biundo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Biundo",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Biundo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 24
                            }
                        ],
                        "text": "To apply McNemar\u2019s test (Everitt, 1977), we divide our available sample of data S into a training set R and a test set T."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 25
                            }
                        ],
                        "text": "Figure 5 shows the probability of making a type I error for each of the five procedures when the data set S contains 300 examples and the overall error rate \u00b2 was varied from 0.10 to 0.40."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57734438,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "79e3c1fd60b277f2f45c4e69e93aca603c9929ed",
            "isKey": true,
            "numCitedBy": 918,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-Contingency-Tables-Biundo",
            "title": {
                "fragments": [],
                "text": "Analysis of Contingency Tables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335246"
                        ],
                        "name": "C. Merz",
                        "slug": "C.-Merz",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Merz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Merz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209099422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cf9167aeb2782651156de5e22cad82ee69a225",
            "isKey": false,
            "numCitedBy": 1982,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-Machine-Learning-Databases-Merz",
            "title": {
                "fragments": [],
                "text": "UCI Repository of Machine Learning Databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240660"
                        ],
                        "name": "B. Dasarathy",
                        "slug": "B.-Dasarathy",
                        "structuredName": {
                            "firstName": "Belur",
                            "lastName": "Dasarathy",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dasarathy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60461418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b1d3ec2e6fe49aaf8dc068b8a812e9ef3f163fa",
            "isKey": false,
            "numCitedBy": 1939,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nearest-neighbor-(NN)-norms:-NN-pattern-techniques-Dasarathy",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor (NN) norms: NN pattern classification techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 100
                            }
                        ],
                        "text": "One approach, advocated by the DELVE project (Hinton, Neal, Tibshirani, & DELVE team members, 1995; Rasmussen, 1996), is to subdivide S into a test set and several disjoint training sets of size m."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 89
                            }
                        ],
                        "text": "Then the null hypothesis can be written as\nPrR,x[ f\u0302A(x) = f (x)] = PrR,x[ f\u0302B(x) = f (x)], where the notation PrR,x indicates the probability taken with respect to the random draws of the training set R and the test example x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for nonlinear regression. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for nonlinear regression. Unpublished doctoral dissertation"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Kohavi (1995) shows that stratified 10- fold cross-validation produces fairly good estimates in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 154
                            }
                        ],
                        "text": "Hence, we can measure the accuracy of C on the test set and construct a binomial confidence interval (Snedecor & Cochran, 1989; Efron & Tibshirani, 1993; Kohavi, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrappers for performance enhancement and oblivious decision graphs. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Assessing learning procedures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of ID 3 and backpropagation forEnglish texttospeech mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "& DELVE team members Assessing learning procedures using DELVE"
            },
            "venue": {
                "fragments": [],
                "text": "& DELVE team members Assessing learning procedures using DELVE"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 58
                            }
                        ],
                        "text": "For example, suppose we want to sell an e-mail system that learns to recognize and filter junk mail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Assessing learning procedures using DELVE"
            },
            "venue": {
                "fragments": [],
                "text": "Assessing learning procedures using DELVE"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received October"
            },
            "venue": {
                "fragments": [],
                "text": "Received October"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "Figure 3 shows the measured values of \u00b2(x) for a population of 7670 points with respect to the C4.5 decision tree algorithm (Quinlan, 1993) trained on randomly drawn training sets of 100 examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 42
                            }
                        ],
                        "text": "To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for empirical learning"
            },
            "venue": {
                "fragments": [],
                "text": "C4.5: Programs for empirical learning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "A second simple statistical test is based on measuring the difference between the error rate of algorithm A and the error rate of algorithm B (Snedecor & Cochran, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 80
                            }
                        ],
                        "text": "This estimate focuses on the probability of disagreement of the two algorithms (Snedecor & Cochran, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "Hence, we can measure the accuracy of C on the test set and construct a binomial confidence interval (Snedecor & Cochran, 1989; Efron & Tibshirani, 1993; Kohavi, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical methods (8th ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rigorous learning curve bounds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Assessing learning procedures using DELVE (Tech. Rep.) Toronto: University of Toronto, Department"
            },
            "venue": {
                "fragments": [],
                "text": "DELVE team members"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and regression trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and regression trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "& DELVE team members Assessing learning procedures using DELVE (Tech. Rep.) Toronto"
            },
            "venue": {
                "fragments": [],
                "text": "& DELVE team members Assessing learning procedures using DELVE (Tech. Rep.) Toronto"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Assessing learning procedures using DELVE (Tech. Rep.) Toronto: University of Toronto, Department of Computer Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Approximate-Statistical-Tests-for-Comparing-Dietterich/22f0579f212dfb568fbda317cba67c8654d84ccd?sort=total-citations"
}