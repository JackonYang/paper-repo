{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144844988"
                        ],
                        "name": "Feng-hsiung Hsu",
                        "slug": "Feng-hsiung-Hsu",
                        "structuredName": {
                            "firstName": "Feng-hsiung",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng-hsiung Hsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 177
                            }
                        ],
                        "text": "Roughly three years ago, IBM Research was looking for a major research challenge to rival the scientific and popular interest of Deep Blue, the computer chess-playing champion (Hsu 2002), that also would have clear relevance to IBM business interests."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "Finally, the Jeopardy Challenge represents a unique and compelling AI question similar to the one underlying DeepBlue (Hsu 2002) \u2014 can a computer system be designed to compete against the best humans at a task thought to require high levels of human intelligence, and if so, what kind of technology, algorithms, and engineering is required?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "Finally, the Jeopardy Challenge represents a unique and compelling AI question similar to the one underlying DeepBlue (Hsu 2002) \u2014 can a computer system be designed to compete against the best humans at a task thought to require high levels of human intelligence, and if so, what kind of technology,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 109812703,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "c493e050b6e4086de44e52bededc25cf19a62d8d",
            "isKey": true,
            "numCitedBy": 243,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nOn May 11, 1997, as millions worldwide watched a stunning victory unfold on television, a machine shocked the chess world by defeating the defending world champion, Garry Kasparov. Written by the man who started the adventure, Behind Deep Blue reveals the inside story of what happened behind the scenes at the two historic Deep Blue vs. Kasparov matches. This is also the story behind the quest to create the mother of all chess machines. The book unveils how a modest student project eventually produced a multimillion dollar supercomputer, from the development of the scientific ideas through technical setbacks, rivalry in the race to develop the ultimate chess machine, and wild controversies to the final triumph over the world's greatest human player. \nIn nontechnical, conversational prose, Fenghsiung Hsu, the system architect of Deep Blue, tells us how he and a small team of fellow researchers forged ahead at IBM with a project they'd begun as students at Carnegie Mellon in the mid-1980s: the search for one of the oldest holy grails in artificial intelligence -- a machine that could beat any human chess player in a bona fide match. Back in 1949 science had conceived the foundations of modern chess computers but not until almost fifty years later -- until Deep Blue -- would the quest be realized. \nHsu refutes Kasparov's controversial claim that only human intervention could have allowed Deep Blue to make its decisive, \"uncomputerlike\" moves. In riveting detail he describes the heighteing tension in this war of brains and nerves, the \"smoldering fire\" in Kasparov's eyes. Behind Deep Blue is not just another tale of man versus machine. This fascinating book tells us how man as genius was given an ultimate, unforgettable run for his mind, no, not by the genius of a computer, but of man as toolmaker."
            },
            "slug": "Behind-Deep-Blue:-Building-the-Computer-that-the-Hsu",
            "title": {
                "fragments": [],
                "text": "Behind Deep Blue: Building the Computer that Defeated the World Chess Champion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791072"
                        ],
                        "name": "T. Strzalkowski",
                        "slug": "T.-Strzalkowski",
                        "structuredName": {
                            "firstName": "Tomek",
                            "lastName": "Strzalkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Strzalkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60071948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1b873a39941f73b415d9463d0b0e21b3495b484",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated question answering - the ability of a machine to answer questions, simple or complex, posed in ordinary human language - is one of todays most exciting technological developments. It has all the markings of a disruptive technology, one that is poised to displace the existing search methods and establish new standards for user-centered access to information. This book gives a comprehensive and detailed look at the current approaches to automated question answering. The level of presentation is suitable for newcomers to the field as well as for professionals wishing to study this area and/or to build practical QA systems. The book can serve as a \"how-to\" handbook for IT practitioners and system developers. It can also be used to teach advanced graduate courses in Computer Science, Information Science and related disciplines. The readers will acquire in-depth practical knowledge of this critical new technology."
            },
            "slug": "Advances-in-Open-Domain-Question-Answering-Strzalkowski-Harabagiu",
            "title": {
                "fragments": [],
                "text": "Advances in Open Domain Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book gives a comprehensive and detailed look at the current approaches to automated question answering, and will acquire in-depth practical knowledge of this critical new technology."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215762892,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0d8bf6be792deb9b7e499b361a8332f0dce68089",
            "isKey": false,
            "numCitedBy": 536,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The TREC question answering track is an effort to bring the benefits of large-scale evaluation to bear on the question answering problem. The track contained two tasks in TREC 2002, the main task and the list task. Both tasks required that the answer strings returned by the systems consist of nothing more or less than an answer in contrast to the text snippets containing an answer allowed in previous years. A new evaluation measure in the main task, the confidence-weighted score, tested a system\u2019s ability to recognize when it has found a correct answer. The goal of the question answering (QA) track is to foster research on systems that retrieve answers rather than documents in response to a question, with particular emphasis on systems that can function in unrestricted domains. Now in its fourth year, the tasks in the track have evolved over the years to increase the realism of the task and to focus research on particular aspects of the problem deemed important to improving the state-of-the-art. All of the tasks have involved finding answers to closed-class questions within a large corpus of news text. This paper provides an overview of the TREC 2002 QA track. This year\u2019s track contained two tasks, the main task and the list task. Both tasks were also run in TREC 2001, but systems were required to return exact answers this year. That is, the text string returned by the system in response to a question was required to consist of a complete answer and nothing else, in contrast to earlier years where systems could return text strings that simply contained an answer. To make the paper self-contained, the first section recaps the tasks and evaluation procedures used in the first three tracks. The following sections then describe this year\u2019s tasks. 1 Evolution of the TREC QA Track The task in the first two QA tracks (TRECs 8 and 9) was the same. For each question in the question set, systems retrieved a ranked list of up to five text snippets that contained an answer to the question plus a document that supported the answer. The collection of documents from which the support was drawn was a large set of newswire and newspaper articles. The questions were restricted to factoid questions such as In what year did Joe DiMaggio compile his 56game hitting streak? and Name a film in which Jude Law acted. Each question was guaranteed to have at least one document in the collection that explicitly answered it. The maximum length of the text snippets was either 50 or 250 bytes, depending on the run type. Human assessors read each string and decided whether the string actually did contain an answer to the question in the context provided by the document. Given a set of judgments for the strings, the score computed for a submission was the mean reciprocal rank. An individual question received a score equal to the reciprocal of the rank at which the first correct response was returned, or zero if none of the five responses contained a correct answer. The score for a submission was then the mean of the individual questions\u2019 reciprocal ranks. The TREC-8 track both defined how answer strings were judged, and established that different assessors have different ideas as to what constitutes a correct answer even for the limited type of questions used in the track. A [document-id, answer-string] pair was judged correct if, in the opinion of the assessor, the answer-string contained an answer to the question, the answer-string was responsive to the question, and the document supported the answer. If the answer-string was responsive and contained a correct answer, but the document did not support that answer, the pair was judged \u201cNot supported\u201d. Otherwise, the pair was judged incorrect. Requiring that the answer string be responsive to the question addressed a variety of issues. Answer strings that contained multiple entities of the same semantic category as the correct answer but did not indicate which of those entities was the actual answer (e.g., a list of names in response to a who question) were judged as incorrect. Certain punctuation and units were also required. Thus \u201c5 5 billion\u2019\u2019 was not an acceptable substitute for \u201c5.5 billion\u201d, nor was \u201c500\u201d acceptable when the correct answer was \u201c$500\u201d. Finally, unless the question specifically stated otherwise, correct responses for questions about a famous entity had to refer to the famous entity and not to imitations, copies, etc. For example, two TREC-8 questions asked for the height of the Matterhorn (i.e., the Alp) and the replica of the Matterhorn at Disneyland. Correct responses for one of these questions were incorrect for the other. See [6] for a very detailed discussion of responsiveness. To test whether assessor opinions vary, each TREC-8 question was independently judged by three different assessors. The separate judgments were combined into a single judgment set through adjudication for the official track evaluation, but the individual judgments were used to measure the effect of differences in judgments on systems\u2019 scores. Assessors opinions did vary. For example, assessors differed on how much of a name was required and on the desired granularity of dates and locations. Fortunately, as with document retrieval evaluation, the relative mean reciprocal rank scores between QA systems remain stable despite differences in the judgments used to evaluate them [5]. The TREC 2001 track modified the main task to make it more realistic and introduced two new tasks, the list task and the context task. All runs were restricted to answer strings of maximum length 50 bytes since the results from the earlier tracks clearly demonstrated that allowing 250-byte answer strings was a much simpler problem. In the main task, the guarantee that a question had an answer in the document collection was eliminated. A system returned the string \u201cNIL\u201d to indicate its belief that there was no answer in the document collection. NIL was marked correct if there was no known answer for that question in the collection and incorrect otherwise. The list task required systems to assemble an answer from information located in multiple documents. Such questions are harder to answer than the questions used in the main task since information duplicated in the documents must be detected and reported only once. Each question in the list task specified a particular kind of information to be retrieved, such as Who are 6 actors who have played Tevye in \u201cFiddler on the Roof\u201d?. Systems returned an unordered list of [document-id, answer-string] pairs where each pair represented a single instance. Results were scored using mean accuracy, which is the ratio of the number of distinct correct responses retrieved to the target number of responses requested. The context task was a pilot evaluation for question answering within a particular scenario or context. The task was designed to represent the kind of dialog processing that a system would need to support an interactive user session. Questions were grouped into different series, and the QA system was expected to track the discourse objects across the individual questions of a series. Unfortunately, the results in the pilot were completely dominated by whether or not a system could answer the particular type of question: the ability to correctly answer questions later in a series was uncorrelated with the ability to correctly answer questions earlier in the series. Thus the task was not repeated in TREC 2002. 2 The TREC 2002 QA Track The TREC 2002 track repeated the main and list tasks from 2001, but with the major difference of requiring systems to return exact answers. The change to exact answers was motivated by the belief that a system\u2019s ability to recognize the precise extent of the answer is crucial to improving question answering technology. The problems with using text snippets containing the answer as responses were illustrated in the TREC 2001 track. For example, each of the answer strings shown in Figure 1 was judged correct for the question What river in the US is known as the Big Muddy?, yet earlier responses are clearly better than later ones. Judging only exact answers correct forces systems to demonstrate that they know precisely where the answer lies in such strings. What constitutes an \u201cexact answer\u201d? As with correctness, exactness is essentially a personal opinion. NIST provided guidelines to the assessors so that questions would be judged similarly, but in the end whether or not an answer was exact was up to the assessor. The guidelines given to the assessors are reproduced in Figure 2. Notice that even \u201cgood\u201d responses that contain a correct answer and justification for that answer were considered inexact for the purposes of this evaluation. A system response consisting of an [document-id, answer-string] pair was assigned exactly one judgment by a human assessor as follows: wrong: the answer string does not contain a correct answer or the answer is not responsive; not supported: the answer string contains a correct answer but the document returned does not support that answer; not exact: the answer string contains a correct answer and the document supports that answer, but the string contains more than just the answer (or is missing bits of the answer);"
            },
            "slug": "Overview-of-the-TREC-2002-Question-Answering-Track-Voorhees",
            "title": {
                "fragments": [],
                "text": "Overview of the TREC 2002 Question Answering Track"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper provides an overview of the TREC 2002 QA track, which defined how answer strings were judged, and established that different assessors have different ideas as to what constitutes a correct answer even for the limited type of questions used in the track."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295799"
                        ],
                        "name": "D. Ferrucci",
                        "slug": "D.-Ferrucci",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ferrucci",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ferrucci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144071952"
                        ],
                        "name": "Adam Lally",
                        "slug": "Adam-Lally",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lally",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lally"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "DeepQA is developed using Apache UIMA,10 a framework implementation of the Unstructured Information Management Architecture (Ferrucci and Lally 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "Speed and Scaleout DeepQA is developed using Apache UIMA,10 a framework implementation of the Unstructured Information Management Architecture (Ferrucci and Lally 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26266327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f8eb04dbafdfda997ac5e06cd6c521f82bf4e4c",
            "isKey": false,
            "numCitedBy": 997,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "IBM Research has over 200 people working on Unstructured Information Management (UIM) technologies with a strong focus on Natural Language Processing (NLP). These researchers are engaged in activities ranging from natural language dialog, information retrieval, topic-tracking, named-entity detection, document classification and machine translation to bioinformatics and open-domain question answering. An analysis of these activities strongly suggested that improving the organization's ability to quickly discover each other's results and rapidly combine different technologies and approaches would accelerate scientific advance. Furthermore, the ability to reuse and combine results through a common architecture and a robust software framework would accelerate the transfer of research results in NLP into IBM's product platforms. Market analyses indicating a growing need to process unstructured information, specifically multilingual, natural language text, coupled with IBM Research's investment in NLP, led to the development of middleware architecture for processing unstructured information dubbed UIMA. At the heart of UIMA are powerful search capabilities and a data-driven framework for the development, composition and distributed deployment of analysis engines. In this paper we give a general introduction to UIMA focusing on the design points of its analysis engine architecture and we discuss how UIMA is helping to accelerate research and technology transfer."
            },
            "slug": "UIMA:-an-architectural-approach-to-unstructured-in-Ferrucci-Lally",
            "title": {
                "fragments": [],
                "text": "UIMA: an architectural approach to unstructured information processing in the corporate research environment"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A general introduction to U IMA is given focusing on the design points of its analysis engine architecture and how UIMA is helping to accelerate research and technology transfer is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701063"
                        ],
                        "name": "M. Maybury",
                        "slug": "M.-Maybury",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Maybury",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maybury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "It has had a long history (Simmons 1970) and saw rapid advancement spurred by system building, experimentation, and government funding in the past decade (Maybury 2004, Strzalkowski and Harabagiu 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15050823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fce69347160f418e26a78229a59e1c87ada20ae",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Question answering systems, which provide natural language responses to natural language queries, are the subject of rapidly advancing research encompassing both academic study and commercial applications, the most well-known of which is the search engine Ask Jeeves. Question answering draws on different fields and technologies, including natural language processing, information retrieval, explanation generation, and human computer interaction. Question answering creates an important new method of information access and can be seen as the natural step beyond such standard Web search methods as keyword query and document retrieval. This collection charts significant new directions in the field, including temporal, spatial, definitional, biographical, multimedia, and multilingual question answering.After an introduction that defines essential terminology and provides a roadmap to future trends, the book covers key areas of research and development. These include current methods, architecture requirements, and the history of question answering on the Web; the development of systems to address new types of questions; interactivity, which is often required for clarification of questions or answers; reuse of answers; advanced methods; and knowledge representation and reasoning used to support question answering. Each section contains an introduction that summarizes the chapters included and places them in context, relating them to the other chapters in the book as well as to the existing literature in the field and assessing the problems and challenges that remain."
            },
            "slug": "New-Directions-in-Question-Answering-Maybury",
            "title": {
                "fragments": [],
                "text": "New Directions in Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Key areas of research and development include current methods, architecture requirements, and the history of question answering on the Web; the development of systems to address new types of questions; interactivity; reuse of answers; advanced methods; and knowledge representation and reasoning used to support question answering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704635"
                        ],
                        "name": "D. Lenat",
                        "slug": "D.-Lenat",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Lenat",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lenat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 51
                            }
                        ],
                        "text": "[2003]), deep semantic relationships (for example, Lenat [1995], Paritosh and Forbus [2005]), or both ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16147141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1e7bf85c7caf1306fa27802218a8e2cdc8f4268",
            "isKey": false,
            "numCitedBy": 2244,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Since 1984, a person-century of effort has gone into building CYC, a universal schema of roughly 105 general concepts spanning human reality. Most of the time has been spent codifying knowledge about these concepts; approximately 106 commonsense axioms have been handcrafted for and entered into CYC's knowledge base, and millions more have been inferred and cached by CYC. This article examines the fundamental assumptions of doing such a large-scale project, reviews the technical lessons learned by the developers, and surveys the range of applications that are or soon will be enabled by the technology."
            },
            "slug": "CYC:-a-large-scale-investment-in-knowledge-Lenat",
            "title": {
                "fragments": [],
                "text": "CYC: a large-scale investment in knowledge infrastructure"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The fundamental assumptions of doing such a large-scale project are examined, the technical lessons learned by the developers are reviewed, and the range of applications that are or soon will be enabled by the technology is surveyed."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143745511"
                        ],
                        "name": "R. F. Simmons",
                        "slug": "R.-F.-Simmons",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Simmons",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F. Simmons"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10329681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7869fc49e9650b164a18f618f02b318ae41709f5",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent experiments in programming natural language question-answering systems are reviewed to summarize the methods that have been developed for syntactic, semantic, and logical analysis of English strings. It is concluded that at least minimally effective techniques have been devised for answering questions from natural language subsets in small scale experimental systems and that a useful paradigm has evolved to guide research efforts in the field. Current approaches to semantic analysis and logical inference are seen to be effective beginnings but of questionable generality with respect either to subtle aspects of meaning or to applications over large subsets of English. Generalizing from current small-scale experiments to language-processing systems based on dictionaries with thousands of entries\u2014with correspondingly large grammars and semantic systems\u2014may entail a new order of complexity and require the invention and development of entirely different approaches to semantic analysis and question answering."
            },
            "slug": "Natural-language-question-answering-systems:-1969-Simmons",
            "title": {
                "fragments": [],
                "text": "Natural language question-answering systems: 1969"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is concluded that at least minimally effective techniques have been devised for answering questions from natural language subsets in small scale experimental systems and that a useful paradigm has evolved to guide research efforts in the field."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113262829"
                        ],
                        "name": "Christine Clark",
                        "slug": "Christine-Clark",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christine Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2248349"
                        ],
                        "name": "Steven J. Maiorano",
                        "slug": "Steven-J.-Maiorano",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Maiorano",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven J. Maiorano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34491971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aee98350f29a407596d782b09bb37e0f94a7a76f",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%."
            },
            "slug": "COGEX:-A-Logic-Prover-for-Question-Answering-Moldovan-Clark",
            "title": {
                "fragments": [],
                "text": "COGEX: A Logic Prover for Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The idea of automated reasoning applied to question answering is introduced and the feasibility of integrating a logic prover into a Question Answering system is shown."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662034"
                        ],
                        "name": "C. Engelman",
                        "slug": "C.-Engelman",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Engelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Engelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1044201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbe42997734010b1dbc9c44a62af29d60aecd16",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural language database query system incorporated in the KNOBS interactive planning system comprises a dictionary driven parser, APE-II, and script interpreter which yield a conceptual dependency conceptualization as a representation of the meaning of user input. A conceptualization pattern matching production system then determines and executes a procedure for extracting the desired information from the database. In contrast to syntax driven Q-A systems, e.g., those based on ATN parsers, APE-II is driven bottom-up by expectations associated with word meanings. The processing of a query is based on the contents of several knowledge sources including the dictionary entries (partial conceptualizations and their expectations), frames representing conceptual dependency primitives, scripts which contain stereotypical knowledge about planning tasks used to infer states enabling or resulting from actions, and two production system rule bases for the inference of implicit case fillers, and for determining the responsive database search. The goals of this approach, all of which are currently at least partially achieved, include utilizing similar representations for questions with similar meanings but widely varying surface structures, developing a powerful mechanism for the disambiguation of words with multiple meanings and the determination of pronoun referents, answering questions which require inferences to be understood, and interpreting ellipses and ungrammatical utterances."
            },
            "slug": "Knowledge-Based-Question-Answering-Pazzani-Engelman",
            "title": {
                "fragments": [],
                "text": "Knowledge Based Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The natural language database query system incorporated in the KNOBS interactive planning system comprises a dictionary driven parser, APE-II, and script interpreter which yield a conceptual dependency conceptualization as a representation of the meaning of user input."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5895004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1291583873fb890e7922ec0dfefd4846df46c9",
            "isKey": false,
            "numCitedBy": 5479,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stacked-generalization-Wolpert",
            "title": {
                "fragments": [],
                "text": "Stacked generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfd4259d305a00f13d5f08841230389f61322422",
            "isKey": false,
            "numCitedBy": 4304,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples."
            },
            "slug": "Optimizing-search-engines-using-clickthrough-data-Joachims",
            "title": {
                "fragments": [],
                "text": "Optimizing search engines using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 65
                            }
                        ],
                        "text": "Motivated by hierarchical techniques such as mixture of experts (Jacobs et al. 1991) and stacked generalization (Wolpert 1992), a metalearner is trained over this ensemble."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 572361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
            "isKey": false,
            "numCitedBy": 4007,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network."
            },
            "slug": "Adaptive-Mixtures-of-Local-Experts-Jacobs-Jordan",
            "title": {
                "fragments": [],
                "text": "Adaptive Mixtures of Local Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases, which is demonstrated to be able to be solved by a very simple expert network."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791311"
                        ],
                        "name": "U. Hermjakob",
                        "slug": "U.-Hermjakob",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Hermjakob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Hermjakob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144995564"
                        ],
                        "name": "M. Rey",
                        "slug": "M.-Rey",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Rey",
                            "middleNames": [
                                "del"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1258994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94fe3e6a5cc74138288515a04a75b0fb712c3d9e",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Webclopedia Question Answering system, in which methods to automatically learn patterns and parameterizations are combined with hand-crafted rules and concept ontologies. The source for answers is a collection of 1 million newspaper texts, distributed by NIST. In general, two kinds of knowledge are used by Webclopedia to answer questions: knowledge about language and knowledge about the world. The former is embodied both in the Information Retrieval engine that identifies likely answer sentences and the CONTEX parser that analyses the input question and candidate answers. The latter is embodied in a QA typology of some 140 nodes, a concept ontology of some 10,000 concepts, and the IR engine\u2019s ranking algorithm that takes typical document structure into account."
            },
            "slug": "Knowledge-Based-Question-Answering-Hermjakob-Hovy",
            "title": {
                "fragments": [],
                "text": "Knowledge-Based Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "The Webclopedia Question Answering system is described, in which methods to automatically learn patterns and parameterizations are combined with hand-crafted rules and concept ontologies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37614272"
                        ],
                        "name": "Jeongwoo Ko",
                        "slug": "Jeongwoo-Ko",
                        "structuredName": {
                            "firstName": "Jeongwoo",
                            "lastName": "Ko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeongwoo Ko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144287919"
                        ],
                        "name": "Eric Nyberg",
                        "slug": "Eric-Nyberg",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nyberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nyberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145388187"
                        ],
                        "name": "Luo Si",
                        "slug": "Luo-Si",
                        "structuredName": {
                            "firstName": "Luo",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luo Si"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11834211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41824a8ae2cbc93c127101c3ecc8095b43ebab0c",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models have been applied to various information retrieval and natural language processing tasks in the recent literature. In this paper, we apply a probabilistic graphical model for answer ranking in question answering. This model estimates the joint probability of correctness of all answer candidates, from which the probability of correctness of an individual candidate can be inferred. The joint prediction model can estimate both the correctness of individual answers as well as their correlations, which enables a list of accurate and comprehensive answers. This model was compared with a logistic regression model which directly estimates the probability of correctness of each individual answer candidate. An extensive set of empirical results based on TREC questions demonstrates the effectiveness of the joint model for answer ranking. Furthermore, we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking."
            },
            "slug": "A-probabilistic-graphical-model-for-joint-answer-in-Ko-Nyberg",
            "title": {
                "fragments": [],
                "text": "A probabilistic graphical model for joint answer ranking in question answering"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A probabilistic graphical model is applied for answer ranking in question answering which estimates the joint probability of correctness of all answer candidates, from which the probability of Correctness of an individual candidate can be inferred."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990264"
                        ],
                        "name": "Praveen K. Paritosh",
                        "slug": "Praveen-K.-Paritosh",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Paritosh",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Praveen K. Paritosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11226861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da34f3b459aad01222ffe301a877d8f999ffd456",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Back of the envelope (BotE) reasoning involves generating quantitative answers in situations where exact data and models are unavailable and where available data is often incomplete and/or inconsistent. A rough estimate generated quickly is more valuable and useful than a detailed analysis, which might be unnecessary, impractical, or impossible because the situation does not provide enough time, information, or other resources to perform one. Such reasoning is a key component of commonsense reasoning about everyday physical situations. We present an implemented system, BotE-Solver, that can solve about a dozen estimation questions like \"What is the annual cost of healthcare in USA?\" from different domains using a library of strategies and the Cyc knowledge base. BotE-Solver is a general-purpose problem solving framework that uses strategies represented as suggestions, and keeps track of problem solving progress in an AND/OR tree. A key contribution of this paper is a knowledge level analysis [Newell, 1982] of the strategic knowledge used in BotE reasoning. We present a core collection of seven powerful estimation strategies that provides broad coverage for such problem solving. We hypothesize that this is the complete set of back of the envelope problem solving strategies. We present twofold support for this hypothesis: 1) an empirical analysis of all problems (n=44) on Force and Pressure, Rotation and Mechanics, Heat, and Astronomy from Clifford Swartz's \"Back-of-the-Envelope Physics\" [Swartz, 2003], and 2) an analysis of strategies used by BotE-Solver."
            },
            "slug": "Analysis-of-Strategic-Knowledge-in-Back-of-the-Paritosh-Forbus",
            "title": {
                "fragments": [],
                "text": "Analysis of Strategic Knowledge in Back of the Envelope Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an implemented system that can solve about a dozen estimation questions like \"What is the annual cost of healthcare in USA?\" from different domains using a library of strategies and the Cyc knowledge base, and hypothesizes that this is the complete set of back of the envelope problem solving strategies."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 171
                            }
                        ],
                        "text": "Another step in the content-acquisition process is to identify and collect these resources, which include databases, taxonomies, and ontologies, such as dbPedia,7 WordNet (Miller 1995), and the Yago8 ontology."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "WordNet: A Lexical Database for English."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52886585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this project is to provide lexical resources for natural language research. The primary emphases are on the further development and dissemination of the on-line lexical database, WordNet. A secondary goal is to learn how to develop contextual representations for different senses of a polysemous word, where a contextual representation is comprised of topical and local context for each sense."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The goal of this project is to provide lexical resources for natural language research and to learn how to develop contextual representations for different senses of a polysemous word."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782853"
                        ],
                        "name": "Mark Dredze",
                        "slug": "Mark-Dredze",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Dredze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Dredze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 1
                            }
                        ],
                        "text": "(Dredze, Crammer, and Pereira 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 708332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7af09246bae1d2d9abada79f441ba25858c69ef9",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training."
            },
            "slug": "Confidence-weighted-linear-classification-Dredze-Crammer",
            "title": {
                "fragments": [],
                "text": "Confidence-weighted linear classification"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Empirical evaluation on a range of NLP tasks show that the confidence-weighted linear classifiers introduced here improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13888,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713476"
                        ],
                        "name": "M. McCord",
                        "slug": "M.-McCord",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McCord",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McCord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 38540559,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "d6bdd8bc6e1cad210ae9efab92872b08ff5a1363",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Slot Grammar makes it easier to write practical, broad-coverage natural language grammars, for the following reasons. (a) The system has a lexicalist character; although there are grammar rules, they are fewer in number and simpler because analysis is largely data-driven through use of slots taken from lexical entries. (b) There is a modular treatment of different grammatical phenomena through different rule types, for instance rule types for expressing linear ordering constraints. This modularity also reduces the differences between the Slot Grammars of different languages. (c) Several grammatical phenomena, such as coordination and extraposition, are treated mainly in a language-independent shell provided with the system."
            },
            "slug": "Slot-Grammar:-A-System-for-Simpler-Construction-of-McCord",
            "title": {
                "fragments": [],
                "text": "Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Several grammatical phenomena, such as coordination and extraposition, are treated mainly in a language-independent shell provided with the Slot Grammars system."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language and Logic"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107389564"
                        ],
                        "name": "T. Smith",
                        "slug": "T.-Smith",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2398669"
                        ],
                        "name": "M. Waterman",
                        "slug": "M.-Waterman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Waterman",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Waterman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20031248,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "40c5441aad96b366996e6af163ca9473a19bb9ad",
            "isKey": false,
            "numCitedBy": 10037,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Identification-of-common-molecular-subsequences.-Smith-Waterman",
            "title": {
                "fragments": [],
                "text": "Identification of common molecular subsequences."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4159208"
                        ],
                        "name": "S. Morrison",
                        "slug": "S.-Morrison",
                        "structuredName": {
                            "firstName": "Sherie",
                            "lastName": "Morrison",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Morrison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36306765,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d887eeda2fe75da58dde8c0bda667d90b5512097",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A new design for bispecific antibodies enables efficient production of stable molecules with good pharmacodynamic properties."
            },
            "slug": "Two-heads-are-better-than-one-Morrison",
            "title": {
                "fragments": [],
                "text": "Two heads are better than one"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new design for bispecific antibodies enables efficient production of stable molecules with good pharmacodynamic properties and helps to reduce uncertainty in the design of new drugs."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Biotechnology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13477554"
                        ],
                        "name": "K. Fernow",
                        "slug": "K.-Fernow",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Fernow",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fernow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27239232,
            "fieldsOfStudy": [],
            "id": "aeb4d3e0ed9799136cf5c5011540a6ed8d23b287",
            "isKey": false,
            "numCitedBy": 3746,
            "numCiting": 180,
            "paperAbstract": {
                "fragments": [],
                "text": "Table of"
            },
            "slug": "New-York-Fernow",
            "title": {
                "fragments": [],
                "text": "New York"
            },
            "venue": {
                "fragments": [],
                "text": "American Potato Journal"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 51
                            }
                        ],
                        "text": "Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to produce promising results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Developed in part under the U.S. government AQUAINT program3 and in collaboration with external teams and universities, PIQUANT was a classic QA pipeline with state-of-the-art techniques aimed largely at the TREC QA evaluation (Voorhees and Dang 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "While the TREC QA evaluation allowed the use of the web, PIQUANT focused on question answering using local resources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "The DeepQA Approach Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to produce promising results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "We spent minimal effort adapting OpenEphyra, but like PIQUANT, its performance on Jeopardy clues was below 15 percent accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Both the 2005 PIQUANT and 2007 OpenEphyra systems had less than 50 percent accuracy on the TREC questions and less than 15\npercent accuracy on the Jeopardy clues."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "Our most obvious baseline is the QA system called Practical Intelligent Question Answering Technology (PIQUANT) (Prager, Chu-Carroll, and Czuba 2004), which had been under development at IBM Research by a four-person team for 6 years prior to taking on the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 71
                            }
                        ],
                        "text": "In particular, we added question-analysis\nFALL 2010 75\ncomponents from PIQUANT and OpenEphyra that identify answer types for a question, and candidate answer-generation components that identify instances of those answer types in the text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "PIQUANT performed in the 33 percent accuracy range in TREC evaluations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The PIQUANT and OpenEphyra baselines demonstrate the performance of state-of-the-art QA systems on the Jeopardy task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "An initial 4-week effort was made to adapt PIQUANT to the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The result of the PIQUANT baseline experiment is illustrated in figure 4."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two Heads Are Better Than One in Question-Answering"
            },
            "venue": {
                "fragments": [],
                "text": "Paper presented at the Human Language Technology Conference, Edmonton, Canada, 27 May\u20131 June. Dredze, M.; Crammer, K.; and Pereira, F. 2008. Confidence-Weighted Linear Classification. In Proceedings of the Twenty-Fifth International Conference on Machine Learning (ICML). Princeton,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 398,
                                "start": 9
                            }
                        ],
                        "text": "After merging, the system must rank the hypotheses and estimate confidence based on their merged scores. We adopted a machine-learning approach that requires running the system over a set of training questions with known answers and training a model based on the scores. One could assume a very flat model and apply existing ranking algorithms (for example, Herbrich, Graepel, and Obermayer [2000]; Joachims [2002]) directly to these score profiles and use the ranking score for confidence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "The DeepQA approach encourages a mixture of experts at this stage, and in the Watson system we produce shallow parses, deep parses (McCord 1990), logical forms, semantic role labels, coreference, relations, named entities, and so on, as well as specific kinds of analysis for question answering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2293,
                                "start": 6
                            }
                        ],
                        "text": "Scoring. The scoring step is where the bulk of the deep content analysis is performed. Scoring algorithms determine the degree of certainty that retrieved evidence supports the candidate answers. The DeepQA framework supports and encourages the inclusion of many different components, or scorers, that consider different dimensions of the evidence and produce a score that corresponds to how well evidence supports a candidate answer for a given question. DeepQA provides a common format for the scorers to register hypotheses (for example candidate answers) and confidence scores, while imposing few restrictions on the semantics of the scores themselves; this enables DeepQA developers to rapidly deploy, mix, and tune components to support each other. For example, Watson employs more than 50 scoring components that produce scores ranging from formal probabilities to counts to categorical features, based on evidence from different types of sources including unstructured text, semistructured text, and triple stores. These scorers consider things like the degree of match between a passage's predicate-argument structure and the question, passage source reliability, geospatial location, temporal relationships, taxonomic classification, the lexical and semantic relations the candidate is known to participate in, the candidate's correlation with question terms, its popularity (or obscurity), its aliases, and so on. Consider the question, \"He was presidentially pardoned on September 8, 1974\"; the correct answer, \"Nixon,\" is one of the generated candidates. One of the retrieved passages is \"Ford pardoned Nixon on Sept. 8, 1974.\" One passage scorer counts the number of IDF-weighted terms in common between the question and the passage. Another passage scorer based on the Smith-Waterman sequence-matching algorithm (Smith and Waterman 1981), measures the lengths of the longest similar subsequences between the question and passage (for example \"on Sept. 8, 1974\"). A third type of passage scoring measures the alignment of the logical forms of the question and passage. A logical form is a graphical abstraction of text in which nodes are terms in the text and edges represent either grammatical relationships (for example, Hermjakob, Hovy, and Lin [2000]; Moldovan et al. [2003]), deep semantic relationships (for example, Lenat [1995], Paritosh and Forbus [2005]), or both ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Slot Grammar: A System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Developed in part under the U.S. government AQUAINT program3 and in collaboration with external teams and universities, PIQUANT was a classic QA pipeline with state-of-the-art techniques aimed largely at the TREC QA evaluation (Voorhees and Dang 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "While the TREC QA evaluation allowed the use of the web, PIQUANT focused on question answering using local resources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "The DeepQA Approach Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to produce promising results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "We spent minimal effort adapting OpenEphyra, but like PIQUANT, its performance on Jeopardy clues was below 15 percent accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Both the 2005 PIQUANT and 2007 OpenEphyra systems had less than 50 percent accuracy on the TREC questions and less than 15\npercent accuracy on the Jeopardy clues."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 113
                            }
                        ],
                        "text": "Our most obvious baseline is the QA system called Practical Intelligent Question Answering Technology (PIQUANT) (Prager, Chu-Carroll, and Czuba 2004), which had been under development at IBM Research by a four-person team for 6 years prior to taking on the Jeopardy Challenge."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 71
                            }
                        ],
                        "text": "In particular, we added question-analysis\nFALL 2010 75\ncomponents from PIQUANT and OpenEphyra that identify answer types for a question, and candidate answer-generation components that identify instances of those answer types in the text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "PIQUANT performed in the 33 percent accuracy range in TREC evaluations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The PIQUANT and OpenEphyra baselines demonstrate the performance of state-of-the-art QA systems on the Jeopardy task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "An initial 4-week effort was made to adapt PIQUANT to the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The result of the PIQUANT baseline experiment is illustrated in figure 4."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Multi-Strategy, Multi-Question Approach to Question Answering"
            },
            "venue": {
                "fragments": [],
                "text": "New Directions in Question-Answering, ed. M. Maybury. Menlo Park, CA: AAAI Press."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Developed in part under the U.S. government AQUAINT program3 and in collaboration with external teams and universities, PIQUANT was a classic QA pipeline with state-of-the-art techniques aimed largely at the TREC QA evaluation (Voorhees and Dang 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "While the TREC QA evaluation allowed the use of the web, PIQUANT focused on question answering using local resources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "The DeepQA Approach Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to produce promising results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "We spent minimal effort adapting OpenEphyra, but like PIQUANT, its performance on Jeopardy clues was below 15 percent accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Both the 2005 PIQUANT and 2007 OpenEphyra systems had less than 50 percent accuracy on the TREC questions and less than 15\npercent accuracy on the Jeopardy clues."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "Our most obvious baseline is the QA system called Practical Intelligent Question Answering Technology (PIQUANT) (Prager, Chu-Carroll, and Czuba 2004), which had been under development at IBM Research by a four-person team for 6 years prior to taking on the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 71
                            }
                        ],
                        "text": "In particular, we added question-analysis\nFALL 2010 75\ncomponents from PIQUANT and OpenEphyra that identify answer types for a question, and candidate answer-generation components that identify instances of those answer types in the text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "PIQUANT performed in the 33 percent accuracy range in TREC evaluations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The PIQUANT and OpenEphyra baselines demonstrate the performance of state-of-the-art QA systems on the Jeopardy task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "An initial 4-week effort was made to adapt PIQUANT to the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The result of the PIQUANT baseline experiment is illustrated in figure 4."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two Heads Are Better Than One in Question-Answering. Paper presented at the Human Language Technology Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Two Heads Are Better Than One in Question-Answering. Paper presented at the Human Language Technology Conference"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Developed in part under the U.S. government AQUAINT program3 and in collaboration with external teams and universities, PIQUANT was a classic QA pipeline with state-of-the-art techniques aimed largely at the TREC QA evaluation (Voorhees and Dang 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "While the TREC QA evaluation allowed the use of the web, PIQUANT focused on question answering using local resources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "The DeepQA Approach Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to produce promising results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "We spent minimal effort adapting OpenEphyra, but like PIQUANT, its performance on Jeopardy clues was below 15 percent accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Both the 2005 PIQUANT and 2007 OpenEphyra systems had less than 50 percent accuracy on the TREC questions and less than 15\npercent accuracy on the Jeopardy clues."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "Our most obvious baseline is the QA system called Practical Intelligent Question Answering Technology (PIQUANT) (Prager, Chu-Carroll, and Czuba 2004), which had been under development at IBM Research by a four-person team for 6 years prior to taking on the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 71
                            }
                        ],
                        "text": "In particular, we added question-analysis\nFALL 2010 75\ncomponents from PIQUANT and OpenEphyra that identify answer types for a question, and candidate answer-generation components that identify instances of those answer types in the text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 51
                            }
                        ],
                        "text": "Early on in the project, attempts to adapt PIQUANT (Chu-Carroll et al. 2003) failed to pro-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "PIQUANT performed in the 33 percent accuracy range in TREC evaluations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The PIQUANT and OpenEphyra baselines demonstrate the performance of state-of-the-art QA systems on the Jeopardy task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "An initial 4-week effort was made to adapt PIQUANT to the Jeopardy Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The result of the PIQUANT baseline experiment is illustrated in figure 4."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two Heads Are Better Than One in QuestionAnswering"
            },
            "venue": {
                "fragments": [],
                "text": "Paper presented at the Human Language Technology Conference, Edmonton, Canada, 27 May-1 June."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5659511"
                        ],
                        "name": "Bill Broyles",
                        "slug": "Bill-Broyles",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Broyles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Broyles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208811440,
            "fieldsOfStudy": [],
            "id": "0aea520a25198f6b3f385a09b158da2f7ec5cf1f",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Notes-Broyles",
            "title": {
                "fragments": [],
                "text": "Notes"
            },
            "venue": {
                "fragments": [],
                "text": "Edinburgh Medical Journal"
            },
            "year": 1928
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118508626"
                        ],
                        "name": "Warren Bower",
                        "slug": "Warren-Bower",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "Bower",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Warren Bower"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 169069114,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "04b58ae736315304388616a38d54b98a33eb66b5",
            "isKey": false,
            "numCitedBy": 1081,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "New-directions-Bower",
            "title": {
                "fragments": [],
                "text": "New directions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1937
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39743766"
                        ],
                        "name": "J. Stockwell",
                        "slug": "J.-Stockwell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stockwell",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stockwell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123903112,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "ec6f57686cb24f044eff96c7ea9c9a13f2a152d4",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-the-open:-By-Henry-C.-Tracy-with-an-by-6-\u00d7-Stockwell",
            "title": {
                "fragments": [],
                "text": "Towards the open: By Henry C. Tracy with an Introduction by Julian Huxley. xx-257 pages, 6 \u00d7 in., cloth. New York, E. P. Dutton & Co., 1927. Price, $3.50"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1928
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39962304"
                        ],
                        "name": "Darren Gehring",
                        "slug": "Darren-Gehring",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Gehring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Gehring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686971"
                        ],
                        "name": "T. Graepel",
                        "slug": "T.-Graepel",
                        "structuredName": {
                            "firstName": "Thore",
                            "lastName": "Graepel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graepel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60533697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "958f001b6f348f7c353260b289bed185fffac847",
            "isKey": false,
            "numCitedBy": 980,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Large-Margin-Rank-Boundaries-for-Ordinal-Regression-Gehring-Graepel",
            "title": {
                "fragments": [],
                "text": "Large Margin Rank Boundaries for Ordinal Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Watson is named after IBM's founder, Thomas J. Wat- son"
            },
            "venue": {
                "fragments": [],
                "text": "Watson is named after IBM's founder, Thomas J. Wat- son"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "OAQA is intended to directly engage researchers in the community to help replicate and reuse research results and to identify how to more rapidly advance the state of the art in QA (Ferrucci et al 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards the Open Advancement of Question Answer Systems. IBM Technical Report RC24789, Y orktown Heights"
            },
            "venue": {
                "fragments": [],
                "text": "Towards the Open Advancement of Question Answer Systems. IBM Technical Report RC24789, Y orktown Heights"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "OAQA is intended to directly engage researchers in the community to help replicate and reuse research results and to identify how to more rapidly advance the state of the art in QA (Ferrucci et al 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards the Open"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 1
                            }
                        ],
                        "text": "(Dredze, Crammer, and Pereira 2008)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Con!dence-Weighted Linear Classi!cation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twenty-Fifth International Conference on Machine Learning (ICML). Princeton, NJ: International Machine Learning Society."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "generally receives the most attention and analysis from those interested in game strategy, and there exists a growing catalogue of heuristics such as \"Clavin's Rule\" or the \"Two-Thirds Rule\" (Dupee 1998) as well as identification of those critical score boundaries at which particular strategies may be used (by no means does this make it easy or rote; despite this attention, we have found evidence that contestants still occasionally make irrational Final Jeopardy bets)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and analysis from those interested in game strategy, and there exists a growing catalogue of heuristics such as \u201cClavin\u2019s Rule\u201d or the \u201cTwo-Thirds Rule\u201d (Dupee 1998) as well as identification of those critical score boundaries at which particular strategies may be used (by no means does this make\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to Get on Jeopardy.' ... and Win: Valu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "10. incubator.apache.org/uima"
            },
            "venue": {
                "fragments": [],
                "text": "10. incubator.apache.org/uima"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From our academic partners, Manas Pathak (CMU) James Allen (UMass)"
            },
            "venue": {
                "fragments": [],
                "text": "Hideki Shima (CMU) Pallika Kanani (UMass), Boris Katz (Massachusetts Institute of Technology), Alessandro Moschitti, and Giuseppe Riccardi"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 150
                            }
                        ],
                        "text": "\u2026and analysis from those interested in game strategy, and there exists a growing catalogue of heuristics such as \u201cClavin\u2019s Rule\u201d or the \u201cTwo-Thirds Rule\u201d (Dupee 1998) as well as identification of those critical score boundaries at which particular strategies may be used (by no means does this make\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to Get on Jeopardy! ... and Win: Valuable Information from a Champion"
            },
            "venue": {
                "fragments": [],
                "text": "How to Get on Jeopardy! ... and Win: Valuable Information from a Champion"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 27
                            }
                        ],
                        "text": "It has had a long history (Simmons 1970) and saw rapid advancement spurred by system building, experimentation, and government funding in the past decade (Maybury 2004, Strzalkowski and Harabagiu 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural Language QuestionAnswering Systems: 1969"
            },
            "venue": {
                "fragments": [],
                "text": "Communications of the ACM 13(1): 15-30"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random jitter has been added to help visualize the distribution of points"
            },
            "venue": {
                "fragments": [],
                "text": "Random jitter has been added to help visualize the distribution of points"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Multi - Strategy , Multi - Question Approach to Question Answering"
            },
            "venue": {
                "fragments": [],
                "text": "New Directions in Question - Answering"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5. sourceforge.net/projects/openephyra"
            },
            "venue": {
                "fragments": [],
                "text": "5. sourceforge.net/projects/openephyra"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "OAQA is intended to directly engage researchers in the community to help replicate and reuse research results and to identify how to more rapidly advance the state of the art in QA (Ferrucci et al 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards the Open Advancement of Question Answer Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards the Open"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge Discovery and Data Mining (KDD). New York: Association for Computing Machinery"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 30th Annual International ACM SIGIR Conference,"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Watson is named after IBM's founder"
            },
            "venue": {
                "fragments": [],
                "text": "Watson is named after IBM's founder"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 128
                            }
                        ],
                        "text": "One could assume a very flat model and apply existing ranking algorithms (for example, Herbrich, Graepel, and Obermayer [2000]; Joachims [2002]) directly to these score profiles and use the ranking score for confidence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimizing Search Engines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New Directions in QuestionAnswering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 5,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 50,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Building-Watson:-An-Overview-of-the-DeepQA-Project-Ferrucci-Brown/3ff2862a8121cc823a8eb72f3e0a97bbf25c82ec?sort=total-citations"
}