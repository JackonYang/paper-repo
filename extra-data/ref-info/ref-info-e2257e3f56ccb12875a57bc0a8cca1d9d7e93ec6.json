{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 274
                            }
                        ],
                        "text": "\u2026learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al., 2009; Arora & Livescu, 2012); and reducing sample complexity of prediction problems using unlabeled data (Kakade & Foster, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6792954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1caea0662cc8c62ce3ef49d8f54ba7ca2b39c92c",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In the multi-view regression problem, we have a regression problem where the input variable (which is a real vector) can be partitioned into two different views, where it is assumed that either view of the input is sufficient to make accurate predictions -- this is essentially (a significantly weaker version of) the co-training assumption for the regression problem. \n \nWe provide a semi-supervised algorithm which first uses unlabeled data to learn a norm (or, equivalently, a kernel) and then uses labeled data in a ridge regression algorithm (with this induced norm) to provide the predictor. The unlabeled data is used via canonical correlation analysis (CCA, which is a closely related to PCA for two random variables) to derive an appropriate norm over functions. We are able to characterize the intrinsic dimensionality of the subsequent ridge regression problem (which uses this norm) by the correlation coefficients provided by CCA in a rather simple expression. Interestingly, the norm used by the ridge regression algorithm is derived from CCA, unlike in standard kernel methods where a special apriori norm is assumed (i.e. a Banach space is assumed). We discuss how this result shows that unlabeled data can decrease the sample complexity."
            },
            "slug": "Multi-view-Regression-Via-Canonical-Correlation-Kakade-Foster",
            "title": {
                "fragments": [],
                "text": "Multi-view Regression Via Canonical Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work provides a semi-supervised algorithm which first uses unlabeled data to learn a norm (or, equivalently, a kernel) and then uses labeled data in a ridge regression algorithm (with this induced norm) to provide the predictor."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38120884"
                        ],
                        "name": "Kamalika Chaudhuri",
                        "slug": "Kamalika-Chaudhuri",
                        "structuredName": {
                            "firstName": "Kamalika",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamalika Chaudhuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37564609"
                        ],
                        "name": "Karthik Sridharan",
                        "slug": "Karthik-Sridharan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Sridharan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 131
                            }
                        ],
                        "text": ", 2007); learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al., 2009; Arora & Livescu, 2012); and reducing sample complexity of prediction problems using unlabeled data (Kakade & Foster, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 150
                            }
                        ],
                        "text": "\u2026learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al., 2009; Arora & Livescu, 2012); and reducing sample complexity of prediction problems using unlabeled data (Kakade & Foster, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2800834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ec1f149e494cf589163f6fc46cb6f02e44540f",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering data in high dimensions is believed to be a hard problem in general. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via Principal Components Analysis (PCA) or random projections, before clustering. Here, we consider constructing such projections using multiple views of the data, via Canonical Correlation Analysis (CCA).\n Under the assumption that the views are un-correlated given the cluster label, we show that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature. We provide results for mixtures of Gaussians and mixtures of log concave distributions. We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure)."
            },
            "slug": "Multi-view-clustering-via-canonical-correlation-Chaudhuri-Kakade",
            "title": {
                "fragments": [],
                "text": "Multi-view clustering via canonical correlation analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Under the assumption that the views are un-correlated given the cluster label, it is shown that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365054"
                        ],
                        "name": "R. Arora",
                        "slug": "R.-Arora",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 59
                            }
                        ],
                        "text": "We use a scalable KCCA algorithm based on incremental SVD (Arora & Livescu, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 174
                            }
                        ],
                        "text": "\u2026learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al., 2009; Arora & Livescu, 2012); and reducing sample complexity of prediction problems using unlabeled data (Kakade & Foster, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 215
                            }
                        ],
                        "text": "In practice solving KCCA may not be straightforward, as the kernel matrices become very large for real-world\ndata sets of interest, and iterative SVD algorithms for the initial dimensionality reduction can be used (Arora & Livescu, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 175
                            }
                        ],
                        "text": "Often a further regularization is done by first projecting the data onto an intermediatedimensionality space, between the target and original dimensionality (Ek et al., 2008; Arora & Livescu, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Estimating the covariance matrices with regularization also reduces the detection of spurious correlations in the training data, a.k.a. \u201coverfitting\u201d (De Bie & De Moor, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1580352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d032cb993d5eafea73be85ee6db13d30f8d35ef",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning transformations of acoustic feature vectors for phonetic frame classification, in a multi-view setting where articulatory measurements are available at training time but not at test time. Canonical correlation analysis (CCA) has previously been used to learn linear transformations of the acoustic features that are maximally correlated with articulatory measurements. Here, we learn nonlinear transformations of the acoustics using kernel canonical correlation analysis (KCCA). We present an incremental SVD approach that makes the KCCA computations feasible for typical speech data set sizes. In phonetic frame classification experiments on data drawn from the University of Wisconsin X-ray Microbeam Database, we find that KCCA provides consistent improvements over linear CCA, as well as over single-view unsupervised dimensionality reduction."
            },
            "slug": "Kernel-CCA-for-multi-view-learning-of-acoustic-Arora-Livescu",
            "title": {
                "fragments": [],
                "text": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In phonetic frame classification experiments on data drawn from the University of Wisconsin X-ray Microbeam Database, it is found that KCCA provides consistent improvements over linear CCA, as well as over single-view unsupervised dimensionality reduction."
            },
            "venue": {
                "fragments": [],
                "text": "MLSLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750713"
                        ],
                        "name": "B. Moor",
                        "slug": "B.-Moor",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Moor",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66721764"
                        ],
                        "name": "Kasteelpark Arenberg",
                        "slug": "Kasteelpark-Arenberg",
                        "structuredName": {
                            "firstName": "Kasteelpark",
                            "lastName": "Arenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kasteelpark Arenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5034909,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3898f4c600cf7d27f7e3e14fca3f95f369bc75d3",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "By elucidating a parallel between canonical correlation analysis (CCA) and least squares regression (LSR), we show how regularization of CCA can be performed and interpreted in the same spirit as the regularization applied in ridge regression (RR). Furthermore, the results presented may have an impact on the practical use of regularized CCA (RCCA). More specifically, a relevant cross validation cost function for training the regularization parameter, naturally follows from the derivations."
            },
            "slug": "On-the-Regularization-of-Canonical-Correlation-Bie-Moor",
            "title": {
                "fragments": [],
                "text": "On the Regularization of Canonical Correlation Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2479037"
                        ],
                        "name": "F. Rudzicz",
                        "slug": "F.-Rudzicz",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rudzicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rudzicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 27
                            }
                        ],
                        "text": ", 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5986697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76a37840eb20198601503bc9cacbeed27e41688d",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for acoustic-articulatory inversion whose targets are the abstract tract variables from task dynamic theory. Towards this end we construct a non-linear Hammerstein system whose parameters are updated with adaptive kernel canonical correlation analysis. This approach is notably semi-analytical and applicable to large sets of data. Training behaviour is compared across four kernel functions and prediction of tract variables is shown to be significantly more accurate than state-of-the-art mixture density networks. Index terms: acoustic-articulatory inversion, kernel canonical correlation analysis, task dynamics."
            },
            "slug": "Adaptive-Kernel-Canonical-Correlation-Analysis-for-Rudzicz",
            "title": {
                "fragments": [],
                "text": "Adaptive Kernel Canonical Correlation Analysis for Estimation of Task Dynamics from Acoustics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A non-linear Hammerstein system whose parameters are updated with adaptive kernel canonical correlation analysis and prediction of tract variables is shown to be significantly more accurate than state-of-the-art mixture density networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110993317"
                        ],
                        "name": "Tae-Kyun Kim",
                        "slug": "Tae-Kyun-Kim",
                        "structuredName": {
                            "firstName": "Tae-Kyun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tae-Kyun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731402"
                        ],
                        "name": "Shu-Fai Wong",
                        "slug": "Shu-Fai-Wong",
                        "structuredName": {
                            "firstName": "Shu-Fai",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu-Fai Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": ", 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 201
                            }
                        ],
                        "text": "\u2026natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney & Covell,\n2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4642018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baac4ebffdc5955cd5a8a5cf15dfab098c7d0704",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new framework, namely tensor canonical correlation analysis (TCCA) which is an extension of classical canonical correlation analysis (CCA) to multidimensional data arrays (or tensors) and apply this for action/gesture classification in videos. By tensor CCA, joint space-time linear relationships of two video volumes are inspected to yield flexible and descriptive similarity features of the two videos. The TCCA features are combined with a discriminative feature selection scheme and a nearest neighbor classifier for action classification. In addition, we propose a time-efficient action detection method based on dynamic learning of subspaces for tensor CCA for the case that actions are not aligned in the space-time domain. The proposed method delivered significantly better accuracy and comparable detection speed over state-of-the-art methods on the KTH action data set as well as self-recorded hand gesture data sets."
            },
            "slug": "Tensor-Canonical-Correlation-Analysis-for-Action-Kim-Wong",
            "title": {
                "fragments": [],
                "text": "Tensor Canonical Correlation Analysis for Action Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A time-efficient action detection method based on dynamic learning of subspaces for tensor CCA for the case that actions are not aligned in the space-time domain is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763894"
                        ],
                        "name": "D. Hardoon",
                        "slug": "D.-Hardoon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hardoon",
                            "middleNames": [
                                "Roi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hardoon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144762404"
                        ],
                        "name": "J. Miranda",
                        "slug": "J.-Miranda",
                        "structuredName": {
                            "firstName": "Janaina",
                            "lastName": "Miranda",
                            "middleNames": [
                                "Mour\u00e3o"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Miranda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730391"
                        ],
                        "name": "M. Brammer",
                        "slug": "M.-Brammer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brammer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "\u2026of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 31
                            }
                        ],
                        "text": ", 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 93
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learning features for a single view\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3378265,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "cea4f5c295729f8ea7fcc694fafcb27b96be2866",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unsupervised-analysis-of-fMRI-data-using-kernel-Hardoon-Miranda",
            "title": {
                "fragments": [],
                "text": "Unsupervised analysis of fMRI data using kernel canonical correlation"
            },
            "venue": {
                "fragments": [],
                "text": "NeuroImage"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "Contrastive divergence (Bengio & Delalleau, 2009) has had great success as a pretraining technique, as have many variants of autoencoder networks, including the denoising autoencoder (Vincent et al., 2008) used in the present work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 93
                            }
                        ],
                        "text": "In our experiments, we initialize the parameters of each layer with a denoising autoencoder (Vincent et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143683998"
                        ],
                        "name": "S. Akaho",
                        "slug": "S.-Akaho",
                        "structuredName": {
                            "firstName": "Shotaro",
                            "lastName": "Akaho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akaho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 46
                            }
                        ],
                        "text": "Kernel canonical correlation analysis (KCCA) (Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004) is an extension of CCA in which maximally correlated nonlinear projections, restricted to reproducing kernel Hilbert spaces with corresponding kernels, are found."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5702881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d374c48cf79d8dfb767e27474a619542c0d75406",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Canonical correlation analysis is a technique to extract common features from a pair of multivariate data. In complex situations, however, it does not extract useful features because of its linearity. On the other hand, kernel method used in support vector machine is an efficient approach to improve such a linear method. In this paper, we investigate the effectiveness of applying kernel method to canonical correlation analysis. Keyword: multivariate analysis, multimodal data, kernel method, regularization"
            },
            "slug": "A-kernel-method-for-canonical-correlation-analysis-Akaho",
            "title": {
                "fragments": [],
                "text": "A kernel method for canonical correlation analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The effectiveness of applying kernel method to canonical correlation analysis is investigated, which shows an efficient approach to improve such a linear method."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484138"
                        ],
                        "name": "C. Ek",
                        "slug": "C.-Ek",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Ek",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856452"
                        ],
                        "name": "Jonathan Rihan",
                        "slug": "Jonathan-Rihan",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rihan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Rihan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321919"
                        ],
                        "name": "Gr\u00e9gory Rogez",
                        "slug": "Gr\u00e9gory-Rogez",
                        "structuredName": {
                            "firstName": "Gr\u00e9gory",
                            "lastName": "Rogez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gr\u00e9gory Rogez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 158
                            }
                        ],
                        "text": "Often a further regularization is done by first projecting the data onto an intermediatedimensionality space, between the target and original dimensionality (Ek et al., 2008; Arora & Livescu, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7519208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5876e6f1e69a9893ec018ea022d87ade3426a438",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We are interested in the situation where we have two or more representations of an underlying phenomenon. In particular we are interested in the scenario where the representation are complementary. This implies that a single individual representation is not sufficient to fully discriminate a specific instance of the underlying phenomenon, it also means that each representation is an ambiguous representation of the other complementary spaces. In this paper we present a latent variable model capable of consolidating multiple complementary representations. Our method extends canonical correlation analysis by introducing additional latent spaces that are specific to the different representations, thereby explaining the full variance of the observations. These additional spaces, explaining representation specific variance, separately model the variance in a representation ambiguous to the other. We develop a spectral algorithm for fast computation of the embeddings and a probabilistic model (based on Gaussian processes) for validation and inference. The proposed model has several potential application areas, we demonstrate its use for multi-modal regression on a benchmark human pose estimation data set."
            },
            "slug": "Ambiguity-Modeling-in-Latent-Spaces-Ek-Rihan",
            "title": {
                "fragments": [],
                "text": "Ambiguity Modeling in Latent Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a latent variable model capable of consolidating multiple complementary representations, and extends canonical correlation analysis by introducing additional latent spaces that are specific to the different representations, thereby explaining the full variance of the observations."
            },
            "venue": {
                "fragments": [],
                "text": "MLMI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "Kernel canonical correlation analysis (KCCA) (Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004) is an extension of CCA in which maximally correlated nonlinear projections, restricted to reproducing kernel Hilbert spaces with corresponding kernels, are found."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 69
                            }
                        ],
                        "text": "Parameters of both transformations are jointly learned to maximize the (regularized) total correlation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7691428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d4f4601940d5b13455541a643a39538bb54b6f3",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms."
            },
            "slug": "Kernel-independent-component-analysis-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Kernel independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A class of algorithms for independent component analysis which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space is presented, showing that these algorithms outperform many of the presently known algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143842629"
                        ],
                        "name": "T. Melzer",
                        "slug": "T.-Melzer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Melzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Melzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060020641"
                        ],
                        "name": "M. Reiter",
                        "slug": "M.-Reiter",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "Kernel canonical correlation analysis (KCCA) (Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004) is an extension of CCA in which maximally correlated nonlinear projections, restricted to reproducing kernel Hilbert spaces with corresponding kernels, are found."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33570889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "260e6ff494c26202bcd2929de40cf7d1c732b4f9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA) with applications in regression and object recognition. The non-linear transformation of the input data is performed using kernel-methods. Although, in this respect, our approach is similar to other generalized linear methods like kernel-PCA, our method is especially well suited for relating two sets of measurements. The benefits of our method compared to standard feature extraction methods based on PCA will be illustrated with several experiments from the field of object recognition and pose estimation."
            },
            "slug": "Nonlinear-Feature-Extraction-Using-Generalized-Melzer-Reiter",
            "title": {
                "fragments": [],
                "text": "Nonlinear Feature Extraction Using Generalized Canonical Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA), which is especially well suited for relating two sets of measurements, with applications in regression and object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 155
                            }
                        ],
                        "text": "Deep networks have been used widely to learn representations, for example using deep Boltzmann machines (Salakhutdinov & Hinton, 2009), deep autoencoders (Hinton & Salakhutdinov, 2006), and deep nonlinear feedforward networks (Hinton et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2013 by the author(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 237
                            }
                        ],
                        "text": "There has been a resurgence of interest in such models following the advent of various successful unsupervised methods for initializing the parameters (\u201cpretraining\u201d) in such a way that a useful solution can be found (Hinton et al., 2006; Hinton & Salakhutdinov, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14645,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763894"
                        ],
                        "name": "D. Hardoon",
                        "slug": "D.-Hardoon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hardoon",
                            "middleNames": [
                                "Roi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hardoon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540580"
                        ],
                        "name": "S. Szedm\u00e1k",
                        "slug": "S.-Szedm\u00e1k",
                        "structuredName": {
                            "firstName": "S\u00e1ndor",
                            "lastName": "Szedm\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szedm\u00e1k"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 101
                            }
                        ],
                        "text": "Kernel canonical correlation analysis (KCCA) (Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004) is an extension of CCA in which maximally correlated nonlinear projections, restricted to reproducing kernel Hilbert spaces with corresponding kernels, are found."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "Kernel CCA finds pairs of nonlinear projections of the two views (Hardoon et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6b5b20151c752beb74508f813699fa5216dedfa",
            "isKey": false,
            "numCitedBy": 2670,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text. The semantic space provides a common representation and enables a comparison between the text and images. In the experiments, we look at two approaches of retrieving images based on only their content from a text query. We compare orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model."
            },
            "slug": "Canonical-Correlation-Analysis:-An-Overview-with-to-Hardoon-Szedm\u00e1k",
            "title": {
                "fragments": [],
                "text": "Canonical Correlation Analysis: An Overview with Application to Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text and compares orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365054"
                        ],
                        "name": "R. Arora",
                        "slug": "R.-Arora",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2013 by the author(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 160
                            }
                        ],
                        "text": "\u2026natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney & Covell,\n2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2380455,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a66169aef77dc88accac73d52c9f947e7f8a1e5a",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Canonical correlation analysis (CCA) and kernel CCA can be used for unsupervised learning of acoustic features when a second view (e.g., articulatory measurements) is available for some training data, and such projections have been used to improve phonetic frame classification. Here we study the behavior of CCA-based acoustic features on the task of phonetic recognition, and investigate to what extent they are speaker-independent or domain-independent. The acoustic features are learned using data drawn from the University of Wisconsin X-ray Microbeam Database (XRMB). The features are evaluated within and across speakers on XRMB data, as well as on out-of-domain TIMIT and MOCHA-TIMIT data. Experimental results show consistent improvement with the learned acoustic features over baseline MFCCs and PCA projections. In both speaker-dependent and cross-speaker experiments, phonetic error rates are improved by 4-9% absolute (10-23% relative) using CCA-based features over baseline MFCCs. In cross-domain phonetic recognition (training on XRMB and testing on MOCHA or TIMIT), the learned projections provide smaller improvements."
            },
            "slug": "Multi-view-CCA-based-acoustic-features-for-phonetic-Arora-Livescu",
            "title": {
                "fragments": [],
                "text": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The behavior of CCA-based acoustic features on the task of phonetic recognition is studied, and to what extent they are speaker-independent or domain-independent."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "\u2026, f \u2217 2 ) = argmax\nf1\u2208H1,f2\u2208H2 corr (f1(X1), f2(X2)) (6)\n= argmax f1\u2208H1,f2\u2208H2 cov (f1(X1), f2(X2))\u221a var (f1(X1)) var (f2(X2)) ,\nTo solve the nonlinear KCCA problem, the \u201ckernel trick\u201d is used: Since the nonlinear maps f1 \u2208 H1, f2 \u2208 H2 are in RKHS, the solutions can be expressed as linear\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 24
                            }
                        ],
                        "text": "Contrastive divergence (Bengio & Delalleau, 2009) has had great success as a pretraining technique, as have many variants of autoencoder networks, including the denoising autoencoder (Vincent et al., 2008) used in the present work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14266633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b543de6755fc1612b2fb449e0282727d0835d9cf",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncationrunning only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps k and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-k is a good descent direction even for small k."
            },
            "slug": "Justifying-and-Generalizing-Contrastive-Divergence-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "Justifying and Generalizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables, shows that its residual term converges to zero, justifying the use of a truncation of only a short Gibbs chain."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 227
                            }
                        ],
                        "text": "Deep networks have been used widely to learn representations, for example using deep Boltzmann machines (Salakhutdinov & Hinton, 2009), deep autoencoders (Hinton & Salakhutdinov, 2006), and deep nonlinear feedforward networks (Hinton et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 216
                            }
                        ],
                        "text": "There has been a resurgence of interest in such models following the advent of various successful unsupervised methods for initializing the parameters (\u201cpretraining\u201d) in such a way that a useful solution can be found (Hinton et al., 2006; Hinton & Salakhutdinov, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939367"
                        ],
                        "name": "Paramveer S. Dhillon",
                        "slug": "Paramveer-S.-Dhillon",
                        "structuredName": {
                            "firstName": "Paramveer",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paramveer S. Dhillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 93
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "\u2026biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 141
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learning features for a single view\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 37
                            }
                        ],
                        "text": ", 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 519902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5db592bef4b5ff231e1de92588907808f00bfbb4",
            "isKey": true,
            "numCitedBy": 253,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model the context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate low dimensional context-specific word representations from unlabeled data. These representation features can then be used with any supervised learner. LR-MVL is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (NER) and chunking problems."
            },
            "slug": "Multi-View-Learning-of-Word-Embeddings-via-CCA-Dhillon-Foster",
            "title": {
                "fragments": [],
                "text": "Multi-View Learning of Word Embeddings via CCA"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Low Rank Multi-View Learning (LR-MVL) is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (NER) and chunking problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 183
                            }
                        ],
                        "text": "The most closely related work is that of Ngiam et al. on multimodal autoencoders (Ngiam et al., 2011) and of Srivastava and Salakhutdinov on multimodal restricted Boltzmann machines (Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 261
                            }
                        ],
                        "text": "A natural next step is therefore to test the representations produced by deep CCA in the context of prediction tasks and to compare against other nonlinear multi-view representation learning approaches that optimize other objectives, e.g., (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 710430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5726c7b40fcc454b77d989656c085520bf6c15fa",
            "isKey": false,
            "numCitedBy": 1489,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time."
            },
            "slug": "Multimodal-learning-with-deep-Boltzmann-machines-Srivastava-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Multimodal learning with deep Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A Deep Boltzmann Machine is proposed for learning a generative model of multimodal data and it is shown that the model can be used to create fused representations by combining features across modalities, which are useful for classification and information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 112
                            }
                        ],
                        "text": "We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "\u2026(Sargin et al., 2007); learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al., 2009; Arora & Livescu, 2012); and reducing sample complexity of prediction problems using unlabeled data\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1145503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fd700f4a010d765c506841de9884df394c1de1c",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for spectral clustering with paired data based on kernel canonical correlation analysis, called correlational spectral clustering. Paired data are common in real world data sources, such as images with text captions. Traditional spectral clustering algorithms either assume that data can be represented by a single similarity measure, or by co-occurrence matrices that are then used in biclustering. In contrast, the proposed method uses separate similarity measures for each data representation, and allows for projection of previously unseen data that are only observed in one representation (e.g. images but not text). We show that this algorithm generalizes traditional spectral clustering algorithms and show consistent empirical improvement over spectral clustering on a variety of datasets of images with associated text."
            },
            "slug": "Correlational-spectral-clustering-Blaschko-Lampert",
            "title": {
                "fragments": [],
                "text": "Correlational spectral clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed method uses separate similarity measures for each data representation, and allows for projection of previously unseen data that are only observed in one representation (e.g. images but not text)."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47778994"
                        ],
                        "name": "A. Lahiri",
                        "slug": "A.-Lahiri",
                        "structuredName": {
                            "firstName": "Ahbik",
                            "lastName": "Lahiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41227297"
                        ],
                        "name": "B. Prochnow",
                        "slug": "B.-Prochnow",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Prochnow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Prochnow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 82
                            }
                        ],
                        "text": "The most closely related work is that of Ngiam et al. on multimodal autoencoders (Ngiam et al., 2011) and of Srivastava and Salakhutdinov on multimodal restricted Boltzmann machines (Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 241
                            }
                        ],
                        "text": "A natural next step is therefore to test the representations produced by deep CCA in the context of prediction tasks and to compare against other nonlinear multi-view representation learning approaches that optimize other objectives, e.g., (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We experimented with a stochastic method based on mini-batches, but obtained much better results with full-batch optimization using the L-BFGS second-order optimization method (Nocedal & Wright, 2006) which has been found to be useful for deep learning in other contexts (Le et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6076653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "053912e76e50c9f923a1fc1c173f1365776060cc",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining."
            },
            "slug": "On-optimization-methods-for-deep-learning-Le-Ngiam",
            "title": {
                "fragments": [],
                "text": "On optimization methods for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2916561"
                        ],
                        "name": "Alexei Vinokourov",
                        "slug": "Alexei-Vinokourov",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Vinokourov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei Vinokourov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 93
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 148
                            }
                        ],
                        "text": "\u20261984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 116
                            }
                        ],
                        "text": "CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learning features for a single view\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 37
                            }
                        ],
                        "text": ", 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 761978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18b8a90cd3668c826a10e01b2a9c680469ff28de",
            "isKey": true,
            "numCitedBy": 282,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the first and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reflect some semantic similarity. Certain patterns of English words that relate to a specific meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we first demonstrate that the correlations detected between the two versions of the corpus are significantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reflect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and significantly superior to LSI on the same data."
            },
            "slug": "Inferring-a-Semantic-Representation-of-Text-via-Vinokourov-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390603950"
                        ],
                        "name": "Mingyu Kim",
                        "slug": "Mingyu-Kim",
                        "structuredName": {
                            "firstName": "Mingyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145578392"
                        ],
                        "name": "Juhan Nam",
                        "slug": "Juhan-Nam",
                        "structuredName": {
                            "firstName": "Juhan",
                            "lastName": "Nam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juhan Nam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 82
                            }
                        ],
                        "text": "The most closely related work is that of Ngiam et al. on multimodal autoencoders (Ngiam et al., 2011) and of Srivastava and Salakhutdinov on multimodal restricted Boltzmann machines (Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 241
                            }
                        ],
                        "text": "A natural next step is therefore to test the representations produced by deep CCA in the context of prediction tasks and to compare against other nonlinear multi-view representation learning approaches that optimize other objectives, e.g., (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 27
                            }
                        ],
                        "text": "on multimodal autoencoders (Ngiam et al., 2011) and of Srivastava and Salakhutdinov on multimodal restricted Boltzmann machines (Srivastava & Salakhutdinov, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 352650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80e9e3fc3670482c1fee16b2542061b779f47c4f",
            "isKey": false,
            "numCitedBy": 2432,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning."
            },
            "slug": "Multimodal-Deep-Learning-Ngiam-Khosla",
            "title": {
                "fragments": [],
                "text": "Multimodal Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work presents a series of tasks for multimodal learning and shows how to train deep networks that learn features to address these tasks, and demonstrates cross modality feature learning, where better features for one modality can be learned if multiple modalities are present at feature learning time."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 105
                            }
                        ],
                        "text": "Deep networks have been used widely to learn representations, for example using deep Boltzmann machines (Salakhutdinov & Hinton, 2009), deep autoencoders (Hinton & Salakhutdinov, 2006), and deep nonlinear feedforward networks (Hinton et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 877639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85021c84383d18a7a4434d76dc8135fc6bdc0aa6",
            "isKey": false,
            "numCitedBy": 2024,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks."
            },
            "slug": "Deep-Boltzmann-Machines-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Deep Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new learning algorithm for Boltzmann machines that contain many layers of hidden variables that is made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87339519"
                        ],
                        "name": "M. Kanehisa",
                        "slug": "M.-Kanehisa",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Kanehisa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kanehisa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "\u2026across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8629843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d74bfa3505b5350a7c84732f3d51cf9fd66bd8b3",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm to extract features from high-dimensional gene expression profiles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression profiles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. \n \nFunction prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classifier uses the vector of features instead of the original expression profile to predict the functional class of a gene."
            },
            "slug": "Graph-Driven-Feature-Extraction-From-Microarray-and-Vert-Kanehisa",
            "title": {
                "fragments": [],
                "text": "Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This approach is validated by showing a consistent increase in performance when a state-of-the-art classifier uses the vector of features instead of the original expression profile to predict the functional class of a gene."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 38
                            }
                        ],
                        "text": "Canonical correlation analysis (CCA) (Hotelling, 1936; Anderson, 1984) is a standard statistical technique for finding linear projections of two random vectors that are maximally correlated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122166830,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45db76270416a42517a21c63a77e9c4260fa979a",
            "isKey": false,
            "numCitedBy": 5596,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of correlation and regression may be applied not only to ordinary one-dimensional variates but also to variates of two or more dimensions. Marksmen side by side firing simultaneous shots at targets, so that the deviations are in part due to independent individual errors and in part to common causes such as wind, provide a familiar introduction to the theory of correlation; but only the correlation of the horizontal components is ordinarily discussed, whereas the complex consisting of horizontal and vertical deviations may be even more interesting. The wind at two places may be compared, using both components of the velocity in each place. A fluctuating vector is thus matched at each moment with another fluctuating vector. The study of individual differences in mental and physical traits calls for a detailed study of the relations between sets of correlated variates. For example the scores on a number of mental tests may be compared with physical measurements on the same persons. The questions then arise of determining the number and nature of the independent relations of mind and body shown by these data to exist, and of extracting from the multiplicity of correlations in the system suitable characterizations of these independent relations. As another example, the inheritance of intelligence in rats might be studied by applying not one but s different mental tests to N mothers and to a daughter of each"
            },
            "slug": "Relations-Between-Two-Sets-of-Variates-Hotelling",
            "title": {
                "fragments": [],
                "text": "Relations Between Two Sets of Variates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2211263"
                        ],
                        "name": "Youssef Mroueh",
                        "slug": "Youssef-Mroueh",
                        "structuredName": {
                            "firstName": "Youssef",
                            "lastName": "Mroueh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youssef Mroueh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293163"
                        ],
                        "name": "E. Marcheret",
                        "slug": "E.-Marcheret",
                        "structuredName": {
                            "firstName": "Etienne",
                            "lastName": "Marcheret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marcheret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782589"
                        ],
                        "name": "Vaibhava Goel",
                        "slug": "Vaibhava-Goel",
                        "structuredName": {
                            "firstName": "Vaibhava",
                            "lastName": "Goel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vaibhava Goel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 351326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bceab5ec127b496b83a62131ab12e0b502f86e45",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of 41% under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of 35.83% demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of 34.03%."
            },
            "slug": "Deep-multimodal-learning-for-Audio-Visual-Speech-Mroueh-Marcheret",
            "title": {
                "fragments": [],
                "text": "Deep multimodal learning for Audio-Visual Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built is studied, demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120536806"
                        ],
                        "name": "M. E. Sargin",
                        "slug": "M.-E.-Sargin",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Sargin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Sargin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696666"
                        ],
                        "name": "Y. Yemez",
                        "slug": "Y.-Yemez",
                        "structuredName": {
                            "firstName": "Y\u00fccel",
                            "lastName": "Yemez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yemez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749677"
                        ],
                        "name": "E. Erzin",
                        "slug": "E.-Erzin",
                        "structuredName": {
                            "firstName": "Engin",
                            "lastName": "Erzin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Erzin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747853"
                        ],
                        "name": "A. Tekalp",
                        "slug": "A.-Tekalp",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tekalp",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tekalp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 254
                            }
                        ],
                        "text": "\u2026natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney & Covell,\n2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026(Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learning features for a single view when another view is available for representation learning but not at prediction time\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 86
                            }
                        ],
                        "text": ", 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learning features for a single view when another view is available for representation learning but not at prediction time (Blaschko & Lampert, 2008; Chaudhuri et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2257674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd7c133ae44b9a6117c4b1453f3e6be9335b4858",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well-known that early integration (also called data fusion) is effective when the modalities are correlated, and late integration (also called decision or opinion fusion) is optimal when modalities are uncorrelated. In this paper, we propose a new multimodal fusion strategy for open-set speaker identification using a combination of early and late integration following canonical correlation analysis (CCA) of speech and lip texture features. We also propose a method for high precision synchronization of the speech and lip features using CCA prior to the proposed fusion. Experimental results show that i) the proposed fusion strategy yields the best equal error rates (EER), which are used to quantify the performance of the fusion strategy for open-set speaker identification, and ii) precise synchronization prior to fusion improves the EER; hence, the best EER is obtained when the proposed synchronization scheme is employed together with the proposed fusion strategy. We note that the proposed fusion strategy outperforms others because the features used in the late integration are truly uncorrelated, since they are output of the CCA analysis."
            },
            "slug": "Audiovisual-Synchronization-and-Fusion-Using-Sargin-Yemez",
            "title": {
                "fragments": [],
                "text": "Audiovisual Synchronization and Fusion Using Canonical Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new multimodal fusion strategy for open-set speaker identification using a combination of early and late integration following canonical correlation analysis (CCA) of speech and lip texture features is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290352"
                        ],
                        "name": "M. Slaney",
                        "slug": "M.-Slaney",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Slaney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Slaney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1396777509"
                        ],
                        "name": "M. Covell",
                        "slug": "M.-Covell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Covell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Covell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2013 by the author(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 275
                            }
                        ],
                        "text": "\u2026natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney & Covell,\n2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6627214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4b280573d84447b87a646be5f7fe4f670bd6ee",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "FaceSync is an optimal linear algorithm that finds the degree of synchronization between the audio and image recordings of a human speaker. Using canonical correlation, it finds the best direction to combine all the audio and image data, projecting them onto a single axis. FaceSync uses Pearson's correlation to measure the degree of synchronization between the audio and image data. We derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problems caused by computing the correlation matrices."
            },
            "slug": "FaceSync:-A-Linear-Operator-for-Measuring-of-Video-Slaney-Covell",
            "title": {
                "fragments": [],
                "text": "FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The optimal linear transform is derived to combine the audio and visual information and an implementation that avoids the numerical problems caused by computing the correlation matrices is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 123
                            }
                        ],
                        "text": "To prove (15), we use the fact that for a matrix X, \u2207||X||tr = UV \u2032, where X = UDV \u2032 is the singular value decomposition of X (Bach, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3262509,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4042e97b0ce7a978599eccbe970aa790a2360a48",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled."
            },
            "slug": "Consistency-of-trace-norm-minimization-Bach",
            "title": {
                "fragments": [],
                "text": "Consistency of trace norm minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper extends some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss and provides an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678451"
                        ],
                        "name": "K. Choukri",
                        "slug": "K.-Choukri",
                        "structuredName": {
                            "firstName": "Khalid",
                            "lastName": "Choukri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Choukri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693574"
                        ],
                        "name": "G. Chollet",
                        "slug": "G.-Chollet",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Chollet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Chollet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 149
                            }
                        ],
                        "text": "\u20262002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2013 by the author(s)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62578287,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "03829093e42f96791d2240bfb8649f0094d9947e",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptation-of-automatic-speech-recognizers-to-new-Choukri-Chollet",
            "title": {
                "fragments": [],
                "text": "Adaptation of automatic speech recognizers to new speakers using canonical correlation analysis techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32238160"
                        ],
                        "name": "A. Lewis",
                        "slug": "A.-Lewis",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Lewis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 42
                            }
                        ],
                        "text": "This is easily derived from Theorem 1 of (Lewis, 1996):\nTheorem 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "This is easily derived from Theorem 1 of (Lewis, 1996):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9989214,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2843d6c16f54fd5070e9d9e0c19c14a9f43ed4f",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A spectral function of a Hermitian matrix X is a function which depends only on the eigenvalues of X, \u03bb1X \u2265 \u03bb2X \u2265... \u2265 \u03bbnX, and hence may be written f\u03bb1X, \u03bb2X,..., \u03bbnX for some symmetric function f. Such functions appear in a wide variety of matrix optimization problems. We give a simple proof that this spectral function is differentiable at X if and only if the function f is differentiable at the vector \u03bbX, and we give a concise formula for the derivative. We then apply this formula to deduce an analogous expression for the Clarke generalized gradient of the spectral function. A similar result holds for real symmetric matrices."
            },
            "slug": "Derivatives-of-Spectral-Functions-Lewis",
            "title": {
                "fragments": [],
                "text": "Derivatives of Spectral Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A spectral function of a Hermitian matrix X is a function which depends only on the eigenvalues of X if and only if the function f is differentiable at the vector \u03bbX, and the formula for the derivative is given."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172770680"
                        ],
                        "name": "N. L. Johnson",
                        "slug": "N.-L.-Johnson",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Johnson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. L. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 252
                            }
                        ],
                        "text": "\u2026similarly placing wi2 into A2 \u2208 Rn2\u00d7k, we obtain the following formulation to identify the top k \u2264 min(n1, n2) projections:\nmaximize: tr(A\u20321\u03a312A2) subject to: A\u20321\u03a311A1 = A \u2032 2\u03a322A2 = I.\n(4)\nThere are several ways to express the solution to this objective; we follow the one in (Mardia et al., 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 87
                            }
                        ],
                        "text": "There are several ways to express the solution to this objective; we follow the one in (Mardia et al., 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4206943,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "182f77976b08d832b5cdc7debdaeacc300c8e723",
            "isKey": false,
            "numCitedBy": 5733,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical AnalysisBy Prof. T. W. Anderson. (Wiley Publications in Mathematical Statistics.) Pp. xii + 374. (New York: John Wiley and Sons, Inc.; London: Chapman and Hall, Ltd., 1958.) 100s. net.Some Aspects of Multivariate AnalysisBy Prof. S. N. Roy. (Indian Statistical Series, No. 1.) Pp. viii + 214. (New York: John Wiley and Sons, Inc.; Calcutta: Indian Statistical Institute; London: Chapman and Hall, Ltd., 1957.) 64s. net.The Analysis of Multiple Time-SeriesBy M. H. Quenouille. (Griffin's Statistical Monographs and Courses, No. 1.) Pp. 105. (London: Charles Griffin and Co., Ltd., 1957.) 24s."
            },
            "slug": "Multivariate-Analysis-Johnson",
            "title": {
                "fragments": [],
                "text": "Multivariate Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33731715"
                        ],
                        "name": "J. Westbury",
                        "slug": "J.-Westbury",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Westbury",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Westbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47641812"
                        ],
                        "name": "P. Milenkovic",
                        "slug": "P.-Milenkovic",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Milenkovic",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Milenkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3461646"
                        ],
                        "name": "G. Weismer",
                        "slug": "G.-Weismer",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Weismer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weismer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2352997"
                        ],
                        "name": "Raymond D. Kent",
                        "slug": "Raymond-D.-Kent",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Kent",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond D. Kent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "The second set of experiments uses speech data from the Wisconsin X-ray Microbeam Database (XRMB) (Westbury, 1994) of simultaneous acoustic and articulatory recordings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Hyperparameter optimization selected the number of hidden units per layer in the DCCA-50-2 model as 1641 and 1769 for the MFCC and XRMB views respectively."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121260171,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "761593973b2f86863fbfa8370d06083986f921f0",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A broad review of literature describing lingual function during speech shows that speaker samples per study are typically small (N<3 in more than 80% of all cases), and that speech samples, and representational and analysis conventions are highly variable. Similar conclusions can be drawn for other articulators. Thus it is fair to argue that there is still not available any valid, statistically\u2010defensible sense of normal speech motor behavior, against which disordered articulatory behavior can be compared. Accordingly, a large\u2010sample, 50\u2010speaker x\u2010ray microbeam speech database will be developed at the University of Wisconsin, incorporating point\u2010parametrized representations of lingual, labial, mandibular, and velar movements in association with the resulting acoustic sound pressure wave, for a rich set of utterances and oral motor tasks, and lengthy recording interval (circa 18 min/speaker). The database is intended to be uniform across speakers in task inventory and descriptive kinematic framework; suffi..."
            },
            "slug": "X\u2010ray-microbeam-speech-production-database-Westbury-Milenkovic",
            "title": {
                "fragments": [],
                "text": "X\u2010ray microbeam speech production database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52476463"
                        ],
                        "name": "A. Vogler",
                        "slug": "A.-Vogler",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Vogler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vogler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 55
                            }
                        ],
                        "text": "Canonical correlation analysis (CCA) (Hotelling, 1936; Anderson, 1984) is a standard statistical technique for finding linear projections of two random vectors that are maximally correlated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 55
                            }
                        ],
                        "text": "We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63409966,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ecae668ad78e5ab6ef099808fea219f815cf5c4",
            "isKey": false,
            "numCitedBy": 3862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": " "
            },
            "slug": "An-Introduction-to-Multivariate-Statistical-Vogler",
            "title": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The introduction to multivariate statistical analysis is universally compatible with any devices to read, and will help you to cope with some harmful bugs inside their desktop computer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761880"
                        ],
                        "name": "A. Haghighi",
                        "slug": "A.-Haghighi",
                        "structuredName": {
                            "firstName": "Aria",
                            "lastName": "Haghighi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Haghighi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075292388"
                        ],
                        "name": "P. Liang",
                        "slug": "P.-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400419309"
                        ],
                        "name": "Taylor Berg-Kirkpatrick",
                        "slug": "Taylor-Berg-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Taylor",
                            "lastName": "Berg-Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taylor Berg-Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "\u2026et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 37
                            }
                        ],
                        "text": ", 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri & Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013), computer vision (Kim et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7185434,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3709b6cb2ed14c04b60e38d5f75e89c41317e93d",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types."
            },
            "slug": "Learning-Bilingual-Lexicons-from-Monolingual-Haghighi-Liang",
            "title": {
                "fragments": [],
                "text": "Learning Bilingual Lexicons from Monolingual Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421780"
                        ],
                        "name": "L. Montanarella",
                        "slug": "L.-Montanarella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Montanarella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Montanarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92493912"
                        ],
                        "name": "M. Bassani",
                        "slug": "M.-Bassani",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Bassani",
                            "middleNames": [
                                "Rosa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bassani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13771080"
                        ],
                        "name": "O. Br\u00e9as",
                        "slug": "O.-Br\u00e9as",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Br\u00e9as",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Br\u00e9as"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 121
                            }
                        ],
                        "text": "The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 122
                            }
                        ],
                        "text": "The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 97605374,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "4eec9816d69f0a566ff9f1b9a218cd10d4036a50",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of Curie-point pyrolysis mass spectrometry (Py/MS) to 33 certified wine samples from different regions of Spain, France and Italy is reported. The resulting mass spectra were submitted to principal component analysis, canonical variates analysis and cluster analysis producing a final dendrogram, using average linkage clustering. A good classification was achieved according to large geographical areas (country of origin), while no correlation was found to grape species or to local areas of cultivation. The same data were submitted to artificial neural networks analysis using the back-propagation algorithm and dividing the data into a training and a test data set. More than 90% of the test wine samples were correctly identified according to their country of origin after training of the artificial neural network. Again no correlation was found between pyrolysis mass spectra and grape species or area of cultivation. Nevertheless, the technique could be applied to rapid screenings of wine samples for the tentative identification of country of origin. In combination with other techniques (IR-MS, NMR) it can give complementary information about the samples and their regions of origin."
            },
            "slug": "Chemometric-classification-of-some-european-wines-Montanarella-Bassani",
            "title": {
                "fragments": [],
                "text": "Chemometric classification of some european wines using pyrolysis mass spectrometry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299292"
                        ],
                        "name": "K. B. Petersen",
                        "slug": "K.-B.-Petersen",
                        "structuredName": {
                            "firstName": "Kaare",
                            "lastName": "Petersen",
                            "middleNames": [
                                "Brandt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. B. Petersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122191331"
                        ],
                        "name": "M. S. Pedersen",
                        "slug": "M.-S.-Pedersen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "Syskind"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026that may account for deep networks\u2019 success in other tasks\u2014high model complexity, the ability to concisely represent a hierarchy of features for modeling real-world data distributions\u2014could be particularly useful in a setting where the output space is significantly more complex than a single label."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 197
                            }
                        ],
                        "text": "The derivation of the gradient is not entirely straightforward (involving, for example, the gradient of the trace of the matrix square-root, which we could not find in standard references such as (Petersen & Pedersen, 2012)) and is given in the appendix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 9
                            }
                        ],
                        "text": "60 from (Petersen & Pedersen, 2012) for the derivative of an inverse,\n\u2202(T \u2032T )cd \u2202(\u03a3\u030211)ab = \u2211 ij \u2202(T \u2032T )cd \u2202(\u03a3\u0302\u2212111 )ij \u2202(\u03a3\u0302\u2212111 )ij \u2202(\u03a3\u030211)ab\n= \u2212 \u2211 ij (\u03a3\u0302 \u22121/2 22 \u03a3\u030221)ci(\u03a3\u030212\u03a3\u0302 \u22121/2 22 )jd(\u03a3\u0302 \u22121 11 )ia(\u03a3\u0302 \u22121 11 )bj = \u2212(\u03a3\u0302\u22121/222 \u03a3\u030221\u03a3\u0302\u2212111 )ca(\u03a3\u0302\u2212111 \u03a3\u030212\u03a3\u0302 \u22121/2 22 )bd = \u2212(T \u2032\u03a3\u0302\u22121/211 )ca(\u03a3\u0302\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1221763,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "9682782c26bb418741304436bb3a721ffc96c0f0",
            "isKey": false,
            "numCitedBy": 2371,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Acknowledgements: We would like to thank the following for contributions and suggestions: Bill Baxter, Brian Templeton, Christian Rishoj, Christian Schroppel Douglas L. Theobald, Esben Hoegh-Rasmussen, Glynne Casteel, Jan Larsen, Jun Bin Gao, Jurgen Struckmeier, Kamil Dedecius, Korbinian Strimmer, Lars Christiansen, Lars Kai Hansen, Leland Wilkinson, Liguo He, Loic Thibaut, Miguel Barao, Ole Winther, Pavel Sakov, Stephan Hattinger, Vasile Sima, Vincent Rabaud, Zhaoshui He. We would also like thank The Oticon Foundation for funding our PhD studies."
            },
            "slug": "The-Matrix-Cookbook-Petersen-Pedersen",
            "title": {
                "fragments": [],
                "text": "The Matrix Cookbook"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026that may account for deep networks\u2019 success in other tasks\u2014high model complexity, the ability to concisely represent a hierarchy of features for modeling real-world data distributions\u2014could be particularly useful in a setting where the output space is significantly more complex than a single label."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 177
                            }
                        ],
                        "text": "We experimented with a stochastic method based on mini-batches, but obtained much better results with full-batch optimization using the L-BFGS second-order optimization method (Nocedal & Wright, 2006) which has been found to be useful for deep learning in other contexts (Le et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 198120256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c3bfffdcd313c549b2045980855ea001d6f13b",
            "isKey": false,
            "numCitedBy": 10652,
            "numCiting": 270,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side."
            },
            "slug": "Numerical-Optimization-Nocedal-Wright",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization, responding to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "We use the MNIST handwritten image dataset (LeCun & Cortes, 1998), which consists of 60,000 train images and 10,000 test images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60282629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2",
            "isKey": false,
            "numCitedBy": 4402,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough."
            },
            "slug": "The-mnist-database-of-handwritten-digits-LeCun-Cortes",
            "title": {
                "fragments": [],
                "text": "The mnist database of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An improved articulated bar flail having shearing edges for efficiently shredding materials and an improved shredder cylinder with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft are disclosed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577099"
                        ],
                        "name": "T. W. Anderson",
                        "slug": "T.-W.-Anderson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 55
                            }
                        ],
                        "text": "Canonical correlation analysis (CCA) (Hotelling, 1936; Anderson, 1984) is a standard statistical technique for finding linear projections of two random vectors that are maximally correlated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 90
                            }
                        ],
                        "text": "The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Montanarella et al., 1995), biology and neurology (Vert & Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124434321,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "86a937b1a27aa5cc60e9c10c7c175bc0c8b46b2f",
            "isKey": false,
            "numCitedBy": 906,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Introduction-to-Multivariate-Statistical-2nd-Anderson",
            "title": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical Analysis, 2nd Edition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "We use the MNIST handwritten image dataset (LeCun & Cortes, 1998), which consists of 60,000 train images and 10,000 test images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The MNIST database of hand - written digits ,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 158
                            }
                        ],
                        "text": "Often a further regularization is done by first projecting the data onto an intermediatedimensionality space, between the target and original dimensionality (Ek et al., 2008; Arora & Livescu, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ambiguity modelling in latent spaces"
            },
            "venue": {
                "fragments": [],
                "text": "In MLMI,"
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 24
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 43,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Canonical-Correlation-Analysis-Andrew-Arora/e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6?sort=total-citations"
}