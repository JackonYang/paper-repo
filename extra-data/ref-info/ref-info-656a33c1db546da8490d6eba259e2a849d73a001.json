{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5254307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0658717c77b812cab5943f13564e1aa2f06f6e71",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A general statistical description of the problem of learning from examples is presented. Learning in layered networks is posed as a search in the network parameter space for a network that minimizes an additive error function of a statistically independent examples. By imposing the equivalence of the minimum error and the maximum likelihood criteria for training the network, the Gibbs distribution on the ensemble of networks with a fixed architecture is derived. The probability of correct prediction of a novel example can be expressed using the ensemble, serving as a measure to the network's generalization ability. The entropy of the prediction distribution is shown to be a consistent measure of the network's performance. The proposed formalism is applied to the problems of selecting an optimal architecture and the prediction of learning curves. >"
            },
            "slug": "A-statistical-approach-to-learning-and-in-layered-Levin-Tishby",
            "title": {
                "fragments": [],
                "text": "A statistical approach to learning and generalization in layered neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The proposed formalism is applied to the problems of selecting an optimal architecture and the prediction of learning curves and the Gibbs distribution on the ensemble of networks with a fixed architecture is derived."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103188660"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "A.R.",
                            "lastName": "Barron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "An alternative approach is given by Barron (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 51276807,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "72d761afbe35634213849419ff63fad5bc9fabeb",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>"
            },
            "slug": "Statistical-properties-of-artificial-neural-Barron",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "The results of White (1989b) and of Kuan and White (1989) can be used to construct tests of the irrelevant input hypothesis and the irrelevant hidden unit hypothesis in ways directly analogous to those discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 84
                            }
                        ],
                        "text": "Such an assumption is implausible for the analysis of time series data, so Kuan and White (1989) apply results of Kushner and Clark (1978) and Kushner and Huang (1979) to establish consistency and limiting distribution results for dependent sequences of random variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 4.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 30
                            }
                        ],
                        "text": "One such test is described by White (1989c), and its properties are investigated by Lee et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 43
                            }
                        ],
                        "text": "Several of these are described by Kuan and White (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 23
                            }
                        ],
                        "text": "As a specific example, White (1989b) proves that learning methods that solve equation 4.2 (locally) for squared error performance are asymptotically efficient relative to the method of backpropagation regardless of the local minimizer to which convergence occurs (see Theorems 5 and 6 of the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 5.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 9
                            }
                        ],
                        "text": "Kuan and White (1989) discuss a modification of backpropagation that has asymptotic efficiency equivalent to the (local) solution of equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 3.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "The following four results are proven in White (1989b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 5.2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "White (1989a) contains an extensive discussion of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122513242,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09a4cba077f0ca74049e68e70967712bd0bdec5c",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We investigate the properties of a recursive estimation procedure (the method of \u201cback-propagation\u201d) for a class of nonlinear regression models (single hidden-layer feedforward network models) recently developed by cognitive scientists. The results follow from more general results for a class of recursive m estimators, obtained using theorems of Ljung (1977) and Walk (1977) for the method of stochastic approximation. Conditions are given ensuring that the back-propagation estimator converges almost surely to a parameter value that locally minimizes expected squared error loss (provided the estimator does not diverge) and that the back-propagation estimator is asymptotically normal when centered at this minimizer. This estimator is shown to be statistically inefficient, and a two-step procedure that has efficiency equivalent to that of nonlinear least squares is proposed. Practical issues are illustrated by a numerical example involving approximation of the Henon map."
            },
            "slug": "Some-Asymptotic-Results-for-Learning-in-Single-White",
            "title": {
                "fragments": [],
                "text": "Some Asymptotic Results for Learning in Single Hidden-Layer Feedforward Network Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3243917"
                        ],
                        "name": "P. Shoemaker",
                        "slug": "P.-Shoemaker",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Shoemaker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shoemaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19419511,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "385f68e2d50236024a349aa0584282ec53a43112",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network models are considered as mathematical classifiers whose inputs comprise random variables generated according to arbitrary stationary class distributions, and the implication of learning based on minimization of sum-square classification error over a training set of these observations for which class assignments are absolutely determined is addressed. Expectations for network outputs in such cases are weighted least-squares approximations to a posteriori probabilities for the classes, which justifies interpretation of network outputs as indicating degree of confidence in class membership. The author demonstrates this with a straightforward proof in which class probability densities are regarded as primitives and which for simplicity does not rely on probability theory or statistics. The author cites more detailed results giving conditions for consistency of the estimators and discusses some issues relating to the suitability of neural network models and back-propagation training for approximation of conditional probabilities in classification tasks."
            },
            "slug": "A-note-on-least-squares-learning-procedures-and-by-Shoemaker",
            "title": {
                "fragments": [],
                "text": "A note on least-squares learning procedures and classification by neural network models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author demonstrates this with a straightforward proof in which class probability densities are regarded as primitives and which for simplicity does not rely on probability theory or statistics."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 217
                            }
                        ],
                        "text": "When T is chosen to satisfy a certain mild condition, it can be shown that optimal weights w* have a fundamental information theoretic interpretation, related to the statistical mechanical interpretation discussed by Tishby et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "When T is chosen to satisfy a certain mild condition, it can be shown that optimal weights w* have a fundamental information theoretic interpretation, related to the statistical mechanical interpretation discussed by  Tishby et al. (1989) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15012839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10440620da8a43a1b97e3da4b1ff13746306475",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>"
            },
            "slug": "Consistent-inference-of-probabilities-in-layered-Tishby-Levin",
            "title": {
                "fragments": [],
                "text": "Consistent inference of probabilities in layered networks: predictions and generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework and the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2203810"
                        ],
                        "name": "Thierry J. Guillerm",
                        "slug": "Thierry-J.-Guillerm",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Guillerm",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thierry J. Guillerm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264754"
                        ],
                        "name": "N. E. Cotter",
                        "slug": "N.-E.-Cotter",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Cotter",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. E. Cotter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35684355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae903c80df38256327705488bd5162d5e46180f1",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The convergence of neural networks when the mapping is accompanied by noise is discussed. An average method is proposed for cases in which the network configuration leads to a noisy energy function during the learning. The proposed method features time-windowed weight averaging, which proves efficient in the presence of Gaussian noise. Temporal averaging, rather than increasing the network size, may be chosen in order to avoid adding local minima. The analysis and examples are based on feedforward network architectures. The filtering observed through the networks indicates that neural networks may be used for multidimensional nonlinear filtering"
            },
            "slug": "Neural-networks-in-noisy-environment:-a-simple-for-Guillerm-Cotter",
            "title": {
                "fragments": [],
                "text": "Neural networks in noisy environment: a simple temporal higher order learning for feed-forward networks"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The convergence of neural networks when the mapping is accompanied by noise is discussed and an average method is proposed for cases in which the network configuration leads to a noisy energy function during the learning."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "The results of White (1989b) and of Kuan and White (1989) can be used to construct tests of the irrelevant input hypothesis and the irrelevant hidden unit hypothesis in ways directly analogous to those discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 84
                            }
                        ],
                        "text": "Such an assumption is implausible for the analysis of time series data, so Kuan and White (1989) apply results of Kushner and Clark (1978) and Kushner and Huang (1979) to establish consistency and limiting distribution results for dependent sequences of random variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, if W is restricted to be a single cone of the type described by  Hecht-Nielsen (1989)  (this eliminates interchangeability), then multiple minima are no longer guaranteed.,However, it is now well established that hidden layer feedforward networks with as few as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989; Funahashi 1989;  Hecht-Nielsen 1989;  Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 4.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 30
                            }
                        ],
                        "text": "One such test is described by White (1989c), and its properties are investigated by Lee et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 43
                            }
                        ],
                        "text": "Several of these are described by Kuan and White (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 23
                            }
                        ],
                        "text": "As a specific example, White (1989b) proves that learning methods that solve equation 4.2 (locally) for squared error performance are asymptotically efficient relative to the method of backpropagation regardless of the local minimizer to which convergence occurs (see Theorems 5 and 6 of the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 5.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 9
                            }
                        ],
                        "text": "Kuan and White (1989) discuss a modification of backpropagation that has asymptotic efficiency equivalent to the (local) solution of equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 3.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "The following four results are proven in White (1989b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(White 1989b, Proposition 5.2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "White (1989a) contains an extensive discussion of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14381644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b1237609c9d95a0fb33dc95697f6b78cf7935ee",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The author presents a statistical test of the hypothesis that a given multilayer feedforward network exactly represents some unknown mapping subject to inherent noise against the alternative that the network neglects some nonlinear structure in the mapping, leading to potentially avoidable approximation errors. The tests are based on methods that statistically determine whether or not there is some advantage to be gained by adding hidden units to the network.<<ETX>>"
            },
            "slug": "An-additional-hidden-unit-test-for-neglected-in-White",
            "title": {
                "fragments": [],
                "text": "An additional hidden unit test for neglected nonlinearity in multilayer feedforward networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The author presents a statistical test of the hypothesis that a given multilayer feedforward network exactly represents some unknown mapping subject to inherent noise against the alternative that the network neglects some nonlinear structure in the mapping, leading to potentially avoidable approximation errors."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122959051"
                        ],
                        "name": "T. Ash",
                        "slug": "T.-Ash",
                        "structuredName": {
                            "firstName": "Timur",
                            "lastName": "Ash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ash"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 148
                            }
                        ],
                        "text": "The search for an appropriate technique for determining network complexity has been the focus of considerable effort to date (e.g., Rumelhart 1988; Ash 1989; Hirose et al. 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40941575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212a4ab68c4489eca22984ecd297e986693e5200",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given. A novel method called dynamic node creation (DNC) that attacks issues of training large networks and of testing networks with different numbers of hidden layer units is presented. DNC sequentially adds nodes one at a time to the hidden layer(s) of the network until the desired approximation accuracy is achieved. Simulation results for parity, symmetry, binary addition, and the encoder problem are presented. The procedure was capable of finding known minimal topologies in many cases, and was always within three nodes of the minimum. Computational expense for finding the solutions was comparable to training normal backpropagation (BP) networks with the same final topologies. Starting out with fewer nodes than needed to solve the problem actually seems to help find a solution. The method yielded a solution for every problem tried. BP applied to the same large networks with randomized initial weights was unable, after repeated attempts, to replicate some minimum solutions found by DNC.<<ETX>>"
            },
            "slug": "Dynamic-node-creation-in-backpropagation-networks-Ash",
            "title": {
                "fragments": [],
                "text": "Dynamic node creation in backpropagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel method called dynamic node creation (DNC) that attacks issues of training large networks and of testing networks with different numbers of hidden layer units is presented, which yielded a solution for every problem tried."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14470590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7f0bc85b339d781c2e0c7e6db8e339b6b9fec2",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "K.M. Hornik, M. Stinchcombe, and H. White (Univ. of California at San Diego, Dept. of Economics Discussion Paper, June 1988; to appear in Neural Networks) showed that multilayer feedforward networks with as few as one hidden layer, no squashing at the output layer, and arbitrary sigmoid activation function at the hidden layer are universal approximators: they are capable of arbitrarily accurate approximation to arbitrary mappings, provided sufficiently many hidden units are available. The present authors obtain identical conclusions but do not require the hidden-unit activation to be sigmoid. Instead, it can be a rather general nonlinear function. Thus, multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial. In particular, sigmoid activation functions are not necessary for universal approximation.<<ETX>>"
            },
            "slug": "Universal-approximation-using-feedforward-networks-Stinchcombe-White",
            "title": {
                "fragments": [],
                "text": "Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Rinnooy Kan et al. (1985) (RBT) give a survey of results from this literature that is directly relevant to finding the solution to equation 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 70
                            }
                        ],
                        "text": "As an example, we describe the multilevel single linkage algorithm of Rinnooy Kan et al. (1985). This method is a variant of the \u201dmultistart\u201d method, which has three steps:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 86
                            }
                        ],
                        "text": "This is easily recognized as the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986). Thus, the method of backpropagation can be viewed as an application of the Robbins-Monro (1951) stochastic approximation procedure to solving the first-order conditions for a nonlinear least-squares regression problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 192
                            }
                        ],
                        "text": "\u2026as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989; Funahashi 1989; Hecht-Nielsen 1989; Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": true,
            "numCitedBy": 6388,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, it is now well established that hidden layer feedforward networks with as few as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989;  Funahashi 1989;  Hecht-Nielsen 1989; Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 206
                            }
                        ],
                        "text": "\u2026as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989; Funahashi 1989; Hecht-Nielsen 1989; Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144371774"
                        ],
                        "name": "S. M. Carroll",
                        "slug": "S.-M.-Carroll",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Carroll",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2203239"
                        ],
                        "name": "B. Dickinson",
                        "slug": "B.-Dickinson",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dickinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 164
                            }
                        ],
                        "text": "\u2026as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989; Funahashi 1989; Hecht-Nielsen 1989; Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18058503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4bd5422c82009290a5cd71457388f0780530d6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.<<ETX>>"
            },
            "slug": "Construction-of-neural-nets-using-the-radon-Carroll-Dickinson",
            "title": {
                "fragments": [],
                "text": "Construction of neural nets using the radon transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699388"
                        ],
                        "name": "L. Ljung",
                        "slug": "L.-Ljung",
                        "structuredName": {
                            "firstName": "Lennart",
                            "lastName": "Ljung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ljung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "White (198913) applies results of  Ljung (1977)  and Walk (1977) to obtain consistency and limiting distribution results for equations 4.6 and 4.7 and the method of backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "White (198913) applies results of Ljung (1977) and Walk (1977) to obtain consistency and limiting distribution results for equations 4.6 and 4.7 and the method of backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123327817,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d6af193556d970da366332d1301ea0d5ea5511c2",
            "isKey": true,
            "numCitedBy": 807,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive algorithms where random observations enter are studied in a fairly general framework. An important feature is that the observations my depend on previous \"outputs\" of the algorithm. The considered class of algorithms contains, e.g., stochastic approximation algorithm, recursive identification algorithm, and algorithms for adaptive control of linear systems. It is shown how a deterministic differential equation can be associated with the algorithm. Problems like convergence with probability one, possible convergence points and asymptotic behavior of the algorithm can all be studied in terms of this differential equation. Theorems stating the precise relationships between the differential equation and the algorithm are given as well as examples of applications of the results to problems in identification and adaptive control."
            },
            "slug": "Analysis-of-recursive-stochastic-algorithms-Ljung",
            "title": {
                "fragments": [],
                "text": "Analysis of recursive stochastic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how a deterministic differential equation can be associated with the algorithm and examples of applications of the results to problems in identification and adaptive control."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 69
                            }
                        ],
                        "text": "Use of the genetic algorithm for function optimization is treated by Goldberg (1989) (see also Davis 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The genetic algorithm (Holland 1975) proceeds by viewing the opposite of A,, that is, -An, as a fitness function and w as a \u201dDNA vector.\u201d Use of the genetic algorithm for function optimization is treated by  Goldberg (1989)  (see also Davis 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38613589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e62d1345b340d5fda3b092c460264b9543bc4b5",
            "isKey": false,
            "numCitedBy": 58106,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
            },
            "slug": "Genetic-Algorithms-in-Search-Optimization-and-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search Optimization and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This book brings together the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145692989"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Phillips",
                            "middleNames": [
                                "C.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 133
                            }
                        ],
                        "text": "If convergence to a flat occurs, then the estimated weights 3, have a limiting distribution that can be analyzed using the theory of Phillips (1989) for \"partially identified\" models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118156376,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7fcb839c29ebf11d404790d163960a9064070b6d",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies a class of models where full identification is not necessarily assumed. We term such models partially identified. It is argued that partially identified systems are of practical importance since empirical investigators frequently proceed under conditions that are best described as apparent identification. One objective of the paper is to explore the properties of conventional statistical procedures in the context of identification failure. Our analysis concentrates on two major types of partially identified model: the classic simultaneous equations model under rank condition failures; and time series spurious regressions. Both types serve to illustrate the extensions that are needed to conventional asymptotic theory if the theory is to accommodate partially identified systems. In many of the cases studied, the limit distributions fall within the class of compound normal distributions. They are simply represented as covariance matrix or scalar mixtures of normals. This includes time series spurious regressions, where representations in terms of functionals of vector Brownian motion are more conventional in recent research following earlier work by the author."
            },
            "slug": "Partially-Identified-Econometric-Models-Phillips",
            "title": {
                "fragments": [],
                "text": "Partially Identified Econometric Models"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper studies a class of models where full identification is not necessarily assumed, and focuses on two major types of partially identified model: the classic simultaneous equations model under rank condition failures; and time series spurious regressions."
            },
            "venue": {
                "fragments": [],
                "text": "Econometric Theory"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588939"
                        ],
                        "name": "B. Yuhas",
                        "slug": "B.-Yuhas",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Yuhas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yuhas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32165157"
                        ],
                        "name": "M. Goldstein",
                        "slug": "M.-Goldstein",
                        "structuredName": {
                            "firstName": "Moise",
                            "lastName": "Goldstein",
                            "middleNames": [
                                "H."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32439364"
                        ],
                        "name": "R. E. Jenkins",
                        "slug": "R.-E.-Jenkins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jenkins",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. E. Jenkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1131056,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "4516f30bec0aecdf5569133ebe0b26a8f54d9c76",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "It is demonstrated that multiple sources of speech information can be integrated at a subsymbolic level to improve vowel recognition. Feedforward and recurrent neural networks are trained to estimate the acoustic characteristics of a vocal tract from images of the speaker's mouth. These estimates are then combined with the noise-degraded acoustic information, effectively increasing the signal-to-noise ratio and improving the recognition of these noise-degraded signals. Alternative symbolic strategies such as direct categorization of the visual signals into vowels are also presented. The performances of these neural networks compare favorably with human performance and with other pattern-matching and estimation techniques. >"
            },
            "slug": "Neural-network-models-of-sensory-integration-for-Yuhas-Goldstein",
            "title": {
                "fragments": [],
                "text": "Neural network models of sensory integration for improved vowel recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is demonstrated that multiple sources of speech information can be integrated at a subsymbolic level to improve vowel recognition and compare favorably with human performance and with other pattern-matching and estimation techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955900"
                        ],
                        "name": "C. Hwang",
                        "slug": "C.-Hwang",
                        "structuredName": {
                            "firstName": "Chii-Ruey",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hwang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "\u2026randomness or fuzziness in the true relation between X and Y.\nWhite (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "White (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981;  Geman and Hwang 1982;  White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning procedure for 8, under fairly general conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14650349,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2d00d92e9ff17ce7fe991e700a1ce8ced639c2c8",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum likelihood estimation often fails when the parameter takes values in an infinite dimensional space. For example, the maximum likelihood method cannot be applied to the completely nonparametric estimation of a density function from an iid sample; the maximum of the likelihood is not attained by any density. In this example, as in many other examples, the parameter space (positive functions with area one) is too big. But the likelihood method can often be salvaged if we first maximize over a constrained subspace of the parameter space and then relax the constraint as the sample size grows. This is Grenander's \"method of sieves.\" Application of the method sometimes leads to new estimators for familiar problems, or to a new motivation for an already well-studied technique. We will establish some general consistency results for the method, and then we will focus on three applications."
            },
            "slug": "Nonparametric-Maximum-Likelihood-Estimation-by-the-Geman-Hwang",
            "title": {
                "fragments": [],
                "text": "Nonparametric Maximum Likelihood Estimation by the Method of Sieves"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48385057"
                        ],
                        "name": "E. Wan",
                        "slug": "E.-Wan",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35552556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82faa6788ed8ec6fc2222b83bffccf2bb203e307",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The relationship between minimizing a mean squared error and finding the optimal Bayesian classifier is reviewed. This provides a theoretical interpretation for the process by which neural networks are used in classification. A number of confidence measures are proposed to evaluate the performance of the neural network classifier within a statistical framework."
            },
            "slug": "Neural-network-classification:-a-Bayesian-Wan",
            "title": {
                "fragments": [],
                "text": "Neural network classification: a Bayesian interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The relationship between minimizing a mean squared error and finding the optimal Bayesian classifier is reviewed and a number of confidence measures are proposed to evaluate the performance of the neural network classifier within a statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17354,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1933935"
                        ],
                        "name": "D. Ruck",
                        "slug": "D.-Ruck",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ruck",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30603673"
                        ],
                        "name": "S. Rogers",
                        "slug": "S.-Rogers",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rogers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786615"
                        ],
                        "name": "M. Kabrisky",
                        "slug": "M.-Kabrisky",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kabrisky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabrisky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693682"
                        ],
                        "name": "M. Oxley",
                        "slug": "M.-Oxley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Oxley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oxley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832848"
                        ],
                        "name": "B. Suter",
                        "slug": "B.-Suter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Suter",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13199036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b217788dd6d274ad391ee950e6f6a34033bd2fc7",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function. The result is demonstrated for both the two-class problem and multiple classes. It is shown that the outputs of the multilayer perceptron approximate the a posteriori probability functions of the classes being trained. The proof applies to any number of layers and any type of unit activation function, linear or nonlinear."
            },
            "slug": "The-multilayer-perceptron-as-an-approximation-to-a-Ruck-Rogers",
            "title": {
                "fragments": [],
                "text": "The multilayer perceptron as an approximation to a Bayes optimal discriminant function"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98362967"
                        ],
                        "name": "Ian Domowitz",
                        "slug": "Ian-Domowitz",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Domowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Domowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 148
                            }
                        ],
                        "text": "This is in fact true under appropriate conditions on I , W , and { Zt} discussed in the econometrics literature by White (1981,1982, 1984a, 1989a1, Domowitz and White (19821, and Gallant and White (1988). It is useful to give a brief description of the underlying heuristics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 96
                            }
                        ],
                        "text": "A specification test using single hidden layer feedforward networks has been proposed by White (1989~) and investigated by Lee et al. (1989). Most such specification tests are \u201cblind to certain alternatives, however, in that they will fail to detect certain departures from Ho no matter how large is n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122107786,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "2189478f6383cf09f3b1492f8a6b9909c57dadfa",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Misspecified-models-with-dependent-observations-Domowitz-White",
            "title": {
                "fragments": [],
                "text": "Misspecified models with dependent observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117841"
                        ],
                        "name": "A. Gallant",
                        "slug": "A.-Gallant",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gallant",
                            "middleNames": [
                                "Ronald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gallant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751657"
                        ],
                        "name": "D. Nychka",
                        "slug": "D.-Nychka",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Nychka",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nychka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1306,
                                "start": 32
                            }
                        ],
                        "text": "When p(&, 82) = 0, we view O1 and 8, as identical. The pair (0, p) is known as a \"metric space.\" For any function space 0 there are usually many different possible choices for p . However, once a suitable metric is specified, we can define stochastic convergence in terms of the chosen metric. The property of strong (p-) consistency of 8, for 8, holds when p(8,, 8,) --t 0 (as n --+ 00) a.s. - P. The property of weak ( p - ) consistency of 8, for 0, holds when p(8,,8,) -+ 0 prob - P. Because weak consistency is often easier to establish, we focus only on weak consistency, and drop the explicit use of the word \"weak.\" In a very precise sense, then, a \"consistent\" learning procedure for 8, is one for which the probability that 8, exceeds any specified level of approximation error relative to 8, as measured by the metric p tends to zero as the sample size n tends to infinity. Procedures that are not consistent will always make errors in classification, recognition, forecasting, or pattern completion (forms of generalization) that are eventually avoided by a consistent procedure. The only errors ultimately made by a consistent procedure are the inherently unavoidable errors ( E = Y - 8,W) arising from any fundamental randomness or fuzziness in the true relation between X and Y. White (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning procedure for 8, under fairly general conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 39
                            }
                        ],
                        "text": "For this reason, Fourier series (e.g., Gallant and Nychka 1987) and spline functions (e.g., Wahba 1984; Cox 1984) are commonly used in this context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 5
                            }
                        ],
                        "text": "procedure. White (1989a) contains an extensive discussion of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 4
                            }
                        ],
                        "text": "See Gallant and White (1988, chap. 5) for a proof of a more general result. The steps of the proof of the present result are completely analogous to those of Gallant and White. The following four results are proven in White (1989b). The notation has been adapted to correspond to that used here."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 121519004,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "da588a04e39cc38762828bb42d288b938b5e0bee",
            "isKey": true,
            "numCitedBy": 855,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Often maximum likelihood is the method of choice for fitting an econometric model to data but cannot be used because the correct specific ation of (multivariate) density that defines the likelihood is unknown. In this situation, simply put the density equal to a Hermite series and apply standard finite dimensional maximum likelihood methods. Model parameters and nearly all aspects of the unknown density itself will be estimated consistently provided that the length of the series increases with sample size. The rule for increasing series length can be data dependent. The method is applied to nonlinear regression with sample selection. Copyright 1987 by The Econometric Society."
            },
            "slug": "Semi-nonparametric-Maximum-Likelihood-Estimation-Gallant-Nychka",
            "title": {
                "fragments": [],
                "text": "Semi-nonparametric Maximum Likelihood Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146284512"
                        ],
                        "name": "Hai Huang",
                        "slug": "Hai-Huang",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hai Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such an assumption is implausible for the analysis of time series data, so Kuan and White (1989) apply results of Kushner and Clark (1978) and  Kushner and Huang (1979)  to establish consistency and limiting distribution results for dependent sequences of random variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "Such an assumption is implausible for the analysis of time series data, so Kuan and White (1989) apply results of Kushner and Clark (1978) and Kushner and Huang (1979) to establish consistency and limiting distribution results for dependent sequences of random variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119843387,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f1d6b97447767abbe14200e8868b4ade8075b5e4",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general form of the stochastic approximation algorithm$X_{n + 1} = X_n + a_n h(X_n ,\\xi _n )$, where h is not necessarily additive in $\\xi _n $. Such algorithms occur frequently in applications to adaptive control and identification problems, where $\\{ \\xi _n \\} $ is usually obtained from measurements of the input and output, and is almost always complicated enough that the more classical assumptions on the noise fail to hold. Let $a_n = {A / {(n + 1)^\\alpha }}$, $0 < \\alpha \\leqq 1$, and let $X_n \\to \\theta $ w.p. 1. Define $U_n = (n + 1)^{{\\alpha / 2}} (X_n - \\theta )$. Then, loosely speaking, it is shown that the sequence of suitable continuous parameter interpolations of the sequence of \u201ctails\u201d of $\\{ U _n \\} $ converges weakly to a Gaussian diffusion. From this we can get the asymptotic variance of $U _n $ as well as other information. The assumptions on $\\{ \\xi _n \\} $ and $h( \\cdot , \\cdot )$ are quite reasonable from the point of view of applications."
            },
            "slug": "RATES-OF-CONVERGENCE-FOR-STOCHASTIC-APPROXIMATION-Kushner-Huang",
            "title": {
                "fragments": [],
                "text": "RATES OF CONVERGENCE FOR STOCHASTIC APPROXIMATION TYPE ALGORITHMS"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work considers the general form of the stochastic approximation algorithm X_{n + 1} = X_n + a_n h(X_n ,\\xi _n )$, where h is not necessarily additive in $\\xi_n $, and shows that the sequence of suitable continuous parameter interpolations of the sequences of \u201ctails\u201d of $\\{ U _n \\} $ converges weakly to a Gaussian diffusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123057104,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "d5203cc7a2f667f2bfda8dbf22edac71b1447efb",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Under general conditions given here, the least squares estimator for the parameters of a misspecified nonlinear model converge strongly to the parameters of a (weighted) least squares approximation to the true model. With additional conditions, the least squares estimator is asymptotically normal. A new, specification-robust estimator of the covariance matrix is obtained, which simplifies to the usual estimator when the model is correct up to an independent additive error. The properties of the approximation and the covariance estimator are exploited to yield new tests for model misspecification. These results are applied to two examples in economics."
            },
            "slug": "Consequences-and-Detection-of-Misspecified-Models-White",
            "title": {
                "fragments": [],
                "text": "Consequences and Detection of Misspecified Nonlinear Regression Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582515"
                        ],
                        "name": "J. D. Bryngelson",
                        "slug": "J.-D.-Bryngelson",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Bryngelson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Bryngelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47289680"
                        ],
                        "name": "S. Southard",
                        "slug": "S.-Southard",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Southard",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Southard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62557393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f494d374fcbbc99bfcd787feec9c9d596824fcd",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-protein-structure-predictor-based-on-an-energy-Bryngelson-Hopfield",
            "title": {
                "fragments": [],
                "text": "A protein structure predictor based on an energy model with learned parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741615"
                        ],
                        "name": "B. Hajek",
                        "slug": "B.-Hajek",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Hajek",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hajek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Hajek (1985, 1988) gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hajek (1985, 1988)  gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9928671,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a98f9f0385285db083112025d8e5e4f186215d4a",
            "isKey": true,
            "numCitedBy": 150,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Annealing is the process of slowly cooling a physical system in order to obtain states with globally minimum energy. By simulating such a process, near globally-minimum-cost solutions can be found for very large optimization problems. The purpose of this paper is to review the basic theory of simulated annealing, to survey its recent applications, and to survey the theoretical approaches that have been used to study the technique. The applications include image restoration, combinatorial optimization (eg VLSI routing and placement), code design for communication systems and certain aspects of artificial intelligence. The theoretical tools for analysis include the theory of nonstationary Markov chains, statistical physics analysis techniques, large deviation theory and singular perturbation theory."
            },
            "slug": "A-tutorial-survey-of-theory-and-applications-of-Hajek",
            "title": {
                "fragments": [],
                "text": "A tutorial survey of theory and applications of simulated annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "The basic theory of simulated annealing is reviewed, its recent applications are surveyed, and the theoretical approaches that have been used to study the technique are surveyed."
            },
            "venue": {
                "fragments": [],
                "text": "1985 24th IEEE Conference on Decision and Control"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The genetic algorithm ( Holland 1975 ) proceeds by viewing the opposite of A,, that is, -An, as a fitness function and w as a \u201dDNA vector.\u201d Use of the genetic algorithm for function optimization is treated by Goldberg (1989) (see also Davis 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38743,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456647"
                        ],
                        "name": "R. Davies",
                        "slug": "R.-Davies",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Davies",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Davies"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Hypothesis testing in the present context has been studied by Davies (1977, 1987); the analysis is complicated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hypothesis testing in the present context has been studied by  Davies (1977, 1987) ; the analysis is complicated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209835858,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eb4d812c9bf8a08cf086a6f0a51b25d72bef9da6",
            "isKey": false,
            "numCitedBy": 1451,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We wish to test a simple hypothesis against a family of alternatives indexed by a one-dimensional parameter, 0. We use a test derived from the corresponding family of test statistics appropriate for the case when 0 is given. Davies (1977) introduced this problem when these test statistics had normal distributions. The present paper considers the case when their distribution is chi-squared. The results are applied to the detection of a discrete frequency component of unknown frequency in a time series. In addition quick methods for finding approximate significance probabilities are given for both the normal and chi-squared cases and applied to the two-phase regression problem in the normal case."
            },
            "slug": "Hypothesis-testing-when-a-nuisance-parameter-is-the-Davies",
            "title": {
                "fragments": [],
                "text": "Hypothesis testing when a nuisance parameter is present only under the alternative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The method of simulated annealing ( Kirkpatrick et al. 1983;  Cerny 1985) proceeds by leading an \"energy landscape\" over the state space W. It is desired to settle into a low energy state, the lowest being 8,. Different annealing strategies arise depending on whether W is a finite set or is a continuum, but the basic idea is to start at some initial weight vector and compute the \"energy\" (value of 1,) for a nearby weight ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "The method of simulated annealing (Kirkpatrick et al. 1983; Cerny 1985) proceeds by leading an \"energy landscape\" over the state space W ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39637,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116308413,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dcc9e825771c36a3e5ce09ed0edecb7cef3f9554",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a number of results which describe the behavior of the maximum likelihood estimator of the parameters of a dynamic model which is incorrectly or incompletely specified. We provide conditions which ensure the existence of the Quasi-Maximum Likelihood Estimator (QMLE) and its consistency for the parameters of an approximation to the unknown true probability density which has optimal information theoretic properties. The ability of the QMLE to consistently estimate certain parameters of interest despite misspecification is investigated. We give conditions ensuring the asymptotic normality of the QMLE together with conditions under which its asymptotic covariance matrix may be consistently estimated. Two model specification tests are briefly discussed."
            },
            "slug": "Maximum-Likelihood-Estimation-of-Misspecified-White",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Estimation of Misspecified Dynamic Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144312915"
                        ],
                        "name": "J. Ortega",
                        "slug": "J.-Ortega",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ortega",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ortega"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001942"
                        ],
                        "name": "W. Rheinboldt",
                        "slug": "W.-Rheinboldt",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Rheinboldt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rheinboldt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 76
                            }
                        ],
                        "text": "A discussion of these and much additional relevant material can be found in Ortega and Rheinboldt (1970) and Rheinboldt (1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A discussion of these and much additional relevant material can be found in  Ortega and Rheinboldt (1970)  and Rheinboldt (1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39585209,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ef098e0154da4b210a6ee11b84ca30bd3e445ac6",
            "isKey": false,
            "numCitedBy": 7887,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Classics Edition Preface Acknowledgments Glossary of Symbols Introduction Part I. Background Material. 1. Sample Problems 2. Linear Algebra 3. Analysis Part II. Nonconstructive Existence Theorems. 4. Gradient Mappings and Minimization 5. Contractions and the Continuation Property 6. The Degree of a Mapping Part III. Iterative Methods. 7. General Iterative Methods 8. Minimization Methods Part IV. Local Convergence. 9. Rates of Convergence-General 10. One-Step Stationary Methods 11. Multistep Methods and Additional One-Step Methods Part V. Semilocal and Global Convergence. 12. Contractions and Nonlinear Majorants 13. Convergence under Partial Ordering 14. Convergence of Minimization Methods An Annotated List of Basic Reference Books Bibliography Author Index Subject Index."
            },
            "slug": "Iterative-solution-of-nonlinear-equations-in-Ortega-Rheinboldt",
            "title": {
                "fragments": [],
                "text": "Iterative solution of nonlinear equations in several variables"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Convergence of Minimization Methods An Annotated List of Basic Reference Books Bibliography Author Index Subject Index."
            },
            "venue": {
                "fragments": [],
                "text": "Computer science and applied mathematics"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53480826,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "395bc88e0c22c646acd1d95d69ccca9c03e4113d",
            "isKey": false,
            "numCitedBy": 4664,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (QMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification."
            },
            "slug": "Maximum-Likelihood-Estimation-of-Misspecified-White",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Estimation of Misspecified Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The results of White (1989b) and of Kuan and White (1989) can be used to construct tests of the irrelevant input hypothesis and the irrelevant hidden unit hypothesis in ways directly analogous to those discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Kushner (1987) has studied a modification of equation 4.7 guaranteed to converge w.p.1 to a global solution to equation 4.1 as n + 00."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119339305,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f5081943e0bdc65dc4af7da55a073a5118afe77f",
            "isKey": true,
            "numCitedBy": 152,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The asymptotic behavior of the systems $X_{n + 1} = X_n + a_n b( {X_n ,\\xi _n } ) + a_n \\sigma ( X_n )\\psi_n $ and $dy = \\bar b( y )dt + \\sqrt {a( t )} \\sigma ( y )dw$ is studied, where $\\{ {\\psi _n } \\}$ is i.i.d. Gaussian, $\\{ \\xi _n \\}$ is a (correlated) bounded sequence of random variables and $a_n \\approx A_0/\\log (A_1 + n )$. Without $\\{ \\xi _n \\}$, such algorithms are versions of the \u201csimulated annealing\u201d method for global optimization. When the objective function values can only be sampled via Monte Carlo, the discrete algorithm is a combination of stochastic approximation and simulated annealing. Our forms are appropriate. The $\\{ \\psi _n \\}$ are the \u201cannealing\u201d variables, and $\\{ \\xi _n \\}$ is the sampling noise. For large $A_0 $, a full asymptotic analysis is presented, via the theory of large deviations: Mean escape time (after arbitrary time n) from neighborhoods of stable sets of the algorithm, mean transition times (after arbitrary time n) from a neighborhood of one stable set to another, a..."
            },
            "slug": "Asymptotic-global-behavior-for-stochastic-and-with-Kushner",
            "title": {
                "fragments": [],
                "text": "Asymptotic global behavior for stochastic approximation and diffusions with slowly decreasing noise effects: Global minimization via Monte Carlo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229585"
                        ],
                        "name": "D. Clark",
                        "slug": "D.-Clark",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Clark",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Clark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117328031,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0009c5a2b4b07751a99bcf407d95e911a3064d0f",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I. Introduction.- 1.1. General Remarks.- 1.2. The Robbins-Monro Process.- 1.3. A \"Continuous\" Process Version of Section 2.- 1.4. Regulation of a Dynamical System a simple example.- 1.5. Function Minimization: The Kiefer-Wolfowitz Procedure.- 1.6. Constrained Problems.- 1.7. An Economics Example.- II. Convergence w.p.1 for Unconstrained Systems.- 2.1. Preliminaries and Motivation.- 2.2. The Robbins-Monro and Kiefer-Wolfowitz Algorithms: Conditions and Discussion.- 2.3. Convergence Proofs for RM and KW-like Procedures.- 2.3.1. A Basic RM-like Procedure.- 2.3.2. One Dimensional RM and Accelerated RM Procedures.- 2.3.3. A Continuous Parameter RM Procedure.- 2.3.4. The Basic Kiefer-Wolfowitz Procedure.- 2.3.5. Random Directions KW Methods.- 2.4. A General Robbins-Monro Process: \"Exogenous Noise\".- 2.4.1. The Case of Bounded h(*,*).- 2.4.2. Unbounded h(*,*): Exogenous Noise.- 2.5. A General RM Process State Dependent Noise.- 2.5.1. Extensions and Localizations of Theorem 2.5.2.- 2.6. Some Applications.- 2.7. Mensov-Rademacher Estimates.- III. Weak Convergence of Probability Measures.- IV. Weak Convergence for Unconstrained Systems.- 4.1. Conditions and General Discussion.- 4.2. The Robbins-Monro and Kiefer-Wolfowitz Procedures.- 4.2.1. The Basic Robbins-Monro Procedure.- 4.2.2. The One-Dimensional Robbins-Monro Procedure.- 4.2.3. The Kiefer-Wolfowitz Procedure.- 4.2.4. A Case Where the Limit Satisfies a Generalized ODE.- 4.2.5. A Continuous Parameter KW Procedure.- 4.3. A General Robbins-Monro Process: Exogenous Noise.- 4.4. A General RM Process: State Dependent Noise.- 4.5. The Identification Problem.- 4.6. A Counter-Example to Tightness.- 4.7. Boundedness of {Xn} and Tightness of {Xn(*)}.- V. Convergence w.p.1 For Constrained Systems.- 5.1. A Penalty-Multiplier Algorithm for Equality Constraints.- 5.1.1. A Basic RM-like Algorithm, Conditions and Discussion.- 5.1.2. The Noise Condition, Discussion and Generalization.- 5.1.3. Boundedness of {Xn}.- 5.1.4. Proof of the Main Theorem.- 5.1.5. Constrained Function Minimization and Other Extensions.- 5.2. A Lagrangian Method for Inequality Constraints.- 5.2.1. The Algorithm and Conditions.- 5.2.2. The Convergence Theorem 18.- 5.2.3. A Non-Convergent but Useful Algorithm.- 5.2.4. An Application to the Identification Problem.- 5.3. A Projection Algorithm.- 5.4. A Penalty-Multiplier Method for Inequality Constraints.- VI. Weak Convergence: Constrained Systems.- 6.1. A Multiplier Type Algorithm for Equality Constraints.- 6.1.1. Boundedness of {Xn}.- 6.1.2. The Noise Condition, Discussion.- 6.1.3. The Convergence Theorem.- 6.2. The Lagrangian Method.- 6.3. A Projection Algorithm.- 6.4. A Penalty-Multiplier Algorithm for Inequality Constraints.- VII. Rates of Convergence.- 7.1. The Problem Formulation.- 7.2. Conditions and Discussions.- 7.3. Rates of Convergence for Case 1, the KW Algorithm.- 7.4. Discussion of Rates of Convergence for Two KW Algorithms."
            },
            "slug": "wchastic.-approximation-methods-for-constrained-and-Kushner-Clark",
            "title": {
                "fragments": [],
                "text": "wchastic. approximation methods for constrained and unconstrained systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Robbins-Monro and Kiefer-Wolfowitz Algorithm for Inequality Constraints and the Weak Convergence of Probability Measures, a simple example, and the Convergence Theorem, a proof of the Main Theorem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738371"
                        ],
                        "name": "R. Serfling",
                        "slug": "R.-Serfling",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Serfling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Serfling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "In particular, large deviation inequalities and Berry-Esseen inequalities (see, e.g., Serfling 1980) can provide information on the rate at which convergence to the limiting set or limiting distribution occurs under specific conditions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122686903,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0d2e8d420f3932945ab92c44bc9e44ac78cf8db6",
            "isKey": false,
            "numCitedBy": 3682,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Preliminary Tools and Foundations. The Basic Sample Statistics. Transformations of Given Statistics. Asymptotic Theory in Parametric Inference. U--Statistics. Von Mises Differentiable Statistical Functions. M--Estimates. L--Estimates. R--Estimates. Asymptotic Relative Efficiency. Appendix. References. Author Index. Subject Index."
            },
            "slug": "Approximation-Theorems-of-Mathematical-Statistics-Serfling",
            "title": {
                "fragments": [],
                "text": "Approximation Theorems of Mathematical Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001942"
                        ],
                        "name": "W. Rheinboldt",
                        "slug": "W.-Rheinboldt",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Rheinboldt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rheinboldt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 109
                            }
                        ],
                        "text": "A discussion of these and much additional relevant material can be found in Ortega and Rheinboldt (1970) and Rheinboldt (1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61656662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49a2ab169c929ef4fea7d3a8cc66db280c56bd59",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis second edition provides much-needed updates to the original volume. Like the first edition, it emphasizes the ideas behind the algorithms as well as their theoretical foundations and properties, rather than focusing strictly on computational details; at the same time, this new version is now largely self-contained and includes essential proofs. Applied scientists will find this new edition most useful."
            },
            "slug": "Methods-for-solving-systems-of-nonlinear-equations-Rheinboldt",
            "title": {
                "fragments": [],
                "text": "Methods for solving systems of nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This second edition provides much-needed updates to the original volume and emphasizes the ideas behind the algorithms as well as their theoretical foundations and properties, rather than focusing strictly on computational details."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145198149"
                        ],
                        "name": "V. Cern\u00fd",
                        "slug": "V.-Cern\u00fd",
                        "structuredName": {
                            "firstName": "Vladim\u00edr",
                            "lastName": "Cern\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cern\u00fd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The method of simulated annealing (Kirkpatrick et al. 1983;  Cerny 1985 ) proceeds by leading an \"energy landscape\" over the state space W. It is desired to settle into a low energy state, the lowest being 8,. Different annealing strategies arise depending on whether W is a finite set or is a continuum, but the basic idea is to start at some initial weight vector and compute the \"energy\" (value of 1,) for a nearby weight ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 60
                            }
                        ],
                        "text": "The method of simulated annealing (Kirkpatrick et al. 1983; Cerny 1985) proceeds by leading an \"energy landscape\" over the state space W ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122729427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc643e958e762ab72da9277ca64ef3bf74f90e4f",
            "isKey": false,
            "numCitedBy": 2194,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a Monte Carlo algorithm to find approximate solutions of the traveling salesman problem. The algorithm generates randomly the permutations of the stations of the traveling salesman trip, with probability depending on the length of the corresponding route. Reasoning by analogy with statistical thermodynamics, we use the probability given by the Boltzmann-Gibbs distribution. Surprisingly enough, using this simple algorithm, one can get very close to the optimal solution of the problem or even find the true optimum. We demonstrate this on several examples.We conjecture that the analogy with thermodynamics can offer a new insight into optimization problems and can suggest efficient algorithms for solving them."
            },
            "slug": "Thermodynamical-approach-to-the-traveling-salesman-Cern\u00fd",
            "title": {
                "fragments": [],
                "text": "Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is conjecture that the analogy with thermodynamics can offer a new insight into optimization problems and can suggest efficient algorithms for solving them."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47071493"
                        ],
                        "name": "J. Davidson",
                        "slug": "J.-Davidson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davidson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Davidson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117841"
                        ],
                        "name": "A. Gallant",
                        "slug": "A.-Gallant",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gallant",
                            "middleNames": [
                                "Ronald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gallant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 154835046,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "900b07497a7dcc5d93f1d7cf7d8bbf269ebdd8c9",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "2. The data generation process and optimization estimators 3. Consistency of optimization estimators 4. More on near epoch dependence 5. Asymptotic mormality 6. Estimating asymptotic cavariance matrices 7. Hypothesis testing"
            },
            "slug": "A-Unified-Theory-of-Estimation-and-Inference-for-Davidson-Gallant",
            "title": {
                "fragments": [],
                "text": "A Unified Theory of Estimation and Inference for Nonlinear Dynamic Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13533363,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "37807e97c624fb846df7e559553b32539ba2ea5d",
            "isKey": false,
            "numCitedBy": 1805,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Universal-approximation-of-an-unknown-mapping-and-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 59
                            }
                        ],
                        "text": "This is easily recognized as the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 271
                            }
                        ],
                        "text": "When ~ ( y , 0) = (y - 0)\u2019/2, we have\nVKZ, w) = -of(., w)(y - f(z, w)) so that\nG, = Gn-l + qnVf(X, , G,-l)(Y, - f(X,, Gn-l)), n = 1,2, . . . .\nLearning in Artificial Neural Networks 449\nThis is easily recognized as the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 60
                            }
                        ],
                        "text": "network using some variant of the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 225
                            }
                        ],
                        "text": "In many of these cases, success has been achieved by the now rather simple expedient of appropriately training a hidden layer feedforward network using some variant of the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": true,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741615"
                        ],
                        "name": "B. Hajek",
                        "slug": "B.-Hajek",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Hajek",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hajek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Hajek (1985, 1988) gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hajek (1985, 1988)  gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2287187,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "df511a5d9d12bff681438e2dbe2ecef70268c9c9",
            "isKey": true,
            "numCitedBy": 1273,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A Monte Carlo optimization technique called \u201csimulated annealing\u201d is a descent algorithm modified by random ascent moves in order to escape local minima which are not global minima. The level of randomization is determined by a control parameter T, called temperature, which tends to zero according to a deterministic \u201ccooling schedule.\u201d We give a simple necessary and sufficient condition on the cooling schedule for the algorithm state to converge in probability to the set of globally minimum cost states. In the special case that the cooling schedule has parametric form Tt = c/log1 + t, the condition for convergence is that c be greater than or equal to the depth, suitably defined, of the deepest local minimum which is not a global minimum state."
            },
            "slug": "Cooling-Schedules-for-Optimal-Annealing-Hajek",
            "title": {
                "fragments": [],
                "text": "Cooling Schedules for Optimal Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A Monte Carlo optimization technique called \u201csimulated annealing\u201d is a descent algorithm modified by random ascent moves in order to escape local minima which are not global minima."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286518"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Further general discussion of the meaning of Kullback-Leibler Information in a related context is given in White (1989a, chaps. 2-51. Viewing learning as related to Kullback-Leibler Information in this way implies that learning is a quasi-maximum likelihood statistical estimation procedure. White (1989a) contains an extensive discussion of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 95
                            }
                        ],
                        "text": "Use of the genetic algorithm for function optimization is treated by Goldberg (1989) (see also Davis 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 9
                            }
                        ],
                        "text": "See also Davis (1987) and van Laarhoven (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58113852,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "d0f1a43944f9c80768e3398f3ee732d7301c196d",
            "isKey": false,
            "numCitedBy": 1426,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A detergent composition mainly for automatic laundering machines which comprises, on the basis of 100 parts by weight of total composition, at least 60 parts of soap and no more than 10 parts of a mixture of surfactants comprising 10 to 30% of at least one non-ionic polyoxyalkylated surfactant and 90 to 70% of an anionic surfactant selected essentially from alpha -sulfonated fatty acids derivatives, the remainder of the composition comprising at least one ingredient selected from alkaline detergent additives, bleaching agents, optical brighteners, fragrances, antiredeposition agents and enzymes. The non-ionic surfactants are preferably fatty acid amides derived from tallow, copra or palm-oil condensed with polyoxyethylene residues. The anionic surfactants are preferably alpha -sulfonated fatty esters or amides derived from tallow, copra or palm-oil. The proper combination of said non-ionic and anionic surfactants with soaps impart to the laundering compositions an excellent detergent ability and foam control even in very soft waters and non-polluting properties."
            },
            "slug": "Genetic-Algorithms-and-Simulated-Annealing-Davis",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms and Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A detergent composition mainly for automatic laundering machines which comprises, on the basis of 100 parts by weight of total composition, at least 60 parts of soap and no more than 10 parts of a mixture of surfactants which impart an excellent detergent ability and foam control even in very soft waters and non-polluting properties."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121982591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "97c50afbf5f9106dd1e1fa04397ab805c316f699",
            "isKey": false,
            "numCitedBy": 1667,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Linear Model and Instrumental Variables Estimators. Consistency. Laws of Large Numbers. Asymptotic Normality. Central Limit Theory. Estimating Asymptotic Covariance Matrices. Functional Central Limit Theory and Applications. Directions for Further Study. Solution Set. References. Index."
            },
            "slug": "Asymptotic-theory-for-econometricians-White",
            "title": {
                "fragments": [],
                "text": "Asymptotic theory for econometricians"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48801723"
                        ],
                        "name": "F. Downton",
                        "slug": "F.-Downton",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Downton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Downton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 132
                            }
                        ],
                        "text": "A useful review of recursive estimation methods such as the RobbinsMonro and Kiefer-Wolfowitz procedures has recently been given by Ruppert (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4224834,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "555a7921231b86464a8db34ada9edade765bf8f0",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic ApproximationBy M. T. Wasan. (Cambridge Tracts in Mathematics and Mathematical Physics, No. 58.) Pp. x + 202. (Cambridge University Press: London, June 1969.) 70s; $9.50."
            },
            "slug": "Stochastic-Approximation-Downton",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144584328"
                        ],
                        "name": "G. Nielson",
                        "slug": "G.-Nielson",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Nielson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nielson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120830229,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7513a529028dfee2f55cbb86c8e267c914ae7c4d",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A theorem that characterizes spline functions that both smooth and interpolate is given. A bivariate generalization is presented which permits interpolation and smoothing of information which is not necessarily on a rectangular grid. A theorem which involves reproducing kernels for Hilbert spaces unifies this theory."
            },
            "slug": "Multivariate-Smoothing-and-Interpolating-Splines-Nielson",
            "title": {
                "fragments": [],
                "text": "Multivariate Smoothing and Interpolating Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A discussion of these and much additional relevant material can be found in Ortega and Rheinboldt (1970) and  Rheinboldt (1974) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 83
                            }
                        ],
                        "text": "In particular, White (1988) discusses use of the method of cross-validation (e.g., Stone 1974) to determine network complexity appropriate for a training set of given size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62698647,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7b28610d2d681a11398eb614de0d70d7de41c20c",
            "isKey": false,
            "numCitedBy": 7502,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance."
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101681617"
                        ],
                        "name": "P. Billingsley",
                        "slug": "P.-Billingsley",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Billingsley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Billingsley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "The conditions stated rely on concepts of probability, measure, and integration that are well presented in, for example, Bartle (1966) and Billingsley (1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "Given domination of 1 and compactness of W , it follows from Theorem 16.8(i) of Billingsley (1979) that X is continuous on W ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122274264,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1d4e9d95dd060a84e9054047e0314261949a7804",
            "isKey": true,
            "numCitedBy": 2179,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability. Measure. Integration. Random Variables and Expected Values. Convergence of Distributions. Derivatives and Conditional Probability. Stochastic Processes. Appendix. Notes on the Problems. Bibliography. List of Symbols. Index."
            },
            "slug": "Probability-and-Measure-Billingsley",
            "title": {
                "fragments": [],
                "text": "Probability and Measure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "This is in fact true under appropriate conditions on I , W , and { Zt} discussed in the econometrics literature by White (1981,1982, 1984a, 1989a1, Domowitz and White (19821, and Gallant and White (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 33
                            }
                        ],
                        "text": "We have described recent work of White (1988) along these lines, establishing that arbitrary mappings can indeed be learned using multilayer feedforward networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 27
                            }
                        ],
                        "text": "Theorem 7 of the appendix (White 1988) gives precise conditions on 0, { Zt} (equivalently, P), $1, {q,} and {A,} that ensure the consistency\n454 Halbert White\nof 8, for 0, in 0 in the sense of the root mean square metric p2,\nNote that the integral is taken with respect to the environment measure p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "Theorem 7 of the appendix (White 1988) gives precise conditions on 0, { Zt} (equivalently, P), $1, {q,} and {A,} that ensure the consistency"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "In particular, White (1988) discusses use of the method of cross-validation (e.g., Stone 1974) to determine network complexity appropriate for a training set of given size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "White (1988) considers approximations obtained using single hidden layer feedforward networks with output functions\nativity), p(Bl, 6 2 ) = p(&, 8,) (symmetry), and p(&, I p(&, B3) + do3, 82)\n4 f\"c., WQ) w10 + c wlj$di-hoj), (5.1) where wq = (w;, wi)' is the s x 1 (s = q(r + 2) + 1) vector of\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "\u2026are the inherently unavoidable errors ( E = Y - 8,W) arising from any fundamental randomness or fuzziness in the true relation between X and Y.\nWhite (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 1
                            }
                        ],
                        "text": "(White 1988, Theorem 4.5)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks can learn arbitrary mappings: Connectionist nonparametric regression with automatic and semi-automatic determination of network complexity"
            },
            "venue": {
                "fragments": [],
                "text": "UCSD Department of Economics Discussion Paper."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 320,
                                "start": 88
                            }
                        ],
                        "text": "The \"temperature\" is lowered at an appropriate rate so as to control the probability of jumping away from relatively good minima. Hajek (1985, 1988) gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2. See also Davis (1987) and van Laarhoven (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 58
                            }
                        ],
                        "text": "White (1989a) contains an extensive discussion of this subject. Relevant discussion of these and related issues in a connectionist context is provided in a paper by Golden (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "White (198913) applies results of Ljung (1977) and Walk (1977) to obtain consistency and limiting distribution results for equations 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 439,
                                "start": 323
                            }
                        ],
                        "text": ", N from the uniform distribution over W and initiate local search from each weight, unless (1) w1 is too close to the boundary of W (within a distance 7 > 0, in RBT\u2019s notation); (2) 20\u2019 is too close to a previously identified local minimizer (within a distance w > 0 in RBT\u2019s notation); or (3) there is a weight vector w3,j # i, such that i,(w3) < i,(wz) and w3 is close to w2 (within a distance T N > 0 in RBT\u2019s notation). Timmer (1984) proves that if there is a finite number of minima and if ?-N is chosen appropriately and tends to zero as N -+ 00, then any local minimum iir, (and consequently global minimum 8,) will be found within a finite number of iterations, with probability 1; the reader is referred to Timmer (1984) for further discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2214,
                                "start": 283
                            }
                        ],
                        "text": "2, the local nature of the analysis of limiting distributions and the freedom allowed by the ability to choose W in any convenient way (and in particular as a small neighborhood of the set to which the learning method ultimately converges) means that the foregoing remarks apply not just to learning methods that globally solve equation 4.2, but also to methods that find a local solution to 4.2. Accordingly, we can think of w* as being a unique local solution to 4.2 in the discussion to follow. In Theorem 2 of the appendix, we give conditions ensuring that the limiting distribution of f i ( G n - w*) is the multivariate normal distribution with mean vector zero and an s x s covariance matrix (say c*) that can be given a precise analytic expression. We refer to C\u2019 as the \u201dasymptotic covariance matrix\u201d of 8,. The smaller this covariance matrix is (as measured by, say, tr C* or det C*) the more tightly the distribution of 6, is concentrated around w*, with less consequent uncertainty about the value of w*. It is therefore desirable that C\u2019 be small, but there are fundamental limits on how small C* can be. When two learning methods yield weights 61, and &, respectively, that are both consistent for w+, one with asymptotic covariance matrix C;, and other with asymptotic covariance matrix C;, the method yielding the smaller asymptotic covariance matrix is preferable, because that method makes relatively more \u201cefficient\u201d use of the same sample information. In certain cases, it can be shown that C; - C; is a positive semidefinite matrix, in which case it is said that the second method is \u201casymptotically efficient\u201d relative to the first method. Thus, study of the limiting distribution of alternative learning methods can yield insight into the relative desirability of different learning methods. As a specific example, White (1989b) proves that learning methods that solve equation 4.2 (locally) for squared error performance are asymptotically efficient relative to the method of backpropagation regardless of the local minimizer to which convergence occurs (see Theorems 5 and 6 of the appendix). In this sense the method of backpropagation is statistically inefficient. Kuan and White (1989) discuss a modification of backpropagation that has asymptotic efficiency equivalent to the (local) solution of equation 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2068,
                                "start": 30
                            }
                        ],
                        "text": "the procedure of failing to reject H o whenever nG~S\u2018(S&S\u2019)-\u2019SG, fails to exceed the 1 - a percentile of the xi distribution (for some typically small value of a, say a = 0.05 or a = 0.01) leads to incorrect rejection of the irrelevant input hypothesis with (small) probability approximately equal or less than a. As n becomes large the probability of correctly rejecting the irrelevant input hypothesis with this procedure tends to one. This procedure is an application of standard techniques of statistical inference. It allows us to determine whether specific input(s) are irrelevant, to the extent permitted by the sample evidence by controlling the probability of incorrectly rejecting Ho. This approach has obvious applications in investigating the appropriateness of given network architectures. The irrelevant hidden unit hypothesis is of exactly the same form, that is, H o : Sw\u2019 = 0, except that now the q x s selection matrix S picks out weights associated with q hidden units hypothesized to be irrelevant. As before, the alternative is Ha : Sw\u2019 + 0. Similar reasoning can be used to develop an irrelevant hidden unit test statistic. However, there are some rather interesting difficulties in the development of the limiting distribution of G,, under Ho. Problems arise because when HO is true, the optimal weights from input units to the irrelevant hidden unit(s) are not locally unique - they have no effect on network output. This problem is known in the statistics literature as that in which \u201cnuisance parameters are identified only under the alternative hypothesis.\u201d The LMG family now plays an essential and unavoidable role in the analysis. Hypothesis testing in the present context has been studied by Davies (1977, 1987); the analysis is complicated. As one should expect from the LMG family, the resulting test statistic distributions are no longer x2. However, certain techniques can be adopted to avoid these difficulties, yielding a x\u2019, statistic for testing the irrelevant hidden unit hypothesis. One such test is described by White (1989c), and its properties are investigated by Lee et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1852,
                                "start": 283
                            }
                        ],
                        "text": "2, the local nature of the analysis of limiting distributions and the freedom allowed by the ability to choose W in any convenient way (and in particular as a small neighborhood of the set to which the learning method ultimately converges) means that the foregoing remarks apply not just to learning methods that globally solve equation 4.2, but also to methods that find a local solution to 4.2. Accordingly, we can think of w* as being a unique local solution to 4.2 in the discussion to follow. In Theorem 2 of the appendix, we give conditions ensuring that the limiting distribution of f i ( G n - w*) is the multivariate normal distribution with mean vector zero and an s x s covariance matrix (say c*) that can be given a precise analytic expression. We refer to C\u2019 as the \u201dasymptotic covariance matrix\u201d of 8,. The smaller this covariance matrix is (as measured by, say, tr C* or det C*) the more tightly the distribution of 6, is concentrated around w*, with less consequent uncertainty about the value of w*. It is therefore desirable that C\u2019 be small, but there are fundamental limits on how small C* can be. When two learning methods yield weights 61, and &, respectively, that are both consistent for w+, one with asymptotic covariance matrix C;, and other with asymptotic covariance matrix C;, the method yielding the smaller asymptotic covariance matrix is preferable, because that method makes relatively more \u201cefficient\u201d use of the same sample information. In certain cases, it can be shown that C; - C; is a positive semidefinite matrix, in which case it is said that the second method is \u201casymptotically efficient\u201d relative to the first method. Thus, study of the limiting distribution of alternative learning methods can yield insight into the relative desirability of different learning methods. As a specific example, White (1989b) proves that learning methods that solve equation 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 88
                            }
                        ],
                        "text": "The \"temperature\" is lowered at an appropriate rate so as to control the probability of jumping away from relatively good minima. Hajek (1985, 1988) gives a useful survey and some theorems establishing conditions under which simulated annealing ultimately delivers the solution 8, to equation 4.2. See also Davis (1987) and van Laarhoven (1988). It is useful to recognize that such procedures leave us twice removed from the optimal weights w*, in a certain sense."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2126,
                                "start": 30
                            }
                        ],
                        "text": "the procedure of failing to reject H o whenever nG~S\u2018(S&S\u2019)-\u2019SG, fails to exceed the 1 - a percentile of the xi distribution (for some typically small value of a, say a = 0.05 or a = 0.01) leads to incorrect rejection of the irrelevant input hypothesis with (small) probability approximately equal or less than a. As n becomes large the probability of correctly rejecting the irrelevant input hypothesis with this procedure tends to one. This procedure is an application of standard techniques of statistical inference. It allows us to determine whether specific input(s) are irrelevant, to the extent permitted by the sample evidence by controlling the probability of incorrectly rejecting Ho. This approach has obvious applications in investigating the appropriateness of given network architectures. The irrelevant hidden unit hypothesis is of exactly the same form, that is, H o : Sw\u2019 = 0, except that now the q x s selection matrix S picks out weights associated with q hidden units hypothesized to be irrelevant. As before, the alternative is Ha : Sw\u2019 + 0. Similar reasoning can be used to develop an irrelevant hidden unit test statistic. However, there are some rather interesting difficulties in the development of the limiting distribution of G,, under Ho. Problems arise because when HO is true, the optimal weights from input units to the irrelevant hidden unit(s) are not locally unique - they have no effect on network output. This problem is known in the statistics literature as that in which \u201cnuisance parameters are identified only under the alternative hypothesis.\u201d The LMG family now plays an essential and unavoidable role in the analysis. Hypothesis testing in the present context has been studied by Davies (1977, 1987); the analysis is complicated. As one should expect from the LMG family, the resulting test statistic distributions are no longer x2. However, certain techniques can be adopted to avoid these difficulties, yielding a x\u2019, statistic for testing the irrelevant hidden unit hypothesis. One such test is described by White (1989c), and its properties are investigated by Lee et al. (1989). Statistical inference plays a fundamental role in modern scientific research."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate stochastic approximation methods"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Math. Stat. Carroll, S. M., and Dickinson, B. W. 1989. Construction of neural nets using the Radon transform. In"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38930704"
                        ],
                        "name": "Chung-Ming Kuan",
                        "slug": "Chung-Ming-Kuan",
                        "structuredName": {
                            "firstName": "Chung-Ming",
                            "lastName": "Kuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung-Ming Kuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "The results of White (1989b) and of Kuan and White (1989) can be used to construct tests of the irrelevant input hypothesis and the irrelevant hidden unit hypothesis in ways directly analogous to those discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Kuan and White (1989) discuss a modification of backpropagation that has asymptotic efficiency equivalent to the (local) solution of equation 4.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "Such an assumption is implausible for the analysis of time series data, so Kuan and White (1989) apply results of Kushner and Clark (1978) and Kushner and Huang (1979) to establish consistency and limiting distribution results for dependent sequences of random variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 34
                            }
                        ],
                        "text": "Several of these are described by Kuan and White (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63866748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31ac7f1988051080107a945c920ef74dbaa82c1c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-M-estimation,-nonlinear-regression-and-Kuan-White",
            "title": {
                "fragments": [],
                "text": "Recursive M-estimation, nonlinear regression and neural network learning with dependent observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158720"
                        ],
                        "name": "Harro Walk",
                        "slug": "Harro-Walk",
                        "structuredName": {
                            "firstName": "Harro",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harro Walk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 51
                            }
                        ],
                        "text": "White (198913) applies results of Ljung (1977) and Walk (1977) to obtain consistency and limiting distribution results for equations 4.6 and 4.7 and the method of backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "White (198913) applies results of Ljung (1977) and  Walk (1977)  to obtain consistency and limiting distribution results for equations 4.6 and 4.7 and the method of backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119733417,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b176707be5da7f0f83efa4545fdc6e06dc68e2ae",
            "isKey": true,
            "numCitedBy": 76,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-invariance-principle-for-the-Robbins-Monro-in-a-Walk",
            "title": {
                "fragments": [],
                "text": "An invariance principle for the Robbins-Monro process in a Hilbert space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 59
                            }
                        ],
                        "text": "This is easily recognized as the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 245
                            }
                        ],
                        "text": "When ~ ( y , 0) = (y - 0)\u2019/2, we have\nVKZ, w) = -of(., w)(y - f(z, w)) so that\nG, = Gn-l + qnVf(X, , G,-l)(Y, - f(X,, Gn-l)), n = 1,2, . . . .\nLearning in Artificial Neural Networks 449\nThis is easily recognized as the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 60
                            }
                        ],
                        "text": "network using some variant of the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 199
                            }
                        ],
                        "text": "In many of these cases, success has been achieved by the now rather simple expedient of appropriately training a hidden layer feedforward network using some variant of the method of backpropagation (Werbos 1974; Parker 1982; Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": true,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "White (198913) applies results of Ljung (1977) and Walk (1977) to obtain consistency and limiting distribution results for equations 4.6 and 4.7 and the method of backpropagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of recursive stochastic algorithms, IEEE Truns"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic Control AC-22, 551-575."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16577556"
                        ],
                        "name": "M. T. Wasan",
                        "slug": "M.-T.-Wasan",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wasan",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. T. Wasan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125688137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8cc7e79deefc13bccd86ae570cc868b7ec33735",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Approximation-Wasan",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456647"
                        ],
                        "name": "R. Davies",
                        "slug": "R.-Davies",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Davies",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Davies"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Hypothesis testing in the present context has been studied by Davies (1977, 1987); the analysis is complicated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126110526,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0995b16fdfb9bbb40b2b9659d52a97174933cf3d",
            "isKey": false,
            "numCitedBy": 360,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hypothesis-Testing-when-a-Nuisance-Parameter-is-the-Davies",
            "title": {
                "fragments": [],
                "text": "Hypothesis Testing when a Nuisance Parameter is Present Only Under the Alternatives"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884313"
                        ],
                        "name": "D. Balmer",
                        "slug": "D.-Balmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Balmer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Balmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61764504,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "4198230e3dd782f6cc221c3eba3ce8b27a1d3786",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-and-Computational-Aspects-of-Simulated-Balmer",
            "title": {
                "fragments": [],
                "text": "Theoretical and Computational Aspects of Simulated Annealing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926999"
                        ],
                        "name": "C. Boender",
                        "slug": "C.-Boender",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Boender",
                            "middleNames": [
                                "G.",
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Boender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145071526"
                        ],
                        "name": "A. Kan",
                        "slug": "A.-Kan",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kan",
                            "middleNames": [
                                "H.",
                                "G.",
                                "Rinnooy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772468"
                        ],
                        "name": "L. Stougie",
                        "slug": "L.-Stougie",
                        "structuredName": {
                            "firstName": "Leen",
                            "lastName": "Stougie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Stougie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145631562"
                        ],
                        "name": "G. Timmer",
                        "slug": "G.-Timmer",
                        "structuredName": {
                            "firstName": "Gerrit",
                            "lastName": "Timmer",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Timmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, White (1988) discusses use of the method of cross-validation (e.g.,  Stone 1974 ) to determine network complexity appropriate for a training set of given size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123095990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56de39e907094ff6d249960c7ca49f1ae13e1159",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Global-optimization-:-a-stochastic-approach-Boender-Kan",
            "title": {
                "fragments": [],
                "text": "Global optimization : a stochastic approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393852197"
                        ],
                        "name": "Bartle",
                        "slug": "Bartle",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bartle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145212438"
                        ],
                        "name": "R. Gardner",
                        "slug": "R.-Gardner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gardner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The conditions stated rely on concepts of probability, measure, and integration that are well presented in, for example,  Bartle (1966)  and Billingsley (1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123574280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1433b5c1a9088e35b93bc939612ec980773c2da6",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Elements-Of-Integration-Bartle-Gardner",
            "title": {
                "fragments": [],
                "text": "The Elements Of Integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227161"
                        ],
                        "name": "J. Kiefer",
                        "slug": "J.-Kiefer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kiefer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kiefer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50226899"
                        ],
                        "name": "J. Wolfowitz",
                        "slug": "J.-Wolfowitz",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Wolfowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wolfowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 30
                            }
                        ],
                        "text": "One such procedure is that of Kiefer and Wolfowitz (1952)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One such procedure is that of  Kiefer and Wolfowitz (1952) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122078986,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f4e3738a90f9cc806a25c8739e0d8b892ee7d1ff",
            "isKey": false,
            "numCitedBy": 1841,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Estimation-of-the-Maximum-of-a-Function-Kiefer-Wolfowitz",
            "title": {
                "fragments": [],
                "text": "Stochastic Estimation of the Maximum of a Regression Function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103079397"
                        ],
                        "name": "J. Blum",
                        "slug": "J.-Blum",
                        "structuredName": {
                            "firstName": "Julius",
                            "lastName": "Blum",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119732291,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9f4e17b76950083e96bf05c18ad88d1c7d85a3ad",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multidimensional-Stochastic-Approximation-Methods-Blum",
            "title": {
                "fragments": [],
                "text": "Multidimensional Stochastic Approximation Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851442"
                        ],
                        "name": "R. Jennrich",
                        "slug": "R.-Jennrich",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jennrich",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jennrich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 137
                            }
                        ],
                        "text": "Given domination of 1, compactness of W , and the assumption that (2,) is i.i.d., it follows from the uniform law of large numbers (e.g. Jennrich 1969, Theorem 2) that supyEw lAn(w) -"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119909850,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "94d6a5ed17042dfac7a756202bfb019ff84080fb",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymptotic-Properties-of-Non-Linear-Least-Squares-Jennrich",
            "title": {
                "fragments": [],
                "text": "Asymptotic Properties of Non-Linear Least Squares Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085912845"
                        ],
                        "name": "Ing Rj Ser",
                        "slug": "Ing-Rj-Ser",
                        "structuredName": {
                            "firstName": "Ing",
                            "lastName": "Ser",
                            "middleNames": [
                                "Rj"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ing Rj Ser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 118122223,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "52f863329807fbf9479ace645fb0700b1263bdd2",
            "isKey": false,
            "numCitedBy": 2291,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-Theorems-of-Mathematical-Statistics-Ser",
            "title": {
                "fragments": [],
                "text": "Approximation Theorems of Mathematical Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101628567"
                        ],
                        "name": "P. Nowosad",
                        "slug": "P.-Nowosad",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Nowosad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nowosad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115910769,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5d7e97c4d9af7b6df966b2d8c06904217e1934d1",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-elements-of-integration-Nowosad",
            "title": {
                "fragments": [],
                "text": "The elements of integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 73
                            }
                        ],
                        "text": "However, if W is restricted to be a single cone of the type described by Hecht-Nielsen (1989) (this eliminates interchangeability), then multiple minima are no longer guaranteed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 222
                            }
                        ],
                        "text": "\u2026as a single hidden layer are capable of arbitrarily accurate approximation to an arbitrary mapping provided that sufficiently many hidden units are available [see Carroll and Dickinson 1989; Cybenko 1989; Funahashi 1989; Hecht-Nielsen 1989; Hornik et al. 1989a,b (HSW); Stinchcombe and White 19891."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63607042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e96586f330e8d7011317b298524f66990008704c",
            "isKey": false,
            "numCitedBy": 514,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-the-Back-Propagation-Neural-Network-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "Theory of the Back Propagation Neural Network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on specification testing against nonparametric"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026are \u201cblind to certain alternatives, however, in that they will fail to detect certain departures from Ho no matter how large is n. Recent work of Wooldridge (1989) has exploited the nonparametric estimation capabilities of series estimators, a special class of sieve estimators, to obtain\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 149
                            }
                        ],
                        "text": "\u2026the true relation between X and Y.\nWhite (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning procedure for 8, under fairly general\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on sieve estimation with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation of misspecified dynamic mod"
            },
            "venue": {
                "fragments": [],
                "text": "Misspecificution Analysis ,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 142
                            }
                        ],
                        "text": "\u2026in the true relation between X and Y.\nWhite (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning procedure for 8, under\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on sieve estimation with dependent observations"
            },
            "venue": {
                "fragments": [],
                "text": "Nonparametric and Semiparametric Methods in Econometrics and Statistics ,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on specification testing against nonparametric alternatives"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some asymptotic results for learning in single hidden layer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some asymptotic results for learning in single hidden layer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "J . Am . Stat . Assoc . , forthcoming ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate smoothing splines"
            },
            "venue": {
                "fragments": [],
                "text": "S I A M J . Numerical Anal ."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 216
                            }
                        ],
                        "text": "The relation between choice of performance measure K and approximation of the probabilistic relation between X and Y is established at a deep level by considerations related to the theory of information initiated by Shannon (1948) and Wiener (1948)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of communication. Bell System Tech"
            },
            "venue": {
                "fragments": [],
                "text": "J"
            },
            "year": 1948
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 30
                            }
                        ],
                        "text": "See also Davis (1987) and van Laarhoven (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorefical and Computational Aspects of Simulated Annealing. Centrum voor Wiskunde en Informatica"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 132
                            }
                        ],
                        "text": "The search for an appropriate technique for determining network complexity has been the focus of considerable effort to date (e.g., Rumelhart 1988; Ash 1989; Hirose et al. 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing. Plenary lecture"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 284
                            }
                        ],
                        "text": "\u2026on artificial data-generating assumptions, statisticians have developed so-called \u201cresampling techniques\u201d that permit rather accurate estimation of finite sample distributions for 6, when { Zt} is a sequence of independent identically distributed (i.i.d.) random variables (see, e.g., Efron 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The lacknife, the Bootstrap and Other Re-sampling Plans"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "Rules suggesting that s x 10 may be adequate (e.g., Baum and Haussler 1989) are relevant to classification problems in which the relation between X and Y is nonprobabilistic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 54
                            }
                        ],
                        "text": "Results for some probabilistic cases are discussed in Haussler (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalizing the PAC model for neural net and other learning applications"
            },
            "venue": {
                "fragments": [],
                "text": "UCSC Computer Research Laboratory Tech. Rep. UCSC-CRL-"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 149
                            }
                        ],
                        "text": "\u2026from any fundamental randomness or fuzziness in the true relation between X and Y.\nWhite (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 64
                            }
                        ],
                        "text": "White (1988) uses statistical theory for the \"method of sieves\" (Grenander 1981; Geman and Hwang 1982; White and Wooldridge 1989) to establish that multilayer feedforward networks can be used to obtain a consistent learning procedure for 8, under fairly general conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abstract Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley, New York."
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cross - validated spline methods for the estimation of multivariate functions from data on functionals"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics : An Appraisal ,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 11
                            }
                        ],
                        "text": "Results of Andrews (1988) may be applicable to obtain the limiting distributions"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 11
                            }
                        ],
                        "text": "Results of Andrews (1988) may be applicable to obtain the limiting distributions of linear and nonlinear functionals of 8,."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic normality of series estimators for various nonparametric and semi-parametric estimators"
            },
            "venue": {
                "fragments": [],
                "text": "Yale University, Cowles Foundation Discussion Paper"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 35,
            "methodology": 29,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 83,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-in-Artificial-Neural-Networks:-A-White/656a33c1db546da8490d6eba259e2a849d73a001?sort=total-citations"
}